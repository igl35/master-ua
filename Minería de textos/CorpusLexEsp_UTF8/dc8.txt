Son precisamente linfocitos T los productores de la interleukina - 2,cuando han sido activados por antígenos y por la interleukina-l,sustancia que producen otros glóbulos blancos,los macrófagos,que son los verdaderos perros de presa de la sangre: a diferencia de los linfocitos,atacan de manera indiscriminada a cualquier cuerpo extraño,y son capaces de digerir hasta cien bacterias antes de morir indigestados.
¿Cómo actúa entonces la interleukina - 2? Dirigiéndose a los receptores específicos que,para acogerla,han surgido en la superficie de otros linfocitos T virgenes,a los que ha estimulado la presencia de un antígeno,y activando la proliferación de esos linfocitos: como en una orgía romana,los linfocitos se excitan unos a otros,y éste fue uno de los puntos de partida del descubrimiento de Steven Rosenberg,ya que manipulando la interleukina se podía lograr una tasa de reproducción altísima.
El otro punto de partida consiste en un avance cualitativo sobre el desarrollo de una antigua vía terapéutica (mucho más antigua de lo que pudiera creerse,pues data de 1890 ; ver recuadro): hacer que las propias defensas naturales del organismo se vuelvan en contra de las células malignas.
En otras palabras sublevar al sistema inmunológico contra el cáncer,modificando la respuesta biológica,normalmente moderada e insuficiente,declarando una guerra total.
Este es el método que Rosenberg denomina inmunoterapia adoptiva.
Pero hay antecedentes en el empleo de armas biológicas similares,la mayoría de data muy reciente,a las que conviene pasar revista para poder apreciar más claramente la singularidad del nuevo descubrimiento.
Sin duda el interferón - - interferón alfa,para ser precisos - - es la más conocida del gran público.
Se trata de una proteína que el organismo produce de modo natural ante una infección por virus.
Fue descubierto en 1967,pero hasta los años 70 no hay datos que lo acrediten como arma contra el cáncer,salvo una experiencia de incierto resultado que fue realizada en Suecia con un extraño caso de tumor óseo.
Pero fue en 1978 cuando comenzó a utilizarse de modo sistemático en los Estados Unidos,y más aún a partir de 1981 cuando pudo ser sintetizado en el laboratorio.
Sin embargo,salvo algún éxito espectacular en raros tipos de leucemia,no demostró más que muy modestos resultados en el tratamiento de tumores pulmonares,hepáticos y prostáticos.
Es decir,los de principal incidencia en cuanto a mortalidad.
Pariente próximo del alfa es el interferón gamma ; también se trata de una proteína,que en este caso es producida principalmente por las ya mencionadas células T. Según parece,logra una mayor vulnerabilidad de las células tumorales,revelando su presencia al sistema inmunitario.
De un modo similar al empleado por Rosenberg,puede también combinarse con otros glóbulos blancos,los monocitos,verdadera legión en el conjunto de los glóbulos blancos.
Aún no puede hablarse de resultados,pues las experiencias con interferón gamma acaban de comenzar.
Más proteínas,pero que en este caso actúan de manera conjunta,constituyen el grupo denominado CSF (de Colony Stimulating Factor).
El CSF actúa estimulando la función de los macrófagos,esos fagocitos insaciables e indiscriminados que son los basureros del sistema inmune,y que del mismo modo que atacan a toda sustancia extraña,son capaces de aniquilar sin miramientos a las células malignas.
Es posible que en el curso de este año se inicie el tratamiento experimental con pacientes humanos.
Muy reciente es también el comienzo de experiencias clínicas con el llamado TNF (Factor de Necrosis Tumoral),una proteína que fabrican también los leucocitos,y para la que existen,posiblemente,receptores específicos en diversos tipos de células tumorales.
Por lo tanto su eficacia se extendería,como en el caso del método de Rosenberg,a varias clases de cáncer.
Se piensa que el TNF estimula también la acción de los macrófagos y del interferón gamma.
Otras dos sustancias del sistema inmune han sido descubiertas hace poco más de un año: la citolisina y la leucorregulina.
De la primera se supone que ha cumplido un importante papel en la batalla contra los tumores librada por las células asesinas de Rosenberg.
En cuanto a la leucorregulina,se sospecha que puede paralizar el crecimiento de las células tumorales.
Por su extrema juventud,estas técnicas aún no han superado la primera fase experimental,es decir la etapa del tubo de ensayo.
Por su descubrimiento de los anticuerpos monoclonales,los científicos César Milstein y George Kohler recibieron el Premio Nobel de Medicina en 1984.
Se trata,en síntesis,de una nueva técnica consistente en fusionar una célula productora de anticuerpos (un linfocito B) con una célula cancerosa,que como se sabe crece a gran velocidad.
El resultado es la rápida obtención de una gran cantidad de células productoras de anticuerpos del tipo buscado,que son monoclonales porque provienen de una misma célula madre.
Las aplicaciones clínicas de estos anticuerpos son variadísimas,sobre todo en diagnóstico,pero en el tratamiento del cáncer se encuentran en una fase preliminar,experimental,con seres humanos.
Además es un método caro.
Los anticuerpos así obtenidos pueden conducir sustancias tóxicas hacia un tumor,o bien actuar selectivamente sobre algún factor determinante del crecimiento tumoral.
Sobre la base del hallazgo de Robert Gallo,Rosenberg comenzó a observar in vitro cómo actuaba la IL - 2 sobre los linfocitos humanos.
Descubrió entonces la aparición de un efecto citotóxico,letal para las células malignas.
Y comenzó la larga marcha.
En las primeras experiencias con animales enfermos,comprobó que ni la IL - 2 ni los linfocitos T activados producían efecto alguno sobre los tumores cuando se administraban separadamente.
En el caso de los linfocitos,quizá se debiera a que su cantidad era insuficiente.
El paso siguiente fue inyectar juntas la IL - 2 con los linfocitos activados,y comprobó que los tumores retrocedían.
Sólo quedaba ahora poner en práctica el método con pacientes humanos.
Pero las estrictas normas de investigación científica no permiten este tipo de pruebas sin comprobaciones previas del potencial tóxico de las sustancias que serán utilizadas.
Esta fue la razón de que los primeros 66 pacientes no respondieran al tratamiento,pues no hubo más remedio que administrarles las sustancias por separado para verificar su inocuidad.
Entre los diversos tipos de leucocitos o glóbulos blancos de la sangre,están los motrofogos feroces comedores de lodo cuando encuentran),los linfocitos B lubricantes de anticuerpos) y los linfocitos T o alvéolos asesinos dibujo A).
La aparición de un tumor activa a algunos linfocitos T (dibujo B),pero aunque éstos se multiplican,lo hacen a una velocidad relativamente bajo: es habitual que el tumor crezcamos rápidamente IC) y por eso lo normal en cánceres es que el tumor desborde la capacidad defensiva del cuerpo (D).
En el método de Rosenberg el comienzo es el mismo IE y F),pero una vez detectado el tumor se extraen con uno máquina especial los linfocitos T (leucohresis,dibujo G).
In vitro se excita la reproducción de la actividad " asesina " de estos linfocitos mediante lo interuterino que ha sido obtenida mediante Ingeniería Genética,Al paciente se le reinyectan los linfocitos T mas interleukino,para mantenerlos estimulados 11).
Estos atacan ferozmente a las células del tumor sin dañar en lo más mínimo a las demás células sanas,Es cierto que un siglo escaso no representa antigüedad en el trayecto milenario de las ciencias médicas,pero en el caso de la inmunoterapia como arma contra el cáncer 1890 es sin duda una fecha muy remota,si tenemos en cuenta que los tratamientos basados en el sistema inmune constituyen la última novedad en Oncología.
Fue en 1890,precisamente,cuando un médico de Nueva York,el doctor William B. Coley,advirtió que un paciente con cáncer de garganta mejoraba tras haber contraído una infección por estreptococos.
No dudó Coley ni un momento en administrar a sus pacientes cancerosos una vacuna de su invención,compuesta por estreptococos y otros microorganismos patógenos.
Y proclamó a los cuatro vientos haber logrado curas asombrosas.
Pero su curioso método no prosperó,entre otras cosas por la aparición de nuevos recursos terapéuticos,como la radiación.
Pero ya fuese por aquel primer destello,o por el progresivo conocimiento del sistema inmune,los investigadores comenzaron a plantearse interrogantes: algunos raros casos de tumores incurables que desaparecían sin dejar rastros,por ejemplo,hacían sospechar una intervención de las propias defensas.
Vinieron luego los trasplantes de órganos con sus problemas de rechazo,estimulando la investigación en el siempre difícil y complejo dominio de la Inmunología.
Lo demás es historia reciente.
Stonehengen (hacia 2800 aC).
- - Alguna tribu neolítica seminómada de la región,utilizando posiblemente cornamentas de ciervos a modo de picos,excava en el suelo calizo de la zona una fosa más o menos regular de unos 100 metros de diámetro,unos 6 metros de anchura y una profundidad de entre 1,5 y 2 metros.
La tierra extraída la amontonan por la parte exterior del círculo formando un terraplén de unos dos metros de alto.
Luego erigen en la fosa sendas piedras en los puntos en que un eje Sudoeste Nordeste corta la circunferencia y,unos metros más allá,siguiendo el mismo eje,un bloque solitario,de piedra arenisca,de 35 toneladas de peso,conocido como la piedratalón o heelstone.
Este bloque de piedra local está situado justo en el centro de la entrada,al final de una larga avenida de acceso.
Aquellos primeros constructores excavan también,aunque todo indica que los rellenan al poco tiempo,56 agujeros poco profundos y más o menos equidistantes entre sí en el interior de la fosa,conocidos como los agujeros de Aubrey en honor a John Aubrey,que fue el primero en estudiarlos ya en el siglo XVII.
Stonehenge-ll (hacia 2100 Ac).
- - Unos 700 ar os después de aquellos primeros trabajos se erigen dos circunferencias concéntricas de monolitos en el interior del círculo primitivo.
Se trata de pequeños bloques de piedras ígneas,llamadas bluestones o piedras azules,la mayor parte de las cuales se traen de las montañas galesas de Prescelly,que distan unos 320 Km del lugar.
Estas piedras,transportadas primero siguiendo la línea sur de la costa galesa y luego por río,son finalmente arrastradas hasta el lugar de su emplazamiento.
La entrada del primer círculo está alineada aproximadamente con la salida del Sol en el solsticio de verano (21 de Junio: día en que el astro alcanza su punto más alto por sobre el horizonte) y coincide con la continuación de la parte central de la avenida,unida ahora ya con el río Avon,distante unos tres kilómetros y medio.
Esta doble circunferencia no se completa nunca y todo indica que se desmonta en el período siguiente.
Stonehenge-lll (hacia 1600 Ac).
- - Entre los años 2000 Ac y 1500 Ac el monumento es remodelado prácticamente en su totalidad.
En algún momento de este período se erigen,formando un círculo de unos 30 metros de diámetro,80 bloques enormes de piedra tallada y pulida,rematados por un dintel continuo,llamados sarsens (palabra derivada posiblemente de saracen o extranjero),de 50 toneladas de peso cada uno y de algo más de 9 metros de altura,traídos de las colinas de Marlborough,a unos 30 Km de distancia del lugar.
Esta construcción encierra en su interior un recinto en forma de herradura integrado por cinco trilitos (cada uno formado por dos bloques de piedra a la manera de montantes coronados por una piedra horizontal superior o dintel).
La estructura sarsen de Stonehenge es única en Europa,tanto por su magnitud como por el trabajo de acabado de las piedras,así como por su encajamiento y la horizontalidad casi perfecta de los dinteles.
En etapas inmediatas posteriores se cavan nuevos agujeros,se reerigen parte de las piedras galesas (bluestones) y se desmantelan de nuevo.
En 1978 el arqueólogo británico John Evans obtiene un nuevo permiso para realizar trabajos de campo y reabre las excavaciones realizadas anos atrás por Atkinson.
Estos trabajos permiten demostrar a Evans que entre la construcción de StonehengeI y Stonehenge-ll transcurren centenares de años,ya que encuentra y analiza una amplia muestra de conchas de pequeños moluscos en las fosas abiertas,hecho que evidencia que algunas zanjas,después de excavadas,fueron abandonadas durante decenas de años y resultaron invadidas por la maleza y por todo tipo de vegetación de matorral.
Richard Atkinson,sin embargo,sin quitar importancia a este descubrimiento,sentencia: " Hasta hoy la mayoría de los trabajos que se han escrito sobre Stonehenge son meras especulaciones o análisis pseudocientíficos.
Pura diversión,de hecho.
Hoy sabemos quién,cuándo,cómo y dónde,pero,presumiblemente,nunca daremos con la clave del por qué fue construido ".
Esta es precisamente la pregunta que se repiten una y otra vez todos los estudiosos interesados por Stonehenge: ¿por qué,por qué se construyó en este lugar? ¿Cuáles fueron los motivos de generaciones y generaciones de hombres que en un período enorme,que abarca unos 1.500 años,levantaron este sorprendente monumento megalítico? Las posibles respuestas a estas preguntas son casi innumerables,algunas verosímiles y otras - - naturalmente - - obra de la más sofisticada fantasía.
Las primeras interpretaciones consideraron a Stonehenge como un gran corral para el ganado,una suntuosa tumba funeraria para una insigne reina bretona,un templo donde los sacerdotes druidas adoradores del Sol realizaban sus sacrificios humanos o incluso un lugar de culto de los romanos.
Pero a principio de siglo el astrónomo británico Norman Lockyer comprueba que uno de los ejes principales del monumento está orientado hacia la salida del Sol en el solsticio de verano,y señala que la razón de Stonehenge es claramente la de un observatorio del movimiento solar y un gran calendario para determinar la duración de los años.
En 1963,otro astrónomo,Gerald Hawkins,retoma la interpretación astronómica y asegura que Stonehenge es un complejo ordenador para determinar los eclipses de Sol y de Luna.
El libro de Hawkins,titulado Stonehenge Decoded (Stonehenge decodificado),es un auténtico bestseller en 1965.
Richard Atkinson escribe sobre el mismo: " El trabajo de Hawkins es lo peor que se ha escrito sobre el tema.
Es tendencioso,arrogante,sin fundamento e inconvincente.
Además está plagado de errores.
Si los hombres prehistóricos hubieran querido disponer de un ordenador para efectuar cálculos astronómicos no era necesario ni mucho menos que construyeran uno tan inmenso ".
Para algunos científicos,Stonehenge seguirá conservando su misterio por el resto de los siglos.
Y así,según algunas apreciaciones,serían necesarias excavaciones por un período de 75 años para empezar a hallar algunas de las claves de su construcción.
No obstante,cualquier excavación puede ser muy destructiva para el lugar,ya que los arqueólogos no saben exactamente lo que deben buscar.
Esto sucedió entre 1919 y 1926 con los trabajos del coronel William Hawley,quien al utilizar métodos muy rudimentarios de excavación se llevó por delante más de la mitad de los secretos que cobija la zona.
Atkinson considera que,en la duda,lo mejor es abstenerse: " Sería un crimen permitir nuevas destrucciones en Stonehenge hasta que no sepamos exactamente lo que andamos buscando ".
Mantener " legible " al conjunto,sin embargo,será cada vez más difícil,ya que la mayoría de los daños que ha sufrido el conjunto,desde la Edad Media hasta hace bien poco,son en su mayoría totalmente irreparables.
En la época medieval se sabe que se desmantelaron muchas piedras para emplearlas en la construcción de viviendas.
Ya en el siglo pasado,algunos hoteles de la vecina ciudad de Salisbury prestaban a sus clientes picos y martillos de mano para que pudieran llevarse algún pedacito del Neolítico a sus hogares.
En 1961 un grupo Para ser enteramente sinceros,la verdad es que la Estadística es una ciencia muy útil,necesaria y simpática,pero no muy divertida.
En ciertas otras ramas de as Matemáticas se puede encontrar cierto misterio y hasta cierta poesía,de las cuales la Estadística en general carece.
La Topología,por ejemplo,nos introduce en un mundo de puentes que se entrecruzan y laberintos que saltan hacia otras dimensiones ; el Algebra conserva mucho del misterio de cabalistas judíos y pitagóricos greco-árabes en sus más recónditos vericuetos ; hasta el humilde y pedestre Cálculo Infinitesimal,tan poco agraciado estéticamente,tiene en el jardín de las series convergentes el fragante encanto de los números trascendentes inesperados,la sorpresa de encontrarse con pi o con la e neperiana en el fondo de una serie que iba de cualquier otra cosa (en apariencia).
Pero dentro de su bien llevada fealdad,de su tosquedad de ciencia fregona,la Estadística encierra un elemento que debe reivindicarse como una verdadera joya.
Se trata de una variable de las tantas que miden el modo en que una miríada de datos se dispersan alrededor de ciertas tendencias centrales,pero es una variable distinta,una verdadera maravilla.
Se la llama coeficiente de Pearson y se la simboliza con la letra griega ro minúscula (p).
La definición de la ro de Pearson tiene la fealdad de las fórmulas estadísticas.
pero no nos adelantemos ; se obtiene,dice la ley,de la siguiente ecuación: donde XO es el valor medio de una lista de valores denominados con la letra X: YO es el valor medio de otra lista Y ; X e Y son los valores correspondientes de ambas listas,es decir,los que ocupan el mismo lugar.
y que naturalmente van variando según se avanza en ambas listas ; los puntos entre expresiones representan multiplicación y la letra griega sigma mayúscula indica que los respectivos valores deben irse sumando de principio a fin de las listas.
Todo esto está muy bien,se soluciona en diez minutos con calculadora y en un par de segundos con ordenador,pero... ¿para qué sirve? Supongamos que X e Y miden valores correspondientes de dos magnitudes cuya relación se desconoce: se calcula ro y se sabe si existe correlación,cuán fuerte es y si es directa o inversa.
No sé si se advierte la trascendencia de este indicador mágico: el valor de ro indica si dos tipos de acontecimientos tienen algo que ver entre sí,y en su caso si tienen relación directa o si se trata de hechos totalmente independientes,sin relación entre si.
En algunos casos el cálculo de ro puede parecer obvio porque ya sabemos cuál es la relación entre dos cosas,pero la gracia es justamente que permite determinar mediante una herramienta matemática concreta la existencia o no de relación antes de que sepamos por otra vía s ¡una cosa es función de la otra.
Si,por ejemplo,cogemos 456 trozos de cable,medimos su longitud y su resistencia eléctrica,y llamando X a la longitud e Y a la correspondiente resistencia calculamos ro,encontraremos que el valor del coeficiente será casi de 1,lo que indica correlación total y directa (- - 1 indica correlación total pero inversa,como podría darse entre resistencia eléctrica y superficie de la sección del cable,y el 0 denota falta de toda relación ; son posibles valores intermedios).
Por supuesto,esa relación no es novedad para quien sepa las leyes elementales de la electricidad,pero la gracia está en que incluso cien años antes de que naciera Ohm,si alguien hubiera sabido Estadística,podría mediante el cálculo de ro establecer que la resistencia de un cable depende por completo de la longitud del mismo (aunque no sólo de eso).
Calculando ro para diversas posibles variables - - superficie de la sección,color de la cubierta aislante,tamaño de los zapatos del electricista,cualquier variable que uno pueda imaginar - -,su valor nos iría diciendo que algunas cosas sí tienen que ver con la resistencia eléctrica,y otras no.
Todo esto puede parecer bastante superfluo,pero no lo es.
Solemos pensar en términos de las ciencias exactas,donde las relaciones de las cosas son bastante claras,y con frecuencia se sospecha la ley que las relaciona entre sí antes de ir a buscar,experimentalmente,una confirmación a esa sospecha.
Pero esa no tiene por qué ser siempre la situación.
En ciencias biológicas y sociales ciertas hipótesis de correlación son oscuras,inasibles,y el uso de la ro es vital para poder determinar si verdaderamente dos cosas tienen un vínculo.
¿Depende el grado de éxito en los estudios de EGB del salario sumado de padre y madre? ¿La supervivencia como expectativa de vida a los 40 años tiene relación inversa con la cantidad de cigarrillos fumados por día? ¿El número de horas de sueño permite establecer el número probable de camarones arrastrados por la corriente? No son estas correlaciones tan obvias como la longitud de un cable y su resistencia,e incluso la ro puede determinar vínculos matemáticos mucho antes de que se conozca la ley exacta,porque una de las virtudes de esta mágica piedra de toque es que descubre vínculos escondidos dentro de leyes complejas donde participan muchas variables (su valor será positivo y distinto de 0,aunque cuando se utilicen cables de diferentes longitudes y diversas secciones,pese a la doble dependencia).
Estamos acostumbrados a que las relaciones entre cosas sean descubiertas por la intuición y la genialidad de los científicos,y que la ausencia de reacción significativa sea también el golpe triunfal con que la Razón derrota a la Superstición.
Que dos tablas de valores correspondientes y una sencilla fórmula matemática puedan decirnos lo mismo es algo que desafía nuestro orgullo.
Naturalmente no basta con que exista la correlación para que se pueda establecer de inmediato la correcta relación de causa-efecto: En los pasillos del Registro Civil se ve mucha gente con corbata - - decía el epistemólogo Mario Bunge - - y de ello no se puede deducir que llevar corbata provoque el casamiento).
Pero desde luego un valor alto de ro indica que algo pasa ; establecerlo queda,por suerte,para los científicos.
No sé si el propio Pearson era consciente de que estaba fundamentando el edificio entero de la Ciencia.
Después de todo lo esencial del pensamiento científico no está en la experimentación,como suele creerse,sino en el establecimiento de relaciones causales entre los fenómenos que se describen.
La ro es la única medida objetiva de si algo es una afirmación digna de ser sometida al juicio de la Ciencia o si se trata de una pura tontería,una afirmación sin fundamento.
Karl Pearson nació en Londres en 1857 y murió en la misma ciudad 79 años más tarde,en 1934.
Matemático,considerado uno de los padres de la Estadística moderna,también estudió Leyes,trabajó a favor de partidos políticos radicales de la época y escribió dos o tres novelas.
Dentro de ese polifacético espectro de intereses,trató de poner un fundamento matemático a ciertos problemas biológicos relacionados con la herencia y la evolución,y en el University College de Londres - - del cual fue profesor de Geometría muchos años - - conoció a sir Francis Galton,el primero en aplicar la psicometría es decir,la medición,mediante tests,de la inteligencia y otras variables psicológicas.
Fue justamente en el ámbito de la psicometría que Pearson aplicó su coeficiente de correlación,es decir,en un terreno en el que las conjeturas y las suposiciones sin fundamento suelen plagar la investigación.
Ahora,gracias a él,la Psicología es una ciencia un poco más exacta,más seria,más científica,aunque ro pueda seguir usándose en todos los ámbitos del conocimiento para distinguir verdades de supercherías.
EL lunes 27 de enero,por la mañana,el coordinador de ALGO Lluís Borras,echó una última lectura a los diarios del día y llamó a la imprenta ordenando-que comenzaran a imprimir.
Aunque el material de la revista se prepara con bastante anticipación,se lo mantiene " en suspenso " hasta último momento por si ocurriera algún hecho que exigiera modificar alguna información o actualizarla.
Pero todo estaba en orden,y el número de febrero se podía dar por completado.
Al día siguiente,ya impresa la edición,llegaba a la Redacción la noticia del estallido,en pleno vuelo,del transbordador Challenger.
Para ALGO la noticia era doblemente amarga,porque en sus páginas y en su portada el transbordador era el principal protagonista,de una forma amable y optimista,a través de las anécdotas de los científicos y de las alegres y mordaces ilustraciones de TEX.
En ese número de febrero,al final del artículo,se prometía a los lectores una segunda parte: es que,tras leer la evaluación hecha por los hombres de ciencia puestos a juzgar el transbordador,era necesario preguntarse - - y contestar - - qué futuro le espera al space shuttle.
Por un momento,a la luz del accidente y de la consiguiente suspensión de misiones por tiempo indefinido,parecía oportuno reemplazar el artículo de Discover por una crónica de los fatales sucesos del martes 28.
Pero después de una reflexión más profunda se decidió que ALGO no debía adoptar una actitud de sección necrológica.
En la historia de las ciencias y de la tecnología los accidentes son solamente anécdotas.
También en Astronáutica han habido otras víctimas fatales antes de ahora,y su avance prosigue.
Así,pues,se decidió continuar.
El autor del artículo,Gerard O'Neill,es profesor emérito de Física en el Instituto de Estudios Avanzados de Princeton,uno de los colegios universitarios más prestigiosos del mundo,que supo contar en sus filas con científicos de la talla del propio Albert Einstein.
En el texto vislumbra los posibles nuevos derroteros para el transbordador,incluido un ambicioso proyecto industrial en el espacio que hasta hace muy poco hubiera parecido ciencia-ficción.
O'Neill presenta esta doble aventura - - la estrictamente astronáutica y la industrial - - como un reto al hombre norteamericano,quizá rescatando aquel concepto del presidente Kennedy,para quien el espacio sería la nueva frontera a conquistar por un pueblo que creció en fuerza y desarrollo,cien años antes,durante la conquista del Oeste.
A pesar del encendido nacionalismo,o cierto desorden en su exposición,hemos mantenido el texto íntegro: sin saberlo - - pues escribió el artículo antes del accidente del Challenger - -,el profesor O'Neill también estaba explicando a sus compatriotas las razones morales por las cuales la conquista del espacio no puede,pese a todo,abandonarse.
En 1983,Frederick Jackson Turner,contemplando el crecimiento de los Estados Unidos desde una estrecha franja de la Costa Este hasta abarcar todo un continente,percibió un hondo significado en la apertura de sucesivas fronteras hacia el Oeste.
Llegó así a la conclusión de que los pioneros,en consonancia con el desierto,fueron necesariamente duros e independientes.
En ellos se encarnó el ideal del hombre y la mujer de la frontera,individual y americano.
Turner estaba convencido de que su éxito,ante tal desafío,había proporcionado a América una sociedad más libre y autosuficiente,y con ella,una expresión más acendrada de fe democrática.
Ante nosotros se abre ahora una nueva frontera en el espacio.
Y creo que ella ofrece,a los norteamericanos como individuos y a los EE. UU. como nación,una oportunidad aún más grande que el movimiento hacia el Oeste en la pasada centuria.
Nuestros antepasados,los pioneros,tenían que ser pragmáticos para sobrevivir y prosperar en su frontera.
Tal debería ser nuestro ánimo cuando nos preparamos para hacer frente al desafío de esta nueva frontera: el Sistema Solar.
Mi acercamiento práctico al desarrollo espacial proviene de una experiencia muy reciente,en los inicios de un nuevo reto al espacio: el sistema de satélites Geostar.
Con el Geostar se abre camino un nuevo servicio comercial de satélites.
Mediante un sistema de relés especiales en alta órbita,el Geostar permitirá enlaces directos de ida y vuelta entre transmisores minúsculos,baratos y alimentados a pilas,y un centro de computación en tierra.
Los servicios incluirán: Guía posicional y de navegación,con un error de pocos metros,en cualquier parte del territorio y aguas costeras de los Estados Unidos.
Servicios de localización,incluidas emergencias,para despacho de camiones,taxis,ambulancias,coches policiales... Servicio de mensajes telegráficos de ida y vuelta entre cualquier par de transmisiones,incluidos mensajes de salvamento y prevención del crimen.
El Geostar se financia con recursos privados,sin un céntimo de aportación por parte del gobierno.
El primer relé orbital del Geostar,acaba de construirlo la división Astro Electrónica de la RCA,y fue programado para ascender a órbita alta en un satélite (RTE en el mes de diciembre de 1985.
Los principios en los que se ha basado el rápido y amplio avance del Geostar - - han transcurrido apenas 20 meses desde que se fundó la compañía - - se adecúan perfectamente,en mi opinión,al logro de un exitoso programa espacial para América.
Partimos de este lema: " Que sea simple,y que funcione ".
Por otra parte,no creemos que sea siempre necesario gastarse un montón de dinero para conseguir un resultado importante.
Si el objetivo es claro,suele ser posible alcanzarlo rápidamente ; por lo general ensamblando,de manera diversa,piezas de tecnología ya existente.
Finalmente,y por encima de todo,mantenemos abiertas nuestras opciones.
La NASA de los años 60,con un claro objetivo - - el proyecto Apolo - - y un programa ajustado,logró alcanzar su meta de manera soberbia.
Cuando acabó lo del Apolo,los Estados Unidos llevaban una década de ventaja,en tecnología espacial,a cualquier país del mundo.
Y en algunos aspectos aún conservan esa preeminencia,pero en los 70 faltaron objetivos claros y precisos.
En ese lapso de diez años,otros países obtuvieron resultados con mayor rapidez y menor esfuerzo,aplicando tecnología desechada por la NASA.
No podemos permitirnos seguir siendo tan derrochadores.
Estamos en un mundo altamente competitivo,en el que los rivales no son ya sólo los soviéticos,sino los europeos,los japoneses y también los chinos.
En París,un funcionario de la ESA (Agencia Espacial Europea),me dijo lo siguiente: " Los errores que cometió la NASA,a comienzos de los años 70,han permitido que creáramos la ESA y,mediante los cohetes Ariane,sustraerle mercados a los Estados Unidos ".
Enumeró esos errores y,como el más notable,citó la decisión de suspender y luego abandonar,por completo,la producción de vehículos de lanzamiento desechables,para concentrarlo todo en un vasto,y mal definido,nuevo programa: el transbordador espacial.
Por cierto que el transbordador es una gran conquista tecnológica,tripulado por valerosos y competentes hombres y mujeres.
Pero ¿para qué sirve? No es del todo apropiado para lanzar satélites comerciales,pues no se eleva hasta la altura conveniente: alcanza apenas una órbita baja,a unos 400 Km. de la Tierra.
Muchos satélites comerciales necesitan situarse en una órbita estacionaría,es decir,alta: 36.000 Km. Hasta ahora,de estos satélites altos provienen todos los beneficios del comercio espacial.
Tres de los cuatro fallos en los lanzamientos de satélites por el transbordador,se han producido en los grandes cohetes que los satélites deben transportar para alcanzar sus órbitas,es decir,para desplazarse desde el punto en que los deja el transbordador hasta la altura donde realmente tienen que llegar.
Es por esta carencia del transbordador por lo que muchas compañías han escogido el Ariane para sus lanzamientos de satélites,aún cuando este cohete europeo convencional,no recuperable,lleva diez años de retraso tecnológico.
Ahora se trata de que los Estados Unidos emprendan otro programa tan espectacular como mal definido: la estación espacial en órbita baja.
Pienso que es necesario construir algún tipo de estación espacial.
Pero la estación de la que ahora se habla ¿puede proporcionarnos alguna ventaja sobre el Skylab de hace doce años? Y los europeos ya se están preparando para salir al paso con una estación espacial propia,tal como hicieron con el Ariane mientras desperdiciábamos toda la década de los años 70.
A comienzos del próximo decenio,los europeos contarán con lanzadores que puedan ascender indistintamente hasta una órbita alta o baja.
El Ariane 5,recientemente aprobado por el gobierno francés y la ESA,será capaz de transportar cargas útiles de casi cualquier forma y tamaño,pues a diferencia del transbordador americano,no estará restringido por una estructura aerodinámica que pueda resistir el reingreso en la atmósfera.
Y si fuese rentable recuperar las máquinas del Ariane 5,sería posible conseguirlo mediante el empleo de sencillos cuerpos de reentrada,tema en el que los franceses tienen ya 20 años de experiencia técnica.
Los europeos también están desarrollando otro proyecto sensato,un ingenio de nivel tecnológico más bien bajo,el espacio plano Hermes.
Un año después el ladrón se vio ante un tribunal,acusado de asesinato por la muerte súbita de la señora Pizzamiglio.
El resultado de la autopsia reveló al jurado que en su corazón se hallaban unas cicatrices en forma de banda,de tejido dañado por un repentino aporte de sustancias parecidas a la adrenalina.
En algunas personas un ataque de terror puede liberar masivamente este tipo de sustancias.
Estos tejidos dañados serían los responsables de la aceleración de los latidos del corazón,pues forman cortocircuitos en el delicado sistema eléctrico de ese músculo.
El testimonio de los peritos médicos,que relacionaron el susto de la víctima con el tejido dañado,fue suficiente para que el jurado declarara a Michael Stewart,de 20 años de edad,culpable de asesinato.
En este caso,que actualmente está en apelación,es uno de los pocos en los EE. UU. en los que el defendido ha sido declarado culpable de este tipo de delito.
La señora Pizzamiglio fue una víctima de la llamada muerte súbita por fallo cardíaco,la principal causa de muerte en el mundo industrializado,y que causa alrededor de 330.000 muertes cada año en los EE. UU. (más de 200.000 en hombres).
El ataque cardíaco repentino causa una de cada tres muertes entre los varones adultos,es decir,más que todos los tipos de cáncer en conjunto.
Las víctimas se hallan a menudo en la plenitud de su vida y generalmente tienen una apariencia saludable: trabajan,cuidan su jardín o hacen footing hasta minutos o segundos antes de que un cataclismo interno les paralice el corazón.
Los cardiólogos están alcanzando ahora algunos logros sorprendentes en la comprensión de estos fenómenos,en parte debido a que ahora se evitan un gran número de fallecimientos cardíacos gracias a la intervención médica.
Los hallazgos sugieren que la causa de la muerte súbita puede recaer hasta cierto punto en factores emocionales que,o bien crean lentamente en el corazón una predisposición para la muerte,o bien la provocan de modo abrupto en una crisis.
Existen dos clases comunes de muerte por causa cardíaca.
El infarto de miocardio (IM) es lo que el hombre de la calle conoce como ataque al corazón y es la consecuencia de la arteriosclerosis coronaria.
Esta enfermedad se anuncia a sí misma de un modo característico,con brotes de angina de pecho (dolor en el tórax) de intensidad progresiva,hasta que,después de meses o años,una o más arterias coronarias se obstruyen con una placa de grasa y ya no pueden suministrar sangre oxigenada de forma eficiente al músculo cardíaco.
Si se forma un trombo o coágulo en una de esas arterias cuyo calibre está disminuido,se bloquea el aporte sanguíneo a una parte del corazón y el tejido afectado se necrosa (muere).
A menos que la sangre encuentre otras vías colaterales de llegar al área afectada o intervenga la mano del hombre,el proceso puede continuar durante horas y colapsar el corazón.
El síncope,paro cardíaco o verdadera muerte súbita,el otro tipo de desenlace fatal de origen cardíaco,comienza en el 80 al 90 % de los casos con un torrente de impulsos eléctricos que alteran súbitamente el ritmo del corazón.
Los ventrículos,esas poderosas cámaras inferiores con forma de abdomen,caen de repente en una arritmia acelerada y salvaje,con gran aceleración (llamada taquicardia) y el proceso puede degenerar en tan sólo 30 o 45 segundos en la anarquía de la fibrilación ventricular una situación caótica en la que el corazón sólo puede temblar en lugar de bombear.
La fibrilación ventricular es tan frecuentemente fatal que los médicos llaman " sobrevivientes " a quienes hayan superado este estado de muerte súbita mediante los métodos de reanimación cardio-pulmonar,o mediante desfibrilación eléctrica.
Estudios recientes señalan que tal vez un 15 o 20 % de las víctimas de síncope cardíaco no padecen arteriosclerosis coronaria detectable,y la mitad no son fumadores ni hipertensos,ni padecen exceso de colesterol ni obesidad.
Por lo menos un 75 por ciento no muestran el tejido cicatricial que denota haber padecido infarto de miocardio con anterioridad.
En otras palabras,un considerable porcentaje de muertes súbitas no son del todo explicables como el resultado inevitable de una enfermedad cardíaca subyacente o de los " factores de riesgo " que conducen a ella.
Sin embargo,la gente joven con corazones vigorosos raramente resulta afectada,incluso estando en primera línea de fuego en pleno combate,por lo cual los médicos piensan que la mayoría de los paros cardiacos,en gente mayor son debidos a alguna enfermedad preexistente,aunque no siempre haya sido previamente diagnosticada.
Pero incluso cuando la enfermedad ya ha desestabilizado en forma evidente al corazón,los estudiosos de la muerte súbita se preguntan si es la mente o el cuerpo lo que finalmente traiciona a la victima.
Thomas Graboys,cardiólogo de la Facultad de Medicina de Harvard,se pregunta qué es lo que realmente mata a la persona: " Incluso las personas que padecen graves enfermedades cardiacas viven durante años hasta que llega un día en que repentinamente mueren.
Algo desencadena esa muerte y la evidencia sugiere de forma creciente que factores como el miedo,la depresión y el aislamiento,pueden jugar un papel importante,especialmente en corazones enfermos y vulnerables ".
Algunos investigadores creen que este asesino de uno de cada cinco norteamericanos es un tipo de disfunción temporal.
El cardiólogo Bernard Lown (1),colega de Graboys,lo llama un accidente eléctrico y cree que tal vez sea evitable.
Varios científicos están intentando seguir las vías intrincadas y elusivas a través de las cuales el cerebro controla al corazón,para poder aislar al supuesto mensajero químico de la muerte.
James Skinner,un neurofisiólogo de la Escuela de Medicina Baylor,en Houston,afirma: " Creemos que se trata de una sustancia química,probablemente un neuropéptido o una enzima,que no funciona correctamente en el lóbulo frontal de algunas personas.
Esto no destruye la inteligencia,la percepción o la memoria,pero aumenta la potencialidad letal de los trabajos que provocan stress,las arteriopatias coronarias y otras causas que pueden desencadenar una fibrilación ventricular ".
Skinner cree que en algunas personas este mediador químico interfiere con la capacidad del cerebro) para determinar el momento apropiado de poner en marcha una respuesta del tipo " lucha o huye ",con aumento del ritmo cardíaco.
En las personas sanas,el lóbulo frontal (la parte pensante del cerebro) controla estos mensajes de pánico,pero alguien que tenga este mecanismo alterado por acción de ese agente químico desconocido no puede frenar la taquicardia una vez desencadenada y esto puede llevarle a la arritmia y a la muerte súbita.
Skinner se ha propuesto una tarea difícil: encontrar un antídoto al mediador químico de la muerte.
En esta investigación,tanto él como el equipo de Harvard dirigido por Lown están rompiendo con su propia tradición,ya que los cardiólogos investigadores han sido durante mucho tiempo reacios a ligar la muerte súbita con circunstancias psicológicas.
Pero desde que Meyer Friedman,un cardiólogo del Hospital Mount Zion,de San Francisco,señaló que el tipo de personalidad superdinámica conocida en Estados Unidos como " tipo A " constituía un factor de riesgo adicional en las enfermedades del corazón,han proliferado los estudios sobre las variables psicológicas y sociales que pueden intervenir en la muerte súbita.
De esos estudios se derivan las siguientes consideraciones: Cerca del 20 por ciento de los que mueren de ataque cardíaco o son reanimados después de sufrir disritmias cardiacas graves,han experimentado un alto stress psicológico durante las últimas 24 horas.
Por ejemplo el primer día del secuestro del vuelo 847 de TWA murió el padre del ingeniero Benjamin Zimmermann,que padecía del corazón.
Los viudos y viudas recientes tienen una tasa de mortalidad superior en un 40 por ciento respecto a los casados de la misma edad.
Las muertes por causa cardíaca aumentan drásticamente en los hombres durante el primer ano de retirarse del trabajo y convertirse en pensionistas.
La ira es una de las emociones que frecuentemente precede a la muerte.
Mediante un experimento de laboratorio,que consistía en arrebatar la comida a unos perros hambrientos,el Dr. Richard Verrier,especialista en Clínica Cardiovascular en la Escuela de Salud Pública de Harvard,ha demostrado que la ira aumenta de modo significativo la vulnerabilidad de estos animales a la fibrilación.
Por otra parte,las investigaciones de un psiquiatra de Harvard,el Dr. Peter Reich,sugieren que algunas muertes súbitas en el género humano podrían haber estado precedidas de ataques de ira.
La liberación masiva de ciertos mediadores químicos llamados catecolaminas (que son segregadas por las glándulas suprarrenales y el sistema nervioso simpático en momentos de extremo stress) podría romper unas pequeñas fibras musculares llamadas fibras de Purkinje.
Al igual que el tejido cicatricial que resulta de un infarto de miocardio,estas " miocardiopatias de stress " producen agujeros o pozos muertos eléctricos que pueden ocasionar un cortocircuito en el sistema de conducción del corazón.
En multitud de ocasiones se observa cómo el ritmo cardíaco se acelera súbitamente como respuesta al stress que supone hablar en público o conducir con un tráfico denso y ruidoso.
Mediante un aparato portátil de electrocardiogramas,Graboys demostró que un ardiente forofo del Club Boston Celtics,que gozaba de perfecta salud,experimentó bruscos episodios de taquicardia ventricular en cuatro ocasiones,mientras miraba en la televisión la final de baloncesto Philadelphia-Celtics.
Otro investigador sugiere que el aislamiento prolongado,la frustración profesional,la depresión,la ira o el enfado,no sólo pueden provocar en el momento una respuesta fatal del corazón,sino que además pueden aumentar a largo plazo la probabilidad de padecer muerte súbita.
Se han realizado estudios comparativos en varias comunidades de japoneses y americanos en California,que ponen de manifiesto que el principal factor de riesgo coronario - - más importante que la tasa de colesterol,la dieta,la presión de la sangre o el tabaco - - era la pérdida de la seguridad que proporcionan los lazos familiares estables de la cultura japonesa tradicional.
Otro estudio llevado a cabo en una ciudad de Pennsylvania,habitada mayoritariamente por americanos de origen italiano,gente muy vital,de buen comer y con fuertes lazos familiares,indica la misma conclusión.
Otros estudios muestran que el aislamiento por si solo podría cuadruplicar el riesgo de muerte por enfermedad coronaria.
Las últimas investigaciones incluso arrojan dudas sobre la teoría de que los individuos con estudios superiores y personalidad de ejecutivo sean más vulnerables a la muerte súbita.
En un seguimiento,de cinco años de duración,de 1.739 pacientes que se recobraban de infarto de miocardio,cuyos corazones ya dañados y lábiles desde el punto de visto eléctrico les hacían fácilmente vulnerables a la fibrilación,mostró que las victimas con 8 años de escolaridad o menos,tenían una tasa de mortalidad tres veces superior a la de sus compañeros mejor instruidos.
Los trabajos difíciles,repetitivos y rutinarios sobre los que la persona no ejerce ningún control y que no ofrecen posibilidad alguna de prosperar,aumentan el riesgo de muerte súbita.
Por otra parte,el riesgo de accidente y el caos automovilístico serían responsables de la hipertensión que padecen muchos conductores de autobuses.
Durante los años 60 los jóvenes ingenieros espaciales del Centro Espacial Kennedy comenzaron a morir de muerte súbita con una frecuencia que doblaba la tasa general para los hombres de su edad.
" La CIA pensó que los ingenieros estaban siendo envenenados ",recuerda Robert Eliot,un cardiólogo que colabora con la NASA y que actualmente es el director del Centro Nacional de Medicina para la Prevención del Stress,en Arizona.
" Pero estos hombres también tenían una alta incidencia de divorcios,alcoholismo y suicidios.
El problema subyacente era el peso excesivo de la responsabilidad de su trabajo.
" Según Eliot,las autopsias revelaron que más del 80 por ciento de estos hombres presentaba signos de miocardiopatía de stress.
Otros investigadores han vuelto su atendía el famoso estudio sobre el corazón realizado con la población de la ciudad de Framingham hace 35 años,para reexaminar la muerte súbita en las mujeres.
Se han encontrado con que dos terceras partes de estas víctimas de paro cardíaco no tenían ninguna manifestación previa de enfermedad.
Un estudio basado en los datos de Framingham muestra que las experiencias frustrantes o deprimentes,como hospitalizaciones por enfermedades psiquiátricas,una muerte reciente en la familia,un matrimonio con un trabajador no cualificado,sin esperanza alguna de ascenso social,un trabajo con un jefe odioso o tener a su cuidado más de un niño pequeño,aumentan enormemente la susceptibilidad de las mujeres a padecer enfermedad coronaria.
Más que determinar la importancia de la enfermedad cardíaca subyacente,las nuevas investigaciones buscan la repercusión del stress en el proceso general de deterioro cardíaco.
Este trío es conocido - - al menos por mí - - como " el bueno,el malo y el feo ".
Pero existen otras numerosas posibilidades,como los indios Hopi,los pigmeos de los bosques de Ituri,los Kwakiutl,del Canadá,los esquimales Inuit o los Yir Yoront,para no mencionar los estudios comparativos inter-culturales * *.
De modo que no te faltarán ejemplos,aunque hayas cateado Antropología tres veces.
Además,las tribus están llegando a los medios: hace poco,la televisión pública de Estados Unidos lanzó un programa semanal titulado " Mundo en extinción ",permitiendo que su ascendente nivel de audiencia tomara contacto con gente que iba en la otra dirección (ya que puede decirse que extinguirse de la faz de la Tierra es el non plus ultra de la situación descendente).
Estoy convencido de que siempre,en algún lugar del mundo,existe una tribu para apoyar cualquier punto de vista.
Si,por ejemplo,quieres argumentar a favor del adulterio,estás en excelente posición para encontrar ejemplos contundentes.
Es sabido que el sexo ha sido siempre una de las bazas principales de la Antropología.
Desde luego,contamos con él en nuestra propia cultura,pero solemos creer que es mucho mejor en sitios exóticos,como la Micronesia.
La variedad no es la única sal de la vida,pero si de la Etnología.
Sugiero la lectura de Los Ulithi,un estilo micronesio de vida,por William A. Lessa,publicado en 1966,que encontré en una caja de viejos libros de bachillerato de mi mujer.
Hay una muchacha desnuda en la cubierta,y recomiendo pasar directamente al capítulo siete,titulado " Conducta sexual " (como tuve que hacerlo yo,pues buscaba material para este artículo).
Allí encontré una atractiva descripción de la fiesta del pi supuhui,expresión que,traducida libremente del ulitiano,significa algo así como " fiesta de los cien magreos ".
La idea en que se apoya esta festividad es que cada cual escoge a otro del sexo opuesto y se lo lleva al huerto (en Micronesia,a los matorrales).
A los esposos no les está permitido ese día escogerse entre sí,sino que han de unirse con terceras personas,y no creo que les importe.
Lo bueno,a mi entender,de la fiesta del pi supuhui,es que no se celebra en una fecha determinada,ni siquiera una vez al año,sino cuando alguien lo sugiere.
Me imagino a un ulithi proponiendo a gritos un pi supuhi como el estudiante que echa a correr hacia el comedor gritando " ¡Maricón el último! ".
A veces es difícil imaginar qué punto de vista puede sostenerse con la conducta de determinada tribu.
Los Jaté de Nueva Guinea son caníbales (o al menos lo eran en 1960) y sujetan los Párpados y labios de sus víctimas con huesecillos de ala de murciélago,para impedir que escape el alma y les produzca una indigestión a los comensales.
Aunque los Jalé tienen razones religiosas para tratar a sus enemigos de esa forma,hasta que están bien a punto,sospecho que la razón por la que se los comen es más gastronómica que sagrada: la gente tiene igual o mejor sabor que el cochinillo.
Apostaría a que los Jalé pueden sostener discusiones sobre quién ha sido - - o puede ser - - más apetecible a la brasa,pero aún no hallé la forma de meter el tema en una tertulia entre amigos.
Hay dos formas de encarar las argumentaciones antropológicas: hacer uso de ellas,o despreciarlas.
Yo estoy a favor de utilizarlas,y aconsejo ser siempre el primero en sacar a pueblos primitivos en la conversación.
Esto te da una gran ventaja,similar al servicio en el tenis.
Tu oponente (confieso que considero a toda conversación como una competencia,de una u otra forma) debe devolver la bola desde el sitio a que tú la has lanzado.
Por ejemplo,puedes lanzar de entrada un casi imparable rito iniciático de circuncisión,y subir rápidamente a la red con un matrimonio exogámico.
Es casi seguro que ganarás el set,si el adversario no rompe tu servicio con una volea de ceremonias funerarias.
Por supuesto,puede que tú seas una de esas personas que valoran el rigor intelectual,la honestidad,la amistad y el juego limpio,por encima de una victoria en la discusión.
Lo considero un lamentable error,aunque sé,por mi bagaje antropológico,que esto suele ocurrirle a los blandos (es el primer principio de la Etnografía Comparativa).
Si perteneces a esta tipología,y te sientes acosado por inescrupulosos mencionadores de incomparables tribus y pueblos primitivos,hay formas de defenderte.
En primer lugar,debes prestar mucha atención a la forma en que se describe la tribu en cuestión.
Si la discusión se centra,digamos,en si las codornices deben tomarse con vino tinto o rosado,o en si la auténtica paella lleva o no lleva guisantes,un verdadero antropólogo citará rápidamente a los Jalé por su nombre,dará un precisa ubicación geográfica,y una detallada descripción de sus costumbres culinarias,con una erudición no inferior a la del mesonero mayor de Castilla.
Al menos,sabrás que tienes que vértelas con una tribu real.
Pero esto ocurre muy rara vez.
La mayor parte de las personas que hablan tanto de una u otra tribu no son siquiera antropólogos y a menudo no saben de qué tribu están hablando.
Todo lo que hacen es lanzar un prolongado suspiro,dirigirte una mirada de superioridad,y colocarte en la desventajosa posición de devolver su servicio: " Existe una tribu... " No te lo creas.
Pregunta inmediatamente de qué tribu se trata,y si el nombre no sale a relucir,expresa sin vacilar que tienes serias dudas sobre su existencia.
Sé que puede parecer un tanto rudo,pero la gente suele citar tribus que creen que sus profesores citaron quince años atrás,y que en realidad no son más reales que los Gazonga (a los que,debo admitir,he inventado).
Los Gururumba,por otra parte,son reales,lo cual demuestra que la realidad es más sonora que la ficción.
Pero yo no soy el único que se inventa tribus.
Ludwig Wittgenstein,el filósofo,también lo hacía,aunque con mayor honestidad.
Pudo escribir,por ejemplo: " Imaginemos que los individuos de una tribu fueran enseñados a no expresar sentimientos de ningún tipo ".
(Gente de esta clase,de existir,nunca hubiera usado letra cursiva).
Otra cosa que se puede esperar de los aficionados a las tribus,es que veneren una versión un tanto desangelada del mito del " noble salvaje ".
Las tribus son,para muchos de nosotros,algo así como el equivalente humano de las medicinas de hierbas: unas gentes que viven en bosques y selvas,que no llevan ropas a la moda o ni siquiera ropas,son consideradas como más " naturales ",y,por lo tanto,mejores.
Esto no es necesariamente cierto.
Consideremos el modo de vestir de los Jalé.
Es posible que las vainas para el pene sean moralmente superiores a los tejanos ceñidos o los pantalones posmodernos,y es verdad que difícilmente se rebajarían a aceptar que la arruga es bella,pero los hombres Jalé utilizan una ridícula sarta de anillos a la cintura,para no mencionar otros pueblos que se acogotan con collares,se atraviesan la nariz con anillos,se tiñen la piel o el pelo,o se cubren de ceniza de pies a cabeza.
Podríamos afirmar que,para cada situación,hay una tribu " buena " y otra " mala ".
Si alguien está tratando de convencerte de cuánto mejor,más sana y más natural es la vida de los nobles salvajes,comparada con la violencia y la drogadicción que azotan a las más civilizadas capitales del mundo,menciónales a los Yanomamo.
No sólo son fanáticos de la guerra,que practican con desenfrenada crueldad,sino que su afición favorita - - cuando no están peleando - - es pasarse las tardes sentados en la selva húmeda,aspirando un verde polvillo alucinógeno.
Aunque la tribu de tu oponente sea real,aún no estás perdido.
¿Cómo sabemos que los nativos decían la verdad cuando aseguraron al antropólogo que Cunga Cunga robó la receta secreta del maíz cocido para que la tribu no muriera de hambre cuando escasearan los piñones? No me atrevería a sospechar que los buenos salvajes pueden ser unos mentirosos,si no fuera por el frentazo que se pegó,postmortem,Margaret Mead.
No se si recuerdas que hace 50 años la célebre antropóloga volvió de Samoa asegurando que las muchachas adolescentes salían a acostarse por ahí antes de estar casadas,y que sus padres no se molestaban en ahuyentar a los galanes merodeadores.
Hace un tiempo,Derek Freeman declaró que esto no era cierto,y que los padres samoanos jamás permitirían que sus hijas ligaran alegremente con el primero que encuentren,por mas que luciera vaina y anillos de oro.
En su opinión (la de Freeman) los buenos salvajes de Samoa se quedaron con Margaret contándole mentiras,una divertida costumbre local llamada tau,que podría traducirse como " colarle un gol ".
La discusión sigue abierta y no me arriesgaría a afirmar quién tiene razón.
Pero nos otorga un respiro.
Si un nativo es capaz de mentir a Margaret Mead,es capaz de mentirle a cualquiera.
E incluso podría ser que urdir historias para contárselas a los antropólogos fuera una de las diversiones favoritas de ciertas tribus: Primer nativo: Vamos a contarle mentiras.
Segundo nativo: Vale.
Le diré que nos comemos a la gente.
Primer nativo: ¡Genial! Yo contaré que mi hija se acuesta con cualquiera.
Segundo nativo: Es un poco fuerte,pero será divertido.
Tercer nativo: ¡Eh,chicos! ¿Qué tal si me pongo unos anillos en la cintura? Si todo lo demás falla,y uno se encuentra acorralado por un argumentador incansable que cuenta con una verdadera tribu,queda aún el recurso de un contraargumento final,basado en la lógica más cruda.
Estoy diciendo esto por puro altruismo,ya que mi mujer va a leer este artículo,y jamás podré volver a utilizar una tribu para ganarle una discusión sobre lo que es psicológicamente más sano.
Pongamos por caso aquel argumento de la tribu que amamanta largamente a SuS niños y éstos son muy saludables.
El contraargumento es: " ¿Quién dijo que son muy saludables? " n Como bien saben los antropólogos profesionales,sólo porque alguien,en alguna parte,haga algo,no quiere decir que eso sea bueno.
Este punto fue expresado más sintéticamente por el psicoanalista de un amigo mío.
Mi amigo,casi al final de la sesión,trajo a colación una tribu,con la intención de sugerir que si los nativos podían salir adelante solos,sin tener que pagar 80 dólares la hora,quizá él también podría hacerlo.
" ¿Quién sabe si realmente salen adelante solos? - - retrucó el psicoanalista - -.
No sabemos nada de ellos,puesto que no tienen psicoanálisis ".
Supongo que fue entonces cuando mi amigo le dio el talón de aquel mes,sin rechistar.
Los Kung son cazadores-recolectoreA,tratan bien a sus niños y no contraen préstamos hipotecarios.
Los Ik son un pueblo que degenera fácilmente hacia un feroz egoísmo cuando se enfrenta al peligro de morir de hambre.
Los labio-estirado resultan familiares a todo el que haya mirado alguna vez cualquier revista antropológica.
¿Quién puede resistirse a comparar a los Machiguenga,de la selva húmeda del Amazonas,con los Parisinos,de Francia? Sentimientos como el amor,el odio,el altruismo o,desde luego,el egoísmo,son otras tantas respuestas a este ciego objetivo genético,totalmente ajeno a todo rasgo de voluntad o siquiera de conciencia.
" Defenderé la tesis de que la unidad fundamental de selección,y por tanto del egoismo,no es la especie ni el grupo,ni siquiera,estrictamente hablando,el individuo - - anuncia Dawkins - -.
Es el gen,la unidad de la herencia.
A algunos biólogos este planteamiento les podrá parecer,al principio,una posición extrema.
Espero que cuando aprecien en qué sentido lo afirmo,estén de acuerdo en que es una posición,en esencia,ortodoxa,aún cuando expresada de una manera insólita ".
Hecha esa declaración y abierto este paraguas,el sociobiólogo da por enunciada su teoría,y se da a la ardua tarea de demostrarla.
Para ello recurre a un variado arsenal de argumentos y afirmaciones,desde espectaculares bombas de profundidad lanzadas desde las primeras páginas para despejar el terreno,hasta amenas y eruditas explicaciones científicas,citas de colegas prestigiosos,descripción de sorprendentes observaciones etológicas o áridas formulaciones matemáticas.
Es irresistible la tentación de exponer un par de ejemplos del primer caso,como cuando afirma que " un feto humano,sin más sentimientos humanos que una ameba,goza de una reverencia y una protección legal que excede en gran medida a la que se le concede a un chimpancé adulto " ; o cuando,al describir el macabro apareamiento de la mantis religiosa,cuya hembra devora la cabeza del macho durante el coito,supone que " ya que en la cabeza del insecto se encuentran localizados algunos centros nerviosos inhibitorios,es posible que la hembra mejore la actuación sexual del macho al devorarle la cabeza.
De ser así,es un beneficio adicional.
El beneficio primordial es que consigue una buena comida.
" Además de la exposición y defensa de una inquietante y revulsiva interpretación científica,El gen egoísta es también,a su manera,un instructivo manual de introducción a la Biología Genética.
Y como tal,comienza donde comenzó todo: en aquel célebre " caldo primigenio " que hace tres o cuatro mil millones de años constituía los mares de la Tierra,y en el que se cocinó el origen de la vida.
" Recientes experimentos de laboratorio - - relata Dawkins - - en los que se simularon las condiciones químicas de la Tierra antes de que se produjese la vida,dieron como resultado sustancias orgánicas llamadas purina y pirimidina.
Ambas son componentes de la molécula genética,denominada ADN (ácido desoxirribonucleico) ".
Esta molécula,o más precisamente una forma precursora de ella,flotaba en el caldo original.
No era necesariamente la más grande,ni la más completa.
Accidentalmente,se formó con la extraordinaria propiedad de poder crear copias de sí misma.
Dawkins la denomina el reproductor,y según él ese impulso reproductivo no sólo no ha terminado,sino que es el motor de la larga historia de la evolución de la vida y de su permanencia actual.
Cada copia del reproductor producía,como es lógico,nuevas reproducciones.
En un principio,el caldo se pobló de réplicas idénticas,pero algunos errores de reproducción produjeron moléculas distintas,que se reprodujeron a su vez.
La proliferación,agregación y convivencia de estos distintos reproductores - - descendientes todos del gran reproductor original - - dio pie a la diversidad de la vida,y también a la lucha evolutiva por la supervivencia.
" El caldo primigenio no podía mantener a un número infinito de moléculas reproductoras - - explica el autor - -.
Por una parte,el tamaño de la Tierra es finito,pero otros factores limitativos deben,también,haber sido importantes.
" Algunos reproductores encuentran formas de protegerse a sí mismos,tanto de las agresiones del medio como de sus rivales,ya sea por medios químicos o por medio de una " coraza " de proteínas a su alrededor.
Dawkins supone que estos caparazones se hicieron cada vez más complejos,adquiriendo funciones diversas al servicio de una mejor perpetuación de sus " amos ",los reproductores,que comenzaron a utilizarlas como habitáculos,vehículos y máquinas de supervivencia.
Y a partir de allí,echa a volar su bien dotada imaginación lírica: " ¿Llegaría a tener algún fin este gradual perfeccionamiento de las técnicas y artificios empleados por los reproductores para asegurarse su propia continuidad en el mundo? ¿Qué misteriosas máquinas de autopreservación producirían los milenios? En cuatro mil millones de años,¿cuál sería el destino de los antiguos reproductores? No murieron,porque son maestros en el arte de la supervivencia.
Pero no se les debe buscar flotando libremente en el mar ; ellos renunciaron a esa desenvuelta libertad hace mucho tiempo.
Ahora abundan en grandes colonias,a salvo dentro de gigantescos y lerdos robots,encerrados y protegidos del mundo exterior,comunicándose con él por medio de rutas indirectas y tortuosas,manipulándolo por control remoto.
Se encuentran en ti y en mí ; ellos nos crearon en cuerpo y mente,y su preservación es la razón última de nuestra existencia.
Aquellos reproductores han recorrido un largo camino.
ahora se les conoce con el término de genes,y nosotros somos sus máquinas de supervivencia.
" Pese a estas efusiones que rozan la ciencia-ficción,Dawkins no deja de arropar su teoría en una revalorización de la obra de Darwin y Mendel,ni de demostrar un actualizado conocimiento de su disciplina,tanto en las citas como en la extensa y bien seleccionada bibliografía.
Después de un largo capítulo dedicado a exponer la estructura y funcionamiento de la cadena ADN y las leyes genéticas,bajo el inspirado titulo de " Las espirales inmortales ",llega al punto álgido de su demostración,aquél en que debe explicar cómo las características y conductas de las innumerables y diversas especies vivas - - y cada uno de sus individuos - -,que él llama " máquinas de genes ",responden siempre y en cada caso a una programación predeterminada por la estructura genética.
" Un pulpo no se parece en nada a un ratón,y ambos son muy diferentes de un roble - - observa Dawkins - -.
Sin embargo,en su química fundamental son casi uniformes,y en especial,en lo que se refiere a los reproductores que portan,los genes,son básicamente el mismo tipo de moléculas para todos nosotros,desde las bacterias hasta los elefantes.
" Todos portamos,pues,esas tiránicas cadenas de ADN,distribuidas en las células de todo ser viviente,y un gen puede ser considerado como una unidad que sobrevive a través de un gran número de seres sucesivos e individuales.
Para Dawkins,la peculiaridad de un gen es que no se vuelve senil ; no es más probable que muera cuando tienen un millón de años que cuando tienen cien.
Esa especie de eternidad,siempre potencial y condicionada al éxito en su designio de supervivencia,se consigue desarrollando y programando la máquina de genes que le ha tocado en suerte,sea una ameba o un lector de ALGO,por ejemplo para obtener ese fin.
Quede claro,una vez más,que ese fin no es necesariamente la supervivencia de la propia máquina - - aunque coincida con ella a menudo - -,ni siquiera la de los genes que porta,sino del tipo o clase genética de que forman parte.
Con ello explica Dawkins ciertas conductas animales y humanas que podrían aparecer como contradictorias de su teoría.
" Para una máquina de supervivencia,otra máquina de supervivencia (que no sea su propio hijo u otro pariente cercano) constituye una parte de su entorno,al igual que una roca,un río o un bocado de alimento.
Es algo que obstruye el camino o que puede ser utilizado.
Difiere de una roca o un río en un aspecto importante: tiene tendencia a devolver el golpe.
" Con esta audaz e implacable afirmación (que de paso echa por tierra toda solidaridad o altruismo entre especies,o comunidades de cualquier tipo sin parentesco directo,lo cual incluye a la pareja),Dawkins convierte tanto a la Naturaleza como a la civilización y la cultura en un campo de batalla entre máquinas de genes,y a toda apariencia contraria en una estrategia,simulación o estratagema de un programa genético en un propio beneficio.
La agresión,por ejemplo,en sus variantes de valor y miedo,ataque,huida o persecución,y destrucción o perdón del adversario,obedecería siempre a un veloz y completo análisis de posibilidades,con respuestas alternativas programadas por los genes a partir de su prolongada experiencia en sobrevivir.
Pero Dawkins se cuida bien de distinguir entre el " programa " introducido en cada máquina por la herencia genética,y la decisión que adopta cada individuo en el rol de " ejecutivo " de la ardua empresa.
Porque,como él mismo declara," Una sociedad humana basada simplemente en la ley de los genes,de un egoismo cruel universal,sería una sociedad muy desagradable en la cual vivir ".
Teseo se propone matar al Minotauro,monstruo oculto en el centro del laberinto,que se alimentaba de doncellas y mancebos cretenses.
Si conseguía su objetivo,el hilo le permitiría luego a Teseo encontrar el camino de vuelta: desde el interior de la madre Tierra,saldría a la luz unido a su cordón.
Como en efecto sucedió.
Por supuesto,si el visitante ha seguido la norma de girar a la izquierda en cada bifurcación,habrá llegado al centro y desandado el camino en no más de un cuarto de hora.
Laberintos ajardinados como el de Hampton Court,abundan en el Reino Unido: Alkborough,Hilton,Somerleyton Hall,son también célebres.
En Francia son notables los del Jardín Botánico de París,o el de Noissy-le-Roi.
No faltan en España,como el de los reales jardines de La Granja (en la provincia de Segovia),o el del parque llamado precisamente Para encontrar el centro del laberinto,sin embargo,el hilo carecía de utilidad.
Teseo pudo encontrarlo,en cambio,por el socorrido método de ensayo y error,o bien aplicando una sencilla regla,en cuyo caso podría haber vuelto a la entrada sin necesidad de hilo alguno.
La cita Jorge Luis Borges en El jardín de senderos que se bifurcan: " La casa queda lejos de aquí,pero usted no se perderá si toma ese camino de la izquierda y en cada encrucijada del camino doble a la izquierda ".
La regla vale para un gran número de laberintos,y uno de los más famosos en el que es aplicable se encuentra apenas a media hora de Londres: el laberinto de Hampton Court,que desde 1690 viene confundiendo a los curiosos.
Por sólo 30 peniques - - - unas 65 pesetas - - se puede disfrutar de los sobresaltos de extraviarse en sus caminos enmárcados por setos de boj.
Siempre,claro,que se ignore su secreto.
A un turista no informado de la regla,como la mayoría,el recorrido por los senderos de Hampton Court le llevará a plantearse la necesidad de girar a derecha o izquierda al llegar a la primera bifurcación.
Y así en las siete restantes que podrá encontrar en los 800 metros de su extensión.
Pero lo más probable es que crea encontrarse con muchas más,pues entrando y saliendo de callejones sin salida,atravesando recodos,volverá a pasar una y otra vez por los mismos sitios por donde ha pasado sin recordarlos.
Pronto perderá la confianza en el mapa mental que se ha ido trazando desde el comienzo.
Se olvida de buscar el centro del laberinto,ya no le importa más que encontrar la salida antes del Laberinto,en el barrio barcelonés de Horta,antigua finca del marqués de Alfarrás.
No en todos los laberintos,como ya se insinuó,es válida la regla de girar a la izquierda en cada bifurcación.
Hay uno especialmente diabólico que ha diseñado el especialista Greg Bright: el de la espléndida mansión de Longleat,en Wiltshire (Inglaterra).
Seis puentes que se entrecruzan confieren al laberinto de Bright una tercera dimensión,con lo que se frustran todas las soluciones fáciles.
Pero tampoco se cumple la regla,en todos los laberintos trazados sobre un plano,Un ejemplo soberbio es el laberinto de la finca del millonario neoyorquino Armand Erpf,quien confió su diseño al ya más probable es no hallar ninguno de los centros,y volver a encontrarnos en la entrada sin habérnoslo propuesto.
La regla de la izquierda tampoco es aplicable en el laberinto de Ayrton.
Cuando la regla no se cumple,podemos estar seguros de que el constructor ha introducido alguno de sus diabólicos trucos.
El más frecuente es trazar un recorrido en torno al centro,y entonces colocarlo a la derecha de la única bifurcación que conduce a él,de modo que al aplicar la regla de la izquierda se pasará siempre de largo (ver fig.
3).
Y no pensemos que será posible deshacer esta trampa aplicando una regla de giro a la derecha,si lo hacemos así,el camino hacia el centro quedará a la izquierda.
¿Estaremos condenados entonces a girar eternamente? La verdad,mucho más humillante,está prefigurada en el diseño del laberinto de Ayrton.
En r la primera bifurcación (figura 3),giramos astutamente a la izquierda,de modo que ya estamos en el camino principal hacia nuestro objetivo,pero resulta que ese patio central está situado a la derecha,más allá de otra bifurcación que pasamos por alto con aire de entendidos.
Después de un rato,volvemos a encontrarnos con la primera bifurcación,pero no la reconocemos,y por tanto giramos a la izquierda,para encontrarnos al cabo del camino en la abertura por donde habíamos entrado.
Imaginemos a Teseo tratando de explicarle a una doncella ansiosa,que no había encontrado al Minotauro.
Se sentiría traicionada.
A Erpf le había fascinado la lectura de El hacedor de laberintos imaginaria autobiografía del mítico Dédalo,constructor del más célebre de todos los laberintos,escrita por el propio Ayrton.
El laberinto concebido por Ayrton,y situado en Catskills,no tiene solamente un centro,que es lo habitual en este tipo de construcciones,sino dos: uno de ellos contiene un Minotauro de bronce,el otro un Dédalo del mismo material.
Su más de medio kilómetro de sendas en espiral no está flanqueado por frágiles setos vegetales,sino por sólidas paredes de ladrillo y hormigón.
Pero el resultado seria el mismo.
Si algún topólogo nos dice que un laberinto está " múltiplemente conectado ",quiere expresar que contiene al menos un recorrido cerrado.
Lo que significa la existencia de más de un camino uniendo por lo menos un par de puntos de decisión.
En el múltiplemente conectado laberinto de Hampton Court,los dos recodos que contiene no nos juegan esa mala pasada,puesto que allí se cumple la regla de la izquierda.
Y en la intrincada biblioteca donde se pierde el hermano Guillermo de Baskerville,sus múltiples conexiones se entrelazan de manera tan compleja,que sólo saldrá de allí por pura casualidad.
Hubiera necesitado,como pensaba,algo con qué hacer marcas.
Y es esto lo que nos hará falta para resolver un laberinto de múltiples conexiones.
Supongamos que en cada punto donde tengamos que hacer una elección,marquemos con alguna señal nuestro camino de entrada y el que hemos escogido.
Observemos ahora la figura 4. Poco después de entrar se nos presenta la primera opción: un punto donde se unen tres caminos,si también contamos el de entrada.
En éste hacemos una marca,y otra en el de la izquierda,que es el que habremos escogido.
El sitio en el que el camino se bifurca hacia el centro (pero no lo sabemos) es otra unión de tres caminos.
Y aunque la regla de la izquierda no nos permite,por supuesto,girar a la derecha,volvemos a hacer dos marcas.
Seguimos un poco más adelante y de pronto vemos... ¡dos marcas! Esto quiere decir que hemos tomado por un recodo,de modo que regresamos por el mismo camino.
Pronto volveremos a encontrar dos marcas,pero hay un tercer camino sin marcar.
Lo seguimos... ¡y hemos llegado a nuestra meta! El recurso parece que funciona.
¿Podemos establecerlo como norma? Es esto lo que pretendía el texto recordado por el hermano Guillermo,pero la apresurada traducción del latín era un tanto confusa,y podía conducir a errores.
Una versión muchísimo más clara es la que propone Joe O'Rourke,un científico profesor de Informática: Gire a la izquierda en cada bifurcación.
Más exactamente: r gire por la vía situada más a la izquierda que aún no haya sido explorada.
Esto requiere que se marquen las vías por las que se ha girado.
Siga así hasta que encuentre un callejón sin salida,o su equivalente funcional: una conjunción cuyas salidas han sido todas exploradas.
En cualquiera de los dos casos,dé media vuelta y decida qué hacer en esa conjunción de acuerdo con las mismas reglas.
Esto significa: " Observar la regla de la izquierda ; pero teniendo en cuenta que un recodo puede despistarnos,emplear marcas para no caer dos veces en la misma trampa ".
Es lo que en la jerga informática se denomina búsqueda del " primero en profundidad " (depth first),porque prescinde de toda distracción hasta haber recorrido,lo más profundamente posible,el camino que hemos escogido al comienzo.
Si penetrando " lo más profundamente posible " llegamos a nuestra meta,pues muy bien ; si no es así,retrocedemos hasta una conjunción en la que habremos de decidir de acuerdo con las mismas reglas.
Reglas que nos exigen verificar las marcar que hemos hecho en ese punto y tomar el camino situado más a la izquierda de los que aún no han sido marcados.
Podemos llegar así a otros recodos o callejones sin salida,pero hallaremos finalmente una bifurcación que nos conduzca hacia algo interesante.
No es raro que sea precisamente la ciencia de los ordenadores la que pueda rectificar el laberíntico texto del hermano Guillermo.
Trabajar con ordenadores significa enfrentar laberintos días tras día.
Los programas sirven a una incesante búsqueda de algo a través de un laberinto: un nombre en una lista de correos,el mejor movimiento en una partida de ajedrez.
En escala mayor,los ordenadores de una compañía telefónica dirigen llamadas de ciudad a ciudad por puntos de derivación,y si una línea está sobrecargada,el ordenador busca una ruta alternativa.
El problema es el mismo que plantea atravesar,por ejemplo,el laberinto de Horta.
Y la búsqueda en profundidad del primer camino,tal como se hace en Informática,tiene una serie de ventajas que están implícitas en la frase de Joe O'Rourke " de acuerdo con las mismas reglas ".
O sea,que el programa puede ser repetido con toda eficacia,mediante una estrategia conocida como iteración.
No es ésta,sin embargo,una buena estrategia para el ajedrez,porque podría plantearnos 50 movimientos sucesivos antes de terminar en un mate absurdo.
Por eso son tan torpes las computadoras,Un tumor maligno o cáncer es un conglomerado de miles de millones de células,todas descendientes de una primera célula " fundadora " del tumor.
Esta era una célula normal que cumplía su función regular en algún tejido hasta que sufrió un cambio critico y comenzó a reproducirse y proliferar sin obedecer a los factores externos que normalmente regulan el crecimiento celular.
Pero el cáncer es una enfermedad maligna fundamentalmente por su capacidad de dar lugar a un proceso de diseminación.
El tumor primario localizado en un órgano determinado (mama,colon,vejiga,etc.) puede originar,al cabo de algunos meses o algunos años,tumores secundarios localizados en otros órganos (cerebro,hígado,pulmones).
Estos tumores secundarios son las llamadas metástasis,y pueden producirse en órganos muy distanciados del que dio origen al tumor inicial.
Por ejemplo,un cáncer de mama puede originar metástasis en el cerebro,en las extremidades o en los huesos de la pelvis.
Para que esto ocurra,una célula del tumor original debe separarse de éste (sola o en pequeños grupos),penetrar en un vaso sanguíneo o linfático (los vasos linfáticos drenan los tejidos de su líquido extracelular,comunicándose luego con el sistema sanguíneo),y llegar finalmente al órgano en el que quedará retenida y proliferará.
En muchos cánceres no es el tumor principal el que pone en peligro la vida del enfermo,ya que a menudo es posible eliminarlo por radioterapia o cirugía.
Por el contrario,son los tumores secundarios los que constituyen una amenaza mucho más importante,dado que pueden ser numerosos y no son accesibles a dichas técnicas terapéuticas.
De hecho,la mortalidad del cáncer se debe más a las metástasis que a los tumores primarios.
Por ello,hoy los principales esfuerzos terapéuticos están dirigidos a la destrucción precoz de las metástasis.
El punto clave consiste en detectar cuanto antes la aparición de estas metástasis.
Desde que se trata el tumor primario,los médicos realizan exploraciones radiológicas o de otro tipo para descubrir la existencia de posibles tumores secundarios.
Pero en esta etapa,los nuevos tumores pueden tener un tamaño microscópico,lo que impide su detección por estos métodos convencionales,y sin embargo ya son capaces de generar células emigrantes que pueden producir nuevas metástasis.
Este grave inconveniente empieza a tener solución gracias a un método desarrollado por investigadores de la Clínica de la Universidad de Frankfurt,que permite ver las metástasis,por pequeñas que sean,en una pantalla de ordenador.
El método,que está produciendo una revolución en el diagnóstico y tratamiento del cáncer,consiste en una aplicación del brillante descubrimiento de los anticuerpos monoclonales,por el que el alemán Georges Kohler y el argentino (residente en Cambridge,Inglaterra) César Milstein,recibieron el Premio Nobel en 1984.
En breve será posible no sólo detectar,sino también atacar.
Los anticuerpos (o inmunoglobulinas) son unas proteínas muy especiales,fabricadas por un grupo de glóbulos blancos,los linfocitos B. Cuando un elemento extraño al organismo (como una bacteria o un virus) penetra en él,el sistema inmunitario,que es capaz de discernir entre las estructuras " propias " y " no propias ",lo reconoce como una estructura " extranjera " y responde mediante la fabricación de anticuerpos que reconocen alguna de las moléculas extrañas (por ejemplo,una de las proteínas de la cubierta externa de un virus,algún polisacárido de la pared celular de una bacteria o alguna toxina producida por un microorganismo infectante),y se unen a ella.
La molécula extraña que es capaz de inducir esta respuesta inmune y a la que se une el anticuerpo,se denomina antígeno.
La unión del anticuerpo al antígeno provoca,según el caso,la inactivación de la toxina,la pérdida de la capacidad infectiva en el caso de un virus,o la activación de distintos elementos defensivos del organismo que producen la muerte de las bacterias sobre las que se han fijado los anticuerpos.
También las células humanas tienen en su superficie moléculas antigénicas,es decir,capaces de hacer que un sistema inmunitario produzca anticuerpos contra ellas.
Esto no ocurre con las células del mismo organismo porque el sistema inmunitario las reconoce como propias.
Sin embargo,cuando se realiza un trasplante de órganos,este problema adquiere mucha importancia porque las células del órgano donado sí que provocan la respuesta inmune del organismo receptor,lo que obliga a suministrar drogas inmunosupresoras al paciente.
Desde hace tiempo se sabe que,entre los distintos cambios que presentan las células tumorales,poseen una membrana con una composición distinta de la de las células normales de las que derivan.
Algunas de estas moléculas anormales de la membrana son capaces de inducir una respuesta inmune por parte del organismo portador del tumor,es decir,funcionan como antígenos.
Esto,sin embargo,no suele tener mucha importancia para el desarrollo del tumor ya que éste evoluciona más rápidamente que la respuesta inmune por él generada,y además existen distintos mecanismos por medio de los cuales el tumor puede evitar esta respuesta defensiva del organismo.
Pero estos antígenos tumorales proporcionan una nueva posibilidad terapéutica: fabricar anticuerpos que reconozcan específicamente las células tumorales.
La forma tradicional de obtener anticuerpos consiste en inmunizar a un animal (conejo,cabra,etc.) inyectándole muchas veces,durante varias semanas,una preparación más o menos purificada del antígeno.
La presencia de esta sustancia extraña provoca la respuesta inmune,que incluye la producción de anticuerpos contra dicha sustancia.
A continuación hay que extraerle sangre,separar el suero sanguíneo,aislar los anticuerpos del resto de proteínas y eliminar los otros anticuerpos,que no reaccionan con ese antígeno que se ha inyectado,y que están siempre presentes en la sangre como consecuencia de contactos con otras sustancias extrañas.
A partir de este suero,al que se denomina antisuero,y después de un largo proceso de purificación,se puede obtener un preparado razonablemente puro,pero en cantidad limitada,de anticuerpos que reaccionan con el antígeno.
Sin embargo,por este método se obtiene una mezcla heterogénea de anticuerpos.
En Marzo pasado,cuando España salía de una confrontación electoral - - el Referéndum del día 12,sobre permanencia en la Alianza Atlántica - - y lentamente empezaba a hablarse de las elecciones generales que debían celebrarse medio año más tarde,en Octubre próximo,tuvieron lugar en Francia elecciones para la Asamblea,es decir,para su Congreso de Diputados.
quizá por el clima interelectoral reinante,los españoles prestaron especial atención al hecho de que sus vecinos habían cambiado su sistema electoral,hasta entonces extremadamente complicado,instaurando uno más sencillo.
Sin embargo,el gobierno socialista responsable de ese cambio perdió las elecciones,y algunos análisis no descartaban que los seguidores de Miterrand podrían haber " tomado de su propia medicina ",como dicen en las películas del Oeste,perjudicándose de la reforma que propiciaran.
¿Es tan importante el sistema electoral que se elija? Parecería que en una democracia la cuota de poder político depende sólo de la cantidad de votos obtenidos,pero sin embargo no es así.
Tan importante como la cantidad de votos propios - - sin duda el factor determinante - - es la mayor o menor dispersión de los votos contrarios,el tipo de sistema electoral elegido y el tamaño de las circunscripciones.
Pequeños cambios en estos tres últimos factores pueden modificar dramáticamente la cuenta de diputados adjudicados a un partido político ; así,pues,a la lucha por lograr el favor de los votantes se le suma una lucha anterior,tácita,la que libraran los grupos políticos al fijar las reglas de juego previas,y que reflejaron en su momento las esperanzas políticas de cada grupo: los partidos con posibilidades mayoritarias tratando de favorecer a las mayorías - - pero cuidándose de que un futuro y posible contraste electoral no les dejara demasiado malparados - - y los grupos necesariamente minoritarios tratando de arrancar la mayor cantidad de poder a los partidos grandes.
El sistema más sencillo para elegir diputados es el llamado de pluralidad o de mayoría relativa.
Una de las formas posibles de aplicación es dividir el país en tantas circunscripciones como representantes tenga la Asamblea Legislativa,lo cual exige hacer a veces extrañas cabriolas geográficas para que cada circunscripción tenga aproximadamente el mismo número de votantes.
En cada circunscripción resulta electo diputado el candidato que tenga mayoría de votos.
Este primitivo sistema,poco más o menos el que se utiliza en Gran Bretaña,tiene numerosos inconvenientes,beneficiando desmesuradamente a los grupos que,con independencia de su peso real,tengan sus votos muy concentrados.
Además representa muy mal a los distintos grupos de opinión,ya que la extrema regionalización del cómputo siempre pone el peso más en los candidatos que en los partidos.
Como ejemplo,sea una hipotética nación de 1.000 votantes dividida en 10 circunscripciones de 100 votantes cada una,que debe elegir los 10 diputados de su pequeña Asamblea.
Da la casualidad que un mismo resultado se repite en 9 circunscripciones: el partido A gana por 51 a 49 al partido B,y se adjudica así,uno por uno,esos 9 diputados.
En la décima circunscripción se ha presentado el muy regional partido C que obtiene 52 votos,repartiéndose los 48 restantes entre los partidos B y A por partes iguales (24 votos cada uno).
El resultado es estrafalario: el partido A,con 483 votos,es decir,el 48,3 % obtiene 9 diputados,es decir,el 90 / O del total ; el partido B,con 465 votos,no consigue ni un solo diputado ; pero el partido C,con sólo el 5,2 % de los votos,sí que obtiene un diputado (10 % del total).
Naturalmente es éste un cálculo teórico imaginario: en Inglaterra los resultados son más ajustados a la verdadera distribución de votos entre los grandes partidos (además el sistema no es exactamente como se ha descrito en el ejemplo),pero un sistema electoral debe ser bueno en todas las circunstancias imaginables,y en ese sentido el de pluralidad simple es un sistema malo.
Dentro del sistema de mayorías,y de la misma familia que el de la pluralidad,el sistema de mayoría absoluta - - el bien conocido sistema francés del ballotage - - resulta de aceptable representatividad en la elección de cargos unipersonales,como el de presidente o de gobernador en las naciones federales,pero en cambio es complicado y sufre los mismos inconvenientes que el de pluralidad cuando se lo aplica a elecciones de diputados.
Exige con frecuencia una segunda elección - - cuando en algunas circunscripciones nadie ha alcanzado la mayoría absoluta - -,y suele quedar limitado,en el ballotage o segunda vuelta,sólo a los dos candidatos más votados de la primera.
El ballotage favorece los extraños arreglos entre fuerzas rivales pero decididas a aunarse contra un partido más fuerte,y en ese sentido perjudica la representatividad.
El ejemplo dado más arriba para la mayoría relativa puede utilizarse también para demostrar que en este otro caso,si el partido A sacara 48 votos en cada circunscripción,el partido B,26 y el partido C otro 26,una alianza para el ballotage podría hacer que todos los diputados fueran de los partidos B y C - - en la mitad de las circunscripciones el partido B podría apoyar a C y en la otra mitad al revés,por ejemplo - - y el mayoritario partido A quedaría sin representación.
Además,al favorecerse extraños pactos " contra Natura ",sucede que los ciudadanos pueden y suelen quedar finalmente representados por personas que no son de su misma ideología,lo que hace al sistema proclive a generar frustraciones y por consecuencia " bandazos " del electorado,que cambiará frecuentemente su voto de unas elecciones a las siguientes,restando estabilidad a los gobiernos.
Fue por esa inestabilidad intrínseca de sus Asambleas legislativas por la que los franceses debieron instaurar un régimen parcialmente presidencialista,que equilibrara con un Ejecutivo estable y poderoso las alternativas de sus Cortes.
Frente a los sistemas pluralista y mayoritario,que ponen su peso en la simpatía personal de los candidatos frente a los votantes de su circunscripción - - que suele ser su aldea o su barrio - -,se alzan los denominados genéricamente " sistemas proporcionales ".
En éstos se parte de un concepto que ha sido motivo de muchas polémicas: que el representante va a la legislatura no tanto en nombre de un grupo de ciudadanos de una región,sino en nombre de unas ideas generales,las de un partido político,y que el votante quiere manifestar más su preferencia por unas ideas políticas que por una persona.
En general,los estudios sociológicos y estadísticos respaldan plenamente este punto de vista: en las elecciones para concejales de ayuntamiento,por ejemplo,los ciudadanos a veces ni saben el nombre de los últimos concejales de la lista,y se limitan a otorgar su confianza a un determinado partido político ; naturalmente en las elecciones para diputados el factor ideológico pesa aún más que en las municipales.
Por razones de espacio,el examen de los sistemas de representación proporcional quedará para una próxima entrega,así como los sistemas híbridos estadounidense y australiano.
Generalmente debido a que estos estudios se sirven de voluntarios,pagados o no,y sus orgasmos,muchos de ellos alcanzados mediante masturbación,se observan y graban.
Mucha gente piensa que este sistema es artificioso y poco natural.
A pesar de las inhibiciones sociales que rodean este tema,lo cierto es que,para estudiar científicamente la sexualidad,la observación sigue siendo la base fundamental del método científico.
En 1966 Masters y Johnson publicaron sus observaciones en La respuesta sexual humana y por primera vez describieron lo que realmente ocurre durante la relación sexual.
También ayudaron a sacar los tópicos sexuales del cuarto trastero y los incorporaron a las conversaciones de las fiestas y reuniones mundanas en todo el mundo.
A este cambio tan repentino y generalizado se le llamo,en Estados Unidos,Revolución Sexual.
Las personas que buscaban información sexual seria,ya no estaban a merced del estamento médico,pobremente informado,de las novelas pseudopornográficas o de las torpes conversaciones en voz baja con amigos y familiares.
A pesar de contar con un equipo de registro muy rudimentario,las observaciones de Masters y Johnson sobre los sutiles cambios internos fisiológicos,especialmente en las mujeres,constituyeron la base para las posteriores definiciones científicas.
De todos modos,lo que sus trabajos describían era cómo parece ser el orgasmo,y no cómo es verdaderamente.
Descubrir lo que verdaderamente es el orgasmo constituye un desafío para disciplinas que aún se hallaban en pañales cuando Masters,que ahora tiene 70 años y Johnson,que ahora cuenta con 60,iniciaron sus primeros estudios.
Masters,un hombre lacónico que se apasiona cuando discute sobre sexualidad,se halla ahora trabajando en su cuarto libro,que versa sobre el tratamiento a corto plazo de los problemas sexuales.
Reconoce que La respuesta sexual humana ya no está al día y afirma que si ahora volviera a empezar sus investigaciones " repetiría los mismos experimentos con un equipo mejor ".
Desde luego,la segunda generación de investigadores disponen de nuevas tecnologías que les permiten registrar hasta los más pequeños cambios metabólicos.
Pero a pesar de tan sofisticado equipo,cuanto más profundizan los científicos en el orgasmo,cuanto más intentan definirlo con ordenadores,registros y monitores,más parecen fracasar en contestar la pregunta básica: ¿que es realmente? Para apreciar la dificultad de esta pregunta debe considerarse la intrincada colaboración del cuerpo y la mente durante el orgasmo.
Los trabajos de dos investigadores - - Joseph Bohlen,un psicólogo que ahora es residente de Psiquiatría en la Facultad de Medicina de Illinois del Sur y Gorm Wagner,psicólogo de la reproducción del Instituto Pamun de Copenhague,ilustran vividamente esta conexión.
Fisiológicamente,durante el orgasmo se presentan en ambos sexos contracciones musculares y un sentimiento de placer,focalizado en la región genital,que se esparce por todo el cuerpo.
Pero después de varios años de estudiar con monitores este aspecto físico del orgasmo,tanto Bohlen como Wagner han llegado a la inexorable conclusión de que la esencia del orgasmo decididamente no reside en los genitales.
Decidido a establecer los umbrales metabólicos de la actividad sexual,a fin de cuantificar lo que hasta la fecha sólo había sido visto cualitativamente,Bohlen,que entonces era director del Laboratorio de Psicofisiología de la Facultad de Medicina de Minnesota,decidió en 1975 empezar a investigar,utilizando los equipos más nuevos y medir todo lo que fuera medible en la respuesta sexual.
Subvencionado con fondos estatales durante seis años,estudió el orgasmo durante el coito y la masturbación,correlacionando 13 variables fisiológicas con sus percepciones psicológicas,incluyendo las pulsaciones del corazón,la presión de la sangre,el intercambio de oxígeno,las variaciones de flujo sanguíneo,las contracciones musculares,la circunferencia del pene y la conductibilidad de la piel.
Los voluntarios con los que se hicieron estos experimentos - - cada uno participó en varias ocasiones - - activaban los registros apretando un botón cuando sentían llegar el orgasmo y lo volvían a apretar cuando éste finalizaba.
Las mediciones se acompañaban de un interrogatorio y un cuestionario a fin de tener una visión más completa de la experiencia.
Tal y como se esperaba,Bohlen recogió muchos aspectos psicológicos y descripciones de las actividades sexuales de gran utilidad en la investigación y la práctica médica,que constituyen una valiosa ayuda para los médicos a la hora de dar consejos sexuales a los pacientes con,por ejemplo,riesgo cardiovascular.
Uno de sus estudios,publicado en septiembre de 1984 en el Archives of Internal Medicine,demostraba que hay probablemente algo de verdad en las historias que se cuentan de hombres que mueren en circunstancias relacionadas con la actividad sexual.
En una posición especialmente extenuante - - de las cinco estudiadas - -,Bohlen observó que un hombre medio aumenta sus pulsaciones cardíacas a dos tercios de su máximo posible,y consume hasta un cuarto de su reserva de oxígeno.
Pero la conclusión más importante de Bohler fue sorprendente: encontró poca correlación entre la percepción del orgasmo (lo que el sujeto masculino o femenino decía experimentar) y lo que los aparatos de registro demostraban.
Había supuesto,lógicamente,que la duración e intensidad de fenómenos como las contracciones musculares se corresponderían con la intensidad y duración de la experiencia psicológica,y sin embargo se encontró con que no podía establecer dicha conexión.
En otras palabras,no se hallaba en condiciones de medir el placer.
La disparidad existente entre la percepción y la respuesta fisiológica le dejó perplejo,pero era innegable.
Es difícil sostener que un orgasmo es igual a contracciones si las contracciones pueden presentarse antes de que I a persona empiece a percibir el orgasmo y terminar antes de que éste finalice.
También le resultó sorprendente que las contracciones intensas no fueran un indicativo seguro de mayor placer.
Además,un tercio de las mujeres del grupo de Bohien afirmaron que habían experimentado orgasmos,pero sin contracciones.
La mayoría de los hombres entienden que su orgasmo se da simultáneamente con la eyaculación,que a su vez consiste en una respuesta en dos fases: la emisión (las contracciones de los órganos reproductores internos que reúnen el esperma y el liquido prostático que componen el semen eyaculado) y la eyaculación propiamente dicha (las contracciones de la musculatura estriada responsables de la salida del semen por el pene).
" Conocemos la neurología de la erección y de la eyaculación - - afirma Bohlen - - pero no la del orgasmo.
Está claro que no son paralelas ".
Incluso los hombres y las mujeres a los que les han sido amputados los genitales pueden tener orgasmos,lo cual nos indica que la esencia de la cuestión está localizada en otra parte.
Después de casi una década de experimentación fisiológica Bohlen ha llegado a la conclusión de que el orgasmo es una experiencia cerebral.
Al mismo resultado ha llegado su colega Wagner.
" A pesar de que ahora podemos medir,y de hecho lo hacemos,los cambios físicos que tienen lugar durante el orgasmo,la esencia de la cuestión se nos escapa ",afirma.
Según él,el mayor obstáculo en esta búsqueda es que la percepción del orgasmo sólo puede medirse por el mismo sujeto que lo está experimentando.
En sus estudios sobre la respuesta femenina,el 85 por ciento estimaba que sus orgasmos duraban solo la mitad del tiempo durante el cual los aparatos sensores detectaban cambios significativos.
Después de examinar las mediciones de las respuestas fisiológicas,Wagner concluyó que no había una correlación significativa entre la duración del orgasmo y su intensidad,o entre el aporte sanguíneo interno a la vagina durante el orgasmo y la intensidad o el tiempo requerido para provocar el orgasmo.
Wagner está de acuerdo con Bohlen en que tampoco parece haber una relación directa de causa a efecto entre la eyaculación y el orgasmo.
Según Wagner,la cartografía del orgasmo reflejaría las funciones o procesos cerebrales que tienen lugar durante la excitación,así como las conexiones entre los genitales y el sistema nervioso,que permiten que el mensaje de la excitación llegue hasta el cerebro.
Su descripción del orgasmo," un fenómeno introspectivo,un estado autosensitivo de satisfacciones físicas o mentales ",refleja su convicción de que el cerebro juega un fuerte protagonismo en todo este proceso.
Si bien se ha profundizado en la fisiología y el funcionamiento de los genitales durante la excitación sexual (en particular sobre el fenómeno masculino de la erección y la eyaculación) la esencia de la investigación que se sigue actualmente consiste precisamente en averiguar lo que el orgasmo no es.
" Es un error salirse por la tangente - - afirma Virginia Johnson - -.
La respuesta sexual es una experiencia global del cuerpo y la mente.
El orgasmo puede provocarse con una mínima estipulación mental o física.
Puede incluso ser totalmente espiritual.
La respuesta de cada persona es única y no se pueden establecer recetas rígidas fijadas de antemano ".
Es significativo que tanto Wagner como Bohlen,a partir de investigaciones totalmente independientes,hayan llegado a conclusiones tan similares.
Wagner,en su intento de aplicar las reglas básicas de la investigación científica al tratamiento médico de los problemas sexuales,estuvo buscando alguna especie animal que tuviera una vagina similar a la de la mujer,a fin de poder llevar a cabo sus experimentos vasculares,hormonales y farmacológicos.
Una posibilidad parece ser la oveja.
Ha conseguido evocar los mismos reflejos en una oveja anestesiada que los que tienen lugar en la mujer durante el orgasmo,lo cual indicaría que existen mecanismos similares para ambas.
Su último logro es el descubrimiento de polipéptido intestinal vasoactivo (PIV) en la vagina ; es una sustancia química que tiene el poder de aumentar el riego sanguíneo en diversas partes del cuerpo.
En la vagina el PIV produce una secreción (plasma sanguíneo ultrafiltrado) necesaria como lubricante para que la relación sexual sea placentera.
Este polipeptido puede obtenerse sintéticamente,y por lo tanto,el hallazgo de Wagner podría significar un gran paso adelante en el tratamiento de la dispareunia (coito doloroso) y la anorgasmia (falta de orgasmo) femenina.
Mientras que Wagner continúa centrado en la fisiología del sexo,Bohlen prefiere examinar sus aspectos psicológicos.
Sus conclusiones,después de haberse pasado varios años registrando contracciones y tasas metabólicas,se parecen mucho a las de Wagner.
" La esencia del orgasmo está en la percepción - - afirma - -.
Buscar su naturaleza en los cambios fisiológicos es como buscar unas llaves extraviadas debajo de un farol porque es allí donde hay luz ".
Sin embargo,el cerebro es difícil de iluminar.
La comprensión popular de la relación existente entre los genitales y el cerebro podría resumirse con esta máxima italiana: " Cuando el pene se alza hacia el cielo,el cerebro baja hasta el suelo ".
Es difícil obtener registros del cerebro durante el orgasmo (o lo que parece ser el orgasmo) ya que dura muy poco.
Además estos experimentos requieren de técnicas invasivas que llegan hasta las profundidades del encéfalo y que en muy raras ocasiones podrán realizarse en personas.
La neurología del orgasmo requiere una elaborada colaboración entre los genitales,la médula y el cerebro (especialmente el sistema límbico - - también conocido como centro del placer - -,localizado en el cerebro).
Este conjunto de estructuras curvas ubicado en el mesencéfalo regula las emociones y las reacciones primitivas como el sexo y la agresividad.
Robert Heath,un neurólogo de la facultad de Medicina de Tulane,llegó a provocar un orgasmo en uno de sus pacientes incurables ; en el transcurso del tratamiento estimuló eléctricamente un punto de su cerebro y obtuvo un registro cerebral mediante diminutos electrodos.
Descubrió también que aplicando una descarga eléctrica en un punto localizado en la región septal de la corteza límbica,conseguía un efecto equivalente al experimentado por la totalidad del cerebro durante un ataque epiléptico.
A primera vista,resulta asombroso que algo tan aparentemente trivial como la relación entre la longitud de una circunferencia y la de su diámetro haya preocupado al hombre durante siglos y siga acaparando la atención de matemáticos y legos.
Seguramente se podrían buscar interesantes explicaciones psicológicas a la fascinación de ir (por ejemplo,en relación con la simbología del círculo demediado - - es decir,dividido en dos mitades - - por su diámetro) ; pero las consideraciones puramente matemáticas son más que suficientes para justificar el milenario interés (al menos el científico) despertado por este número.
Ya para el mundo prehelénico,la razón entre la circunferencia y su diámetro tuvo una gran importancia práctica,puesto que en todo tipo de construcciones es necesario efectuar cálculos relativos a círculos y arcos.
Enrollando una cuerda alrededor de cualquier objeto circular,es fácil ver que el trozo necesario para darle la vuelta completa es unas tres veces mayor que su diámetro.
Y precisamente así debió de ser como hallaron las primeras aproximaciones de pi los calculadores primitivos.
En la Biblia,al hablar del Templo de Salomón,se describe un recipiente circular que " tenía diez codos de un borde al otro... y un cordón de treinta codos de largo lo ceñía alrededor ".
(I) Pero la aproximación pi = 3 es demasiado burda para cualquier obra que requiera un mínimo de precisión,y ya los arquitectos fenicios y egipcios manejaban el valor 22 / 7. Esta fracción se acerca tanto a pi que sólo supone un error del orden del 0,04 70,y era plenamente satisfactoria,a efectos prácticos,para los arquitectos e ingenieros de la antigüedad.
(2) Pero los matemáticos griegos,como es natural,no se conformaron con un valor aproximado obtenido experimentalmente,y el cálculo del valor exacto de pi fue de los principales problemas geométricos que se plantearon.
En el siglo iii a. de C.,Arquímedes,utilizando su " método de exhaución ",calculo pi con un error inferior al 0,001 %,aproximación que no fue superada hasta el siglo XVI.
Se ha dicho a menudo,y con razón,que si Arquímedes hubiera tenido a su disposición los números arábigos que ahora utilizamos,habría inventado el Cálculo Integral dos mil años antes que Newton ; de hecho,la falta de un aparato matemático adecuado le impidió generalizar dicho cálculo,pero no utilizarlo para resolver problemas concretos.
Así,Arquímedes dedujo las fórmulas de las áreas y volúmenes de numerosas figuras geométricas mediante el sutil artificio de dividirlas en franjas " todo lo pequeñas que se desee ",y con un criterio similar abordó el cálculo de pi.
Arquímedes razonó que si inscribía en un círculo polígonos de un número de lados cada vez mayor,sus perímetros se irían aproximando cada vez más,tanto como se quisiera,a la longitud de la circunferencia ; el espacio entre el polígono y la circunferencia,a medida que creciera el número de lados de aquél,se iría agotando (de ahí el nombre de método de exhaución) y en el límite sería cero.
Lo mismo se puede hacer con una secuencia de polígonos circunscriptos,en vez de inscriptos,sólo que en este caso la aproximación será por exceso en lugar de por defecto.
En cualquiera de los dos casos podemos calcular la longitud de la circunferencia (o directamente pi,si partimos de un circulo de diámetro igual a uno) con el grado de aproximación que queramos,siempre que utilicemos un polígono con el suficiente número de lados.
Arquímedes imaginó una circunferencia de diámetro igual a uno atrapada entre dos polígonos regulares de 96 lados,uno inscripto y otro circunscripto: es evidente que por ambos lados los polígonos se aproximan bastante a la circunferencia,permitiendo acotar su medida.
Tras los engorrosos cálculos pertinentes concluyó que Pi es algo mayor que 223 / 71 (3,140845) y algo menor que 22 / 7 (3,1428571).
Sacando la media de ambas fracciones,halló el valor 3,1418,lo que supone un error de menos del 0,007 70.
La estimación de Arquímedes no se mejoró hasta el siglo XVI,cuando se empezó a utilizar la fracción 355 / 113,que es una excelente aproximación y sin duda la mejor expresable mediante una fracción sencilla,ya que 355 / 113 = 3,14159292...,que sólo excede del valor real de Pi (3,14159265...) en menos de tres diezmillonésimas,lo que supone un error despreciable,del orden del 0,000008.
En esa misma época el matemático francés François Viete (1540 - 1603),más conocido como Vieta y justamente considerado el padre del Algebra,halló la primera serie infinita de fracciones que converge hacia pi.
Este método es el equivalente algebraico del de Arquímedes: en lugar de un polígono con un número de lados tan grande como queramos,tenemos una serie de fracciones con un número de miembros tan grande como se desee ; cuantos más miembros tomemos,mayor será la aproximación.
(El mismo Vieta,en 1593,utilizó su serie para hallar pi con diecisiete decimales: 3,14159265358979323).
Este método supone el paso definitivo en el cálculo del valor de pi,y es que,de hecho,se sigue utilizando actualmente.
La única dificultad para hallar cada vez más decimales de pi consiste en el tiempo de cálculo necesario.
Hay numerosas series que convergen hacia infinito o hacia 0,aunque no todas con la misma rapidez.
La más sencilla y conocida es la que halló Leibniz en 1673: Sin embargo,esta elegante serie no es de convergencia muy rápida ; simplemente para mejorar la aproximación 355 / 113 hay que emplear muchas horas de engorroso cálculo.
(Ver recuadro " ALGO contra Pi ") En un principio el cálculo de un número cada vez mayor de decimales de ¡r estuvo impulsado,en gran medida,por la posibilidad de hallar en ellos algún signo de periodicidad.
En 1615,el alemán Ludolf von Ceulen calculó pi con 35 decimales (razón por la que en los textos alemanes se alude a veces a pi como " el número de Ludolf "),sin hallar en ellos ningún indicio de repetición periódica.
Cien años después,en 1717,el matemático Sharp dobló el récord de Von Ceulen al calcular el valor de pi con 72 decimales,sin encontrar tampoco signo alguno de periodicidad.
Por fin,en 1761,Johann Lambert demostró que pi es un número irracional,es decir,que no es igual a ninguna posible fracción,con lo que los esforzados buscadores de períodos cejaron en su fútil empeño ; sin embargo,el cálculo de pi con un número cada vez mayor de decimales no decayó.
El calculador humano que más lejos llegó en esta tarea fue William Shanks,que en 1873 publicó el valor de pi con 707 decimales.
Shanks utilizó una serie de convergencia rápida descubierta por Machin en 1706 (y que sigue siendo una de las más eficaces conocidas).
Esa serie es,a su vez,suma de otras dos series: Tras quince años de cálculos manuales,Shanks alcanzó su resultado,batiendo todos los récords (a pesar de que se equivocó en la cifra 528,lo que afectó a todos los decimales de allí en adelante).
Poco después,en 1882,su colega Lindermann demostró que pi,además de irracional,es trascendente,es decir,no es solución de ninguna ecuación concebible (dicho de otra forma,no puede obtenerse a partir de los enteros mediante un número finito de operaciones algebraicas).
Esto,entre otras cosas,significa que la cuadratura del círculo (construcción con regla y compás de un cuadrado de área igual a la de un circulo dado) es imposible.
El siguiente paso lo darían los ordenadores.
En 1949,la computadora ENIAC,trabajando durante unas 70 horas con la serie de Machin,obtuvo el valor de pi con 2.037 decimales (y descubrió,de paso,el error de Shanks).
Tal vez Shanks y otros calculadores posteriores a la demostración de la irracionalidad de pi (y que por tanto ya no tenían la excusa de estar buscando signos de periodicidad) sólo pretendieron batir un récord,o tal vez estuvieran animados por algún impulso místico y esperaran hallar en los decimales de pi una insospechada revelación.
Este tipo de calculadores insaciables han sido llamados alguna vez,peyorativamente," cazadores de dígitos ",y se ha relacionado su obsesiva actividad más con la Cábala o el coleccionismo que con la ciencia.
Hace poco más de un año,Europa aceptó el reto.
Fue en París,aquella tarde del 17 de abril de 1985,cuando Europa tomó su gran decisión.
Treinta y cuatro ministros,representantes de diecisiete países europeos (los doce de la Comunidad Europea y también los de Austria,Finlandia,Noruega,Suecia y Suiza),manifestaron su acuerdo - - con mayor o menor entusiasmo - - con vistas a independizarse tecnológicamente de EE. UU y del Japón.
Y es que precisamente aquel día Eureka - - el gran proyecto europeo - - dejaba de ser un mero plan,que reunía pliegos y pliegos de buenas intenciones,animado sobre todo por Francia,para convertirse en un programa de trabajo serio y coordinado.
Y no es por inmodestia,pero los expertos europeos no pecan de arrogancia cuando aseguran que Europa,ahora mismo,dispone de los cerebros más exquisitos y de las empresas más en punta para desarrollar y hacer rentables los logros más sonados en una gran variedad de campos de investigación.
Sin embargo,la falta de conexión y de intercambio de información entre las distintas líneas de investigación - - que ahora se están llevando a cabo en forma dispersa - -,la pluralidad de intereses nacionales,las dificultades en la introducción de muchos nuevos productos en los propios mercados europeos - - debido sobre todo a su falta de homologación internacional - -,la gran disparidad de idiomas y la infinidad de trabas aduaneras además de otros factores,lleva a que en la práctica los grandes avances tecnológicos europeos lleguen siempre con un par de años de retraso con respecto a los estadounidenses o a los japoneses.
Y,para colmo,resultan más caros.
De nada sirve dominar la teoría cuando falla estrepitosamente la producción y la comercialización.
Pero ahí está Eureka,un ambicioso programa para salir de esta loca espiral que hundiría a Europa más y más en su dependencia tecnológica con respecto a las dos grandes potencias industriales del mundo occidental.
Basándose en la cooperación industrial y científica,en la complementariedad de las investigaciones básicas y en el establecimiento de una serie de objetivos prioritarios que permitan concentrar esfuerzos conjuntos,el Viejo Continente - - que se sabe herido y postergado en cuestiones tecnológicas de punta - - lucha y se resiste a ser simplemente una colonia comercial que deba pagar en dólares o en yens el ritmo que marca el progreso.
La filosofía que emana del programa Eureka es bien simple,pues se trata de aunar objetivos que faciliten la concreción de esfuerzos dispares y la disposición de recursos financieros suficientes para presentar al mercado,en apenas unos años,productos tecnológicos,necesarios y urgentes,para la industria,la telecomunicación,la medicina y la salud ambiental.
Y no se puede decir que en un solo año se haya avanzado poco.
Desde aquel momento en que el presidente estadounidense Ronald Reagan reclamara a Europa su colaboración en el programa norteamericano de la guerra de las galaxias,denominado oficialmente Iniciativa de Defensa Estratégica (SDI en inglés),las posturas de algunos países europeos se han matizado,las del Reino Unido y Alemania.
Pero aquella reunión de París,hace ahora un año tan sólo,cambió realmente las cosas.
La RFA aceptó el proyecto Eureka y lo consideró casi como una iniciativa franco-alemana,el Reino Unido llevó a la reunión un completo borrador para su puesta en marcha,y Francia,fiel a sus principios,dotó al proyecto con la nada despreciable suma de mil millones de francos.
El resto de los países europeos manifestaron su aprobación o matizaron posibles detalles sobre la financiación,la organización y la conveniencia de toda una serie de proyectos que distintas empresas europeas ya habían acordado presentar a la cumbre ministerial en fechas anteriores a la de la reunión.
Y naturalmente que hubo sus más y sus menos,pero a las 3 de la mañana del día 18 de julio de 1985 los representantes de los 17 países de la Europa Occidental estampaban su firma al final de un documento de nueve puntos que se había redactado una y otra vez para que todo el mundo estuviera de acuerdo y nadie se sintiera molesto por su contenido.
Diecisiete países europeos,así como los representantes de las Comunidades Europeas,se han reunido en París para sentar las bases de la tecnología europea,reconocen la importancia y la urgencia para que Europa haga acopio de sus propias energías y reúna sus competencias en el campo de las altas tecnologías en un frente común.
Todas las delegaciones ministeriales,además,expresan unánimemente su firme apoyo a Eureka,que consideran un programa que debe versar sobre una selección de proyectos civiles escogidos entre los más diversos campos de la tecnología.
Sin embargo,en el texto final de la reunión,básicamente por falta de acuerdo unánime,no se señalan los grandes campos de acción que debe tener Eureka,pero en la mente de todos están bien presentes las cinco grandes áreas de actuación prioritaria,apuntadas por Francia y en las que se deben concentrar inversiones y esfuerzos para no quedar aún más descolgados del avance tecnológico se circunscriben en los programas de Euromática (superordenadores,ordenadores numéricos de gran velocidad,mejora del diálogo entre hombre y máquina e inteligencia artificial...) ; Eurobot (robots de tercera generación para su uso en seguridad civil o agrícola,fábricas automatizadas,aplicaciones avanzadas del rayo láser,etc) ; Eurobio (investigación sobre semillas artificiales obtenidas mediante Ingeniería Genética y técnicas de clonación,sistemas de control y de diagnóstico de aplicación médica,etc.) ; Eurocom (telemática en general,o sea,redes informáticas europeas comunes),y Euromat (fabricación de una turbina industrial de gran rendimiento,investigaciones sobre nuevos materiales,etc.).
Además,otros proyectos fueron añadidos posteriormente como la construcción de un tren europeo de gran velocidad (aprovechando la gran experiencia alemana en trenes de levitación magnética),los sistemas que permitan transformar la emisión de luz de los fotones en electricidad y viceversa (Optrónica) y el hallazgo de chips ultrarrápidos en términos de microelectrónica,entre otros.
Y así,aunque la República Federal de Alemania y el Reino Unido hayan manifestado repetidamente que la colaboración y la coordinación tecnológicas europeas no excluyen que en algunos casos éstos y otros países del Viejo Continente puedan colaborar con las propuestas del programa norteamericano de Guerra de las Galaxias,los Estados Unidos parece ser que han interpretado el acuerdo europeo como un desaire hacia sus propios intereses.
Mientras que el programa norteamericano exhibe una clara orientación hacia el dominio militar en el espacio,Eureka se ha definido,ya desde el principio,como una estrategia conjunta de los países europeos para vencer sus vicios tecnológicos nacionales y hacer rentables los hallazgos que periódicamente surgen de los distintos laboratorios de investigación.
Ya en aquel comunicado final de la reunión de París se remarcaba que: " Eureka versará sobre una selección de proyectos civiles escogidos entre los diversos campos de la alta tecnología ".
Semanas más tarde altos cargos del gobierno estadounidense recriminaban a Europa que no cooperaba como debía en la causa común de la defensa conjunta ya fuera en el plano económico,político o en el estratégico.
Pero las críticas han venido tanto de fuera como de dentro de casa.
En la prestigiosa revista científica británica Nature (6 de Junio de 1985) se expresó la idea de que proyecto Eureka era una clara muestra del chauvinismo europeo,un derroche económico y una prueba a la tozudez de François Mitterrand.
Por su parte L'Humanité,periódico francés de tendencia comunista,calificó en su día el proyecto Eureka como la versión regional de la Guerra de las Galaxias estadounidense y algunos sindicatos o colectivos de profesionales se han opuesto también por tacharlo de " armamentista " y " contrario a la distensión ".
Y es que la frontera que separa las aplicaciones tecnológicas civiles de las militares,en cuestiones de alta investigación,es más delgada que un papel de fumar.
Nadie se atreve a dudar de que un superordenador ultrarrápido interesa enormemente a los sistemas de defensa para,por ejemplo,hacer frente y contrarrestar un posible ataque enemigo con docenas e incluso con centenares de misiles simultáneos o para determinar la explotación más ventajosa de un campo petrolífero o bien para someter a un nuevo avión a multitud de ensayos teóricos y prácticos.
De hecho,mirándolo dentro de un uniforme,cualquier avance significativo en el campo de la tecnología civil puede tener una docena de aplicaciones en el campo militar y,a menudo,claro está,sucede al revés,que las investigaciones militares aportan avances sustanciales a la industria civil.
Y de la veraniega reunión constituyente de París se llega a la otoñal de Hannover,el 15 de Noviembre de 1985,donde 18 estados europeos (a los 17 originales se sumó también Turquía) llegan al acuerdo político de impulsar el programa Eureka,de financiar los proyectos con ayudas públicas si fuera preciso (también la RFA y el Reino Unido estuvieron de acuerdo con este punto),de crear una mínima estructura de coordinación y de establecer un mecanismo básico de funcionamiento y de puesta en marcha inmediata.
En vistas a intentar recuperar la ventaja tecnológica impuesta por los EE. UU y el Japón,los proyectos de Eureka deben centrarse en aquellas líneas de investigación y en el logro de aquellos productos cuya aplicación industrial sea inmediata.
Además,estos proyectos de tecnología avanzada de carácter civil - - se vuelve a insistir en este punto - - deben ser fruto de la iniciativa privada.
En este mismo momento,el doctor Lars Olson,prestigioso neurocirujano sueco del acreditado Instituto Karolinska de Estocolmo,quizás esté realizando su quinta operación de implante cerebral en un paciente seriamente aquejado por la enfermedad de Parkinson,una grave dolencia que afecta a las personas de edad.
Sus cuatro anteriores intervenciones,realizadas entre 1982 y 1985 abrieron una puerta a la esperanza en aquellos enfermos sacudidos por esta temible dolencia y han sido los primeros casos en que un ser humano ha recibido un injerto tisular en su propio cerebro.
Durante el curso de este año,el doctor Olson tratará dos nuevos pacientes con su revolucionaria técnica,que consiste en injertar en el cerebro una pequeña parte del tejido de la glándula suprarrenal,cuya característica principal es la de producir dopamina,una sustancia que en el riñón actúa como precursora en las biosíntesis de la noradrenalina y la adrenalina,las dos hormonas que ponen a tono el organismo ante una situación de stress o de emergencia.
Sin embargo,al mismo tiempo,la dopamina es una sustancia transmisora del impulso nervioso,sintetizada también en una zona determinada del cerebro,concretamente en una estructura llamada sustancia negra,que en los pacientes aquejados por la enfermedad de Parkinson se encuentra gravemente dañada.
Además,en estas dos nuevas intervenciones,el equipo del doctor Olson ensayará algunas de las posibles sustancias que incitan a las terminaciones nerviosas a crecer y a prodigar sus interconexiones.
Los trabajos actuales de Olson,así como los de algunos otros neurofisiólogos y neurocirujanos,principalmente suecos,echan por tierra la creencia prácticamente axiomática que hasta hace pocos años preconizaban los manuales de Medicina.
De hecho,hasta finales de la década de los años 60 se creía que las neuronas - - las células especializadas del tejido nervioso - - eran incapaces de crecer en los adultos ; además,se aseguraba que cada individuo disponía de un número fijo e inmutable de neuronas,con unas características ya determinadas en el momento de su nacimiento.
Ahora,con la suma de los nuevos avances conseguidos en el campo de la Neurología,esta cuestión ha dado un vuelco espectacular y aunque por el momento se desconoce aún cómo inducir en vivo la posible reproducción neuronal,se barajan ya un montón de estudios que confirman que,en determinadas circunstancias,estas células tan especializadas funcionalmente son capaces de crecer y de generar brotes,con un trayecto muy definido y en zonas muy concretas,con el fin de suplir en cierta medida algunas lesiones localizadas del tejido cerebral que,por la razón que sea,ha dejado de ser eficaz o que,en el peor de los casos,ha sufrido un proceso de degeneración o de necrosis.
Y en estos días en que ya no despiertan un interés especial los trasplantes del riñón,de hígado,de corazón,de páncreas o de pulmones e incluso la combinación simultánea de algunos de ellos o la de dos o más en un período de tiempo muy corto,el trasplante de cerebro,o más propiamente la implantación cerebral de tejido de características nerviosas se toma como una práctica propia del diablo o entresacada de alguna novela de ciencia-ficción.
Sin embargo,aunque vacilantes y dudando aún en los conceptos más rudimentarios,los trabajos de trasplantes tisulares cerebrales están ahí,pueden ser una práctica bastante usual en un próximo futuro y constituyen una esperanza para solucionar algunas lesiones irreversibles del cerebro.
Así,para Don Gash,neurólogo de la Universidad de Rochester (EE. UU.) y pionero en este campo," los implantes de cerebro humanos serán una técnica experimental habitual dentro de unos cinco años " ; otros científicos son algo más precavidos,como Richard Wyatt,jefe del departamento de Neuropsiquiatría del Instituto Nacional de la Salud Mental estadounidense,en Bethesda (Maryland),quien cree que " necesitamos aún algunos conocimientos básicos para movernos libremente en este campo.
Estos conocimientos podemos tenerlos dentro de cinco años,pero posiblemente tardaremos algunos más ".
Finalmente,otros científicos son de la opinión que solamente siguiendo adelante con las experiencias previstas para esta década,se conseguirá algún día saber lo necesario para que los implantes de tejido nervioso sean un éxito no ya sólo en pacientes de la enfermedad de Parkinson sino también en aquellos aquejados por la demencia presenil o enfermedad de Alzheimer,la corea de Huntington (baile de San Vito),algunas esquizofrenias,estados epilépticos y trastornos emocionales,ciertas enfermedades hereditarias o en los casos de accidente en que se hayan sufrido graves lesiones del sistema nervioso central.
Y es que,de hecho,la idea de trasplantar tejidos cerebrales viene ya de muchos años atrás.
Así,al inicio de este siglo,concretamente en 1903,la investigadora Elizabeth Dunn injertó tejido cerebral en ratas ante la incredulidad de sus colegas que mantenían que aquello era imposible.
En los años 60,el fisiólogo británico Geoffrey Raisman,de la Universidad de Cambridge,fue el primero en señalar que,cuando se daña un área de cerebro,las neuronas vecinas emiten prolongaciones (brotes),que crecen en la zona lesionada,al mismo tiempo que asumen algunas de las funciones perdidas.
Otras evidencias de que el cerebro puede crecer y autorrepararse en algún momento de la vida adulta de un organismo fueron comunicadas por Fernando Nottebohm,quien observó que la corteza cerebral (la zona más superficial del cerebro) de algunas aves crecía y cambiaba todos los años cuando el pájaro aprendía nuevos trinos.
También es interesante mencionar los estudios de Marian Diamond,de la Universidad de California,que estudió la prolongación y las nuevas conexiones que establecían los axones de las células nerviosas de las ratas adultas cuando se sometía el animal a ciertos estímulos ambientales.
En esta línea de estudios vanguardistas sobre cirugía cerebral se acaban de cumplir ya 23 años de los espeluznantes trabajos del doctor Robert White y de su equipo.
Corre media mañana del 17 de Enero de 1963 cuando en un quirófano del cuarto piso del Hospital General Metropolitano de Cleveland,en Ohio,el doctor White extrae el cerebro del cráneo de un simio después de largas horas de paciente microcirugía.
Luego,lo acopla inmediatamente a una panoplia de aparatos que cumplen las funciones básicas de los órganos del animal y lo mantendrán en un estado de vida suspendida.
En las pantallas de los monitores de control se observa que el cerebro aislado conserva unos valores normales de consumo de oxígeno y de glucosa.
El electroencefalograma (EEG) muestra también unos valores aparentemente normales.
Por primera vez en la historia de la Biología,un cerebro aislado se mantiene clínicamente estable,vivo fuera del cuerpo.
Dos años más tarde,en 1965,El equipo del doctor White vivisecciona pacientemente la cabeza completa de un simio,mantiene el cerebro vivo dentro de la caja ósea craneal y lo trasplanta en el cuello de otro nono tras un trabajo prodigioso de microcirugía vascular.
El cerebro huésped es irrigado inmediatamente con el aporte sanguíneo del organismo receptor.
El EEG del cerebro trasplantado es normal.
El receptor se mantiene con vida durante 24 horas con dos cerebros,realizando cada uno de ellos una vida independiente.
En 1971,Robert White sigue adelante con sus trabajos sobre la conservación y la supervivencia cerebrales.
En esta ocasión,decapitar un mono y trasplanta su cráneo al cuerpo también decapitado de otro simio.
El cerebro trasplantado supera bien el período postanestésico.
Los ojos siguen la luz,se cierran y se abren.
La boca puede morder.
El animal oye.
El mono con la cabeza prestada come y bebe.
El EEG es normal.
Sobrevive 96 horas con la cabeza completa de un congénere.
Los trabajos del doctor White,aunque experimentales y no privados de un cierto sensacionalismo,de hecho han dado un gran empuje a la ingeniería bioquirúrgica.
Sus experiencias han desarrollado la técnica y el aparataje básicos que han permitido estudiar in situ cierto tipo de tumores cerebrales y algunos procesos neoplásicos y meningíticos ; además,ha demostrado que un cerebro aislado de su cuerpo puede sobrevivir más allá de los tres minutos fatídicos si se mantiene a una temperatura de 10 C,ya que entonces la masa cerebral ve disminuidas sus necesidades de oxígeno y de glucosa.
En el mar se cultivan decenas de especies animales (peces,crustáceos,moluscos) y también algas.
Esta agricultura y ganadería marinas han existido desde hace siglos de una manera más o menos rudimentaria,mediante corrales sumergidos.
Actualmente esta acuicultura ha evolucionado de una manera espectacular y especies tales como las algas,mejillones u ostras suponen cantidades importantes en el total de las capturas de ciertos países (por ejemplo el mejillón en España y las algas en China).
En muchos países existe la necesidad de obtener fuentes adicionales o alternativas de pesca,lo que ha originado la creación de numerosos laboratorios científicos para la puesta a punto de técnicas que permitan el cultivo intensivo y extensivo de especies de alto interés comercial o nutritivo.
No obstante,se olvida a menudo la propia naturaleza de la Naturaleza y valga la redundancia.
La acuicultura por un lado y la agricultura o ganadería terrestres del otro,son perfectamente comparables,ya que lo único que cambia es el sustrato donde se desarrollan.
Muchos estudios actuales se han dirigido hacia especies de gran valor comercial (lubina,lenguado,sepia,etc.) olvidando que se trata de especies que se encuentran en niveles tróficos (de alimentación) elevados.
Por ejemplo,la lubina cuando alcanza la talla comercial se alimenta básicamente de otros peces,lo que equivaldría económicamente a criar tigres para comérselos en un ecosistema terrestre.
Hasta la fecha,el cultivo de tigres no se puede decir que haya tenido mucho éxito... Uno de los problemas con que cuenta la acuicultura en la actualidad es,precisamente,la necesidad de cultivar especies caras y escasas,lo que podría dar rentabilidad al cultivo (caso de la lubina).
Sin embargo,biológicamente suele tratarse de especies de crecimiento lento,típicamente predadoras y que necesitan,por tanto,una fuerte inversión en alimentos,encareciendo el producto hasta límites no rentables.
Se está experimentando en muchos laboratorios con piensos especiales,pero la tecnología está aún poco desarrollada y presenta problemas tanto en la fase adulto como en la larvaria.
Con todo ello,y sin negar que las investigaciones puedan lograr que sea rentable la cría de algunas de estas especies - - más como negocio aislado que como fuente masiva de alimentos - - las especies hacia donde habría que dirigir el mayor esfuerzo deberían ser aquellas de crecimiento rápido y bajo costo alimentario: productores primarios (algas),filtradores (mejillón,ostra) o de niveles tróficos primarios o poco selectivos (lisas).
En España,el cultivo del mejillón se realiza en bateas,que son plataformas situadas en la superficie de las rías,de las cuales cuelgan unas cuerdas que sirven de sustrato a los mejillones.
El rendimiento medio por año es de unas 60 toneladas por batea en las rías gallegas,siendo inferior en otras zonas.
Por ejemplo en el puerto de Barcelona,un parque de cultivo bien cuidado y conservado podría producir hasta 25 toneladas por año.
La producción mundial de mejillones en 1981 fue de algo más de 650 mil Tm,correspondiendo 387 mil al mejillón europeo.
En este continente los países de mayor producción son España (93 mil Tm),Holanda (109 mil),Dinamarca (79 mil) y Francia (80 mil).
Conviene señalar que la producción española está subestimada en las estadísticas y seguramente supera ampliamente las cien mil toneladas.
El área gallega se encuentra en condiciones inmejorables para el cultivo de mejillón,dadas las características de la zona.
Según estudios realizados,la producción de mejillones desconchados por hectárea,se sitúa entre 23 mil y 30 mil Kg. Por lo tanto,el rendimiento de estos cultivos sería unas 70 veces mayor que cualquier otra explotación ganadera que no reciba alimentación supletoria.
Estos rendimientos son,asimismo,muy superiores a los de otros países.
Este tipo de cultivos no está exento de problemas.
Su incidencia en el ecosistema es bastante negativo ya que afecta a otras especies bentónicas.
Los mejillones de una batea filtran alrededor de 160 mil metros cúbicos de agua por día,reteniendo en este período de tiempo media tonelada de partículas de materia orgánica,lo que representa 180 toneladas por año,y que traducido en materias fecales desprendidas suponen unas 100 Tm por aro y batea.
Estas cifras tan importantes alteran de una manera grave la fauna bentónica circundante,acabando con las poblaciones naturales.
La naturaleza filtradora del mejillón plantea,además,algunos problemas de consumo cuando se producen mareas rojas o las aguas están contaminadas.
La militoxina,cuyos efectos son similares a los del curare,se acumula en el mejillón cuando ingiere grandes cantidades de determinadas especies de fitoplancton propias de las mareas rojas.
Las algas y su cultivo son poco conocidas en España,a pesar de que en determinados puntos del litoral se recolectan y aprovechan industrialmente.
El cultivo de algas existía ya en Japón,en forma más o menos rudimentaria,siglos atrás,aunque su máximo desarrollo se sitúa a partir de la posguerra,con un máximo en los últimos anos.
Corea triplicó su producción en pocos años,alcanzando 195 mil Tm en 1978.
En Filipinas el cultivo de una especie de alga roja del género Euchema,pasó de unas 60 a 150 Tm anuales en la década de los 60,a 34 mil Tm en 1978,tras poner en práctica un vasto plan de acuicultura.
Asimismo,Chile pasó de 1.500 Tm en 1969 a más de 10 mil en 1979.
No obstante,el país que ha experimentado un mayor desarrollo en este tipo de cultivos ha sido China,siendo actualmente el mayor productor del mundo (1.650.
000 Tm en 1979).
La especie más explotada es la Laminaria japónica que hasta los años 20 era importada de Japón,para ser posteriormente introducida desde aquel país,extendiéndose por el litoral norteño.
En los anos 50 se experimentaron nuevas formas de cultivo mediante balsas flotantes.
Dado el interés económico que para China suponía este cultivo,se crearon amplios programas de investigación (en 1949 contaban con 10 expertos y en 1963 pasaban de mil).
Aunque es indudable el valor que actualmente tienen los cultivos marinos,su futuro podría buscarse en el enriquecimiento de áreas naturales.
De la misma forma que el abonado de los campos supone un incremento notable en la producción agrícola,la formación de afloramientos artificiales,como ya han sugerido algunos científicos,podrían aumentar la producción pesquera de una manera ostensible.
Estos afloramientos podrían provocarse descargando las aguas residuales,depuradas parcial o totalmente,en zonas determinadas de la plataforma Tales aguas,menos densas que las circundantes,ascenderían arrastrando nutrientes que elevarían la producción primaria.
Esta y otras soluciones similares permitirían al hombre incrementar la productividad de muchas áreas y con resultados probable mente mucho más satisfactorios que cierto tipo de acuicultura.
Constituyen un grupo étnico de cabellos lacios y piel cobriza,que viven actualmente como nómadas en las inmediaciones de los ríos Senegal y Niger.
Son los peúles,que apacientan en esos parajes sus rebaños de ganado bovino En líneas generales,su modo de vida es comparable al de los pastores neolíticos del Tassili.
Es evidente que sólo con estos datos no será posible establecer una descendencia directa.
Es más: a nadie se le había ocurrido plantear esa relación hasta 1957,cuando el arqueologo Lhote presentó en una exposición las copias de las pinturas del tassili realizadas por él y su equipo.
Al ver esos grabados por primera vez,Amadou Hampaté Ba,un estudioso peul,no dudó en descifrarlos y atribuirlos precisamente al pueblo peul.
Se trataría,según él,de imágenes iniciáticas peculiares de los peúles que perduraron mucho tiempo después de la adopción del islamismo.
A su juicio,los datos gráficos eran inconfundibles,y todos ellos referentes a la relación con los rebaños: disposición de los cuernos y las manchas colores,objetos propios de los pastores peúles,sacrificios de animales,manera de ordeñar la leche Todo ello fue expuesto años más tarde en el Journal de la Société des Africanistes,en un artículo titulado " Los frescos de la época bovidiana del Tassili y las Iradiciones de los peúles: hipótesis de interpretación " Aunque no han sido confirmadas las hipótesis de A H Ba,el numero y precisión de las similitudes señaladas,son suficientemente atractivos Como para ahondar lodo lo posible en la investigación.
En el Tassili,como ya advertimos,se encuentra acaso el conjunto más extenso y variado del arte rupestre sahariano,y,sin duda,una de las más ricas colecciones de esta clase en el mundo,distribuida en cientos de espacios al abrigo de las rocas o al aire libre.
Las muestras más primitivas datan de unos 10.000 años atrás,y constituyen un testimonio extraordinario de la vida de nuestros antepasados prehistóricos.
Pues si en el arte paleolítico sólo se representaron animales,en las pinturas del Tassili también vemos gente,familias,escenas domésticas.
Por otra parte,si los ejemplo europeos de Altamira o Lascaux son más antiguos - - su datación se remonta a unos 13.000 años a. C. - -,su número es reducido: no más de cien en Francia y en España,contra los centenares que alberga sólo el Tassili.
Existen similitudes entre las pinturas rupestres europeas y africanas,pero ello probablemente sólo significa que la vida prepastoril o pastoril fue similar en todas partes.
No hay evidencia alguna,hasta ahora,que permita relacionar el arte prehistórico en Europa con el de Africa.
Aún se discute sobre los orígenes de los primeros habitantes del Tassili,pero se acepta que provenían del Mediterráneo meridional y que fueron blancos.
Los primeros vestigios de asentamientos neolíticos en el Sahara fueron observados por los arqueólogos,hace un siglo,en Argelia.
A medida que las tropas francesas avanzaban conquistando el Sur de Argelia,era cada vez mayor el número de grabados en la roca que les salían al paso.
En 1933,el teniente Brenans,de la Legión Extranjera,recorrió en camello el Wadi Djerat,donde fue el primero en advertir los grabados del Hoggar (el Wadi cruza desde el Hoggar el límite norte del Tassili).
Hubo a partir de entonces uno que otro intento de explorar el Tassili.
Hasta que,en 1954,un especialista francés en el Sahara,Henry Lhote,organizo una expedición completa.
A los largo de 16 meses,Lhote y sus asistentes encontraron,y copiaron con esmero,vanos miles de pinturas que,desde entonces,salieron a la luz pública.
" He aquí un descubrimiento de extrema importancia ",afirma todavía Lhote,que,a sus ochenta años,en París,y a través del Museo del Hombre,sigue haciendo todo lo posible para atraer la atención hacia sus hallazgos.
Es indudable que son muchas las pinturas y grabados en la roca que aún no han sido descubiertos en el Tassili y en el Hoggar,en la misma medida en que el Sahara mismo aún no ha sido explorado por completo.
Mientras tanto,sigue faltando un exhaustivo trabajo arqueológico,pese a que se trata de " una de las colecciones más importantes de arte prehistórico en el mundo ",como afirma Ray Inskeep,conservador del Pitt-Rivers Museum del departamento de Etnología y Prehistoria,en Oxford.
Nancy Sandars,autora de una obra tan significativa como Arte prehistórico en Europa,conviene en que " por su significado artístico y social,las pinturas del Tassili deberían ser mucho mejor conocidas ".
Una de las razones de esta carencia es la misma dificultad del terreno ; otra,la falta de fondos,y tal vez,en último término,un escaso interés por la Arqueología en Argelia.
Pero,a pesar de todo,nuevas expediciones se suceden año tras año.
A primera vista,la aridez actual del Tassili no sugiere que allí haya existido nunca una comunidad pastoril.
Pero en realidad,como indicamos antes,el Sahara fue una región templada y bien provista de agua durante el último período glacial.
Luego,a medida que en el Norte fue retrocediendo el hielo,y con él la humedad de la tierra,se produjeron dramáticas alteraciones del clima en esta parte de Africa.
Si hubo aquí asentamientos mesolíticos,de ellos no quedan rastros.
Pero hacia el 10000 a. C.,un pueblo de pastores floreció en una comarca que debió parecerse bastante a lo que es hoy el Far West americano.
Esta fértil región se degradó a impresionante velocidad.
El suelo,rápidamente erosionado,fue barrido cada vez más por los incesantes vientos de la montaña,Y puede que también resultase empobrecido por un exceso de pastoreo.
Aproximadamente hacia el año 1000 a. C. la desolación era total,y hubo que buscar nuevos pastos para el ganado en el interior del Africa negra.
El oasis de Djanet,al pie de la meseta del Tassili,fue la base de la expedición organizada por Lhote,y sigue siendo hoy el único punto de partida practicable para acercarse a las pinturas.
Una pequeña avioneta que hace la línea de los oasis aterriza peligrosamente cada día en una estrecha franja de tierra situada en medio de la desolación.
Sobre una minúscula barraca se lee: " Aeropuerto de Djanet ".
En medio de una nube de arena generada por él mismo,un camión saltarín recorre los escasos kilómetros que dista la ciudad.
Los 3.000 habitantes del oasis son tuaregs,la llamada " gente del velo ",por la especie de turbante - - el shesh - - que cubre la cabeza de los hombres con excepción de los ojos.
Históricamente,los tuaregs han controlado el desierto,frenando los avances de posibles intrusos por el Norte y el Sur,como una barrera entre los árabes y el Africa negra.
Resistieron también a los franceses,que sólo en 1909 pudieron apoderarse de la plaza y cambiaron su nombre por el de Fuerte Charlet.
En lo alto de una colina que se alza sobre el oasis,pueden verse las ruinas del fuerte edificado por la Legión Extranjera.
Restos aún más antiguos se esparcen sobre la colina,los de la ciudadela del gobernador turco.
" Las palomas cantan en las palmeras - - escribió Robert Herisson,oficial médico de la primera expedición francesa que penetró en Djanet - -.
En los huertos,circundados por setos de espino,hay frutas y hortalizas: tomates,melones,uvas,cebollas.
Hemos comprado fruta en sazón.
Después de nuestras privaciones,la vida nos parece dulce y fácil.
" Para un viajero actual no sería preciso cambiar ni una palabra de esa descripción.
Cincuenta mil palmeras rodean las casas bajas de adobe encalado,con una mezquita en medio,y algunas dependencias oficiales más sólidas,incluido el generador eléctrico.
En el mercado o las tiendas,que se extienden a lo largo de la única calle,por supuesto de arena,es posible adquirir todo tipo de artículos de primera necesidad: pan fresco,latas de comestibles,ropa.
Pero nada de " lujos ",como una postal o un periódico.
Nuevas pistas de aterrizaje para los grandes Jumbos,un hotel moderno y otras sofisticaciones están planeadas para convertir a Djanet,con sus pinturas del Tassili,en etapa obligada de un circuito turísticoarqueológicoA,como Creta o el Valle de los Reyes.
Pero de momento son necesarios múltiples permisos para acceder a los tesoros prehistóricos del Tassili.
Al amanecer ascendimos hacia el paso del Tafilalet,salvando empinados desniveles,sobre una senda áspera,rocosa,inestable,especial para torcerse los tobillos.
De vez en cuando.
en medio del camino,aparecen acacias,tamariscos,adelfas y espinos,sin mencionar los huesos de camellos y asnos que dieron un mal paso.
Al despuntar el día,en lo alto de la meseta,se vislumbra un paisaje sobrecogedor.
Las que alguna vez fueron pampas o verdes praderas,se han convertido en llanuras de piedra,vastos anfiteatros negros por la eclosión constante que ha oxidado la roca.
Y circundando el panorama,montañas quebradas,inmensas cordilleras,precipicios ; todos de piedra arenisca y color de albaricoque,púrpura,rosa,un marrón inquietante,según los juegos de la luz y la sombra.
Por todas partes emergen fantásticas imágenes de castillos derruidos,rascacielos agrietados,con uno que otro arco,una puerta e incluso calles petrificadas en un paraje llamado Sefar," capital " del Tassili,por encontrarse allí muchas de las mejores pinturas.
En torno del lugar se experimenta un sentimiento perturbador,de ruina,de catástrofe natural,donde sólo las firmes inscripciones en la roca prueban que alguna vez el hombre pasó por allí.
En el Tassili,a diferencia de Lascaux o Altamira,las pinturas no se hallan únicamente en cuevas,sino también al aire libre,sobre salientes de la roca.
Algunas se han desdibujado,lamentablemente,debido en parte al hábito de salpicarlas con agua para resaltar los colores.
Aparte de la supresión de esta práctica,poco se ha hecho desde tiempos de Lhote para conservar las pinturas,por lo que puede sorprender que hayan soportado tan bien la prueba de su hallazgo.
Algunas de ellas se amontonan unas sobre otras,pero también se las encuentra aisladas.
Es éste un laberinto de piedra en el que sería imposible adentrarse sin un guía,tan imposible como salir de allí.
Sobre la base de las pruebas de radiocarbono se ha llegado a datar las pinturas más antiguas alrededor del 8000 a. C. Según Gale Sieveking,del Museo Británico,una de las máximas autoridades en pintura rupestre,es probable que los primeros agricultores del Sahara se instalaran en Tassili.
Los habitantes de la meseta serían también los autores de las pinturas.
La población más primitiva era prepastoril ; la más tardía,agrícola.
Lhote dividió las pinturas en grupos según su estilo,y le dio el nombre de " Cabezas redondas " al más primitivo de todos.
Sieveking,cauto,muestra sus reservas ante la ingeniosa denominación y se limita a comentar que " podría ser así ".
No obstante," período de las cabezas redondas " es una frase que describe literalmente las misteriosas figuras primitivas,a menudo de tamaño natural,blancas y a veces marrones,desprovistas de rasgos,y en ocasiones con máscaras o cuernos.
El astronauta Hugh Merril flotaba libremente en el espacio,sin necesidad del " cordón umbilical " que lo uniera al transbordador,gracias a una de las nuevas Unidades de Maniobra Tripuladas (UMT) que,calzada a su espalda,le permitía desplazarse en cualquier dirección y con una gran autonomía.
Se disponía a cumplir una tarea de rutina,atendiendo al mantenimiento de un satélite meteorológico que tenía frente a si.
Merril se sentía animoso,quizá ligeramente eufórico,al utilizar por primera vez aquel ingenio que le permitía moverse con seguridad e independencia por el espacio exterior.
De pronto su vista se nubló,el corazón pareció detenerse y perdió el conocimiento.
Había sufrido una lipotimia,un desmayo de origen circulatorio,no demasiado grave ni prolongado en circunstancias normales,que suele remitir por sí mismo,sin consecuencias.
Pero,en medio del espacio,el astronauta desvanecido,llevado por la inercia de su trayectoria,comenzó a alejarse peligrosamente.
Su compañero de tareas,Gary Marlowe,advirtió que algo le sucedía a Hugh.
Utilizando su propia UMT,alcanzó velozmente al astronauta errante.
Al advertir que se hallaba sin sentido,utilizó el sistema de salvamento face to face (cara a cara),que permite sujetar una UMT a otra,sin colaboración del rescatado.
Luego,llevando a Merril acoplado,recorrió algo más de un centenar de metros para regresar al transbordador.
Poco después,ambos bebían café con sus otros compañeros,comentando el incidente y la salvadora versatilidad de las Unidades de Maniobra Tripulada.
Esta escena,imaginaria,lo es sólo en sus personajes y detalles.
Pero puede producirse en cualquier momento,y de hecho ocurrirá más de una vez en las próximas décadas,cuando el espacio se pueble de hombres trabajando en la reparación,mantenimiento o instalación de satélites de todo tipo,o en la construcción de estaciones y laboratorios espaciales.
De hecho,escenas similares han sido ensayadas y simuladas ya en el propio escenario espacial,tanto en lo que en su función de vehículo individual de trabajo,como en funciones de salvamento.
La Unidad de Maniobra Tripulada o MMU (Manned Manouvering Unit) es la realización de la fantasía de la moto espacial,que permite a un astronauta desplazarse libremente en el espacio,autoimpulsandose en diversas direcciones,acuñada por numerosos autores de ciencia ficción.
La verdadera MMU no se parece a una moto,sino más bien a una combinación de chaleco,sillón y mochila,en forma de estructura rígida y funcional que el astronauta en operaciones ajusta a su espalda.
Se trata de un verdadero vehículo autónomo para moverse en medio de gravedad cero,accionado por un generador eléctrico que alimenta los controles de propulsión.
Posee los mandos e indicadores necesarios para el manejo de diversas operaciones de vuelo,y su impulsor no quema nada,se efectúa por chorros de nitrógeno gaseoso.
Utiliza 24 impulsores fijos,que le permiten desplazarse o girar libremente en seis tipos de movimiento.
La unidad utiliza un equipo redundante (que duplica o multiplica sus componentes básicos) para que su conductor actúe en condiciones óptimas de seguridad.
La necesidad de diseñar y construir una moto espacial como la UMm-MMU nació de la creciente gama de herramientas extravehiculares en órbita que exige la nueva etapa de utilización y colonización del Espacio,al margen de su función eventual en la todavía discutida Guerra de las Galaxias.
Hasta hoy han sido probadas ya varias de estas unidades,verificando sus prestaciones para las tareas previstas y comprobando la posibilidad de recargar los tanques de nitrógeno y reemplazar las baterías durante las operaciones.
Durante un reciente vuelo del malogrado transbordador Challenger (la misión STS - 41 C) la UMT voló hasta el satélite Solar Máximum,con el objeto de anular su movimiento giratorio.
El piloto debió realizar unas maniobras muy complejas y precisas,logrando el acoplamiento después de varias tentativas.
Poco después,durante la misión STS-CIA,dos astronautas utilizaron sendas UMT para recuperar a Los satélites Westar V / y Palapa B - 2. Aparte de sus funciones de acoplamiento y estabilización de satélites artificiales,la UMT tiene un amplio campo de actividades en el mantenimiento de éstos,así como será indispensable para transferir elementos estructurales durante una construcción en órbita,como por ejemplo la futura estación espacial de la NASA Pero los diseñadores de la UMT y los directores de su programa de pruebas no ocultan que otra de sus funciones fundamentales será la de vehículo de rescate.
La reciente tragedia del Challenger ofreció al público una dramática muestra de un accidente en el espacio,asociado de algún modo a los accidentes aéreos terrestres: una nave estalla,o se estrella,y todos sus tripulantes mueren.
Pero es muy probable que en el futuro cercano,con estaciones,transbordadores y " operarios " trabajando y desplazándose en órbita,los accidentes del espacio se parezcan cada vez más a los accidentes laborales o de carretera,donde la rapidez y eficacia de los servicios de rescate pueden ser decisivos en la salvación de vidas y equipos.
Un vehículo espacial averiado con escasa reserva de oxígeno,un operario herido o enfermo que debe ser trasladado con urgencia,una falla de materiales que exige reparación inmediata,o nuestro caso imaginario de la lipotimia del astronauta Hugh Merril,son otros tantos ejemplos de situaciones donde la independencia y versatilidad de las UMT pueden resultar esenciales.
la NASA ha realizado ya salvamentos simulados,utilizando las nuevas UMT en tres situaciones básicas de emergencia en el espacio: el auxilio a la tripulación de una nave averiada,que no dispone de trajes presurizados ; el salvamento de un astronauta con traje espacial,que sufre una emergencia durante una operación fuera de su nave (ya sea consciente o sin sentido) y,finalmente,el rescate de un conductor de una UMT,en circunstancias similares,utilizando otra UMT.
En el primer caso,para trasladar tripulantes de una nave no operativa,sin trajes espaciales,se ha desarrollado un ingenioso complemento de las UMT,llamado Esfera de Rescate Personal o PRS (Personal Reseue Sfere).
Se trata de un sistema autónomo de subsistencia,de uso individual,que en síntesis puede describirse como un verdadero " balón de oxígeno " que cada tripulante de la nave averiada puede utilizar para meterse adentro y ser trasladado a la nave de rescate.
la PRS se transporta acoplada a la UMT,por delante del conductor,y el receptor se mete en ella usando la cámara de presión intermedia de u nave y viaja en su interior a través del espacio,hasta llegar a la cabina presurizada de la otra nave.
Por un sistema similar,las UMT pueden usarse para el salvamento de Un astronauta flotando en el vacío.
Si se encuentra consciente y cuenta con reserva de aire respirable,es decir que sus únicos problemas son de transporte,él mismo puede atarse a la UMT de su compañero,y ser arrastrado por éste.
Si está desvanecido,será el otro piloto quien deberá amarrarlo o meterlo en el " balón de oxígeno ",de ser necesario,con la ayuda de un tercer astronauta.
La recuperación de una UMT averiada y su conductor,seguiría un proceso semejante.
La factibilidad de estas situaciones de rescate ha sido comprobada en diversas pruebas y ensayos realizados en el Simulador de Operaciones Espaciales (StS,Space operations Simulator) de la empresa Martin Marietta y en el Laboratorio de Entrenamiento para la Ingravidez Ambiental,en el Johnson Space Center,que posee la NASA en Houston,Texas.
Varios siglos antes de que Colón llegara al Nuevo Continente y lo hollaran los conquistadores,existió en América Central la que se supone una de las más sofisticadas civilizaciones de la Antigüedad,más célebre quizá por lo que de ella se ignora y se imagina que por las escasas certezas que los arqueólogos han ido desvelando sobre ese enigmático pueblo cuyo propio nombre tiene sugerencias de misterio: los mayas.
Desde el descubrimiento de las primeras ruinas e indicios de una cultura precolombina distinta,a mediados del siglo XIX,los mayas fueron considerados como una civilización sabia y superior,incluso románticamente pacifica y democrática,a tenor de la rara perfección de los pocos indicios de su arquitectura y artesanía.
En las húmedas e inhóspitas tierras bajas del Yucatán y parte de lo que hoy es Honduras,Guatemala y Belice,ese pueblo elusivo construyó complejas ciudades con lujosas residencias,palacios de piedra y templos piramidales,inventó un sofisticado calendario,desarrollo técnicas agrícolas sorprendentes y una elaborada escritura jeroglífica que aún no ha sido totalmente descifrada.
Se presume que esos textos registran los hechos y dinastías de sus peculiares gobernantes: tradicionalmente,arqueólogos y exploradores tendieron a describir a los mayas como una civilización remota y etérea,dirigida por reyes filósofos,a los que se llegó a imaginar como los últimos sobrevivientes de la extinguida Atlántida o los hijos de la Tribu Perdida de Israel.
En décadas recientes,los avances en la decodificación de los jeroglíficos y las nuevas excavaciones y descubrimientos,significativamente en las tumbas de Tócale y en la región guatemalteca de Petén,han permitido llenar buena parte de los claros en el conocimiento de la historia y las costumbres de los mayas.
Hoy se sabe que sus reyes no eran contemplativos intelectuales recluidos en un selvático paraíso de las artes y las ciencias,sino seres tan vanidosos y ególatras como cualquier otro gobernante autocrático en el resto del mundo," tan obsesionados por el poder como los emperadores romanos y tan ansiosos por perpetuar su gloria personal como los faraones de Egipto ",según escribió Alfred Meyer en la revista Science.
Por otra parte,sus imponentes construcciones no eran sólo centros protocolares y religiosos,como alguna vez se creyó,sino verdaderas ciudades,en el sentido urbano y moderno de la palabra.
Constituían el núcleo de la vida comercial y política,y en sus calles y edificios traficaban los comerciantes,trabajaban los obreros y artesanos,oficiaban los sacerdotes,sentenciaban los jueces y se paseaban los miembros de la clase dirigente.
Desvelado cada vez más por la moderna Arqueología el misterio de lo ignoto y remoto,queda en pie el genuino y excepcional talento intelectual y artístico de los mayas,aunque su vida social y cultural no difiera tanto como se quiso suponer del resto de las civilizaciones de su tiempo.
No fue,por cierto,una civilización efímera.
Su cronología se extiende desde el año 2000 a. C. hasta la Conquista,en el siglo XVI de nuestra era.
Su historia se inicia con la formación de comunidades agrarias que evolucionan,a través de diversas etapas,hacia una sociedad compleja,estratificada y elitista.
La cultura maya tiene su culminación en el llamado Período Clásico,que va de mediados del siglo III de nuestra era (aunque recientes descubrimientos podrían adelantarlo en 400 años) hasta finales del siglo IX,en el que se desarrollan tanto el tamaño de las ciudades y la sofisticación de la cultura,como el poder y el autoritarismo de los reyes y la aristocracia.
Poco después,una misteriosa catástrofe histórica o social desmantela las ciudades del Yucatán,y marca el principio del fin.
Los mayas emigran hacia el Norte y el Sur,estableciéndose en frágiles asentamientos que pronto caen bajo la influencia de las culturas mexicanas,sobre todo la azteca,con su carga de militarismo,crueldad religiosa y mercantilismo.
" Estos últimos mayas - - dice Meyer - - también comparten con los mexicanos una fatal vulnerabilidad ante los soldados españoles,los misioneros católicos y las enfermedades europeas.
" En medio de la espesa selva del Sur de Yucatán adentrándose una decena de kilómetros desde la frontera mexicana en territorio guatemalteco,se encuentra un grupo de colinas que indican la situación de las ruinas de El Mirador,el más espectacular yacimiento arqueológico de la cultura maya,descubierto en la última década.
Allí,diversos equipos de investigadores están literalmente desenterrando los restos de una vasta y maravillosa ciudad levantada hace dos milenios.
El primer indicio sobre la importancia de El Mirador surgió en 1978,cuando Bruce Dahlin,de la Universidad Católica de Washington,encontró en ese lugar trozos de alfarería que fueron datados como pertenecientes al siglo IV a. C. (Hasta entonces se pensaba que el arte maya sólo se había iniciado 700 años más tarde.
) Con el apoyo de la National Ceographic Society,Dahlin convocó a investigadores de diversas universidades católicas y al prestigioso arqueólogo Ray Matheny,de la Universidad Brigham Young de Utah,que dirigió técnicamente el proyecto.
En 1982,después de varias temporadas de excavaciones e investigaciones,el equipo de Dahlin-Matheny pudo concluir que El Mirador albergaba el testimonio de una amplia y bulliciosa metrópolis,erigida no menos de 500 años antes del supuesto florecimiento de la cultura maya.
Los habitantes de la ciudad que se levantaba en El Mirador - - al igual que sus contemporáneos en otras poblaciones mayas - - eran hombres hábiles y ambiciosos,que habían inventado numerosas técnicas de ingeniería y arquitectura,así como un avanzado sistema de cultivos agrícolas.
Estudiando la antigua metrópolis de El Mirador,en comparación con pueblos más pequeños de la misma época,los arqueólogos han podido establecer cómo y cuándo la cultura maya alcanzó su cima,y hallar en las raíces de la mayor civilización precolombina las semillas de su propia destrucción.
Los hallazgos de El Mirador llevaron a revitalizar el estudio conjunto de los clásicos yacimientos de Tócale y Copán,descubiertos en 1840,para establecer las peripecias y acontecimientos protagonizados por los mayas a través de su prolongada historia.
Pueblo inquieto y laborioso,abría claros en la jungla para sus ciudades y cultivos y dominaba la hostil Naturaleza circundante con una notable sabiduría ecológica.
Es probable que los gobernantes fueran a su vez ingenieros (o viceversa),en cuyas manos estaba planificar y dirigir la construcción de los enormes templos en forma de pirámide y los grandes palacios y monumentos (cuya estructura incluía arcos de ménsula y alcantarillas internas,entre otros avances técnicos) que luego eran pintados con colores brillantes.
Como ya se ha dicho,los mayas poseían una compleja y precisa escritura jeroglífica,y un intrincado calendario que da cuenta de sus grandes conocimientos matemáticos y astronómicos.
Consideremos ahora una característica menos diabólica,pero igualmente interesante,de las curvas monstruosas.
Si observamos las figuras que ilustran sus métodos de generación recurrentes,veremos que todas estas curvas son " autosemejantes ",en el sentido de que se pueden subdividir indefinidamente en partes geométricamente semejantes al todo.
Como dijo en 1905 el matemático italiano E. Cesaro,fascinado por la curva en copo de nieve: " Es esta similitud entre el todo y sus partes,incluso las infinitesimales,lo que nos lleva a considerar la curva de Von Koch como una línea verdaderamente maravillosa entre las líneas.
Si estuviera viva,no seria posible aniquilarla sin suprimirla de un solo golpe,pues renacería sin cesar de las profundidades de sus triángulos,como la vida en el Universo ".
El matemático francés Paul Lévy fue el primero en estudiar sistemáticamente las curvas autosemejantes,y su discípulo Benôlt Mandelbrot tuvo el acierto de ver,en lo que parecía una curiosidad matemática sin la menor aplicación práctica,un poderoso instrumento para el estudio de numerosos fenómenos físicos.
Pero tampoco hay planos,rectas o círculos en la Naturaleza,y ello no impide utilizar la Geometría euclidiana como base de la Topografía y la Mecánica.
En su libro Los objetos fractales * *,Mandelbrot afirma: " Aquéllos a quienes se habla de curvas sin tangentes o de funciones sin derivada empiezan pensando que la Naturaleza no presenta tales complicaciones y que,evidentemente,no nos sugiere esas ideas.
Sin embargo,lo cierto es lo contrario,y la lógica de los matemáticos les ha mantenido más cerca de la realidad que las representaciones prácticas empleadas por los físicos ".
Mandelbrot denomina fractales a las curvas autosemejantes de dimensiones fraccionarias,así como a los objetos y fenómenos naturales que tienen en dichas curvas sus modelos geométricos más adecuados (el neologismo fractal se justifica,entre otras cosas,por el hecho de que las llamadas " dimensiones fraccionarias " muchas veces tienen valores irracionales y,por tanto,no expresables mediante fracciones,por lo que la denominación es equivoca).
En su libro,Mandelbrot demuestra mediante numerosos ejemplos que en la Naturaleza abundan los modelos aleatorios que,estadísticamente,tienen un desarrollo autosemejante.
Consideremos,por ejemplo,las líneas de litoral.
Vistas en un mapa,las costas parecen líneas sencillas y de una longitud fácilmente determinable: para saber cuánto mide una costa concreta,basta con medir la línea que la representa en el mapa y conocer la escala de éste.
Pero a poco que se piense en ello se comprende que tal planteamiento es erróneo,pues según la escala del mapa se tendrán en cuenta más o menos accidentes geográficos: líneas que en un mapamundi parecen lisas,en un mapa nacional o regional aparecen llenas de entrantes y salientes,y si nos desplazamos físicamente a la zona en cuestión veremos miles de sinuosidades que ningún mapa registra.
Es más,si queremos calcular la longitud de la costa con verdadera precisión,deberemos medir el contorno de cada roca de la orilla.
¿Y por qué no de cada guijarro,o de cada grano de arena situado en la línea divisoria tierra / agua? Según el grado de precisión,la longitud de la costa entre los puntos A y B puede ser tan pequeña como la del segmento rectilineo AB... o tan grande como queramos.
Por otra parte,y esto es algo observado experimentalmente,las líneas costeras son estadísticamente autosemejantes,es decir,sus configuraciones a distintas escalas repiten pautas muy similares.
(Vista desde un avión,el aspecto general de una línea costera es siempre muy parecido,independientemente de la altura desde la que se la observe.
) Puesto que son autosemejantes y su longitud crece indefinidamente al ir aumentando el grado de precisión de las mediciones,las líneas litorales tienen un modelo geométrico idóneo en curvas como la de Koch.
Y,de hecho,si alteramos la perfección geométrica de la curva en copo de nieve introduciendo elementos aleatorios en su proceso generativo,se obtienen líneas de litoral muy verosímiles (este método se ha utilizado,por ejemplo,en películas como La guerra de las galaxias para diseñar mediante ordenador paisajes extraterrestres).
La misma autosemejanza se observa en el perfil de las montañas,la superficie de ciertos quesos,la distribución de las estrellas en el firmamento y otros muchos modelos naturales.
Lejos de constituir monstruosas excepciones,los objetos fractales parecen ser la regla.
En los últimos anos,la Geometría Fractal ha permitido abordar matemáticamente procesos físicos de gran complejidad,como la propagación de los terremotos,la superconductividad o el arracimamiento de partículas metálicas.
Lo más sorprendente es que fenómenos tan dispares como la turbulencia de los fluidos en movimiento y la distribución de las galaxias,son expresables mediante curvas fractales de dimensiones muy próximas.
Aún es pronto para decir si se trata de meras coincidencias o si existe algún principio oculto capaz de explicar simultáneamente fenómenos aparentemente desconectados ; pero aunque todavía no se sepa con exactitud por qué el análisis fractal es una herramienta tan versátil,su eficacia ya no deja lugar a dudas y su campo de aplicaciones se amplía cada vez más.
Como ha dicho Mandelbrot: " Entre e dominio del caos incontrolado y el orden excesivo de Euclides,hay,a partir de ahora,una nueva zona de orden fractal ".
Seis millones de norteamericanos enfermos de insuficiencias coronarias,dueños de la ilusión proporcionada por una nueva droga de efectos casi mágicos a la hora de desbloquear las arterias obstruidas por trombos,sintieron una gran frustración al conocer la noticia,dramática para muchos de ellos: la FDA (Administración de Drogas y Alimentos),a través de un grupo de consejeros había desaprobado la entrada en el mercado del activador tisular del plasminógeno,más conocido como TPA (ver ALGO de abril,pág.
78).
La droga es el resultado de un minucioso procedimiento de ingeniería genética llevado a cabo por la empresa norteamericana Genentech.
TPA es el potente anticoagulante natural utilizado por la propia sangre para deshacer minúsculos coágulos provocados por pequeñas lesiones en vasos sanguíneos.
Sin embargo su acción sobre grandes trombos no es posible ya que el organismo humano produce muy pequeñas cantidades de TPA.
En una síntesis que seguramente sonará.
Puede decirse que las investigaciones se orientaron en la búsqueda del gen responsable de la producción natural de TPA,para luego introducirlo en ciertas células de hámster,reproducir esas células mediante cultivo y así contar con enormes cosechas de TPA.
Las pruebas que se realizaron con la nueva droga dieron inmediatamente resultados más que satisfactorios ; tanto,que el Instituto Nacional del Corazón,Pulmón y Sangre norteamericano,decidió suspender los ensayos que llevaba a cabo con estrepokinasa (droga que se experimentaba con los mismos propósitos) al comprobar que el TPA era dos veces más efectivo Por eso quizá el asombro haya invadido a mucha gente - - especialmente a los responsables de Genentech - - cuando la FDA supeditó su entrada en el mercado a un trámite,digamos burocrático.
Si bien la institución nunca dudó de la eficacia de la droga,considera en cambio que faltan pruebas o en cualquier caso que no son suficientes las aportadas para demostrar que el TPA prolonga la vida a los pacientes.
Es decir que Genentech deberá acatar los reglamentos y presentar nuevos estudios que determinen la incidencia de la droga en el índice de mortalidad de los pacientes.
Estos estudios de rutina demorarán al menos un año la introducción del TPA en el mercado norteamericano con un doble perjuicio: por un lado para los enfermos,cuya desazón se puede imaginar,y por otro,para los empresarios de la Genentech,no menos descorazonados al ver escapar el negocio de su vida,puesto que las excelentes perspectivas que aguardaban en la faz comercial les había llevado a planificar una nueva planta para la exclusiva elaboración del TPA.
Los únicos contentos con esta noticia han sido los competidores de Genentech.
De todos modos siempre queda una esperanza para los enfermos: en Europa,la Universidad de Lovaina,Bélgica,y los laboratorios alemanes Boehringer Ingelheim siguen desarrollando sus planes para la producción de TPA,aunque aún no se comercializa en el mercado farmacéutico.
Con el peligro de caer en la obviedad,cabe destacar cómo la Ingeniería Genética sigue permitiendo avances espectaculares.
Y una buena noticia es en este sentido la creación de la primera planta de Ingeniería Genética en España.
La iniciativa de los Laboratorios Serono,en Colaboración con el CDTI,y el Banco de Crédito Industrial,permitirá la construcción de la nueva planta en Tres Cantos,(Madrid),durante 1988.
Cuatro años mas tarde comenzará la producción industrial de hormonas y otros medicamentos mediante técnicas de Ingeniería Genética.
El Síndrome de Inmunodeficiencia Adquirida - - SIDA - - amenaza con convertirse en el mal del siglo.
Su aparición e implacable ataque derribando las defensas del organismo humano y poniéndolo a merced de cualquier enfermedad,está causando estragos en determinados grupos de la población y aterrorizando a la mayoría.
Mientras tanto,científicos del mundo entero trabajan por obtener un medio capaz de detener el temible virus,experimentando diferentes vacunas y terapias.
Pero no sólo trabajan en este sentido los médicos,sino también los mismos pacientes,ahora también sorprendidos por un nuevo virus,el llamado HTLV - 1. Ante la gravedad de la situación,un grupo de pacientes reunidos en el Project Inform,con sede en San Francisco,facilita información a los afectados por el SIDA,dando detalles de la falta de defensas en el organismo humano como consecuencia de la acción del virus.
También el Project Inform proporciona el nombre y numeros de teléfono de los médicos que estudian tratamientos posibles y asesora acerca de las vías legales que han de utilizarse para la obtención de determinadas drogas terapéuticas.
El coordinador del Project Inform,Tom Jefferson,y otros enfermos de SIDA iniciaron hace más de dos años un tratamiento experimental distinto a los ofrecidos por la sanidad gubernamental,empleando más de una droga al mismo tiempo.
Drogas que obtenían en México,como la rivabirina y la isoprinosina.
La primera de ellas es tenida como un potente agente antivírico y la segunda como un agente de ayuda al sistema inmunológico.
Según Jefferson,unos 450 pacientes han recibido el tratamiento y,como en su caso,la enfermedad se ha estabilizado.
A fin de contribuir en las investigaciones,el Project Inform ha creado un extraordinario banco de datos con la evolución patológica y los efectos del tratamiento,un tratamiento no autorizado,pero controlado profesionalmente por destacados médicos norteamericanos.
Uno de ellos,el doctor Barry Ginsell,escribió en Medical World News que " las drogas parecen haber estabilizado a algunos pacientes,pero aún no hay seguridad ".
Lo cierto es que más adelante señala que cuando los pacientes abandonan el tratamiento de rivabirina-isoprinosiI0V su estado clínico decae con rapidez.
No obstante no existe seguridad en cuanto a los resultados finales del tratamiento,algunos creen que el régimen de estas drogas ha mantenido a los pacientes en un estado lo suficientemente aceptable como para experimentar con otras sustancias igualmente esperanzadoras,como la D-penicilamina y la azidotimidina (AZT).
Mientras los pacientes organizan sus propios sistemas de información y defensa (ver Sida I),las buenas y malas noticias científicas se cruzan,alentando esperanzas o sembrando temores.
Entre las buenas noticias está la que dio el Dr. Friedmann Kien en el XVII Congreso Mundial de Dermatología celebrado recientemente en Berlín: la frecuencia del sarcoma de Kaposi en los enfermos de SIDA ha descendido considerablemente.
Esta afección cutánea de tipo canceroso era una de las peores consecuencias del SIDA,y afectaba en 1981 al 1987 de los enfermos de ese mal.
Ahora,por razones aún desconocidas,sólo ataca al 17 % de sidóticos.
El congreso - - que también evaluó los progresos en psoríasis y melanomas malignos,así como algunos fármacos novedosos,como el ácido azelaico,un producto de la empresa alemana Schering que parece de notables efectos sobre el acné - - no pudo llegar a una conclusión sobre las razones de cambio en el SIDA respecto del sarcoma de Kaposi,una complicación que se describió por primera vez en Uganda en 1950 y que mata al sidótico antes de 3 años.
Pero no todas son buenas noticias: al virus del SIDA llamado HIV,se ha agregado recientemente un " pariente cercano ",el HTLV - 1. Este virus es el causante de una forma extraña y mortal de leucemia y de una especie de disfunción neurológica de carácter esclerótico.
Al parecer el grupo de mayor riesgo es el de los drogadictos intravenosos.
Del mismo modo que el virus que provoca el SIDA,su pariente,el HTLV - 1,ataca los linfocitos T,que son pieza fundamental en el sistema inmunológico.
Este virus que está en plena expansión,afecta a distintos grupos y razas,localizándose hasta ahora en el Sur de los EE. UU. y Alaska y también en Japón,Haiti y Jamaica.
Otro punto de contacto entre el HTLV - 1 y el HIV es su origen africano.
De acuerdo con el doctor Robert Gallo,el Instituto Nacional del Cáncer de los EE. UU.,que lo aisló ya en 1978,el HTLV - - 1 quizá se haya originado en la misma especie de mono verde que se cree fue el campo propicio del SIDA.
No opinan lo mismo los científicos japoneses,quienes a través del doctor Yorio Hinuma de la Universidad de Kyoto,creen que el virus llegó al Sur de Japón hace unos 12 mil años y fue llevado allí por los pueblos del Asia Central.
De todos modos el HTLV - 1 no es tan letal como el virus del SIDA,pues para que entre en actividad necesita una exposición repetida dentro de un grupo humano cerrado,como el de los drogodependientes.
Pero,a pesar de que sólo un uno por ciento de los individuos atacados puede desarrollar realmente la enfermedad,el virus puede permanecer inactivo y agazapado durante casi 40 años antes de originar un cáncer u otra enfermedad.
El sistema de ataque es semejante al utilizado por el SIDA,lo cual sugiere que se deberán incrementar los controles en los bancos de sangre de todos los hospitales.
Conjuntamente con estas medidas de control también es de fundamental importancia que la población se informe convenientemente y siga las pautas de prevención.
La explosión tecnológica en el campo de las comunicaciones,depende en buena parte del teléfono.
Bancos,empresas,estamentos oficiales,medios de comunicación,particulares,etc.,establecen sus contactos a través de un insignificante trozo de cobre - - o de fibra óptica - - estirado hasta el infinito.
Y puesto que es muy fácil " pincharlo ",en los últimos años se ha puesto de moda darle al pico telefónico.
No pocos escándalos políticos y financieros han saltado en la última década por la intercepción fraudulenta de las comunicaciones telefónicas.
Aunque la intimidad del correo y las comunicaciones telefónicas están garantizadas por la ley estadounidense,a las nuevas modalidades de comunicación como el correo electrónico,las redes de ordenadores,los teléfonos celulares,etc están completamente desprotegidos.
Tanto es así,que el Congreso de los Estados Unidos ha presentado recientemente un proyecto de ley según el cual las comunicaciones por ordenador y teléfono celular se verán protegidas.
Será delito grave interceptar comunicaciones electrónicas y espiar correspondencia ajena en redes de transmisión de correo electrónico.
Asimismo,la ley dará a cualquier persona o corporación cuya comunicación sea interceptada o desvelada,el derecho a demandar por daños.
Un departamento del Congreso,el Office of Technology Assessment,tras un estudio reveló que las comunicaciones de alta tecnología tanto individuales como corporativas,son frecuentemente interceptadas por agencias gubernamentales.
Según este estudio,el 25 % de los 142 departamentos federales que respondieron,indicó que utilizaba,o tenía intención de utilizar,técnicas de intercepción de comunicaciones electrónicas.
incluso correo electrónico transmisiones vía satélite,frecuencias de radio teléfono y el uso de ordenadores.
Encabezan esos departamentos la Dirección de Drogodependencia,el FBI,el Servicio de Aduanas,el Servicio de Recaudación de Impuestos y la Sección Criminal del Departamento de Justicia... Sin embargo,resulta más inquietante el hecho de que tres de los usuarios más asiduos de estas prácticas - - la Agencia Central de Inteligencia (CIA),la Agencia de Seguridad Nacional y la Agencia de Inteligencia de la Defensa.
Los expertos coinciden en afirmar que el espionaje electrónico es más frecuente de lo que pueda llegar a sospecharse y cuesta,por hablar sólo en términos económicos,millones de dólares por año a las empresas afectadas.
En la mayoría de los casos,los hechos no son denunciados para evitar una caída de la imagen,por temor a la reincidencia y porque los afectados no quieren verse involucrados en un sistema legal que va a hacer muy poco por ellos.
Si ha de creerse en esta investigación,cada vez que alguien,en los Estados Unidos,descuelga el teléfono,interconecta su ordenador en una red o activa su correo electrónico,es muy posible que - - en algún punto de la maraña electrónica - - alguien lo esté escuchando.
El castigo a los niños como método de enseñanza y disciplina se ha mantenido desde hace siglos en algunos países,como un pilar imprescindible de la educación.
Ni las numerosas investigaciones y estudios psicológicos y sociológicos,ni las teorías derivadas de ellos y que desaconsejan esa práctica han podido derrumbar tan sólida y violenta costumbre.
En los Estados Unidos,en 41 de sus estados miembros,están aún autorizados los castigos físicos en las escuelas.
Esta situación,una ofensiva desatada por la Academia de Pediatría Nacional y el caso de una niña de EGB,duramente golpeada en su colegio,que incluso puede ser llevado a la Corte Suprema de Justicia de los Estados Unidos,renovó hace pocas semanas la polémica.
Pese a las obvias dificultades que se presentan para elaborar estadísticas fiables,las últimas estimaciones norteamericanas calculan 1,5 millones de alumnos castigados anualmente.
Datos relevados en el condado de Dallas indican que el año pasado se había castigado cinco veces más alumnos que en 1983 (última referencia estadística).
Pero si el motivo de la polémica,bastante antiguo como quedó dicho,no fuese lo suficientemente importante,nuevos estudios sociales,citados por el semanario Newsweek en un reciente reportaje sobre el tema,revelan que el castigo a los escolares se está ejercitando como una práctica de discriminación racial.
En esas mismas investigaciones se establece que los niños negros son más proclives que los blancos a ser castigados,y que los alumnos blancos humildes están en condiciones desfavorables,los ojos de maestros y profesores,ante estudiante,también blancos pero de clase media.
De opinión contraria,y dejando pigmentación cutánea y condición social a un lado,el doctor Don F. Wilson,presidente de la Asociación de Educadores de Ohio afirma que muchos de los expertos que embisten contra los castigos,cambiarían de opinión si tuviesen que pasar buena parte del día en las aulas.
' en Houston,una encuesta indicó que el 77 por ciento de los profesores,el 60 por ciento de los padres y el 40 por ciento de las víctimas potenciales (alumnos) están a favor del castigo físico,fervientemente convencidos del antiguo apotegma que los británicos elevaron a la categoría de ley implícita durante la era victoriana: la letra con sangre entra.
Sin embargo,quienes reniegan de los castigos ha desatado una ola de juicios contra profesores y miembros de la administración,en muchos casos por aplicar castigos ante faltas insignificantes.
Los ejemplos abundan: una niña de Houston fue agredida por su profesor,mientras buscaba algo en s armario,por haber llegado tarde a clase ; la hija de un psicólogo de Marysville,Ohio,que cursa el primer año de EGB,fue castigada por subrayar las respuestas de un examen en lugar de marcarlas con un círculo.
La polémica se orienta ahora hacia la necesidad de una nueva legislación,aunque muchos psicólogos opinan que no se terminará con estas prácticas,autorizadas o no,mientras los padres que fueron castigados en su infancia,impongan la misma ley a sus hijos,vengándose en sus hijos de la humillación recibida de sus padres.
La mixomatosis es una enfermedad contagiosa que afecta al conejo y que estuvo a punto de provocar su extinción en Francia y España,después de su aparición en Europa en 1952.
Aprovechando el carácter letal y selectivo de la enfermedad,algunos países como Gran Bretaña y Australia inocularon el virus de la misma en la población de conejos con la finalidad de acabar con ellos.
Los científicos franceses,por el contrario,quieren ahora salvarlos empleando a uno de sus transmisores,la pulga,para crearles los anticuerpos necesarios.
Esta peste del conejo es originada por un virus que fue aislado en 1898,por Sanarelli en Montevideo (Uruguay).
Su aparición en forma epidémica en Europa tuvo lugar en 1952 y causó grandes estragos en la población de conejos silvestres,tan buscados por los cazadores en otoño.
El contagio se hace a través de un agente transmisor,como el mosquito o la pulga,el cual inocula el virus en el conejo.
Después de una fase de incubación que puede durar entre tres y cinco días,se producen las primeras manifestaciones clínicas.
Estas consisten en un edema palpebral,es decir hinchazón en los párpados del animal,con secreción purulenta ; inflamación de los ganglios linfáticos ; orejas caídas ; edema en la cabeza y rápido adelgazamiento.
Pero no sólo la vacuna defiende a los conejos de la peste que los asola.
Según estudios realizados a través del Ministerio de Agricultura inglés,las nuevas generaciones de conejos están desarrollando sus propias defensas contra la mixomatosis,de modo que en Gran Bretaña la enfermedad apenas si mata,actualmente,a menos de la mitad de los conejos de campo.
Estos,según el ministerio británico,conforman una población de más de 20 millones de ejemplares,que han causado.
Posteriormente,aparece un tercer edema,acompañado de tumores propensos a la ulceración,en la zona genital.
A esta altura del desarrollo de la mixomatosis el conejo pierde la visión como consecuencia de las lesiones oculares y al cabo de pocos días muere.
En 1979 se descubrió una el verano pasado pérdidas en el campo de más de 100 millones de libras esterlinas,razón por la cual pretenden exterminarlos infectando sus madrigueras.
Otro tanto ocurre en Australia,un continente que ha estado a punto de quedar completamente estéril merced a la acción de millones de conejos,que no tienen enemigos naturales.
Estos animales llegaron a la isla,descubierta en 1779 por el capitán James Cook,a principios del siglo XIX para servir de blanco deportivo a los oficiales de la guarnición.
Ahora,los expertos australianos en vida salvaje han llegado a la conclusión de que,en una pequeña isla deshabitada del Sur,será necesario eliminar al menos unos 30 mil conejos,porque de lo contrario estos roedores arrasarán para siempre con la cubierta vegetal.
Para ello,el Servicio Nacional Australiano de Parques y Vida Salvaje está empleando equipos de arqueros que disparan a las áreas más inaccesibles de la isla - - donde las conejas procrean unas treinta crías cada una al año - - cápsulas con pulgas infectadas de mixomatosis.
Un método similar piensa emplear Francia,para inmunizar a sus conejos y salvarlos de la mixomatosis.
En efecto,científicos de la Escuela Nacional de Veterinaria de Toulousse,apoyados por la Asociación Nacional Francesa para la Caza,trabajan en el modo de que el mismo agente capaz de transmitirle la enfermedad también le inocule su defensa.
En el laboratorio veterinario se están preparando miles de pulgas del género Spilopsyllus,que tras ser sumergidas en una solución concentrada de SG33 serán liberadas en las madrigueras para que inmunicen a los conejos salvajes y puedan aumentar la población y satisfacer asimismo las necesidades de los cazadores franceses.
Según los veterinarios galos se necesitan al menos diez pulgas para inmunizar a un conejo.
De todos modos.
Muchos amantes sempiternos de la molicie y el descanso - - " il dolce fare niente ",como dicen los italianos - - han sentido la más verde de las envidias hacia los diez voluntarios escogidos por el Instituto de Problemas Médico y Biológicos del Ministerio de Salud Pública de la Unión Soviética para realizar un experimento de hipocinesia Y es que no todos los días se presenta tal oportunidad de permanecer constantemente acostados durante tanto tiempo.
Los investigadores soviéticos comenzaron sus experimentos en 1986,con la intención de reproducir en la Tierra los efectos fisiológicos fundamentales de la ingravidez,con el fin de mejorar los medios de entrenamiento para prevenir problemas físicos que emplean los cosmonautas en vuelos largos.
Divididos en dos grupos,los volunarios fueron sometidos a diversas pruebas: cinco de ellos hicieron ejercicios físicos durante dos horas diarias,desde el comienzo del experimento,mientras que los otros cinco no usaron ningún tipo de profilaxis ni realizaron movimientos gimnásticos hasta cuatro meses después del inicio de la prueba.
De momento se desconocen los resultados finales de la prueba.
El perro,con todos los respetos que se merece tan noble animal,ha perdido desde hace algunos años el honorífico titulo de " mejor amigo del hombre ".
El candidato con mejores perspectivas para ocupar la vacante es,sin dudas,el ordenador.
Una noticia proveniente de los Estados Unidos confirma esta posibilidad.
Hace pocas semanas entró en operaciones el Simulador Aerodinámico Numérico en el Centro de Investigación Ames que la NASA montó en Mountain View,California.
Este superordenador,aún en fase de desarrollo,permitirá simular las presiones que sufre un avión de combate F - 16,volando a una velocidad Mach 0,9.
Los beneficios no sólo serán prácticos,al reemplazar la construcción de prototipos para su sometimiento a pruebas,sino fundamentalmente económicos,puesto que la comprobación en túnel de viento de los aviones militares engrosan en altos porcentajes su costo total.
El nuevo sistema de la Ames podrá realizar otras tareas dentro de pocas semanas,cuando le sean conectados dos de los superordenadores más veloces que se conozcan.
Si actualmente la versión más sofisticada del ordenador Cray - 2,de la Cray Research,supera el asombro porque su capacidad de cálculo alcanza los 250 millones de operaciones por segundo y su memoria atesora 256 millones de palabras,todo ello parecerá un juego de niños cuando la NASA conecte su segundo superordenador,que incrementará la capacidad del sistema hasta alcanzar l 000 millones de operaciones por segundo.
Estas posibilidades permitirán su utilización en la investigación genética,la simulación de complejos ciclos químicos y físicos,el estudio de las galaxias y otros misterios cósmicos.
Sin embargo,quienes defienden a los animales y reniegan de la tecnología sostienen que esa máquina es incapaz de rastrear como un buen perro ; cierto es que nadie es perfecto.
Cataratas,era la palabra nefasta que se resignaban a escuchar muchos pacientes en las consultas de sus oftalmólogos,hace poco más de medio siglo,con el temor de caer en una ceguera inevitable.
El cristalino - - la lente del ojo que se ocupa de enfocar las imágenes que vemos,para luego imprimirlas en la retina - - sufría un proceso que lo hace opaco.
El único remedio era,y es,extirparlo y reemplazarlo por una lente intraocular.
Este tratamiento requería hasta hace poco la hospitalización del paciente y anestesia general.
Desde aquel entonces hasta hoy,las sombras se fueron transformando en luz cada vez más plena.
Los medios tecnológicos aplicados a los métodos de tratamiento quirúrgico modernos,han hecho que las cataratas se transformasen en una dolencia que ya no atemoriza.
Lo último en este tipo de intervenciones quirúrgicas lo constituye la extracción de cataratas en régimen ambulatorio.
Este procedimiento médico,habitual en los principales centros especializados norteamericanos,acaba de ser introducido en Barcelona por el doctor Antonio Henriquez de Gaztañado.
La novedosa operación,que permite una rápida curación y evita los costes de prolongados ingresos en hospitales,consiste en una técnica quirúrgica que extrae las cataratas en 20 minutos bajo un régimen de anestesia local.
Luego de la intervención los pacientes pasan a una sala de reposo,para relajarse durante una hora y más tarde son conducidos a sus domicilios donde deben descansar durante dos o tres días antes de reiniciar sus actividades habituales,plenamente recuperados.
Nadie sabe con certeza cuándo un volcán dormido volverá a despertar,ya que las causas de un estallido de magma ocurren a muchos kilómetros de profundidad,de modo que nadie puede garantizar a los científicos que no se convertirán ellos mismos en cenizas o en estatuas de lava.
Lo más parecido a una seguridad,en este sentido,es la débil confianza que los geólogos ponen en la periodicidad de ciertos fenómenos volcánicos.
Pero,como ocurre con las personas,hay volcanes puntuales y otros en los que el capricho parece ser la regla: el fotógrafo danés Lars Bjorn,autor de las imágenes que acompañan este articulo,acompañó hace poco a un grupo de vulcanólogos al cráter del volcán Kilauea,en Hawai,pese a las advertencias de los propios geólogos hawaianos.
Una semana después de haber caminado sobre la costra de lava,sorteando los chorros de gas sulfuroso de las fumarolas,confiados en su buena suerte,se enteraron que el volcán había despertado sin previo aviso y estaba manando lava ardiente en gran cantidad: de haber sido más tardía la visita,la muerte del grupo era segura.
Los volcanes no son,como suelen creer los niños - - y como creían los adultos en otros tiempos - -,una manifestación del calor reinante en el centro de la Tierra.
De ser así,el peso de la columna de magma sería tan grande que jamás podría llegar a salir por la corteza.
No: el magma que sale por los volcanes está mucho más próximo a la corteza,en el llamado manto,una capa situada inmediatamente por debajo de aquella y que mide unos 3 mil kilómetros de espesor (recordar que el radio terrestre es de unos 6.300 km).
En esa zona las rocas están en estado sólido,pero a una altísima temperatura,superior incluso a los 1.000 a 1.500 grados necesarios para fundir diversos tipos de piedras.
Esto se debe a la enorme presión que el peso de la corteza ejerce sobre esas rocas.
(Por cierto,ese calor se debe sólo en parte al calor remanente de los tiempos de formación del planeta: la mayor parte se explica por la radiactividad natural.
) La formación de volcanes y su posterior erupción se debe a grietas o fisuras en la corteza,debidas a la actividad de las placas continentales que se incrustan unas bajo el borde de otras.
Al disminuir la presión,la roca se funde en grandes cavidades llamadas cámaras de magma,y cuando puede escapa hacia arriba.
De la viscosidad del magma,su temperatura y su contenido en gases depende el tipo de volcán que se forme,su violencia y en suma su destructividad.
Cuando el magma es espeso y asciende lentamente,se corre el riesgo de que haya una gran acumulación de gases a altísima presión,lo que unido al endurecimiento y " taponamiento " de la grieta significa el peligro de una erupción explosiva y violenta,como la que destruyó la isla de Krakatoa,en tanto que la lava fluida,poco viscosa y con bajo contenido en gases da lugar a erupciones tranquilas,como las típicas en el archipiélago de las Hawaii.
Pero de todos modos hay volcanes más o menos violentos,pero no hay volcanes inofensivos: debajo del cono enrojecido se esconde toda la furia de la Tierra,una furia que el hombre durante mucho tiempo no podrá controlar.
En el segundo de sus libros,que lleva el nombre de la musa Euterpe,el gran historiador griego Herodoto se esfuerza por explicar cómo fueron construidas las pirámides de Egipto.
Herodoto,que vivió hace dos milenios y medio,visitó el país del Nilo cuando esos monumentos ya tenían,a su vez,dos mil años de existencia.
Sólo pudo conocer los métodos de construcción por referencias de egipcios contemporáneos suyos,teñidas seguramente por la pátina de una leyenda ya entonces milenaria,y sospechosas a los ojos de arqueólogos e historiadores de siglos futuros.
Si existían en su época documentos que explicaran las técnicas empleadas,Herodoto no era capaz de descifrarlos por ignorar las claves de la escritura jeroglífica,secreto celosamente guardado por los sacerdotes,inaccesible al pueblo llano y sobre todo a los extranjeros.
Aun cuando fue llamado Padre de la Historia por su obra monumental - - los nueve libros dedicados a las nueve musas que le convierten en primer historiador de la antigüedad clásica - -,Herodoto fue luego criticado,no siempre con razón,o bien por embustero o bien por un exceso de credulidad,que le llevó a aceptar como verdaderos relatos míticos o errores intencionados.
En el caso de Egipto,por ejemplo,llegó a afirmar que el inmenso lago Moeris era una obra artificial,debida a un legendario rey del que tomó su nombre.
La palabra " moeris ",como luego se supo,significa simplemente " lago ",y éste no era menos natural que el Nilo con el que estaba conectado.
Por éste y otros despropósitos fueron cuestionadas,precautoriamente,todas las informaciones sobre Egipto del historiador y viajero griego del siglo V a. C. Entre ellas la relativa a la construcción de las pirámides,que fue sustituida por especulaciones de arqueólogos,egiptólogos y arquitectos,sin mencionar a ubicuos ocultistas,que hallaron en esas moles campo fértil para desplegar su fantasía.
Según el texto de Herodoto,el procedimiento empleado fue el siguiente: " La elevación (de las pirámides) fue graduada regularmente por lo que algunos denominan peldaños y otros altares.
Una vez terminada la primera hilera,los obreros levantaban las piedras hasta la segunda mediante máquinas construidas con piezas cortas de madera ; de la segunda a la tercera,mediante una máquina similar,pues había tantas máquinas como hileras,o bien una misma máquina,fácil de transportar,se instalaba sucesivamente en cada una de ellas,y así hasta llegar a la cúspide.
" En la gran Pirámide de Kufu (Keops en versión griega),la cúspide se elevaba a una altura de 147 metros.
Aunque se trata de la más célebre,mayor y más perfecta de las pirámides,la gran tumba de Kufu estuvo precedida por otras colosales moradas funerarias.
Primeramente la de Zoser (hacia el 2680 a. C.),que consiste en una sucesión de módulos rectangulares de tamaño decreciente,y que por el aspecto que así adquiere ha sido denominada pirámide escalonada.
Mide en su base 109 metros por 122 y se eleva a una altura de 60 metros.
Fue la primera gran construcción de piedra de la historia y es el más antiguo edificio que todavía se conserva.
Ante la falta de evidencias arqueológicas precedentes,durante mucho tiempo se pensó en una súbita y misteriosa emergencia de la civilización egipcia,que procedería de la legendaria Atlántida.
La sensación de que uno se mueve en el vacío y de que todo gira a nuestro alrededor es propia del vértigo.
Esta sensación responde a una pérdida súbita de la sensación de espacio junto con un trastorno en los reflejos de equilibrio y mantenimiento de la postura.
El primero en estudiar las alteraciones como el vértigo y el desequilibrio provocado por la rotación fue el gran biólogo checo Jan Evangelista Purkinje,en 1820.
Este médico atribuyó tales perturbaciones al cerebelo.
Posteriormente otros investigadores,tales como Pierre Florens y Emile Antoine Méniere,establecieron el origen de otros síntomas fisiológicos propios del vértigo.
La altura es una de las causas mas directas de los primeros síntomas de vértigo: pérdida del equilibrio y falta de coordinación,acompañadas de náuseas.
Estas perturbaciones aparecen cuando el individuo está a unos tres metros del suelo y su intensidad no varia cuando se alcanza mayor altura.
Un estudio muy específico realizado recientemente determina que la respuesta fisiológica no depende estrictamente de la altura,sino,y sobre todo,de la distancia de los objetos integrados en el campo visual del individuo.
De este modo,las alteraciones,el vértigo en sí,no se producen por el miedo a caer sino por la visión de un espacio vacío debajo y alrededor del individuo.
Ahora,diversos investigadores,entre ellos el italiano Fausto Baldissera,de la Universidad de Milán,han dado una aplicación más amplia de los mecanismos del vértigo y el mareo,corrigiendo y ampliando muchas viejas creencias sobre el tema.
El equilibrio corporal depende,debido a la postura erecta humana,de un conjunto de pequeños movimientos que tratan de mantener al centro de gravedad del cuerpo ubicado de tal modo que su proyección vertical en el suelo caiga dentro del perímetro de una superficie de apoyo,determinada por los bordes externos de los pies y las líneas que unen los dos dedos gordos y los dos talones.
Cuando la persona está quieta,esta superficie es bastante limitada,lo que exige al cuerpo constantes correcciones que devuelvan el centro de gravedad a una posición compatible con la verticalidad.
De hecho,el centro de gravedad está sujeto a constantes oscilaciones imperceptibles,ya que la tarea de reacomodación es refleja y automática.
El equilibrio responde a la necesidad de mantener una postura erecta con el mínimo de consumo energético,lo cual se consigue como ya se ha dicho corrigiendo hasta la ínfima desviación que se produzca.
La postura correcta se logra manteniendo alineados los segmentos óseos de las extremidades inferiores,la pelvis y las vértebras,de modo tal que el peso del cuerpo sea soportado por el esqueleto y no por los músculos.
Cuando el cuerpo se halla inmóvil y de pie basta con pequeñas correcciones para mantener la posición óptima,pero esta tarea se torna compleja cuando el cuerpo está en movimiento.
En este caso se requiere que entre en acción un sofisticado control nervioso,capaz de corregir las desviaciones apenas se producen,empleando de modo preciso la fuerza durante el tiempo necesario.
Para llevar a cabo esta operación de control y corrección el cerebro recibe información de tres sistemas sensoriales.
El primero corresponde a los receptores vestibulares,que se hallan en la parte media del laberinto óseo,en el oído interno,y cuya misión es indicar la dirección de la fuerza de gravedad,y por tanto el estado de equilibrio.
El segundo sistema corresponde a los receptores que hay en las propias articulaciones - - musculares y cutáneos - -,que tienen la tarea de indicar la posición de los segmentos óseos y la carga a que son sometidos.
El tercero y último sistema que informa al cerebro es la visión.
El sentido de la vista es el que permite las operaciones correctivas más complejas y sutiles.
Para comprenderlo es necesario analizar la oscilación del cuerpo (o simplemente de la cabeza) que siempre provoca un desplazamiento en dirección contraria de las imágenes impresionadas en la retina.
Si después de un movimiento corporal el campo visual rota levemente,el desplazamiento de la imagen genera una respuesta muscular precisa para llevar al cuerpo a la postura anterior.
La importancia de la vista en el mantenimiento del equilibrio corporal ya fue señalada por el neurólogo alemán Thomas Brandt,quien tras una serie de experimentos específicos registró las oscilaciones corporales.
Cuando la agudeza visual disminuye por debajo de la normal,el control y,consecuentemente,la acción reguladora disminuyen proporcionalmente.
El resultado es que las oscilaciones del cuerpo se tornan más frecuentes y amplias,cosa que puede comprobarse poniéndose de pie y cerrando los ojos.
Algunas anomalías oculares,como la miopía,las cataratas o la remoción del cristalino,afectan el control postural del mismo modo que la ceguera,originando en algunos individuos mareos y sensación de vértigo,síntomas que en más de una ocasión han experimentado quienes usan gafas de alta graduación correctiva en el momento de colocárselas o quitárselas.
Esto no quiere decir que la falta de visión provoca la pérdida total del equilibrio,sino que la regulación postural recae exclusivamente en el sistema laberíntico y por tal motivo se torna más lenta.
La imagen cinematográfica más popular del vértigo es la de un hombre tambaleándose en la cornisa de un rascacielos ; desde su óptica todos los demás edificios giran a su alrededor.
Esta imagen da una idea bastante aproximada de lo que es esta disfunción en realidad.
Esto es así porque también el vértigo provocado por la altura es consecuencia de la pérdida de una referencia visual en relación con la postura.
Los movimientos de la imagen fijada en la retina tienen la misión de activar la regulación postural cuando supera una mínima variación.
Además también la distancia del objeto visualizado,dado que mientras más lejos esté dicho objeto del observador,menor será la variación de su imagen en la retina en el momento en que se produce el movimiento de la cabeza.
Normalmente tenemos a nuestro alrededor infinidad de objetos referenciales a muy corta distancia.
Pero si subimos a la terraza de un edificio de varios pisos,la referencia más concreta y amplia,que es la del propio suelo,está lejos,con lo cual se incrementa el grado de las oscilaciones corporales.
Esta es la razón por la que un individuo sobre una cornisa se mueve con inseguridad y trata de aferrarse a cualquier cosa.
Lo mismo le ocurre a infinidad de personas con sólo asomarse a un balcón.
Cuando se produce la sensación de vacío,el individuo también puede experimentar reacciones más viscerales,como las náuseas.
Este malestar,una de las manifestaciones más características del vértigo provocado por la altura,es originado por la contradicción que se produce entre los mensajes laberínticos y los visuales.
Así,mientras el oído interno remite la señal de que el cuerpo oscila,los ojos no perciben los desplazamientos correspondientes del campo visual.
Esta es la causa que origina también los mareos y náuseas cuando se navega o se viaja en coche o autobús y especialmente mientras se lee en tales ocasiones.
Aquí la disfunción se origina porque los movimientos de los vehículos estimulan el laberinto,pero los ojos no perciben tales movimientos al estar fijos en un libro,por ejemplo.
Claro que el malestar también puede originarse al revés,cuando los ojos perciben un movimiento que no detecta el aparato vestibular.
Los actuales conocimientos sobre el vértigo y el mareo dan plena razón a algunos consejos que los hombres de mar más avezados suelen dar a los novatos.
El mejor modo de evitar que las oscilaciones del barco causen trastornos y náuseas es estar en cubierta,de pie y mirando el horizonte: el ver el movimiento de balanceo - - pues el horizonte sube y baja respecto del borde de cubierta - -,contra lo que pudiera pensarse,no aumenta sino que disminuye el malestar,por haber concordancia entre la información visual y la vestibular.
Por las mismas razones,está contraindicado fijar la vista en la lectura,o en el techo de un camarote cerrado.
En fin,que hay que proporcionar información coherente al cerebro.
Y,por supuesto,bloquear hasta donde sea posible el cuadro emocional: ya se sabe que nadie la pasa peor que quien se sube a un barco con temor,convencido de que se va a marear.
También está el hecho de que los investigadores temen sinceramente el hacer predicciones.
Una hoja que cae de un árbol mecida por la brisa,es un fenómeno demasiado complicado para que la Física actual pueda describirlo totalmente.
Los físicos saben que eventualmente la hoja alcanzará el suelo,pero no pueden decir exactamente cuándo,ni dónde.
Predecir el futuro de todo el Universo es una tarea que los científicos prudentes prefieren dejar a astrólogos y adivinos.
Sin embargo,algunos físicos más osados han utilizado la bola de cristal de la ciencia para intentar desentrañar el lóbrego final del Universo.
Y quizá no sean tan esforzados como puede parecerlo.
Los modelos " Big Bang " que describen el nacimiento del Universo son considerablemente más sencillos que los que describen a una hoja cayendo en un día ventoso,y resulta bastante fácil diseñar los acontecimientos futuros que se derivan de ellos.
En realidad,el problema es que esos modelos son tan simplificados que pueden no tener más semejanza con el Universo real que un retrato cubista de Picasso con su modelo.
Cualquier predicción basada en ellos será necesariamente errónea en muchos detalles.
Pero podemos suponer que nuestra teoría,como el retrato de Picasso,expresa correctamente lo fundamental.
También podemos suponer que las leyes físicas válidas hasta ahora,mantendrán indefinidamente su vigencia.
¿Son estas suposiciones demasiado arriesgadas? De ser así,lo que sigue es pura fantasía.
De no serlo,podremos hechar un vistazo al Apocalipsis.
De acuerdo con la Teoría de la Relatividad de Einstein,el modelo estándar de Big Bang tiene dos destinos posibles: la expansión continua o el colapso (ver ALGO de agosto y julio 87,junio 85,abril 84,etc).
La naturaleza de la catástrofe final (si es que realmente nos espera una catástrofe) depende de cuál de esos destinos sea el que suceda.
Si la materia y la radiación superan la densidad crítica de unos 10 - 29 gramos por centímetro cúbico,el Universo es " cerrado " y puede eventualmente contraerse.
De lo contrario es " abierto ",y deberá expandirse eternamente.
La cuenta de las estrellas,galaxias,polvo cósmico y otras cosas visibles,sugiere que la densidad actual es sólo un diez por ciento de la densidad crítica,y por lo tanto el Universo ha de seguir expandiéndose.
Pero como no podemos estar seguros de haber contado todo,ese porcentaje puede ser una subestimación.
Por otra parte,muchos teóricos suscriben el llamado " modelo inflacionario " - - una teoría cosmológica que afirma que el Universo se expandió súbitamente después de nacer,hasta alcanzar un estado que está justo en el límite entre cerrado y abierto - - y por lo tanto piensan que el Universo debe encontrarse casi exactamente en la densidad crítica.
Si es así,todavía puede colapsarse,pero sólo después de un período de tiempo prácticamente infinito.
Consideraremos esta posibilidad como equivalente a la expansión continua,y entenderemos que,si el Universo ha de contraerse,debe comenzar a hacerlo en el " breve " lapso de unos 15 a 20 mil millones de años.
Aceptados estos supuestos.
En un breve artículo escrito en 1969,el astrofísico Martin Rees diseñó el primer esbozo del final de un Universo cerrado.
Actualmente el Universo está poblado de estrellas,galaxias,grupos de galaxias,e incluso supergrupos.
Virgo,la más cercana y conocida aglomeración de galaxias,contiene unas 2.500 de ellas y se encuentra a una distancia de cerca de 20 megaparsecs (unos 65 millones de años-luz).
A niveles cósmicos está bastante próxima,dado que el radio del Universo observable mide alrededor de 5.000 megaparsecs.
Las agrupaciones de galaxias están habitualmente separadas por decenas o centenas de megaparsecs,y abarcan aproximadamente el uno por ciento del total del espacio.
Supongamos que comienza el Big Crunch,es decir el gran colapso.
Una vez que el Universo se ha contraído a un quinto de su radio actual (lo que le tomaría unos 14.000 millones de años si,como se acepta actualmente,su edad actual es de unos 15.000 millones de años),su volumen puede haber decrecido a 1 / 125 del de hoy.
Los grupos de galaxias llenarían el 100 /! lo del espacio,y en consecuencia se habrían fusionado en una sola.
Si hay mil millones de galaxias en el Universo,un astrónomo que viviera en ese tiempo podría ver dos de ellas colisionando cada 100 años,pero la frecuencia de las colisiones crecería rápidamente.
Cuando el radio hubiera decrecido en otro factor de diez,la densidad media del Universo se aproximaría a un átomo por centímetro cúbico,acercándose a la densidad actual de nuestra galaxia.
Aunque a esas alturas el concepto de galaxia habría perdido su significado: los astrónomos ya no verían las familiares galaxias en forma de espiral o de elipse,sino encontrarían estrellas esparcidas en el cielo más o menos uniformemente.
No observarían muchas gigantes o supergigantes,porque la mayoría de ellas habrían ardido tiempo atrás.
Las sobrevivientes serían estrellas de poca masa y vida larga y estable,como las enanas blancas.
Desde luego,podrían formarse unas pocas nuevas estrellas,jóvenes y brillantes.
Pero como las galaxias se habrán fusionado,esas estrellas no estarán ligadas a ninguna " galaxia materna " (como el Sol a la Vía Láctea),y por lo tanto se lanzarán a vagar sin ayuda a través del espacio,como las moléculas de gas en una tetera hirviente.
Una civilización que orbitara una de esas estrellas vagabundas,podría ver su sol destruido por un encuentro cercano con otra estrella errante.
Podemos imaginar a un astrónomo observando horrorizado cómo una enana blanca irrumpe en su sistema solar a una velocidad cercana a la de la luz,dispuesta a desbaratar su sol por medio de fuerzas nucleares.
Obviamente,esa seria la última observación de nuestro astrónomo.
En verdad,esta sería la visión optimista de un escritor de ciencia ficción.
Las estrellas son objetos diminutos en el espacio,y no comenzarían a entrechocar con frecuencia hasta la verdaderamente última etapa del colapso,cuando el Universo sea sólo una mil millonésima-parte de su radio actual.
Para entonces las estrellas casi seguramente ya no existirían.
Es que hasta ahora no hemos tomado en cuenta la radiación de fondo de las microondas cósmicas,o sea la radiación liberada por el Big Bang.
A medida que el Universo se expande,esa radiación se enfría,y actualmente su temperatura es de 2,7 Kelvin (casi tres grados sobre el cero absoluto).
Por el contrario,una vez que el Universo comience a contraerse,la temperatura de la radiación volverá a elevarse.
Dado que esa temperatura es inversamente proporcional al tamaño del Universo,cuando el Cosmos se haya contraído a una milésima parte de su dimensión actual,la radiación alcanzaría los 2.700 Kelvin,aproximándose a la temperatura superficial de una estrella mediana.
Todo el cielo,en cualquier dirección que uno mirase,sería entonces casi tan brillante como el Sol,y la situación general se habría tornado bastante inhóspita.
Una estrella debe radiar desde la superficie la energía generada por el ardiente horno nuclear que guarda en su interior.
Esto se cumple eficazmente cuando el entorno es muy frío,como sucede en estos tiempos.
Pero si sube la temperatura de la radiación de fondo,para continuar despojándose de energía a un nivel apropiado las estrellas deben aumentar su tamaño,o su temperatura superficial,o ambas cosas a la vez.
Podemos imaginar una fase durante la cual las estrellas se hinchan mientras la temperatura exterior aumenta.
Pero esa situación no duraría demasiado.
Una vez que la temperatura de fondo superara la de una estrella,sus estratos superficiales entrarían literalmente en ebullición.
Los electrones de la materia estelar escaparían de las órbitas de sus protones y neutrones,produciéndose un caldo de núcleos atómicos y electrones libres En realidad,las cosas pueden ser aún peores de lo que esta descripción deja suponer.
Cualquier radiación,como la de la luz o las microondas,ejerce una cierta presión.
Cuando la temperatura de fondo de las microondas excede la de la estrella,la presión de la radiación puede ser mayor que la de la superficie estelar,obligando a la estrella a comprimirse gradualmente.
Eventualmente,puede implotar (explotar hacia adentro) o,cuando la radiación de microondas suba otro factor de 1.000,hasta temperaturas de ignición nuclear,el combustible que quedara en la estrella detonaría y ésta se transformaría en una nova o supernova.
Una civilización que habitara un planeta durante este proceso,podría recurrir a refugios subterráneos para eludir el creciente calor de las microondas,pero no tendría salvación a medio plazo.
Ningún deus ex machina podría evitarle la incineración cósmica.
El colapso continuaría probablemente durante otros 100.000 años,después que toda la forma de vida carbónica hubiera resultado ionizada.
En los instantes finales antes del Bing Crunch aparecerían un número creciente de partículas exóticas,exactamente igual que aparecen ahora con cada nueva generación de aceleradores de partículas.
Las últimas fases según este modelo podrían ser diferentes,si el tiempo que transcurre hasta el colapso fuera lo bastante largo para que la mayor parte de las estrellas y galaxias se hubieran convertido en agujeros negros,una posibilidad que analizaremos en detalle más adelante.
Si así fuera,los agujeros negros podrían fusionarse en los últimos segundos del Universo,y tragarse velozmente toda la materia que pudiera quedar todavía por ahí.
En ese caso,el instante final no estaría dominado por la proliferación de partículas exóticas,sino por agujeros negros cada vez más y más grandes,que finalmente engullirían todo el Universo en el momento del Crunch.
Entonces " los hechos se ofuscan ",como diría Gogol,y nadie puede saber qué ocurriría después.
De acuerdo con la Teoría de la Relatividad de Einstein,el Crunch sería una singularidad.
O sea,un estado en el cual la temperatura,la presión y la densidad del Universo serían infinitas.
Y cuando se alcanza la singularidad,se acaba la Historia.
Imposible continuar.
Pero muchos físicos piensan que las singularidades no provienen de una Naturaleza defectuosa,sino de teorías defectuosas,y que la Relatividad de Einstein podría mejorar sensiblemente tras una boda con la Mecánica Cuántica.
Para ser más precisos,se espera a los 10 - 43 segundos antes del Crunch,las fuerzas cuánticas sean tan grandes como las fuerzas gravitatorias que llevan el colapso.
El efecto que esto pueda tener aún no está claro,ya que los teóricos no han conseguido unificar las fuerzas cuánticas y la gravedad en una única teoría que pueda ofrecernos cierta orientación.
Mas es posible que los efectos cuánticos - - que pueden generar presiones negativas - - consigan detener el colapso y hacer que el Universo " rebote " e inicie una nueva expansión.
De ser así,comenzaría un nuevo Big Bang y se iniciaría un nuevo ciclo.
Esa idea de un universo " oscilatorio " no es precisamente nueva.
Hace ya cincuenta años Richard Tolman,del Caltech,mostró que la entropía de un Cosmos en colapso podía ser ligeramente más alta que en la expansión precedente.
Para nuestro propósito,podemos considerar a la entropía como la media del calor desperdiciado en la mayoría de los procesos físicos ; esa energía no es aprovechable,por contraste con la energía " libre ",disponible para su consumo.
En la Teoría de la Relatividad,mayor entropía implica un mayor tanto,si el Universo rebota en el Big Crunch,el nuevo Big Bang empezará con mayor tasa de expansión,y el Universo creado por él tendrá un radio más grande que el Universo que lo precedió.
Cada nuevo Bang producirá universos más y más grandes,ya que en lugar de amortiguar los rebotes,la entropía los amplificaría.
Pero el modelo de Tolman se apoya en por lo menos dos supuestos no justificados.
El primero es que la singularidad del Big Crunch ha sido evitada de alguna forma ; el segundo,que la entropía al final de un ciclo es igual que al comienzo del siguiente.
Hay también modelos de universos que rebotan sin recurrir a la gravedad cuántica,pero no resultan muy realistas en los detalles.
Hasta hoy,todos los modelos razonables de universos cerrados terminan en la singularidad En consecuencia,el Cosmos-Fénix de Tolman quizás no pueda resurgir de esas cenizas.
El tema de la entropía nos lleva directamente al otro destino posible del Universo: el escenario abierto,donde el Cosmos seguirá expandiéndose para siempre,o al menos durante un tiempo muy prolongado.
Aún aceptando que otras energías,tales como la nuclear,lleguen a cubrir parcialmente el déficit progresivo de " combustibles fósiles ",éstos seguirán siendo utilizados,y en mayor medida,durante los próximos decenios.
Al menos para incrementar el rendimiento agrícola,necesario para alimentar a un doble número de personas,será preciso que se emplee más energía Para reemplazar materiales que se están agotando,y otros que habrá que incorporar para el mantenimiento del " segundo planeta ",se requiere también un mayor consumo energético.
El aluminio,por ejemplo,que abunda," tiene el defecto de requerir para su fabricación muchísima energía eléctrica,que es la forma de energía más preciada (y más costosa) que existe sobre la Tierra " Un eventual exceso en el consumo de energía,sin embargo,puede tener consecuencias catastróficas,como el aumento de la temperatura media de la Tierra,que " se encuentra en el momento más delicado de toda la existencia del planeta ".
El mero incremento de un grado centígrado en el conjunto de la masa atmosférica supone un aumento de la temperatura media en las regiones polares de unos 3 o 4 grados.
Retrocedería entonces el límite de los hielos y el nivel de los mares subiría unas decenas de metros,lo suficiente para sumergir a muchas grandes - - y pequeñas - - ciudades costeras,como Nueva York,San Francisco,Londres,Barcelona,y hasta otras alejadas de la costa,como París o Roma.
Si bien la posibilidad es remota," el peligro es real y podría presentarse mucho antes de lo que se piensa,si no se controla la acumulación de anhídrido carbónico en la atmósfera ".
Esta acumulación origina el fenómeno conocido como " efecto invernadero ": el anhídrido carbónico forma una capa en torno del planeta,que impide la dispersión libre en el espacio del calor de la Tierra,con el aumento consiguiente de la temperatura media.
Esa capa no da más calor por la combustión de materiales fósiles.
Además,la mayor pérdida de bosques que hace prever el crecimiento de la población,contribuye a potenciar el peligro: los árboles se " alimentan ",precisamente,de anhídrido carbónico.
Sin ahondar en cuestiones específicas de degradación del medio ambiente,basta el ejemplo mencionado para entender cómo están vinculadas entre sí las distintas necesidades humanas.
" Hasta hace poco tiempo - - señalan Colombo y Turani - - solía pensarse que el abastecimiento de alimentos,la calefacción de las casas y la disponibilidad de ciertos materiales no estaban relacionados.
Hoy,por el contrario,es preciso tomar conciencia de que todos estos factores guardan una profunda relación y que,a su vez,se relacionan con la cuestión mas general del clima " Sobre esta interrelación gravita el omnipresente factor de la economía " Cuando se habla de la escasez de materias primas,conviene distinguir entre escasez física y escasez económica.
Algunas materias primas empiezan a faltar,por lo que deben buscarse sucedáneos o hacer de ellas un uso muy restringido.
Otras siguen estando disponibles,aunque a un coste más alto que en el pasado.
Entre estas últimas se engloban,prácticamente,todas las materias primas conocidas.
Pero habrá que buscarlas en los yacimientos más pobres,menos cómodos,más profundos y mas lejanos a los lugares de empleo.
Serán más caras.
" Puede que el aluminio y el magnesio,dada su abundancia en la Naturaleza y sus propiedades,se empleen también con mayor frecuencia y en ciertos casos sustituyan a materiales que empezarán a escasear.
Pero su procesamiento exige altos consumos de energía por lo que debe disponerse ante todo de energía suficiente,y si ésta proviniera de combustibles fósiles,prever los riesgos de una catástrofe climática Otras materias primas han escaseado siempre como el oro,aunque el posible agotamiento de este metal no supondría graves consecuencias.
No sucede lo mismo con la plata empleada en la fabricación de películas fotográficas y placas radiográficas,dos usos de valor incuestionable en el mundo actual.
Es vital la obtención de sustitutos adecuados.
Más grave aún es el caso del platino,elemento fundamental como catalizador en los procesos de refinación del petróleo y de los productos intermedios de los que se extraen después fertilizantes.
La producción de derivados del petróleo y el futuro de la agricultura,o sea el alimento de la población mundial,dependen de que se halle una solución a la inquietante escasez del noble metal.
En orden de gravedad creciente,aunque a más largo plazo,la disminución de las reservas de fósforo puede acarrear también el colapso agrícola.
Junto al nitrógeno y el potasio,es uno de los tres elementos básicos para la fabricación de abonos.
El fósforo,hasta ahora,parece insustituible,y no puede concebirse en su ausencia la agricultura moderna.
Si es obvio que en el futuro aumentarán los costes de las materias primas,no es menos cierto que deberá enfrentarse un problema de escasez física " Y hay que subrayar la relación - - apuntan Colombo y Turani - - entre la disponibilidad de materiales necesarios para la construcción del segundo planeta con la disponibilidad de energía.
El consumo energético,a lo largo de las últimas décadas,no ha dejado de crecer,y con él la producción de alimentos,que ha aumentado aún más rápidamente que la población.
No obstante,según la ONU,hay en el mundo al menos 1.000 millones de personas mal nutridas,y más de 700 millones en situación de carencia grave de alimento,con detrimento de su desarrollo físico y psíquico.
Y el drama no se debe a una falta real de reservas alimenticias,sino a cuestiones de carácter político y económico.
La producción de alimentos puede aumentar,en el futuro,hasta unas cantidades más que suficientes,pero es probable que esa mayor disponibilidad se consiga mediante una explotación intensiva de los terrenos ya cultivados,y no de la explotación de nuevas zonas cultivables: resulta más barato aumentar la producción en un terreno ya cultivado que preparar para el cultivo una zona hoy desértica una sabana o una selva.
Si se considera el incremento previsto de la población mundial,y se observa que hoy una hectárea de tierra proporciona - - en pro medio - - la alimentación de 2,6 personas,la misma hectárea deberá garantizar,en el año 2000,la supervivencia de 4 personas,y en el 2030 el sustento de 5,2 habitantes de los 8.000 millones que poblarán el planeta.
Será preciso potenciar el uso de maquinaria insecticidas y fertilizantes,y mejorar los sistemas de riego.
En resumen,hará falta gastar más energía Como está comprobado,son los países subdesarrollados o en vías de desarrollo los que muestran un mayor crecimiento demográfico,y así seguirá siendo hasta el año 2030 y aún después.
Las ciudades,allí,se convertirán en megalópolis,y " surgirán otras nuevas en las zonas más adecuadas para la agricultura en razón del clima y de la disponibilidad de agua Las viviendas y las nuevas plantas industriales no tardarán en sustraer los mejores terrenos.
quedarán disponibles para el cultivo las áreas menos confortables,menos fértiles,quizá selváticas o desiertas.
" No es imposible,sin embargo,preparar para el cultivo un área desierta," sólo requiere una inversión de capital que si no existe hoy no existirá mañana " x Las cifras son muy altas.
Por ello el alimento para los nuevos habitantes del planeta será proporcionado por la agricultura de los países ya industrializados,mediante la explotación intensiva de los terrenos ya empleados hoy para el cultivo de productos alimenticios.
Y mediante un mayor consumo de energía En consecuencia hacia el año 2000,el precio de los alimentos aumentará probablemente al doble en términos reales,y de cuatro a cinco veces más hacia el año 2030.
Los países en vías de desarrollo,por lo tanto,dependerán aún más que hasta hoy de los países más ricos para el suministro de alimentos.
No habrá una escasez " física " de alimentos,pero serán mucho más caros y sobre todo un monopolio,en cierto modo,de los países más poderosos económicamente.
La solución,pues,no está solamente en manos de la tecnología.
Nadie podría cuestionar que el futuro que construyamos depende de factores político-económicos,pero no es menos cierto que el progreso científico y tecnológico podría hacer menos sombrías las actuales perspectivas.
Los autores de El segundo planeta,sin embargo,observan que " en nuestros días ya no existe esa esperanza en la tecnología que podía haber hace veinte o treinta años ".
Esta perdida de confianza no es del todo caprichosa,como lo demuestra por ejemplo,la degradación del medio ambiente.
Es cierto que ésta y otras secuelas del uso abundante de tecnología se deben en gran parte al mal uso de la misma o al uso de tecnologías inapropiadas,pero en todo caso han conducido a la implantación de mayores controles y a largos y costosos procesos antes de lanzar un nuevo producto al mercado.
La desconfianza generada por algunos fallos,y el mayor grado de control que fue su consecuencia encareció los costes de los nuevos productos,que antes de entrar en el mercado deben pasar por rigurosas pruebas.
Disminuyeron las innovaciones y el presupuesto destinado a fomentarlas,que se volcó a la investigación de métodos para abaratar los costes de fabricación de productos conocidos.
Decrece en consecuencia la mano de obra y aumenta el paro.
Pero la empresa corre menos riesgos y obtiene más beneficios.
Con no poca soberbia,los hombres modernos,es decir nosotros,echamos mano del hombre de Neandertal como el mejor ejemplo de humanoide grosero,poco inteligente,basto.
Cualquier buen discípulo de Sigmund Freud diría que el hombre moderno,acomplejado por las indefiniciones de su remota infancia,se burla de sus parientes ante la inseguridad de su procedencia.
Pero teorías aparte,los científicos han reivindicado ahora el buen nombre y honor del Neandertal en una reciente conferencia organizada en la Universidad de Lieja.
El acontecimiento fue celebrado para recordar el descubrimiento de los restos de dos hombres de Neandertal hace cien años en Spy,Bélgica.
Estos fósiles belgas fueron en realidad los que convencieron a los investigadores para iniciar estudios serios,ya que muchos creían que aquellos huesos encontrados en 1856 en la pequeña gruta alemana de Neandertal,no podían pertenecer a seres humanos.
Un siglo de estudios - - mucho o poco,según se mire - - hubo que esperar para que antropólogos y paleontólogos,con ayuda de geólogos y físicos,se rindieran ante la evidencia de que el hombre de Neandertal no era ninguna bestia sino que tenía,por el contrario,costumbres más refinadas de lo que se creía tiempo atrás.
Disipadas hace tiempo las dudas sobre si el hombre encontrado en Neandertal era un idiota raquítico y artrítico,o un cosaco de 1814,los científicos se abocaron a revisar su relación con el hombre actual.
Jean Jacques Hublin,investigador de la Universidad de París VI,sostiene que el hombre de Neandertal habría cohabitado,probablemente,con el primer hombre moderno,nuestro antepasado el no menos célebre hombre de CroMagnon.
Pero desconcierta un tanto a los científicos que los restos neandertalianos hayan desaparecido de Europa hace cerca de 40.000 años,antes de la aparición del hombre moderno,es decir,a finales del paleolítico medio,ya que de ese modo sería imposible explicar que hayan coexistido y competido.
Por el contrario,otros estudios de objetos prehistóricos se decantan por la evolución contemporánea de ambos grupos,puesto que algunas constataciones han demostrado una perfecta continuidad entre las industrias del paleolítico medio y las primeras del superior (períodos donde se ubica respectivamente a ambas líneas).
Sin embargo la diversificación de teorías y opiniones,hablan por sí mismas de que aún se está en la búsqueda.
Otros científicos,por ejemplo,afirmaron en la conferencia de Lieja que existió en Europa,durante la misma época,una línea más avanzada,tipo Cro-Magnon,cercana al hombre moderno,y una línea neandertaliana que sobrevivió debido a sus habilidades de presapiens,aunque sus rasgos remarcados,su cabeza redonda,el cuerpo macizo,sus miembros cortos y sus músculos demasiado especializados,demuestran la imposibilidad de que hubiesen dado nacimiento al hombre de Cro-Magnon.
Y aunque los investigadores perdieron el rastro de estos presapiens en Europa,lo encontraron rápidamente en el Oriente Próximo.
Hace 40.000 años existían en Israel dos tipos humanos diferentes: uno era realmente muy cercano al neandertaliano europeo,mientras que el otro tenía rasgos similares al hombre de Cro-Magnon.
Otras conclusiones hablan de la existencia en Europa de preneandertalianos que no evolucionaron hacia el hombre de tipo moderno sino hacia neandertalianos.
Los hombres de Neandertal,de acuerdo con estas ideas,fueron desarrollando sus industrias paralelamente a sus vecinos del Oriente Próximo,en el período que va entre los 80.000 y los 35.000 años antes de nuestra era.
En eso milenios,los hombres de Cro-Magnon fueron progresando hacia el Oeste,hasta que hace 35.000 años se encontraron en Europa Occidental,coincidiendo con la desaparición de los neandertalianos.
Hasta aquí las explicaciones posibles.
Aunque aún no se ha encontrado la causa de la desaparición de los neandertalianos,que continúa siendo la gran incógnita.
¿Hubo un conflicto entre ambos grupos? ¿Quizá una coincidencia en la explotación de los mismos medios de vida? ¿Alguna desventaja genética? A pesar de las dudas,lo cierto es que luego de 35.000 años el hombre de Neandertal ya no navega por la prehistoria como una bestia,lejana,sino como nuestro primo,el que tuvo mala suerte.
El pequeño renacuajo cuya radiografía se ve en la ilustración ha sufrido una mutación por la cual,en vez de cola,tiene un par de patas extra.
Esto no tendría nada de raro si no fuera que un 70 por ciento de los renacuajos de la charca donde fue recogido muestran,también,alguna anormalidad.
Si bien diversos agentes químicos y físicos pueden inducir mutaciones,ninguno de ellos estaba presente en esa charca del condado de Santa Cruz,en California,y además el 70 % es una tasa de monstruosidad realmente excepcional.
El profesor de BUP que descubrió el " yacimiento " - - Stephen B. Ruth,del Monterey Península College - - encontró renacuajos con hasta 8 patas,y envió varios ejemplares al Dr. Stanley Sessions,un biólogo de la Universidad de California (Irvine),en busca de una explicación científica.
El biólogo fue quien midió el altísimo porcentaje de anormalidad (ya que en muchos casos la anormalidad no es aparente,sino que se expresa en modificaciones de órganos internos).
Aunque las primeras sospechas fueron una alteración genética o la polución del agua de la charca con algún agente químico contaminante,Sessions pronto descubrió que la causa eran unos pequeños parásitos,cuyos huevos infestaban masivamente a los ejemplares adultos tanto como a los renacuajos recién nacidos.
Lo que no se ha dilucidado aún es el mecanismo a través del cual esos parásitos producen los cambios en el embrión.
Institutos del CSIC y Construcciones Aeronáuticas (CASA) desarrollan un sistema de robots móviles avanzados de tercera generación (AMR) para la protección y seguridad civil,cuya primera fase ha sido aprobada en la última Reunión de Ministros europeos para el Programa Eureka.
Así mismo,también en España se realizarán nuevos estudios sobre el comportamiento estructural de la Lanzadera Arianne - 5,según establece un acuerdo específico que firmaron recientemente los responsables del CSIC y del Instituto Nacional de Industria (INI).
Este Acuerdo-Marco afectará a todas las empresas del INI y,por otra,a los cerca de 85 Centros e Institutos del CSIC.
Por primera vez en España,la Investigación y a Industria Pública se unen con el objetivo de aumentar la capacidad de desarrollo tecnológico.
Para los aficionados a los aniversarios,he aquí un dato: el pasado 25 de julio cumplió 9 años Louise Brown,la primera niña concebida por el procedimiento de fertilización in vitro (FIV) en Gran Bretaña.
Pero viajar en la memoria hasta 1978 propone un ejercicio mayor que el recordatorio de aquella fecha: también habrá que recordar la ardua polémica que se desató entre partidarios y detractores.
Los científicos deben atender en estos momentos dos frentes de lucha: el externo,con los eternos ataques éticos,documento vaticano incluido,y el interno,donde deben prestar atención a los problemas que presenta la aplicación de este tratamiento.
Los norteamericanos se quejan,sobre todo,de que la FIV es un procedimiento caro,incómodo y que sus porcentajes de éxito aún son bajos,aunque reconocen que es el mejor camino en la solución de la infertilidad.
No se trata de una contradicción,simplemente ponen en la balanza lo negativo y lo positivo y,a pesar de todo,la FIV sigue ganando.
De los tres millones de parejas - - una de cada cinco - - que según el Centro Nacional de Estadísticas Sanitarias de los Estados Unidos puede ser infértil,la mitad de ellas considera la FIV como la mejor y la única manera de concebir un Hijo.
De todos modos no olvidan el camino de dificultades por el que se debe transitar.
En los Estados Unidos son ya 120 las clínicas que ponen en práctica este método de concepción y según la Sociedad Americana de Fertilidad en la mitad de ellas se ha informado de nacimientos simples.
De todos modos los porcentajes de éxito son bajos: sólo cinco clínicas norteamericanas están a la altura de los investigadores británicos y australianos,quienes lograron buenos resultados en el 50 por ciento de sus pacientes.
Pero,además de las desalentadoras cifras de concepción,también presentan dificultades las leyes norteamericanas,muy restrictivas a la hora de reglamentar los experimentos de reproducción con seres humanos.
Asimismo los científicos estadounidenses tampoco lo tienen fácil para lograr continuidad con sus pacientes,puesto que el número de éstos se reduce a las parejas que pueden afrontar el gasto promedio de 6.000 dólares (unas 800 mil pesetas) que implica cada intento de EiIV,incluidos gastos de traslado,implantación y fertilización del óvulo.
Las críticas arrecian contra la organización que rodea la aplicación de la FIV que no contra el propio método.
Así,Lois Barish,miembro de la organización Resolve Inc.,que se ocupa de aconsejar a las parejas infértiles,adjudica la reducida tasa de éxito en los ensayos,entre otras cosas,a que " las clínicas son incapaces de operar durante las 24 horas del día,y esto no es una pega porque las mujeres ovulamos en cualquier momento y no exactamente durante el horario comercial ".
No terminan aquí las dificultades.
La demanda de las parejas infértiles que creen en este procedimiento supera de tal manera la oferta de las clínicas especializadas,que el desborde se traduce en larguísimas listas de espera en las que las parejas deben guardar entre uno y dos años para ser atendidas.
Ante este panorama,quienes han visto una suculenta posibilidad de cubrir el mercado y brindar a la vez un buen servicio son los investigadores de la clínica de fertilización in vitro Australia Ltd.,con sede en Melbourne.
Es posible que esta empresa abra una clínica en Nueva York para la atención de mil parejas al año.
La verdad es que la mayoría de los médicos de Australia Ltd. son estadounidenses que viajaron a Oceanía para aprender las técnicas de FIV en la Universidad Monash de Melbourne,uno de los centros pioneros en el mundo.
Pero el nombre trata de dar " lustre australiano " a la clínica: después de todo el más prestigioso centro de FIV norteamericano - - la Northern Nevada Fertility Clinic,de la ciudad de Reno - - aún no ha logrado superar un 38 % de embarazos,contra el 50 % de los australianos.
Tan solo 800 pandas gigantes sobreviven actualmente en el mundo.
Cerca de 700 viven en estado salvaje,en 12 grandes reservas localizadas en la provincia de Sichuan,en el borde oriental de la meseta del Tibet,en China.
Los otros 100 están distribuidos en zoológicos de todos los continentes.
Las cifras fríamente lanzadas,quizá no signifiquen demasiado.
Pero lo cierto es que la World Wildlife Fund (organización mundial para preservar la vida salvaje animal),ha hecho un llamado de emergencia a favor de los pandas,al igual que el gobierno chino,porque el último censo realizado revela una tendencia a la disminución,cada vez más notable,de estos animales.
Durante la última década,por ejemplo,y a pesar de las medidas proteccionistas,se registró una reducción de 200 ejemplares en la población total.
Sin embargo,- lo más grave es que los científicos de todo el mundo que tratan de buscar los métodos para inventir esa tendencia se enfrentan a problemas muy difíciles de solucionar.
Ya que no se trata sólo de leyes prohibiendo la caza de ese animal,leyes que ya existen y además se cumplen ; otros factores en los que también tiene influencia la presencia humana,aunque en menor medida,son los que preocupan.
Uno de los problemas es la poca " sociabilidad " de los pandas.
Esta tendencia al ostracismo hace que cuando grupos de personas se instalan dentro de sus reservas,ellos se aíslen en lugares más reducidos,en pequeñas comunidades que no superan los 20 ejemplares cada una John Mackinnon,uno de los responsables del proyecto de conservación de los pandas,asegura que en la actualidad existen más de 35 comunidades aisladas con menos de 20 ejemplares cada una.
Este aislamiento también es provocado por la destrucción del hábitat natural del panda: los bosques de bambú.
En opinión de los científicos sólo el 20 % de los bosques son convenientes como sitio natural para estos osos.
Allí los pandas se alimentan de cañas de bambú,tallos y hojas,que sólo crecen a una altura entre los 1800 y 3600 metros sobre el nivel del mar.
La tala indiscriminada de estos bosques impulsa a los pandas a dividirse en grupos pequeños para " buscarse la vida ".
Sin embargo no parece lógico que estos animales tengan dificultades para vivir en grupos reducidos,teniendo en cuenta su carácter tímido y poco sociable.
En realidad los problemas,capaces de provocar un shock demográfico con peligro de extinción,los tienen para reproducirse.
Y es que las colonias reducidas son propensas a cambios accidentales que pueden ocasionar la muerte del único macho o la única hembra del grupo o que toda una nueva generación sea totalmente de un sexo u otro,con la cual la comunidad quedaría condenada a la desaparición.
El doctor Stephen O'Brien,experto en Genética del Instituto del Cáncer de Frederik (Maryland) sostiene asimismo que diferentes cambios genéticos que se reproducen en estas poblaciones pequeñas dejan a sus habitantes indefensos ante una plaga o dificultan su reproducción.
Y si de dificultades para reproducirse se trata,los pandas tienen varias que responden a su propia naturaleza y en la que el hombre,como depredador,no tiene ninguna responsabilidad.
El Dr. George Schaller,zoólogo de la Sociedad Zoológica de Nueva York,dice que las únicas esperanzas de preservar la especie consisten en reunir los grupos aislados de pandas y,por otra parte,en aumentar el número de crías en cautiverio.
Desde luego ambas posibilidades ofrecen grandes problemas,la primera porque el tiempo,la infraestructura y la inversión necesarias son incalculables ; la segunda porque presenta problemas que conciernen a la propia naturaleza de los pandas.
Las hembras entran en celo S LO dos o tres días al año y si en ese tiempo no encuentran un macho,se frustra la procreación.
Como recurso para las que se encuentran en cautiverio,se empleó con cierto éxito la inseminación artificial en algunos zoológicos,pero la realidad indica que los científicos tienen grandes dificultades para saber exactamente el período de ovulación de las hembras,así que las posibilidades de fracaso son altas.
Pero la dura lucha de la especie por adaptarse a este mundo no termina en los inconvenientes para lograr la reproducción: podría decirse que comienzan allí.
Las estadísticas remarcan que de los 51 pandas que nacieron en el zoológico de Pekín entre 1963 y 1983,sólo 19 vivieron más de dos meses.
El esfuerzo por lograr la supervivencia de los pandas llega hasta extremos poco imaginables,como el hábitat diseñado por el doctor William Conway,director de la Sociedad Zoológica de Nueva York.
Conway hizo construir un recinto que él llama Pandaminium,dentro del cual cada panda posee su propia casa y patio,con salida a una " calle " que conecta al resto de casas,como si fuese uno de nuestros Doblados.
Establecer el retrato-robot del pterosaurio y deducir de él las pautas de arranque y despegue del primer vertebrado que levantó el vuelo ha sido una brillante labor de análisis y deducción nada elemental.
Y se debe al paleobiólogo David Unwin.
La gran mayoría de los investigadores abrigaba el convencimiento de que se trataba de un excelente volador,que despegaba batiendo las alas a la vez que corría y daba saltos cada vez más considerables hasta que su poderoso aleteo bastaba para mantenerle en el aire.
Pero,si tal hubiera sido el caso,dedujo el profesor Unwin,no hay duda de que el animal habría tenido que caminar en una postura natural erecta.
Y esto no era lo que le indicaban las dos pelvis que se han hallado recientemente,una de las cuales ha sido clasificada como perteneciente a un Campylognathoides.
Tras el examen de esta pieza,el científico llegó a la conclusión que la creencia de que el pterosaurio estaba bien dotado para el vuelo era un error.
Su conclusión es que la glena es demasiado superficial.
La disposición de este receptáculo óseo destinado a alojar el extremo del fémur,no podía dar lugar a la forma de andar o de volar que caracteriza a las aves,ya que la glena de los animales que se desplazan en postura erguida muestra una cavidad profunda,casi en forma de cuenco.
Pero a juzgar por el escaso alojamiento que la pelvis del pterosaurio k daña a su fémur,éste sólo habría podido describir un ángulo hacia afuera y hacia el frente,ya que la glena es casi plana y carece de conducto para los ligamentos.
Los estudiantes de Medicina,los médicos novatos y las enfermeras se han visto.
más de una vez,frente a un paciente al que parece imposible encontrarle una vena adecuada para una inyección o para darle suero a través de una canalización.
Además de ser,a veces,una causa de pérdidas de tiempo,esta situación puede ocasionar que se lesione el brazo del paciente " buscando vena ",es decir pinchando una y otra vez infructuosamente.
Estos inconvenientes,y las complicaciones derivadas de los mismos - - la peor de ellas las frecuentes flebitis,es decir inflamaciones de la pared venosa interna - - podrían evitarse,según opinión generalizada de los médicos veteranos,con un mejor entrenamiento de estudiantes y enfermeras.
Por supuesto,pese a su impresionante aspecto,no se trata de un brazo humano sino de una vívida imitación en plástico.
En plásticos,habría que decir,ya que la pieza utiliza diversos materiales para mejor imitar las condiciones de la realidad ; la piel,por ejemplo,imita tan bien a la verdadera piel humana que permite palpar en busca de las venas aunque no se las vea.
Desde luego las venas están llenas de un líquido que hace las veces de sangre artificial.
El equipo completo incluye todo lo necesario para aprender a realizar,en el brazo,inyecciones,extracciones y canalizaciones,además de una generosa cantidad de sangre plástica concentrada lista para diluir en agua.
Con una tirada inicial de 13.000 ejemplares,ha salido al mercado la primera revista española en diskette,PCDisc.
Cada revista,que por el momento aparecerá cada dos meses,incluirá 5 programas compatibles con todos los ordenadores personales del mercado y al menos uno de estos programas será de gestión,con lo que el comprador de la revista puede conseguir una biblioteca de programas con un desembolso de sólo 1.500 ptas.
por número.
Asimismo,una sección de Noticias pone al usuario permanentemente al día de las novedades y noticias del sector informático.
La revista PCDisc es una decidida apuesta por la tecnología.
Esta curiosa enumeración es algo desconcertante,pero intenta reflejar la profusa variedad temática del libro,que no deja de ser uno de sus atractivos.
En realidad,la tarea encarada por Coleman y Freec'man ha sido espigar en el corpus actual de la divulgación científica en Psicología (con gran apoyo en la Sociología,la Estadística y la Neurología) para hilvanar una serie de datos y comprobaciones sobre aquellos temas psicológicos más frecuentes en las inquietudes,conversaciones y discusiones de cada día.
En cierto sentido,una especie de " Psicología de la vida cotidiana ",sencilla,amena,actualizada y con cierto tufillo a Reader's Digest,quizá inevitable en el enfoque adoptado por los autores,pero que no reduce su eficacia informativa A continuación ofrecemos una selección comentada de los numerosos temas tratados por Coleman y Freedman,procurando incidir en aquellos más cercanos al público de estas latitudes y mas interesantes para el lector de ALGO.
La palabra sueño tiene dos acepciones,en la Psicología y en el diccionario: el hecho de dormir,y las imágenes,voces y situaciones que se nos representan (¿o nos representamos?) mientras dormimos.
Hoy sabemos que ambas cosas guardan una estrecha relación,que los sueños influyen en el sueño y viceversa según investigaciones recientes o que están actualmente en curso.
El sueño resulta imprescindible para la vida en tanto descanso periódico del cuerpo y la mente,cuya alteración afecta seriamente el equilibrio,la lucidez y la resistencia de las personas,al punto de llegar a producir graves trastornos e incluso la muerte (mantener despierta a la víctima es una forma de tortura refinada y atrozmente eficaz).
Como veremos más adelante,también los sueños parecen cumplir una función necesaria para el equilibrio psicobiológico del ser humano.
¿Cuántas horas es necesario dormir cada noche? ¿Y cuánto tiempo soñamos mientras dormimos? " Para la mayoría de nosotros es suficiente el descanso de siete u ocho horas de sueño - - afirman Coleman y Freedman - -.
No obstante,existen personas que no se ajustan a este período de tiempo: mientras unas emplean menos horas,otras necesitan más para llegar a sentirse bien.
Tanto Edison como Napoleón necesitaban pocas horas de sueño,en cambio Einstein tenía fama de ser muy dormilón ".
Como se ve,el dormir poco o mucho no parece influir en la inteligencia o en el talento,aunque es cierto que quienes duermen menos horas suelen ser más dinámicos y entusiastas,mientras los de mucho dormir se muestran - - siempre generalizando estadísticamente - - más sensibles a las preocupaciones y menos decididos.
De todas formas,tampoco las diferencias de horas de sueño entre unos y otros parecen ser tan extremas.
De acuerdo a recientes investigaciones en individuos de ambos grupos,los " dormilones " en realidad no duermen verdaderamente las nueve o diez horas que pasan en la cama.
Suelen tardar en conciliar el sueño,levantarse durante la noche,o permanecer despiertos un rato antes de levantarse.
Los que aseguran bastarse con cinco o seis horas de sueño,caen dormidos inmediatamente,duermen todo ese tiempo profundamente,y saltan de la cama al despertar.
Por otra parte,no es fácil saber exactamente a qué hora se ha conciliado el sueño,y los períodos de duermevela o entresueño pueden contabilizarse como sueño o vigilia según los casos.
En resumen,lo que estos estudios han comprobado es que el tiempo diario de verdadero sueño oscila,de acuerdo a los individuos,entre seis y ocho horas,con algunas variables.
¿Qué variables? Una de la ; s más importantes es la edad.
Las horas de sueño necesarias disminuyen lenta pero regularmente a lo largo de la vida del individuo,aunque con una " meseta " bastante estable que abarca la juventud y buena parte de la madurez.
Un recién nacido duerme en promedio unas dieciocho horas al día Durante la infancia,el período de sueño se va reduciendo paulatinamente,hasta estabilizarse en las ocho horas de descanso nocturno alrededor de los veinte años.
Ese será el promedio general hasta llegar a las puertas de la ancianidad,en la que puede presentarse una nueva declinación.
El aserto popular de que " dos viejos duermen menos " es real,pero no inexorable.
Los ancianos que se mantienen activos física y mentalmente,necesitan más o menos las mismas horas de sueño que a los 30 o 40 años.
Pero,lamentablemente,todavía son minoría.
Lo común es que el abuelo jubilado sea el primero en levantarse,a menudo con una hora o más de ventaja sobre el resto de la familia En suma,son la inactividad y la vida sedentaria que suelen acompañar a la vejez las que reducen la necesidad de horas de sueño,aunque las estadísticas pueden no tomar en cuenta los " sueñecitos " diurnos que también son característicos de los ancianos La otra variable que puede alterar el ritmo de las horas de descanso es la tensión psíquica y física que la Psicología anglosajona ha denominado como stress.
" Cuando se encuentran sometidas a crisis emocionales cambios de empleo,aumento de actividad mental,o mayor esfuerzo físico,las personas a menudo modifican sus hábitos de sueño - - dicen Coleman y Freedman - - ; a algunas les cuesta conciliar el sueño y dormir con tranquilidad,y otras necesitan más horas de descanso.
El stress emocional,no físico,tiende a provocar un aumento de las horas destinadas a soñar ".
Pero aún sin mucho stress,una persona que llega a los 70 años habrá tenido en su vida aproximadamente unos 150.000 sueños.
La mayoría de ellos los habrá olvidado,incluso en el momento de despertar,y muy pocos responderán a las tradicionales imágenes de monstruos,persecuciones en decorados tenebrosos caídas en el vacío,aventuras delirantes o fantásticas experiencias eróticas,que la literatura o el cine acostumbran presentarnos como escenas oníricas.
" Cuando se despierta aleatoriamente a los voluntarios sometidos a estudios del sueño y se les pide que relaten lo que estaban soñando,las respuestas suelen ser prosaicas - - aseguran los autores citados - -.
La máxima influencia sobre el contenido de los sueños son los sucesos del día anterior,por lo general muy comunes " ¿En qué quedan entonces las teorías sobre la interpretación de los sueños elaboradas por Sigmund Freud y otros miembros de la escuela psicoanalítica? Coleman y Freedman no parecen concederles mucho crédito,ya que sólo las mencionan al pasar,en un alarde de síntesis: " Freud opinaba que cada sueño significaba algo,que los objetos y los sucesos en los sueños eran simbólicos y que además eran la realización de los deseos - - de algo que el soñador ansía que ocurra - -.
Jung pensaba que los sueños representan símbolos básicos de nuestra cultura y también que pueden pronosticar el porvenir de la persona ".
Y acto seguido insisten: " Muchos psicólogos sostienen que los sueños son meramente libres pensamientos relativos a acontecimientos y experiencias de la vida real,a veces distorsionados,pero sin ningún significado especial ".
Las páginas que los autores de Los secretos de la Psicología dedican a los efectos del alcohol,contienen informaciones que pueden resultar muy útiles a los bebedores irredentos.
Por ejemplo,en el siguiente párrafo: " Cuando uno bebe,el alcohol es absorbido por el estómago y el intestino delgado,pasando al cuerpo y al cerebro.
La absorción resulta más lenta y menos completa en el estómago que en el nivel inferior del intestino delgado.
A ello se debe que sea más fácil permanecer sobrio si se bebe durante la comida que si no se toma ningún alimento,pues al llegar éste al estómago retiene allí,durante más tiempo,la bebida.
Por lo tanto,la rapidez con que se llega al estado de ebriedad depende de la cantidad de alimento ingerido con el alcohol.
" Agregan Coleman y Freedman que los licores de alta graduación bebidos sin alimentos son de más rápida absorción,y que ésta se reduce por la simple ingestión de agua,siendo la leche especialmente eficaz.
No obstante,si el agua es gaseosa se acelera la digestión y el alcohol pasa velozmente a través del estómago y llega al intestino,donde se absorberá totalmente y de prisa " A esto se debe - - explican los autores - - que el whisky con agua gaseosa sea una vía más directa a la ebriedad que el whisky con agua natural ".
El malestar y dolor de cabeza que caracterizan a la " resaca " del día siguiente,no se deben propiamente al alcohol sino más bien a otras sustancias,subproductos de la fermentación.
Otro consejo practico es tomar en cuenta que el vodka y la cerveza tienen un nivel muy bajo en estos subproductos mientras que el vino tiene el cuádruple,y son especialmente peligrosos el whisky y el brandy,tanto más cuanto más añejos.
Por otra parte,el grado de trastorno de la persona ebria parece depender del porcentaje de alcohol en el torrente sanguíneo,con lo cual alguien corpulento,con gran cantidad de líquido en el organismo,soportará mejor la misma cantidad de bebida que un individuo delgado y liviano.
Las mujeres también soportan peor la ingestión de alcohol,al tener un mayor porcentaje de grasa corporal (y por lo tanto menos líquido por kilo) lo que hace que la concentración en sangre sea más alta.
Regresando a su tema que era la Psicología Coleman y Freedman señalan que el alcoholismo puede llevar a un serio trastorno mental,conocido como delirium tremens.
Contra lo que suele creerse,este fenómeno alucinatorio no se produce por exceso de alcohol en el organismo,sino por el contrario,a causa de su carencia brusca después de haber bebido en cantidad durante un tiempo prolongado.
O sea que es un típico síndrome de abstinencia lo que popularmente se denomina mona El efecto es muy intenso,pero acostumbra disminuir y desaparecer en unos días.
" Sus características son la excitación extrema,la hiperactividad nerviosa y una clase de alucinaciones espeluznantes,como por ejemplo monstruos terroríficos,invasiones masivas de insectos,o rebaños de elefantes color rosa " describen los autores.
Al parecer,dicho síntomas se deben en parte a que el alcohol produce la depresión de algunos elementos básicos de las células cerebrales especialmente el magnesio.
" El miedo el terror del delirium tremens son tan in tensos,que aproximadamente el 15 % de los afectados mueren por hipertermia,insuficiencia cardíaca o suicidio ",dicen Caleman y Freedman.
No obstante,las actuales terapias y curas antialcohólicas consiguen en la mayor parte de los casos.
examinar o dominar tal síndrome,en la lucha contra otras consecuencias trágicas del alcoholismo como el deterioro cerebral o la cirrosis hepática,para no mencionar accidentes de carretera.
Al introducirse en el terna del sao,Coleman y Freedman comienzan por aventar ciertos prejuicios sobre las diferencias entre ambos (sexos).
La Psicología ha comprobado que las mujeres no son más sugestionables,emotivas,sociables ni volubles que los hombres,y éstos no son necesariamente más inteligentes,independientes,creativos,sensatos o lógicos que las mujeres.
Y la Du Pont,máxima productora mundial de CFCs,manifestó que el apropiado incentivo económico - - creado por dichas regulaciones permitiría a la empresa poner en el mercado,dentro de un plazo de cinco años,sustancias sustitutivas seguras.
Con todo,algunos investigadores afirmaron que la naturaleza del agujero de ozono no eran tan obvia como había parecido en un principio.
En realidad,hacia el mes de abril,lo único cierto acerca del agujero era que realmente existía,y que no era la obra de seres extraterrestres,como sugirió algún periódico.
Pese a la confusión creada en torno del agujero,las negociaciones internacionales sobre el control de los CFCs llegaron a un consenso: significaban una amenaza para la capa de ozono.
Fue en este punto cuando intervino la Administración Reagan,cuestionando la posible acción del gobierno sin una sólida evidencia científica.
Y la historia comenzó a repetirse.
Mientras el Departamento de Estado y la Agencia para el Medio Ambiente,sin mencionar a científicos y ambientalistas celebraban los progresos en la reunión de Ginebra como un hito en la marcha de la cooperación internacional,miembros de la administración reunidos a puerta cerrada sugerían - - según las mismas fuentes - - " protección personal ".
Se aseguró que la administración llegó a planear una campaña de relaciones públicas para estimular el uso de gafas,lociones y sombreros para protegerse del sol,aconsejando la permanencia en la sombra.
El efecto de los CFCs sobre la capa de ozono ha sido un tema zarandeado desde hace trece años,cuando Molina y Rowland advirtieron del peligro.
Los CFCs eran hasta tal punto químicamente inertes - - señalaron los dos científicos - -,que una vez liberados en la atmósfera nada podría impedir su ascenso hasta la estratosfera.
Allí serían descompuestos por la luz ultravioleta,quedando libre el cloro: una mala noticia para el ozono (1).
Un átomo de cloro no sólo reaccionaría con una molécula de ozono,convirtiéndola en la molécula y el átomo de oxígeno de los que proviene,sino que la reacción sería también catártica,o sea que el cloro sobreviviría para repetirla con decenas de miles de moléculas de ozono.
Rowland y Molina calcularon que en el término de un siglo la tasa de destrucción de ozono se duplicaría,y que la capa de ozono disminuiría entre un siete y un trece por ciento.
Señalaron también los consiguientes problemas de salud (2): cáncer de piel y cataratas en particular,además de perjuicios para las cosechas y sabe Dios cuántas cosas más,puesto que la capa de ozono protege los seres vivos de la luz ultravioleta,y una capa de ozono adelgazada significaría más luz ultravioleta.
Pero esto no era todo: si el promedio de vida en la atmósfera de los CFCs es de cien o más años,ya se ha liberado la suficiente cantidad para que el cloro siga fagocitando el ozono durante buena parte del siglo XXI,aún si la producción fuera detenida inmediatamente.
También determinaron ambos científicos,por extrapolación,que si frenásemos hoy mismo la producción,el nivel de cloro en la atmósfera seguiría elevándose hasta alcanzar seis veces su actual concentración,antes de estabilizarse.
Los CFCs,cuando fueron ideados en 1928,parecieron sustancias químicas perfectas: inodoros,no tóxicos,no inflamables y químicamente inertes resultaban extraordinariamente versátiles,y según se pensó entonces,extraordinariamente seguros.
Se los empleó como refrigerantes en acondicionadores de aire y frigoríficos,como gas inerte en la manufactura de espuma (para cajas de huevos y comidas rápidas,aislamientos y cojines,entre otros usos),y en particular como propelentes de aerosoles.
Hacia 1974,esprays para el cabello,desodorantes y otros productos de la misma línea constituían más el 50 por ciento del mercado de CFCs.
Cuando Rowland y Molina sugirieron que los CFCs podrían prohibirse,se entabló oficialmente la batalla de los esprays en la Guerra del Ozono.
Una industria que barajaba cifras multimillonarias no iba a rendirse sin entrar en combate,y el problema no tardó en embrollarse.
Los industriales basaron su defensa en un afán por evitar los peligros de " actuar precipitadamente ",frase que apareció a menudo en notas de prensa.
Ello podría ocurrir - - se advertía al público - - si el Congreso decidía actuar basándose en la teoría de Rowland y Molina,que no era más que eso: una teoría.
En consecuencia,no se podían tomar medidas ponderadas hasta dar por concluida la investigación,plazo que siempre parecía distar de tres a cinco años.
Dado que los científicos no anunciaban peligros inmediatos a causa de los CFCs,argumentaban los industriales - - poniendo énfasis en lo inmediato - -,los legisladores debían esperar que la evidencia científica fuese indiscutible.
El mismo absurdo argumento que se ha aplicado a la lluvia ácida.
La argucia,sin embargo,fue eficaz.
Y más aún: puso a los científicos a la defensiva.
Por una parte,se vieron acusados de actuar como defensores del medio ambiente,no como científicos.
Por la otra,fueron totalmente incapaces de convencer a los industriales - - y se diría que a muy poca gente - - de que los CFCs eran peligrosos.
Los lobbys industriales se negaron a aceptar nada que no fuese una señal inequívoca,y los científicos tengan buenas razones para creer que tampoco la aceptarían.
Habían probado en el laboratorio que el cloro era una amenaza para el ozono,pero la Du Pont,sobre todo,no aceptó tales pruebas como evidencia suficiente de una disminución del ozono.
La controversia declinó a mediados de 1977,cuando la Food and Drug Administration (FDA) y la Environmental Protection Agency (EPA) se unieron para anunciar la prohibición del uso de CFCs en aerosoles desde agosto de 1978.
Los organismos oficiales habían planeado la medida sólo como un primer paso,pero a comienzos de los 80 disminuía la producción de CFCs y comenzaba la administración Reagan,por lo que fueron pospuestas otras acciones reguladoras.
Para los científicos,aunque no lo supieron en su momento,la prohibición fue un caso clásico de " ganar la batalla y perder al guerra ".
La industria consiguió hallar otros usos que hoy lanzan a la atmósfera tantos CFCs como antes.
En buena parte se trata de productos cuyo empleo comenzó a desarrollarse,precavidamente,cuando se conoció la teoría de Rowland y Molina.
A medida que pasaba el tiempo,parecía más difícil determinar hasta que punto se trataba de un peligro inminente.
La química y la dinámica de la atmósfera son complejas: una caótica y polígama interacción entre la atmósfera,los océanos,el sol y el planeta mismo,sin mencionar al hombre.
Y cuanto más aprendían los científicos sobre la atmósfera,más complicada se tornaba la ciencia.
En la década del 30,se conocían cuatro reacciones químicas vitales en la alta atmósfera.
Medio siglo más tarde,la cifra se había elevado a centenares.
Sólo a partir del estallido de la Guerra del Ozono,sin embargo,los científicos han desarrollado lo que puede llamarse " el gran cuadro ".
Según Michael McElroy,jefe del departamento de ciencias planetarias y de la Tierra de la Universidad de Harvard," Aprendimos que no sólo el ozono,sino la composición de la atmósfera,en su conjunto,estaba experimentando un cambio: cambiaba por razones naturales,en virtud de tensiones,a causa de vertidos industriales,cambiaba incluso por razones consideradas hasta ahora m$ bien propicias ".
Los cambios influían - - junto a los CFCs,que estaban alcanzando niveles peligrosos en la estratosfera,como no dejaban de subrayar los investigadores - - un incremento del metano y del dióxido de carbono,así como del óxido nitroso,generalmente conocido como gas hilarante,del óxido de nitrógeno,monóxido de carbono,fluoruro de hidrógeno,y otra treintena de gases raros.
Y cada uno de ellos,aún cuando pudiera existir en concentraciones de partes por billón en la estratosfera - - lo que se ha comparado con una sola gota en un millar de camiones cisterna - - causa su propio y único efecto en la química de la capa de ozono.
Pero toda esta ciencia,lamentablemente,se veía envuelta en un problema político.
Con cada nueva pieza de información,cada nueva sustancia química deducida de ecuaciones científicas,cada nueva tasa de reacción medida en el laboratorio,los esmeradamente construidos modelos computarizados de la atmósfera tenían que ser rehechos.
Los mismo que las predicciones sobre el daño para el medio ambiente.
Todo comenzó en 1971 con el Programa de Evaluación del Impacto Climático,encaminado a determinar los peligros que podría acarrear una flota de 500 aviones supersónicos,como medida previa para desarollar el proyecto.
El estudio llevó a la conclusión de que una flota semejante reduciría,entre un 10 y un 20 por ciento,la densidad de la capa de ozono.
Una verdadera catástrofe.
Pero el trabajo,que se resumió en un informe de 22 páginas,llegó a través de esta síntesis a una misteriosa deducción: la flota entonces programada - - algunos Concorde europeos y TU 144 soviéticos - - producirían a lo sumo insignificantes " efectos en el clima ".
No era éste,sin embargo,el problema Las palabras ozono,radiación ultravioleta,efectos biológicos,cáncer de piel,que si eran el problema,no aparecían por ninguna parte en las " principales conclusiones científicas " del resumen.
En 1976,dos años después de la advertencia de Rowland y Molina,la Academia Nacional de Ciencias norteamericana publicó un estudio sobre los nocivos efectos de los CFCs.
Preveía que estas sustancias podrían adelgazar la capa de ozono hasta un siete por ciento.
También advertía el informe contra las restricciones inmediatas de los CFCs,pues - - señalaba - - no se perdería mucho por esperar un par de años.
El Grumman TMAP (Teleoperated Mobile Antiarmor Platform) ha sido seleccionado por el Sandia National Laboratories,estadounidense,para el desarrollo de un nuevo concepto de lucha anticarro.
La plataforma movil,teledirigida por tan sólo un soldado,consiste en un programa que en un plazo de catorce meses deberá desarrollar un pequeño y ágil sistema de armas capaz de hacer frente a los carros de combate,ganando una rápida posición de ataque.
Más pequeño aún que los vehículos empleados por los golfistas para trasladarse por el campo,medirá aproximadamente 1,82 metros de largo por 1,21 metros de ancho por 1,37 metros de alto.
El cuerpo,fabricado a base de materiales compuestos,será montado en un chasis de cuatro ruedas,ofreciendo una inusual configuración en diamante: una rueda en el frente,otra en la trasera y un par de ellas en la mitad del eje.
La disposición de las ruedas y la articulación del vehículo le permitirán maniobrar en terrenos abruptos,dándole estabilidad a la plataforma de tiro.
El soldado operador del TMAP usará una unidad de control portátil transportada como una mochila.
Desde una posición reguardada,el infante guiará al vehículo a través de un cable de fibra óptica y un enlace de comunicaciones por radio.
Una vez que un blanco haya sido seleccionado,el soldado dirigirá al TMAP a una posición adecuada,lanzando el ataque.
Preocupados por los males urgentes que nos asolan a diario (guerra,SIDA,declaración de la renta),no nos enteramos de otros problemas que,en poco tiempo más,tal vez los próximos 50 o 70 años,se transformen en verdaderas pesadillas mundiales.
Un ejemplo ; está previsto que a mediados del siglo que viene se habrá agotado el 80 por ciento de las reservas mundiales de petróleo.
Es decir,un " cierre del grifo ",como aquél con el que han amenazado tantas veces los árabes,pero definitivo.
Aunque si el mundo,como dicen,es de los previsores,los europeos podemos reducir nuestras preocupaciones.
La Comunidad Económica Europea demostró una visión positiva del futuro al comenzar las investigaciones sobre fusión nuclear hace ya una década,confiando en esa nueva fuente de energía alternativa como una opción de reemplazo del desaparecido " oro negro ".
Para muestra baste la comparación: la energía producida por un gramo de combustible utilizado en la fusión nuclear (deuteriotritio) es igual a la generada por la combustión de más de 10.000 litros de petróleo.
A mitad de los años setenta,un programa europeo de investigación creó el diseño y comenzó la construcción del JET (Joint European Torus),el primer proyecto mundial de máquina experimental en el sector de la fusión.
La iniciativa transitó un camino rápido y eficaz,impulsado por numerosos laboratorios de todos los países comunitarios.
Los avances pueden calificarse de espectaculares.
El JET instalado en Culham,Gran Bretana,ha logrado elevar la temperatura del plasma donde se produce la fusión hasta superar los 100 millones de grados.
Otras tres máquinas similares están en construcción en Cadereche (Francia),Garching (Alemania) y Frascati (Italia).
Sin embargo,no contentos con estos resultados auspiciosos,los científicos europeos están ideando el continuador del JET,el NET (Next European Torus) que,probablemente sea diseñado entre 1989 y 1990.
Esto no es más que una pieza en el nuevo programa comunitario para la fusión,que abarcará un período de cinco años (1987 - 1991),con un presupuesto aproximado de 150.000 millones de pesetas.
La construcción de un reactor de fusión queda un poco más lejana,posiblemente se concrete a mitad de los noventa,con la perspectiva de que,debido al elevado coste que supone su montaje,el primitivo proyecto europeo se convertiría en un amplio programa de cooperación mundial.
La tradición popular relaciona las bebidas alcohólicas con la alegría de las celebraciones y la desagradable pero inevitable resaca que llega después.
Sólo eso.
Los científicos,caminando por otro sendero,se preocupan por los perjuicios que ocasiona el alcohol.
Después de las últimas investigaciones realizadas por dos grupos científicos norteamericanos,los médicos tienen un nuevo argumento a favor: el consumo de bebidas alcohólicas,aunque moderado,aumenta el riesgo de cáncer de mamas.
Si bien algunos muestreos " retrospectivos " habían hecho responder a mujeres con cáncer de pecho acerca de sus anteriores hábitos de bebida,comparándolos con la evolución de su enfermedad,estas últimas investigaciones han confirmado la relación Los estudios fueron realizados ahora por el equipo del Instituto Nacional del Cáncer de Estados Unidos,dirigido por el Dr Arthur Schatzkin y por un grupo de la Escuela de Salud Pública de Harvard,liderado por el Dr. Walter C. Wilett.
El primero de ellos reconoció a más de 7.000 mujeres entre 24 y 75 años,mientras que el segundo realizó un seguimiento con cerca de 90.000 mujeres entre los 34 los 69 años.
Como conclusión final ambos estudios coinciden en que las mujeres que beben son más propensas a desarrollar cánceres de pecho y que el riesgo es directamente proporcional al alcohol consumido.
Estos estudios fueron realizados teniendo como referencia a mujeres con hábitos moderados de bebida y de acuerdo con esas costumbres han podido formular una tabla que relaciona las cantidades de alcohol consumido con el porcentaje de riesgo de contraer cáncer.
Asi es que las mujeres que consumen hasta dos copas semanales sufren un aumento de posibilidades en enfermarse ; las que beben de 3 a 9 copas por semana tienen un 30 por ciento más de riesgo,mientras que las que beben 9 o más copas aumentan ese peligro en un 60 por ciento.
A los efectos de esta tabla,realizada en base a los resultados de los estudios norteamericanos,una " copa " es equivalente a una lata de cerveza (330 gramos) ; un vaso de vino (112 gramos) o una medida de whisky o coñac (45 cl.
) Los doctores Schatzkin y Willett,a pesar de las evidentes respuestas que han dado sus encuestas de investigación aún no han probado cuáles son específicamente los cambios cancerigenos que el alcohol provoca en el pecho femenino.
Pero desde luego aconsejan dejar de beber a la vista de las probabilidades y del reto de enfermedades que el alcohol ocasiona,desde alta.
presión sanguínea hasta las típicas enfermedades del hígado.
Pero fundamentalmente porque el cáncer de pecho producirá,este año y en EE. UU,la muerte de 41.000 mujeres.
Puesto que las mujeres,a la hora de elegir un método contraceptivo,se dejan llevar por el consejo de su médico,algunos investigadores norteamericanos se preguntaron,recientemente,qué métodos anticonceptivos utilizan las mujeres médicas (esto es,no las esposas de los médicos sino las doctoras).
El estudio abarcó a la mayor parte de las médicas de la región cercana a la ciudad de Chicago,y comprendía a aquellas que tenían entre 25 y 45 años,no importa si solteras o casadas,tuvieran una vida sexual activa y no quisieran tener un hijo en ese momento.
De ellas contestaron a la encuesta 235 médicas.
Los resultados,por demás interesantes,indican que sólo un 10 % utilizan píldoras contraceptivas ; 14 70 tienen colocado un DIU (dispositivo intrauterino,o " espiral ") ; 32 10 utilizan lo que en la jerga médica se llaman " barreras locales " (preservativos,diafragmas o gel espermicida) ; 23 10 se han sometido a una operación de ligadura de trompas ; 80 % utilizan el método de abstinencia periódica y un 13 70 no utilizan ningún método,dejando la cosa librada a la buena de Dios.
Estos resultados no son demasiado discrepantes de los que utilizan corrientemente las mujeres norteamericanas no médicas: 33 70 de píldoras ; 7,5 70 de DIU ; 26 70 de barreras locales ; 12 70 de ligaduras ; 11,5 70 de abstinencia y 10 70 que no utiliza ningún método.
La primera conclusión que se obtiene es que las médicas recomiendan a sus pacientes los mismos métodos que utilizan ellas,lo cual indica la aplicación del principio moral de no proponer a los demás lo que no se considera adecuado para uno mismo.
Aquello de " en casa de herrero.
. " no se aplica en este caso.
El estudio no mostró tampoco diferencias entre las ginecólogas y las que practican otras especialidades.
En cuanto a la comparación con el grueso de la población,lo más significativo es el mayor uso del DIU y de la ligadura y el menor uso de la píldora.
Razones probables: el DIU presenta un cierto riesgo de infección que las médicas se sienten capacitadas para vigilar y controlar en si mismas,y tienen más fácil acceso a una operación de ligadura gratuita o en manos de un colega amigo de toda confianza.
Además,las médicas conocen mejor ciertos efectos secundarios de la píldora y tratan de evitarlos.
Lo asombroso: quizás a causa de un nivel económico más alto,son más las médicas que juegan a la ruleta rusa prescindiendo de todo control: 13 70 contra 10 70 de la población común.
Pensarán que si viene un hijo ya tendrán medios para criarlos.
Superada cierta edad madura,los norteamericanos tienden a variar sus intereses personales: la música empieza a aminorar de ritmo,los seguros de vida son más completos (con pólizas suculentas) y las dietas indiscriminadas son recortadas ante el fantasma que asoma en el horizonte: el colesterol.
La preocupación denodada por conocer los niveles de colesterol,como si se tratase de las cotizaciones fluctuantes del yen y el dólar,tiene su razón de ser.
Cada año 540.000 norteamericanos mueren de ataques al corazón producidos por la obstrucción de sus arterias coronarias con depósitos de grasas ; mientras que otros seis millones de personas padecen de insuficiencias coronarias que pueden producirles la muerte.
Para muchos de ellos,a pesar de la esperanza diaria sobre las nuevas investigaciones,la irreversibilidad de la arterosclerosis parecía un hecho consumado.
Sin embargo,un estudio elaborado por el equipo de investigación de la Universidad del Sur de California,encabezado por el doctor David Blankenhorn,ha abierto recientemente la puerta del optimismo: la demostración científica permitió comprobar que el proceso causante de la insuficiencia coronaria,puede revertirse combinando cierta dieta con algunos medicamentos.
Algunos investigadores pudieron verificar un efecto similar en prácticas con cobayas pero aun no habían arribado a la conclusión,a la prueba concreta evidenciada por los científicos de la Universidad del Sur de California: que la reducción de los niveles de colesterol en la sangre provocan una disminución de la oclusión coronaria,enfermedad motivada por la concentración de depósitos de grasa sobre las paredes de las arterias.
Las pruebas que permitieron tan esperanzadora conclusión consistieron en examinar a 162 hombres cuyas edades fluctuaban entre los 40 y los 59 años.
Todos ellos padecían de insuficiencias coronarias.
Se los dividió en dos grupos que fueron sometidos a tratamientos y dietas diferentes.
El grupo tratado con la dieta más estricta (125 miligramos de colesterol diario y un nivel de grasas que no superaba el 22 por ciento de las calorías totales ingeridas),al cabo de dos años obtuvo una mejora notable evidenciada en una disminución de la obstrucción de las arterias en el 16 por ciento de los casos tratados.
Queda claro,y asi lo expusieron los médicos de la Universidad del Sur de California,que no es ésta la solución definitiva.
El sufrimiento seguirá persiguiendo a quienes tienen problemas en las coronarias,salvo que ahora con excelentes perspectivas de superarlos durante largo tiempo.
Este mensaje esperanzador no es sólo dialéctica: Blankenhorn sostuvo que gracias a este descubrimiento las operaciones de by-pass tendrán mayor aguante y que cada paciente,sometiéndose a una dieta adecuada y tomando drogas que favorezcan la reducción de los niveles de colesterol,puede disminuir los riesgos fatales.
Si un hombre muerde a un perro es noticia periodística,pero si aúlla a los lobos es noticia científica.
Y éste es el caso del etólogo norteamericano Alfred Harrington,quien investigando las implicaciones del famoso lamento nocturno de estas fieras,puso en juego su propia capacidad de imitación.
Los lobos grises de Norteamérica le escucharon y,según explicó,fueron ellos los primeros en sacar conclusiones.
Dedujeron,por el timbre de su voz,que se trataba de un congénere agresivo y muy grande.
Y en consecuencia se callaron y se mantuvieron alejados del lugar en donde se ocultó para realizar el estudio.
Sólo unos cuantos se acercaron hasta unos cincuenta metros de él,pero al no poder verlo retrocedieron.
Harrington observó que éstos poseían voces más profundas y ásperas que los que permanecieron alejados.
El aullido que repetidamente emitiera el etólogo había resultado de tono más grave que el de la mayoría de los lobos,y éstos interpretaron el hecho,de acuerdo con un patrón de la especie,como signo de superioridad física y disposición al ataque,por lo que únicamente se aproximaron los que creían estar capacitados para oponérsele por poseer tonos aún más bajos.
Y es que los mamíferos que poseen cuerdas vocales vibrantes tienden a relacionar tesitura con status.
Y el tono emitido por un lobo conlleva el anuncio de su tamaño y de su potencia combativa.
Así,cuanto más grande sea el animal,más grave será su aullido.
Lo cual,ciertamente es una característica que tampoco ha sido ajena a la humanidad.
Por otra parte,cuando el lobo aúlla puede hacerlo por diversas razones.
Con frecuencia lo que desea es ponerse en contacto con otro lobo,especifica Harrington,para lo cual emitirá un tono entre medio y bajo.
Y también pudiera desear comunicar temor,a la vez que la súplica de no ser atacado.
Harrington recuerda que " en una ocasión lancé un profundo aullido a un lobo que de inmediato me respondió con un inequívoco temblor de voz.
Entonces opté por aullar en tono algo más agudo y trémulo,y la respuesta del otro se volvió de inmediato más confiada y firme ".
Y agrega el etólogo que tampoco necesita el lobo recurrir a su tono más grave para pregonar que es capaz de cuidar de sí mismo.
En síntesis,en la voz del lobo va implícita su personalidad.
Es un auténtico documento de identidad que posee y muestra cada ejemplar.
Y si aúlla,será porque lo que debe comunicar merece la gravedad del tono.
Quizá sea posible matizar,ya que no borrar,el convencimiento tan generalizado de que correr es la forma más rápida y segura de llegar a la condición física ideal.
Hacer " jooging ",es decir,correr enfundado en un chandal para propiciar la sudoración,es,antes que cualquier otra cosa,respirar.
Y respirar con bastante mayor fuerza,profundidad y frecuencia.
Pero,¿respirar qué?... ¿Y desde cuándo? Estos interrogantes,que ningún corredor se ha planteado han sido recientemente tema de estudio en los Estados Unidos.
La investigación ha corrido a cargo del Laboratorio Nacional de Nuevo México,donde se expuso a un grupo de ratas a emanaciones de bióxido de nitrógeno.
A continuación se puso en reposo a la mitad de esas ratas,en tanto que a las demás se les sujetó a una carrera regularmente agitada.
Los toxicólogos especializados en la especialidad pulmonar,Douglas Stavert y Bruce Lehnert,comprobaron que el nivel del daño pulmonar aumenta precisamente después de haberse ejercitado.
Las ratas inhalaron 100 partes por millón de bióxido de nitrógeno,pero las que hicieron ejercicio antes de que hubieran transcurrido 24 horas,resultaron cinco veces más afectadas.
El propósito del experimento no era sumar una nueva moraleja a las muchas que ya se han lanzado,sino mostrar hechos concluyentes.
Hechos como el de que un fumador inhala,por lo menos,200 partes por millón de bióxido de nitrógeno en cada " calada ",así que deben considerar muy seriamente s aún quieren hacerse la ilusión de ir a mejorar su condición física poniéndose a correr.
Lo mismo ocurre con los bomberos veteranos que se han visto expuestos a incendios.
dos ellos no deben hacer ejercicio.
En general,todo el mundo debiera saber qué va a respirar cuando se pone un chandal: sería,sin lugar a dudas,la forma más efectiva de saber si la ilusión se convertirá en realidad.
La pregunta puede parecer poco seria,incluso de pésimo gusto y hasta macabra,pero,desde el punto de vista científico se impone como una necesidad de máxima urgencia: ¿de qué mueren realmente los pasajeros de un avión que se estrella o incendia? La respuesta resulta asombrosa y más que reveladora.
En efecto,habiendo alcanzado un alto nivel en la prevención de las muertes a causa del impacto,el ochenta por ciento de las víctimas en este tipo de accidentes aéreos muere en cuestión de segundos no a causa de las llamas,sino por el espeso y venenoso humo que se produce cuando el fuego entra en contacto con los materiales que componen paredes,techos y asientos.
En especial de estos últimos,debido al poliuretano de los cojines ; la tapicería sintética que los recubre se derrite con mucha facilidad y las llamas se transforman en gas mortal en,apenas,una fracción de segundo.
De todas formas,pensar en un cambio de materiales en todas las flotas aéreas de todas las líneas mundiales fue hasta hoy en un propósito inalcanzable por múltiples razones técnicas.
Sin embargo,al fin se ha hallado una solución que permitirá ofrecer un alto nivel de seguridad a los pasajeros.
Se trata de algodón.
Pero,ciertamente,no un algodón cualquiera sino del género X-Ogen.
El X-Ogen es una tela de algodón de una muy particular densidad,dotada con fibras tan resistentes al fuego que soportan la llama de un soplete (2.700 C) por espacio de varios minutos sin llegar a encenderse.
Como lógica e inmediata consecuencia,la casi totalidad de líneas aéreas mundiales se ha decidido a borrar para siempre el peligro que entrañaban los asientos.
Por eso,en adelante,adoptarán una tapicería compuesta de algodón combinado con fibras sintéticas,aplicado sobre una densa capa de material especial (Kevlar) para bloquear el avance del fuego.
Semejante recubrimiento supondrá inevitablemente un aumento de cien mil litros de combustible para una aeronave de 150 pasajeros,pero nadie duda de que las ventajas en todos los órdenes serán dignas de elogio.
No han sido pocos los que se han visto sorprendidos por las conclusiones presentadas el pasado mes de julio en Barcelona por un grupo de científicos de todo el mundo reunidos bajo la presidencia,siempre destacable en nuestro país,de Joan Oró.
El fin de esta reunión no era otro que el de revisar las teorías vigentes acerca del origen de la vida,bajo la nueva óptica de los datos obtenidos sobre el cometa Halley en su último paso por las cercanías de la Tierra,y que venían a completar de manera fehaciente los ya disponibles sobre otros meteoritos y cometas,sobre Marte y sobre las nubes de materia interestelar Sin embargo,lo que ahora comienza a aceptarse como un hecho probado no constituye,ni mucho menos,una novedad.
Fue el americano Arrhenius,Premio Nobel de Química en 1903,quien ideó el término " Panspermia " para definir su propia teoría,según la cual la vida germinó en nuestro planeta gracias a una lluvia de esporas bacterianas procedentes del espacio exterior.
Desde entonces,han sido muchos los científicos que,no sin el desprecio de gran parte de sus colegas,han defendido esta tesis que,entre otras cosaS suponía una clara matización a la teoría aceptada de la " Sopa primordial ",formulada por Oparin,sobre la que Darwin fundamentó los preludios de su Teoría de la Evolución.
Entre estos detractores,que se sumaban así a las correcciones de los neodarwinistas,pueden encontrarse nombres tan importantes como Fred Hoyle,Francis Crick (Premio Nobel de Biología en 1963),Chandra Wickramasingue,Jaworsky,Pflug y John Crontn.
Entre sus argumentaciones en contra de las tesis darwinistas,destaca la de la imposibilidad de que la Teoría de la Evolución explicase la aparición de las más de 200.000 cadenas de aminoácidos perfectamente ordenadas de las que depende la vida.
Y los acontecimientos han venido a dar la razón a este nuevo planteamiento que ha permanecido invariable desde Arrhenius,aunque en ocasiones se han llegado a aventurar tesis como la de Francis Crick,según el cual en la Tierra habría reemergido una inteligencia que,en otra parte del espacio interestelar,se encontró ante una catástrofe ambiental de dimensiones cósmicas.
Esta inteligencia,para salvarse,se habría dividido en elementos básicos que,dispersados a través del espacial encontraron en la Tierra un ambiente adecuado para su desarrollo.
Francis Crick atribuía asi el protagonismo de las bacterias en la teoría panspermica,argumentando que el código genético de todos los seres vivos,incluidas las especies ya desaparecidas,es el mismo De este modo,serían las bacterias las portadoras de la información básica para propiciar la aparición de la vida,aunque quedaba por explicar como estas formas primigenias de vida latente podían viajar por el espacio sin dejar de mantenerse activas.
Poco a poco,los datos han ido cubriendo esta laguna,dando la razón a las tesis fundamentales de la panspermia: se han localizado bacterias en el interior de reactores nucleares ; algunos tipos han resistido inmutables a los rayos X,a los gamma y a las radiaciones cósmicas ; se han descubierto también en la estratosfera bacterias de características muy diferentes a las que pueden encontrarse entre los 10 billones de toneladas de estos microorganismos que pueblan la Tierra.
Así mismo Pflug como John Cronin,descubrió bacterias fósiles en los les tos de meteoritos caídos sobre la superficie de nuestro planeta.
Sin embargo,lo que realmente obligó a los biólogos a empezar a tomarse en serio esta teoría fue el descubrimiento de moléculas orgánicas complejas en las nubes de materia interestelar: formaldehídos y alcohoL En efecto,este feliz hallazgo resultó fundamental,ya que clarificó definitivamente.
Supongamos que el hombre que ha sido despertado por la onda expansiva del reloj se levanta y va a cerrar la ventana.
Al hacerlo,la fricción desprende largas tiras del marco de aluminio,destrozando su superficie.
Si el material fuera acero,estos destrozos constituirían un excelente punto de partida para la herrumbre.
Por el contrario,la ventana de aluminio se dedicará tranquilamente a reparar por si sola sus heridas,como lo ha hecho siempre.
" Antes de que la persona que ha cerrado la ventana se haya dado la vuelta,empieza a crecer a los lados del marco una nueva capa de óxido de aluminio,procedente de las partes intactas - - describe el autor - -.
Se extiende por encima de las microscópicas estrías,las cubre y las sella,y sólo se detiene después de haber sustituido a la perfección el trozo perdido.
" Mientras la ventana se reconstituye a si misma,su propietario se dispone a lavarse los dientes.
Para ello oprime un tubo metálico o plástico,generando ondas de presión que expulsan un buen trozo de pasta dentífrica.
Iniciando una serie de descripciones en plan " defensa del consumidor " - - que son una de las utilidades secundarias de su libro - -,Bodanis declara que la pasta dentífrica se compone mayoritariamente de agua de grifo y la tiza de yeso que se utiliza para escribir en las pizarras escolares (" la fabricación de pasta dentífrica es una actividad muy lucrativa ",acota con ironía).
El yeso proviene de las tumbas de criaturas oceánicas del cretáceo,y sus partículas conservan toda su dureza,necesaria para enfrentarse desde las cerdas del cepillo con la capa externa del esmalte de dientes,que es la sustancia más dura del cuerpo humano,más resistente que los huesos o las uñas.
El problema es que los fragmentos de yeso excesivamente abrasivos o voluminosos caven en el diente hondos orificios,que servirán de refugio a las bacterias que provocarán daños irreversibles en el esmalte.
Para resaltar el pulido del esmalte,los fabricantes suelen añadir dióxido de titanio,el mismo material que da blancura a la pintura blanca para paredes,o blanqueadores ópticos de los que se utilizan en los detergentes para lavar la ropa.
Sus efectos no duran mucho más allá de la primera mirada a la dentadura en el espejo,apreciando su efímera blancura.
A esta mezcla básica se agregan pequeñas porciones de alga marina,anticongelante,Parafina,menta y formaldehido,destinadas a dar color,sabor,consistencia y aroma a la pasta que nos promete una fresca limpieza bucal.
" No obstante,los estudios realizados demuestran que un cuidadoso cepillado con simple agua del grifo da a menudo el mismo resultado " - - afirma el autor.
El hombre de la casa ha llegado a la cocina.
Arroja el periódico sobre la mesa,provocando el pánico y huida de una gran multitud de pequeñísimas criaturas que pacían sobre la tabla.
La muchedumbre en fuga está compuesta fundamentalmente por pseudomonas,entre otras bacterias,y los omnipresentes e inocuos ácaros del polvo.
Estos diminutos seres - - una asamblea de 100.000 pseudomonas sigue siendo invisible al ojo humano - - se desplaza en dirección al otro extremo de la mesa,con retrocesos,choques y confusiones,porque ninguno de ellos está muy bien dotado para la traslación.
Pero se retuercen,colean,flotan o nadan en el aire para alejarse desesperadamente del periódico,porque en ello les va la vida.
En efecto,cuando el periódico hizo impacto sobre la mesa,dejó caer elementos que componen el papel,tales como cáñamo,lino,lana,amianto,fibras de vidrio,cola y otras sustancias,junto a microscópicos chorros de tinta impresa.
En realidad,la tinta no está adherida a las fibras del papel de periódico,sino que se sostiene en las grandes grietas que existen entre esas fibras.
Cualquier golpe o sacudida provoca el desprendimiento y caída de parte de esa tinta,que resulta fatal para las pseudomonas.
" En la tinta,asi como en aceites y detergentes - - explica Bodanis - -,se encuentra una elevada concentración de venenos antimicrobianos que sirven para mantenerla fresca mientras aguarda en las impresoras ".
Si las pseudomonas en fuga pudieran reflexionar,sin duda lamentarían el ánimo de aventura que las llevó a explorar la tabla de la mesa,añorando el confortable trapo de cocina que las vió nacer Según informa el autor," los microbiólogos dedicados a la sanidad pública consideran que el trapo de cocina (junto con la esponja escurrida inadecuadamente) es uno de los principales propagadores de poblaciones bacterianas dentro del hogar ".
Veamos por qué: Un trapo de cocina es sencillamente un tejido de algodón.
Esto significa que tiene entrelazados en su interior largos túneles rellenos de protoplasma,con minerales,proteínas y grasas solubles Todo el conjunto se mantiene unido gracias a una rejilla de celulosa,formada principalmente por azúcares.
Las bacterias engullirían directamente este sabroso material,si no fuera porque un trapo de cocina presenta manjares aún apetecibles: trozos de miga de pan,grasas que se adhirieron al fregar los platos,y otros restos orgánicos " Almacenados con toda seguridad en el interior de un trapo,cada uno de estos residuos alimenticios dará lugar a miles de microbios especializados - - advierte Bodanis - -.
El segundo rasgo característico de un trapo de cocina es aun peor.
Se trata de su constante humedad.
Las bacterias medran en la humedad.
" En este punto,David Bodanis se siente obligado a aclarar que sus descripciones de los habitantes secretos de la cocina matutina,no deben llevar a interpretaciones erróneas de lo que ha sucedido en ella durante la noche.
Es fácil imaginar que estas pseudomonas,bacterias,microbios especializados,y otros seres del mismo origen que sin duda se encuentran en el trapo,sobre la mesa,en el desagüe,o los estantes,han aprovechado las horas nocturnas para depredar la comida almacenada en la cocina.
" Lo han intentado,si,pero a menudo no han logrado dejar ni siquiera la más mínima señal en ella.
Porque la comida que se guarda en la cocina posee sus propias defensas.
" Tomemos por ejemplo un simple huevo guardado en la nevera.
Se ha pasado la noche respirando con dificultad,ingiriendo los gases que contiene la atmósfera del refrigerador.
El huevo respira a través de unos pequeños agujeros que posee en la superficie de la cáscara,cuya función es proporcionar oxígeno a un eventual embrión que se gestará en su interior Estos orificios son lo bastante amplios como para que una docena de bacterias del tipo pseudomonas o similares se deslicen por ellos a la vez,en busca de la apetitosa y nutritiva yema.
Pero las facilidades que ofrece el huevo acaban allí.
El primer obstáculo que encuentran las bacterias invasoras es una membrana elástica que cubre el fondo del orificio (y forma la película translúcida que recubre un huevo cocido),y que no es fácil de atravesar.
Las pseudomonas se retuercen,sondean el terreno,secretan enzimas disolventes y empujan con todas sus fuerzas,hasta que algunas consiguen atravesar esa primera barrera.
Quizá no lo harían,si conocieran el destino que les aguarda al otro lado.
La clara que envuelve a la yema es en realidad un mar protector,lleno de trampas explosivas de origen químico,capaces de destruir flotas enteras de bacterias en su viaje hacia el centro del huevo.
En un primer momento,las pseudomonas pueden realizar unos minutos de tranquila natación vibratoria,sin ser molestadas.
Sólo cuando se han internado lo bastante,alejándose de la seguridad que representa la pared interna de enzimas,las lisozimas,caen sobre las paredes celulares de las bacterias,abriéndolas y causando gran número de bajas - - relata Bodanis en plan corresponsal de guerra - -.
Oleada tras oleada de lisozimas acosan sin descanso a las bacterias,antes de que cesen las hostilidades.
" Pero algunas pseudomonas han sobrevivido a esos exocets biológicos,y más o menos maltrechas prosiguen su avance hacia la yema.
El alto mando de la clara utiliza entonces un viejo recurso bélico: cortarles las provisiones.
Las bacterias se reaprovisionan durante el avance con los nutrientes que flotan a su alrededor.
Pero el extraño mar blanco comienza a envolver esos nutrientes por medio de acelerados procesos químicos.
El hierro que necesitan las atacantes para impulsar sus hélices es rodeado por huidizas proteínas que lo bloquean químicamente.
También los nutrientes de cinc y de cobre quedan envueltos por ese protector subversivo.
Pese a estar privadas del imprescindible suministro mineral,quizá haya bacterias que siguen adelante,llevadas por el ímpetu restante en sus mecanismos de propulsión.
Si consiguen llegar hasta las vitaminas más próximas (en la zona inmediata abundan la riboflavina y la biotina),pueden intentar subsistir pese a la falta de hierro y alcanzar las jugosas proteínas y grasas.
El insomnio es uno de los síntomas más característicos de los estados de ansiedad que padece el individuo urbano.
Para el insomne,cuyo problema es casi siempre consecuencia de otra disfunción nerviosa,hay algo que se convierte en una constante obsesión: dormir.
Sin embargo,el resultado de recientes investigaciones demuestra que,por extraño que parezca,el drama del insomne no consiste en que duerme poco,sino en que ignora cuánto duerme realmente.
El estado de sueño corresponde a la fase de mayor reposo en la actividad del organismo del individuo,caracterizándose por la pérdida de conciencia y disminución de las funciones del sistema nervioso central.
En este período se produce una disminución del ritmo de las principales actividades orgánicas y pérdida de relación con el mundo exterior.
De todos modos el estado de sueño no se produce abruptamente,es decir que nadie queda dormido le repente sino que cumple paulatinamente una serie de etapas que se traducen en síntomas como el bostezo,la relajación muscular,la pérdida de atención,y más tarde de la percepción visual y auditiva.
Enseguida se reduce el nivel de conciencia y de un modo aparente la relación sensorial con el mundo externo.
Considerando los parámetros de un electroencefalograma (EEG) se han considerado dos tipos básicos de sueño,el lento y el paradójico.
El primero corresponde a una fase en la que el individuo permanece tranquilo y casi inmóvil y durante la cual se suceden a su vez cinco etapas que se repiten cíclicamente durante todo el estado del sueño.
El sueño paradójica,de ensoñación o sueño REM se caracteriza por una creciente actividad ocular y de movimientos corporales.
En esta fase el individuo sueña,de modo que desarrolla una intensa actividad psíquica con un nivel de conciencia muy bajo,lo que le hace creer en esos momentos en la veracidad de sus percepciones oníricas.
Hay casos en los que en ese mismo estado,el individuo duda,lo que hace suponer que en tales ocasiones permanecen algunos elementos conscientes propios del estado de vigilia.
Los períodos de sueño lento o profundo y de sueño paradójico REM se alternan durante toda la noche.
A simple vista la estructura del sueño,tal como ha sido definida puede dar la idea de que se desarrolla de modo uniforme desde el Principio al fin.
Sin embargo no es asi.
El resultado de recientes investigaciones demuestran una notable discrepancia entre una medida del comportamiento durante el inicio del sueño y su correlación con la medida del EEG.
El doctor Peter Birrell,del departamento de Psicología de la New South Wales University,en Australia,ha llevado a cabo una serie de interesantes estudios que permiten profundizar en el campo del sueño,en particular en las perturbaciones que se producen durante el mismo y que originan disfunciones como el insomnio.
El doctor Birrell colocó a sus pacientes un botón en sus muñecas y los cables del EEG en la cabeza.
Los pacientes debían presionar el botón cuando escucharan un timbre muy suave.
La reacción se desarrolló sin inconvenientes durante la primera fase del sueño y durante los primeros cinco minutos de la segunda fase,pero no se produjo a partir de aquí.
Esto significa que los sujetos se comportaron durante una parte de su sueño como si estuvieran despiertos,mientras que el EEG indicaba que dormían.
Lo mismo ocurrió cuando los durmientes fueron despertados varias veces e inmediatamente se les preguntó si estaban despiertos o dormidos.
Para el doctor Peter Birrell estos resultados demuestran que el sueño comienza cuando se accede a la segunda fase,aunque algunos individuos jóvenes afirmaron hallarse despiertos antes de que los despertaran realmente,cuando estaban en las fases más profundas del sueño.
La sensación de hallarse despiertos se debe,según el doctor Birrell,a la incidencia de breves estallidos de ondas alfa,los cuales pueden durar apenas un par de segundos.
La demostración del hecho vendría dada al pedírseles a los durmientes que presionarán el botón sujeto a sus muñecas,cada treinta segundos,cada vez que se despertaran durante la noche.
Asi se comprobó que la presión se realizaba tras un estallido de las ondas alfa y seguían haciéndolo cuando ya estaban profundamente dormidos,de acuerdo con el trazado del EEG.
El resultado de esta investigación ha puesto a los científicos australianos en el camino de una de las más comunes alteraciones del sueño: el insomnio.
En este sentido,el Dr. Birrell expuso en un reciente seminario realizado en Cambridge que las personas que padecen de insomnio posiblemente crean que pasan gran parte de la noche luchando por conciliar el sueño,no porque en realidad duerman mucho menos que los durmientes normales,sino porque soportan un número mayor de perturbaciones o interferencias de las ondas alfa.
Estas instrucciones provocan en el durmiente la sensación de la vigilia... aún en los momentos en que consigue dormirse realmente,lo que a su vez genera un estado de mayor ansiedad.
El resultado de las investigaciones de Birrell hacen suponer que el tratamiento para combatir el insomnio puede ser mucho más eficaz en el futuro.
Cuando el misterioso forastero entró en el cuartel de bomberos,Levey quedó estupefacto.
" Quitando unos kilos de diferencia,era como si me mirara al espejo - - dice - -.
Teníamos el mismo bigote,las mismas patillas,e incluso las mismas gafas ".
Durante un largo momento el gemelo de Paramus lo estudió con aire escéptico.
De pronto,ambos capitanes de bomberos aullaron al unísono.
Más tarde,compartiendo su común afición por la cerveza,Levey y su hermano (cuyo nombre era Mark Newman) comenzaron a ordenar el rompecabezas de su pasado.
Cada uno de ellos sabía que sus padres biológicos lo habían entregado en adopción,pero nunca habían sospechado que al mismo tiempo había sido entregado un gemelo.
A medida que hablaban y bebían,los hermanos descubrieron que tenían más cosas en común que el aspecto y la profesión.
Ambos hombres sólo bebían cerveza Budweiser,y sostenían la botella colocando el dedo meñique en la base.
Los dos eran solteros,y compulsivamente enamoradizos.
Ambos hacían gala de un bronco buen humor.
" Hacíamos las mismas observaciones a la vez,utilizando los mismos gestos - - narra Levey - -.
Era algo fantasmagórico ".
En los meses que siguieron al encuentro,la fantasmagoría se fue esfumando,pero continuaron apareciendo similitudes de conducta.
Aunque Mark y Jerry consideraban su extraña historia,y sus aún más extrañas coincidencias,sólo como pasmosas curiosidades,la comunidad científica veía las cosas de forma muy distinta.
Para los estudiosos de la conducta,empeñados en resolver la antigua y contumaz pregunta de si es el ambiente o los genes lo que nos hace ser como somos,Jerry Levey y Mark Newman,junto a un pequeño grupo de seres como ellos,han representado siempre un preciado tesoro para sus investigaciones: gemelos idénticos,con una base genética exactamente igual y una historia de vida totalmente diferente.
Dado lo escaso de estos sujetos,pocos científicos se han dedicado a la penosa tarea de buscarlos y reunirlos para su estudio.
A lo largo de casi medio siglo,sólo tres investigaciones - - una en 1937 y dos en los años 60 intentaron explorar el misterio de los gemelos criados por separado.
E incluso estos estudios fueron,cuanto menos,precipitados.
El mayor de los tres estudió 44 pares de gemelos idénticos ; el menor,investigó sólo 12 pares.
Todo esto comenzó a cambiar en 1979.
Ese año el psicólogo Thomas Bouchard Jr.,de la Universidad de Minnesota,leyó un artículo sobre un par de gemelos separados que habían sido reunidos.
Casi casualmente,decidió tomar contacto con ellos para estudiarlos.
Después de trabajar con esa pareja,no obstante,el interés de Bouchard comenzó a crecer y su propósito cambió gradualmente: decidió estudiar más gemelos criados por separado.
Pero no una docena,ni siquiera una docena de docenas,sino todos los que pudiera encontrar.
Paralelamente,un proyecto asociado estudiaría gemelos educados juntos,que servirían como base de comparación,como " grupo testigo " de control.
La investigación se desarrolló a lo largo de ocho años.
El equipo de Bouchard se amplió de 3 a 18 personas,incluyendo psicólogos,psiquiatras,oftalmólogos,cardiólogos,patólogos,genetistas y dentistas.
Muchos de los gemelos que sirvieron de sujetos ya se habían reunido antes de ofrecerse como voluntarios o ser reclutados por los investigadores.
Otros llegaron individualmente a Minnesota,esperando encontrar ayuda para reunirse con un gemelo separado de cuya existencia ya sabían.
Una vez incorporados al programa,todos los gemelos fueron sometidos a una maratón de pruebas de seis días de duración.
Sembrados de electrodos,metidos en plataformas rotatorias,portando equipos colgantes que registraban su temperatura y actividad,fueron radiografiados,filmados en vídeo y controlados en un sinfín de variables,desde la presión sanguínea hasta la respuesta cardíaca a la ansiedad.
Tanto en cuestionarios escritos como en entrevistas orales,debieron relatar su historia sexual,angustias vitales,escolaridad y problemas familiares.
A solas,cada uno de ellos respondió por escrito a 15.000 preguntas.
Hasta hoy,77 pares de gemelos reencontrados - - incluyendo a Jerry Levey y Mark Newman - - y cuatro grupos de trillizos reunidos,han viajado hasta lo que ya se conoce como " la ciudad de los gemelos " para ofrecer un servicio a la ciencia y aprender algo más sobre sí mismos.
También han sido estudiados varios cientos de gemelos criados y educados juntos.
" Los gemelos que crecieron separados nos llegan a razón de ocho a diez pares por año - - dice David Lykken,psicofisiólogo miembro del equipo - -.
Dado que ahora los gemelos destinados a la adopción ya no son separados al nacer,disponemos de un conjunto finito.
Pienso que podremos estudiar a todos,antes de jubilarnos.
" No obstante.
ya han comenzado a aparecer resultados.
El equipo ha publicado algunos de sus hallazgos en revistas profesionales,y otros trabajos ya escritos esperan para ser editados.
Bouchard y su colega,Nancy Segal,están preparando un libro que se prevé publicar en 1989.
Y puesto que sus investigaciones pronto saldrán a la luz pública,los investigadores se sienten algo más inclinados a hablar sobre lo que han descubierto hasta ahora.
Y las historias que cuentan son verdaderamente notables.
Está la de los gemelos Jim,de Ohio.
El que Lleven el mismo nombre es obviamente una casualidad.
Pero los Jim también fuman la misma marca de cigarrillos,conducen el mismo modelo de coche,tienen el mismo trabajo,y comparten la misma afición: la carpintería.
Ambos han construido idénticos bancos de madera blanca en torno a los árboles de sus jardines,se mordisquean con similar nerviosismo las uñas y comenzaron a sufrir jaquecas a la misma edad.
Después está la historia de Jack,un judío que pasó diversas etapas de su vida en Trinidad,California,y en Israel.
Su gemelo separado,Oskar,creció como católico en Alemania y perteneció a las Juventudes Hitlerianas.
Pese a antecedentes absolutamente opuestos,los gemelos mostraron personalidades asombrosamente similares al ser estudiadas las mismas excentricidades: los dos descargan el agua del retrete antes y después de usarlo,y les resulta igualmente divertido el fingir un aparatoso estornudo en un ascensor repleto.
También es sorprendente el caso de dos burbujeantes gemelas finlandesas,llamadas Dapne y Bárbara.
Pese a haberse criado en circunstancias socioeconómicas opuestas,ambas son muy " peseteras ".
Pero su tacañería no se manifiesta sólo a la hora de gastar dinero,sino también respecto a sus opiniones,rehusándose a tomar partido incluso en los temas más triviales.
Las dos sufrieron abortos durante su primer matrimonio,pero luego llegaron a tener saludables niños cada una.
Comparten asimismo un irracional terror a las alturas,y poco después de reunirse por primera vez comenzaron a terminar la una las frases de la otra y a dar las mismas respuestas al unísono.
Pero estas espectaculares y extravagantes similitudes no constituyen el objetivo central de los investigadores de Minnesota.
En realidad,cuando los científicos analizan el sentido de tales anécdotas,no las consideran mucho más que eso: coloridas historias que revisten su trabajo,pero que no lo representan apropiadamente.
El verdadero foco de sus estudios,puntualizan,reside en los genes.
Lo que esperan discernir es la heredabilidad de los incontables miles de rasgos y características personales de los seres humanos.
" Heredable - - aclaran - - no significa necesariamente heredado ".
De lo que trata la heredabilidad es de los porcentajes.
Por ejemplo,en qué medida nuestra inteligencia no es legada por los genes,y qué parte de ella proviene de la educación y el medio ambiente en que crecemos.
Para poder responder a estas preguntas,el equipo de Bouchard ha incluido el estudio de gemelos idénticos y gemelos sólo fraternos (mellizos).
Las diferencias entre ambos tipos son considerables,tanto en sus rasgos superficiales como genéticos.
A nivel celular,la concepción de gemelos idénticos es un hecho rutinario,exactamente igual a la concepción de cualquier singleton (que es como se denomina en inglés a quienes nos gestamos en solitario),o sea que un único espermatozoide fertiliza un único óvulo,creando una célula-huevo o cigoto.
Normalmente,ese cigoto inicia inmediatamente el proceso que a lo largo de nueve meses madurará el ser humano.
Pero en el caso de gemelos idénticos,existe un paso previo: el cigoto se divide en dos cigotos.
Dado que ambas nuevas células tienen origen en el mismo espermatozoide y el mismo óvulo,son perfectamente idénticas y comparten su material genético al ciento por ciento.
Los niños que nacerán nueve meses más tarde (llamados gemelos verdaderos,idénticos o monocigóticos),serán casi indistinguiblemente iguales.
El caso de los mellizos no-gemelos es diferente.
Se trata de un accidente en el proceso de ovulación que libera dos óvulos en lugar de uno dentro de las trompas de Falopio maternas.
Ese par de óvulos completamente separados es fertilizado por dos espermatozoides distintos,creando dos cigotos completamente independientes.
Se gestarán dos mellizos fraternos (o mellizos diCigótiCos),con un promedio de la mitad de su material genético en común,ni más ni menos que cualquier otra pareja de hermanos de distinta edad.
No obstante,con una educación a menudo idéntica,sirven como apropiados grupo de control para contrastar las similitudes genéticas de los gemelos monocigóticos.
No siempre es evidente que dos mellizos son gemelos.
La prueba definitiva para saber si un par es monocigótico o dicigótico es un test sanguíneo que analiza veinte variables y que a veces ha arrojado algunas sorpresas.
Una pareja que se suponía fraterna por sus claras diferencias de aspecto y habilidades,resultó ser idéntica.
Las diferencias obedecían a que uno de ellos sufría un problema neurológico inducido por su medio ambiente.
Según Bouchard,es un caso en que " el medio desborda a la Naturaleza ".
Otro " error cigótico " tuvo como sujetos a dos jóvenes hermanas,tan parecidas físicamente que su reencuentro se había desencadenado cuando alguien tomó a una por la otra.
Sin embargo,el test sanguíneo demostró que sólo eran mellizas.
El caso más complicado fue el de dos gemelas nacidas en Japón,y criadas por dos familias distintas en California.
Aunque las hermanas parecían casi indistinguibles,algunos pocos indicios como la forma de las orejas y del óvalo facial eran lo bastante diferentes como para hacer dudar a los investigadores.
Los exámenes y entrevistas demostraron que ambas presentaban uñas hendidas en los mismos dedos,las dos habían sufrido abortos espontáneos,y padecían similares acepciones.

