La energía solar cuenta con muchos detractores,incluso en un país como España en el que más de las tres cuartas partes del territorio reciben al año entre dos mil quinientas y tres mil quinientas horas de insolación.
Se sigue insistiendo en su complementariedad,en su dispersión,en su escaso rendimiento,en su alto precio... Argumentos,todos ellos,no exentos de fundamento ; porque la energía solar,hoy por hoy,no puede soñar en sustituir a las fuentes tradicionales de energía,es efectivamente muy dispersa y requiere instalaciones costosas y de escaso rendimiento.
Pero también habría que añadir inmediatamente que las cifras invertidas en investigación son ridículas al lado de las que se invierten en las demás fuentes energéticas,tanto en la industria del petróleo como en la nuclear.
Si la energía solar tuviese a su disposición una cuantiosa inversión investigadora,muy pronto dejaría de ser costosa y de escaso rendimiento y podría muy bien no sólo complementar a las demás fuentes,sino incluso sustituirlas.
Pero todavía quedan muchos problemas por resolver.
Y,a pesar de que se investiga poco,ya se va avanzando en muchos terrenos.
En España,la Agencia Internacional de la Energía colaboró con los científicos del Centro de Estudios de la Energía en la implantación,en tierras almerienses,de una plataforma solar experimental destinada esencialmente a investigar las posibilidades de generación de energía eléctrica mediante concentración de los rayos del sol hacia un horno que alcanzaría elevadas temperaturas.
El día 15 de octubre de 1986 estas instalaciones fueron transferidas,íntegramente,al Instituto de Energías Renovables (perteneciente a la antigua Junta de Energía Nuclear,hoy denominada Centro de Investigación Energética Medioambiental y Tecnológica,CIEMAT),heredero de aquel Centro de Estudios de la Energía.
Se abre así una nueva etapa en las investigaciones a desarrollar en Almería,en un campo desértico próximo a la localidad de Tabernas,que se ha convertido en el más avanzado y flexible centro mundial de estudios acerca de la energía solar a alta temperatura.
En la nueva óptica investigadora participa activamente la ciencia alemana,que sufragará la mitad del coste que acarreen la operación y el mantenimiento de las instalaciones en los próximos años.
Un coste que se eleva a 1.500 millones de pesetas,que se espera sean recuperados a corto plazo mediante la exportación de tecnología efectuada conjuntamente por alemanes y españoles.
Las investigaciones se van a centrar en los combustibles sintéticos,los materiales cerámicos,los sistemas de almacenamiento.
Los paneles solares (en la foto,paneles fotovoltaicos) se instalan en sitios abiertos y despejados con el fin de que puedan recibir las radiaciones solares directamente y durante el mayor tiempo posible.
término a alta temperatura,las metalurgias especiales requeridas por estas enormes centrales solares,etcétera.
El objetivo principal se centra en torno a la viabilidad de grandes centrales con potencias de 50 y hasta 100 megavatios ; las dos centrales actuales de Almería tienen,respectivamente,0,5 y 1 megavatio.
El camino de la generación de electricidad en grandes centrales,en las que la luz solar es concentrada,mediante enormes espejos móviles llamados heliostatos,hacia un foco con el fin de obtener altas temperaturas,no es el único interesante en cuestiones de energía solar.
Sin duda,resulta interesante para las actuales compañías eléctricas,ya que la corriente generada llegará a nuestras casas a través de un contador de la luz,lo mismo que la electricidad de origen térmico,nuclear o hidroeléctrico.
El proceso tiende a concentrar lo que es disperso (la luz solar) para volver a dispersarlo posteriormente mediante la red eléctrica.
Un proceso cuya filosofía puede ser discutible,por tanto,pero que,sin duda,ahorra el consumo de otras fuentes mucho más contaminantes (el carbón,el petróleo,el átomo).
Ahora bien,la energía solar,por ser dispersa,puede ser utilizada directamente de forma igualmente dispersa.
Eso pretenden las centrales fotovoltaicas,capaces de convertir directamente la luz del sol en electricidad.
Una de las centrales pioneras de este tipo en todo el mundo ha sido instalada en el pueblo madrileño de Guadalix de la Sierra ; se trata de una central de 100 kilovatios,de carácter modular.
Lo cual significa que se estudia la viabilidad de pequeños módulos (con 10 kilovatios por unidad),acoplables entre sí en caso de necesidad.
El problema,aquí también,es la rentabilidad del sistema.
Porque resulta todavía excesivamente caro.
Aunque,como más adelante veremos,se siguen investigando nuevos materiales,sobre todo una mejor utilización del silicio,pero amorfo,que es más barato.
Pero,a pesar de todos estos avances,nuestra moderna tecnología aún no es capaz de conseguir lo que la naturaleza realiza sin ningún esfuerzo: captar en verano el exceso de energía solar (gracias a la clorofila de las hojas de los árboles y la hierba,por ejemplo) y almacenarla para las épocas de escasez en forma de almidón en las patatas,azúcar en la remolacha y en la miel de las abejas,grasa en casi todos los animales y muchos otros productos directamente utilizables.
Este método natural de utilizar la energía solar es el que presenta un mejor porvenir para alimentar energéticamente a la civilización humana ; al menos,en teoría.
Pese a los esfuerzos de los investigadores,la ciencia no ha elucidado aún por completo los mecanismos de la fotosíntesis,ese proceso químico perfeccionado a lo largo de miles de millones de años,que convierte a una modesta hoja verde en una especie de célula fotovoltaica de alto rendimiento y bajo coste.
Pero captar la energía solar no es suficiente.
Hay que almacenarla también para poder reutilizarla posteriormente y,si es posible,sin que se haya disipado mientras tanto.
En climas templados o polares resulta esencial constituir una reserva para el invierno,tanto en los vegetales como en los animales y el hombre.
Así,los árboles de hoja caduca utilizan esa reserva para reconstituir su follaje en primavera y los animales para efectuar sus migraciones sin comer en ruta o para hibernar durante meses sin ingerir alimento.
La naturaleza parece disponer de un solo tipo de célula de energía solar a base de los gránulos verdes que contienen clorofila,los cloroplastos.
Pero,en cambio,existe una gran variedad de almacenamientos de esa energía en función de muy diversos productos químicos,con moléculas tan distintas como la albúmina del huevo,el almidón del trigo o los azúcares de ciertas plantas y animales.
A corto plazo,la fotosíntesis consigue almacenar energía solar en forma de ATP (adenosintrifosfato) y de glucógeno,un azúcar polimerizado que se almacena en el hígado de los animales.
El almacenamiento,a más largo plazo,se hace esencialmente a base de moléculas de grasa.
La utilización de estas reservas energéticas puede producir distintas formas nobles o degradadas de energía: energía mecánica,electricidad.
Los acumuladores de almacenamiento actuales,parecidos a las baterías de coche,son pesados,frágiles,y conservan mal la carga.
Resultarían demasiado caros para acumular la producción eléctrica de un tejado durante todo el verano,y además se degradan después de sólo cinco o,como mucho,diez años de servicio.
Mientras llegan las futuras baterías bioquímicas,embebidas en azúcar o grasa,el hidrógeno podría ser utilizable ya como producto de almacenaje.
La electricidad de las fotopilas descompone el agua en hidrógeno y oxígeno ; el hidrógeno,una vez almacenado,puede regenerar el agua y la electricidad en las pilas de combustible.
Se habla mucho actualmente de electrolizadores eficaces para producir hidrógeno,de almacenamiento denso en depósitos de hidruro de hierro y de pilas de combustible mejoradas.
Con todo,a causa de su inflamabilidad extrema y de su poder detonante,el hidrógeno es evidentemente un producto de almacenamiento menos simpático que la grasa del pájaro bobo o del oso.
Estas dos vías,el silicio amorfo y el almacenamiento a base de hidrógeno,vienen a completar,pues,la vía ya emprendida de forma dominante en España de la generación de electricidad mediante energía solar a alta temperatura.
Son caminos no necesariamente antagónicos,pero sí bastante diferentes.
En lo fotovoltaico reside,sin duda,el futuro ; con fotopilas de producción directa de electricidad y con sistemas biológicos fotosintéticos.
Las grandes centrales,suponiendo que sean rentables y que sigan siendo necesarias en el futuro,también están ahí,y se trata de una tecnología mucho más dominada,más asequible a corto plazo.
Y mucho menos contaminante que cualquier otra gran central que queme petróleo o carbón,o que utilice la fisión del átomo.
En España la energía solar comienza a despertar de un largo letargo.
Quizá ese despertar debiera ser aún más vigoroso,con decidido apoyo estatal,incluso mediante exenciones fiscales,a esta energía barata,no contaminante e inagotable.
Pero bien está lo que bien empieza.
La Plataforma Solar de Almería ya es sólo española ; en su explotación económica comparten las tareas los alemanes.
Bienvenida sea la cooperación si con ella nos damos cuenta de lo absurdo que resultaba que en España dejáramos de lado una vía tan prometedora para resolver los problemas energéticos como la energía solar.
En el país más soleado de Europa.
Imaginemos un avión cuya velocidad límite es de 100 nudos.
La aproximación hacia la pista de aterrizaje se hace con un margen de seguridad del 30 %,es decir,a 130 nudos.
Estamos hablando de la velocidad del aparato respecto al aire.
Es decir,si el viento frontal (se despega y se aterriza siempre cara al viento) es de 20 nudos,la velocidad del avión respecto al suelo es de sólo 110 nudos.
Imaginemos que el viento gire bruscamente 180 grados y que se convierta en viento trasero de 20 nudos.
Aunque la resistencia debida al viento ha desaparecido,el avión guardará por inercia,durante un cierto tiempo su velocidad de 110 nudos respecto al suelo.
Pero su velocidad aérea (factor de sustentación) caerá a 90 nudos,110 menos 20,es decir,por debajo del umbral crítico.
El avión pierde altura bruscamente y se estrella antes de tomar pista.
Ha sido víctima de un caso típico de cizalladura de viento.
Un cambio importante y brutal de la dirección o de la velocidad del viento (a menudo,los dos simultáneamente) entre dos puntos cercanos de la trayectoria del avión.
Es un problema grave.
Sólo en los Estados Unidos,entre 1964 y 1982,fueron víctimas de él 27 aviones de línea,con un saldo de 491 muertos y 206 heridos.
El 2 de agosto de 1985 el Lockheed 1011 del vuelo 191 de la Delta Airlines se estrelló en el aeródromo de Dallas-Fort Worth,USA,a pocos centenares de metros del comienzo de la pista,demostrando que,desgraciadamente,el problema aún no está resuelto.
Lo mismo ocurrió en Madrid (Barajas),con un 747 de Avianca,el día 25 de noviembre de 1983.
Afortunadamente los distintos tipos de cizalladura (excepto los " microbursts " que veremos más adelante) se pueden prever generalmente varias horas antes,con las técnicas actuales.
En realidad se pueden prever las causas de la cizalladura más que la cizalladura en sí.
Por ejemplo,suele propiciarla una inversión de temperatura pero no se da simultaneidad sistemática entre ambos fenómenos.
Hasta 1975 se pensaba que ya estaba hecho el inventario de las causas de cizalladura de viento en las capas bajas de la atmósfera.
Desde entonces un nuevo animal meteorológico ha venido a sumarse a los ya conocidos.
El profesor Théodore Fujita,de la Universidad de Chicago,fue quien hizo el descubrimiento al investigar sobre el accidente fatal del Boeing 727 de la Eastern Airlines en el aeropuerto de New York-J. F. Kennedy el 24 de junio de 1975.
Fue él quien utilizó por primera vez el término Downburst (down: hacia abajo y burst: estallido).
Se trata de una corriente descendente muy fuerte que se transforma en viento violento (outburst) en las proximidades del suelo.
Los downbursts más pequeños han recibido el nombre de microburst.
(Disculpe el lector,estos términos no tienen aún traducción al castellano.
) Las investigaciones realizadas después sobre los accidentes del vuelo Continental 426 en Denver,el 7 de agosto de 1975,y del vuelo Alleghney 121,en Filadelfia,el 23 de junio de 1976,corroboraron las hipótesis de Fujita.
Tres características del microburst: su violencia,su dimensión muy pequeña (2 km de sección) y su duración,asimismo,muy limitada,cuatro a cinco minutos,con un viento que alcanza su máximo después de unos tres minutos.
Si se puede comparar el viento que existe debajo de una tempestad con el agua de un grifo derramándose en el fondo de una pila,lo mismo podemos imaginar un microburst como el chorro poderoso de una manguera dirigida al suelo.
Si el chorro está inclinado,el avión no sólo sufrirá el efecto de una cizalladura,sino de dos.
En efecto,la corriente descendente es oblicua y posee una componente vertical y una componente horizontal.
La cizalladura del viento vertical,es decir,su brusco refuerzo tendente a hacer bajar el morro del avión,se añadiría a la pérdida de velocidad - aire debida a la cizalladura horizontal y activará así la pérdida brutal de altura del avión.
Para comprender mejor este elemento,cuya naturaleza particular nunca había sido identificada hasta ahora,los norteamericanos pusieron en marcha dos proyectos.
El primero,en 1978,se llamó NIMROD (Northem Illinois Research On Downbursts),dirigido por el ya citado profesor Fujita,cuyos trabajos se realizaron en la región de Chicago,llegándose a observar 50 microbursts a lo largo de cuarenta y tres días.
El segundo,en 1984,fue el JAWS (Joint Airport Weather Studies).
Dirigido por el profesor J. Mac Carthy,del NCAR (National Center for Atmospheric Research),se llevó a cabo en Denver,Colorado ; en cuarenta y nueve días se detectaron 186 microbursts.
Aunque los resultados no fueron idénticos,los dos equipos coincidieron en una conclusión: no hay relación entre los microbursts y la intensidad de las precipitaciones de lluvia o granizo.
De un total de 236 microbursts,173 fueron secos,es decir con una pluviosidad inferior a 0,2 milímetros durante el período de viento máximo.
Otra conclusión sorprendente fue la relativa a la nube de origen: no es preciso que esa nube convectiva,con frecuencia aislada,esté en su fase activa ; ni las tormentas ni los rayos,pues,son imprescindibles.
Pero lo más sorprendente es que la mayoría de los microbursts observados no iban asociados a malvados cumulonimbos activos,sino a virga (lluvias que se evaporan antes de llegar al suelo) procedentes de cumulus congestus en vía de disipación.
Cuando el 2 de agosto del año pasado,el Tristan de la Delta Airlines se accidentó en Dallas-Fort Worth,el cielo estaba azul si exceptuamos un pequeño cúmulo que desapareció diez minutos después de la catástrofe... ¿Cómo detectar las cizalladuras? Hasta ahora ninguno de los distintos sistemas ideados resulta completamente fiable.
En efecto,la nueva reglamentación prevé,con objeto de distribuir equitativamente las riquezas submarinas que cada Estado o grupo privado admitido deba,si quiere obtener una concesión en aguas internacionales,presentar dos estudios,del mismo valor comercial y de una superficie que no exceda los 300.000 km2.
La convención adjudicará entonces al demandante uno de los dos enclaves y,lo que resulta inédito en la historia de los mares,concederá el otro a la empresa,una sociedad dependiente de un organismo internacional que funcionaría técnicamente gracias a los países desarrollados en beneficio del Tercer Mundo.
La Afernod (Asociación francesa para la búsqueda de nódulos) viene realizando desde 1970 importantes campañas de prospección entre 110 ° y 160 ° de longitud oeste,20 ' de latitud norte y 15 ° de latitud sur.
De estos trabajos,que colocan a nuestro país vecino a la cabeza en materia de prospección en el Pacífico,se deduce que las zonas más interesantes para explotar se sitúan entre las fallas submarinas de Clarion y Clipperton,es decir,de 110 ° a 160 ° de longitud oeste y de 50 a 200 de latitud norte ; por tanto,en aguas internacionales.
Así,el islote de Clipperton,una especie de paraíso abandonado,se convierte de repente en el punto de mira de muchos países por su situación en el Pacífico y por las aguas que le rodean,especialmente ricas en nódulos.
Pero,en el plano económico,el futuro de los nódulos es todavía discutible.
Los ocho proyectos ya depositados representarían dieciséis explotaciones funcionando a la vez,lo que podría desencadenar el hundimiento de los precios internacionales del níquel,el manganeso,el cobalto y el cobre.
Sólo el proyecto francés prevé ya,para asegurar la rentabilidad,la extracción mínima anual de 1 a 1,5 millones de toneladas de nódulos,lo que equivale a 15.000 toneladas de níquel y 2.000 de cobalto.
También hay que tener en cuenta los gastos de la explotación.
Una sola instalación de recogida exige un equipamiento valorado en 150.000 millones de pesetas: un barco o plataforma de base para la recogida,un aparato de drenaje... Hay que añadir,además,los gastos de funcionamiento,que equivalen anualmente a la mitad más o menos de los de equipamiento,es decir,75.000 millones de pesetas.
Hay que mantener al personal,transportar el mineral al país explotador y tratar luego los nódulos... ¿Merece realmente la pena? ¿Qué seguridad hay de lo que va a encontrarse en el fondo del océano? Desde un punto de vista económico estrictamente teórico,los cuatro metales que se extraerían de los nódulos saldrían al mismo precio con que se compran hoy en el mercado internacional.
En efecto,de una tonelada de nódulos se pueden extraer (teóricamente,también) dos kilos y medio de cobalto,15 de níquel,10 de cobre y 300 de manganeso.
En 1984,eso representaba 8.000 pesetas por el cobalto,14.000 por el níquel,2.600 por el cobre y 18.000 por el ferromanganeso,es decir,el valor comercial de una tonelada de nódulos polimetálicos era de 43.600 pesetas,y su precio de costo (tras recogida y tratamiento) de 43.000.
No se pierde nada,pero tampoco se gana respecto al mercado,suponiendo que éste se mantenga aproximadamente como el de ahora.
El único beneficio es la independencia del suministro,que deja de ser monopolizado por los actuales productores.
Pero no todos los materiales tienen el mismo valor estratégico.
Y,además,el mercado cambia muy rápidamente.
Así,entre 1960 y 1975,el futuro del níquel parecía prometedor ; ahora ya no lo es tanto.
Por el contrario,la crisis del Zaire,que comprometió el suministro de cobalto,que resultaba esencial en las aleaciones de alto rendimiento,revalorizó mucho ese metal.
El cobre no ofrece gran interés,ya que actualmente hay superproducción.
Por último,el manganeso se valora porque es necesario en la fabricación de aceros inoxidables,criogénicos,con propiedades magnéticas permanentes,y en aleaciones de todo tipo.
Pero es muy difícil prever lo que podrá ser el mercado dentro de diez o quince años.
Un informe de la Academia francesa de Ciencias de Ultramar mostraba en 1984 sus reservas sobre el interés económico de la explotación de los nódulos ; el informe estimaba que la forma de tratamiento debía estudiarse mejor y tener en cuenta diferentes técnicas,los precios de la electricidad,del carbón,del fuel,etc. Hay que desconfiar de las actitudes del momento: en 1970,por ejemplo,los estudios norteamericanos juzgaban el proyecto totalmente rentable ; en 1975,las Naciones Unidas lo calificaban incluso de excelente ; en 1976 - 78,los consorcios internacionales revisaban los gastos de explotación al alza,y en 1982,algunos expertos norteamericanos estimaban que la explotación no sería rentable jamás... Una estimación precisa es aún más difícil,porque no se ha decidido todavía el proceso de tratamiento metalúrgico de los nódulos.
¿Se adoptará la técnica pirometalúrgica o la hidrometalúrgica? La pirometalúrgica,a grandes rasgos,recurre a altas temperaturas,y la hidrometalúrgica a elevadas masas de agua.
Pero queda por saber lo que se va a encontrar en el fondo del agua y cómo se recogerá.
Aunque es seguro que hay nódulos en el fondo del Pacífico,y aunque se conoce aproximadamente su composición,su recogida dependerá mucho del lugar.
Evidentemente,es muy distinto recoger nódulos en un fondo liso que en un relieve accidentado,por ejemplo.
El punto crucial,la elección de la técnica de recogida,es menos dudoso.
De 1975 a 1984,el Comisariado para la energía atómica francés,asociado al Afernord,estudió un sistema de dragado con lanzaderas,el extractor libre autónomo o PLA.
Se basa en la idea de que un vehículo submarino de intervención y dragado,capaz de observar y trabajar en fondos de hasta 6.000 metros,recogería los nódulos y los subiría a la superficie (plataforma o buque) ; el aparato tendría que ser autónomo.
Para asegurar una explotación suficiente se pensó en una flotilla de veinte unidades que actuaría en cadena desde el fondo del agua hasta la superficie.
Mientras tanto,se advirtió que un tren de veinte unidades de ese tipo costaría muy caro ; y que resultaría aún más caro hacer perfectamente autónomos a esos veinte aparatos trabajando bajo presiones considerables.
En consecuencia,se abandonó la idea de los aparatos que subirían y bajarían incesantemente,como ascensores de ida y vuelta,para volver a la de un solo aparato de dragado que,sin remontarse a la superficie,limitase su actividad a la colecta de los nódulos y a una propulsión lenta en un itinerario dado,teledirigido desde la superficie.
La expedición de los nódulos hacia la superficie se realizaría por un tubo flexible de 500 metros de largo,fijado al aparato y conectado a una tubería de bombeo enlazada con la estación de superficie,que bajaría hasta un centenar de metros del fondo.
Un modelo que se ha hecho clásico por su utilización en las estaciones petrolíferas de alta mar.
Actualmente se proponen tres variantes para ese mismo tipo de estructura,la primera de ellas,el bombeo hidráulico clásico,consiste en expedir tal cual los nódulos colectados por el vehículo dragador hacia la estación de superficie.
Los relés colocados a lo largo del tubo de subida aseguran la constancia del caudal.
Para que éste sea suficiente,el diámetro del tubo debe situarse entre 25 y 50 centímetros ; pero existe el riesgo de atasco.
El segundo método,el bombeo neumático o air-flit,pretende crear una corriente ascendente,insuflando aire comprimido a diferentes niveles del tubo de subida.
Pero este procedimiento garantiza una capacidad escasa y exige un tubo de gran diámetro para alcanzar.
Y,en todo caso,son observables con los microscopios modernos ; por ejemplo,el microscopio de efecto túnel por el que le ha sido concedido este año el Nobel de Física a los técnicos de la IBM,de Zurich,Rohrer y Binnig.
Pero estos átomos son observables como constituyentes de un cuerpo sólido,ligados unos a otros.
Por el contrario,en un gas los átomos se encuentran mucho más independizados.
Y de ahí el interés del experimento de Ashkin y Chu: por primera vez,los físicos van a disponer de una herramienta ideal para estudiar en detalle las interacciones entre átomos.
Muy especialmente,los procesos que conducen a la formación de una molécula de dos o más átomos y,de forma más general,los estados fundamentales de la materia.
La herramienta en cuestión consta de dos técnicas a cual más compleja.
Una de ellas,a pesar de ser reciente,se halla muy extendida en los laboratorios del mundo entero y se ha hecho ya clásica ; fue bautizada como Laser Cooling (enfriamiento por láser) y consiste en frenar un paquete de átomos lanzados a una velocidad de varios kilómetros por segundo y que pasan brutalmente a la ínfima velocidad de sólo unos metros por segundo (para un átomo,casi la inmovilidad).
La segunda técnica,completamente original y debida a la inventiva de los científicos de los laboratorios Bell,es la que permite la segunda etapa del experimento ; a saber,atrapar esos átomos casi inmóviles en una jaula de luz.
La primera etapa se subdivide,a su vez,en dos tramos ; al menos,experimentalmente,ya que la teoría es la misma.
Una teoría que se basa en las leyes,establecidas por Einstein a principios de este siglo,que rigen los intercambios entre la radiación y la materia.
Según Einstein,la luz,como las demás ondas electromagnéticas,está compuesta de una miriada de fotones,paquetes unitarios de energía (los famosos cuantos de Planck).
En los procesos de interacción entre luz y materia hay conservación de la energía y de la impulsión.
Y,así,un fotón que choca con un átomo puede transferirle su energía.
El átomo puede,posteriormente,reemitir esa energía suplementaria bajo la forma de un nuevo fotón,de longitud de onda diferente o igual,según los casos.
Todo ello significa que la luz ejerce una auténtica presión sobre los átomos,es decir,sobre los objetos.
Lo que pasa es que se trata de una presión inapreciable a escala macroscópica ; la presión luminosa de un foco sobre el balón de fútbol en un partido nocturno es indetectable.
En cambio,si un vehículo espacial dispusiese de una vela adecuada (de varios centenares de kilómetros cuadrados),la presión de la luz solar sería suficiente como para asegurar su propulsión.
A escala atómica,la presión de la radiación luminosa es del mismo orden de magnitud que las demás fuerzas,y así un fotón puede,por ejemplo,expulsar a un electrón de un átomo o bien puede frenar o acelerar el movimiento de ese átomo.
Y,en caso contrario,un chorro de fotones,un haz luminoso,podría frenar a un átomo en movimiento como el chorro de agua de una manguera puede frenar el recorrido de una pelota de juguete.
Así fue como Ashkin y sus colegas frenaron hasta casi inmovilizarlo el haz de átomos de sodio,previamente ralentizados mediante el láser refrigerador.
Fabricar el haz de átomos de sodio no resultó difícil ; tomaron una microgota de ese metal y la hicieron estallar mediante un potente rayo láser.
Todo un caso extremo de la acción de los fotones sobre la materia: una luz suficientemente intensa y concentrada (láser) puede romper las uniones entre átomos y vaporizar el sodio,haciéndole pasar de sólido a gas.
Con ese vapor se obtiene un chorro de átomos de sodio a una velocidad media de 200 metros por segundo.
Ese flujo de átomos se puede canalizar a voluntad hacia un túnel de experimentación en el que se le hace chocar de frente con otro haz de rayos láser,esta vez mucho menos potente,para evitar que los átomos,con el impacto,se dispersen en todas las direcciones.
La frecuencia de esta luz láser caracteriza una onda y nos indica su número de oscilaciones por segundo ; su color,en suma,ya que en la luz visible cada color corresponde a una frecuencia bien determinada.
Cuando un fotón luminoso del láser choca con un átomo de sodio será absorbido por éste si la frecuencia del fotón corresponde a la diferencia de energía entre el estado fundamental del átomo y su estado excitado.
Esta moción de estado fundamental (y excitado) de un átomo no es fácil de comprender,porque no existe a escala macroscópica ; sólo a nivel atómico.
Los átomos son capaces de absorber y emitir ciertas radiaciones electromagnéticas bien definidas ; cuando el átomo no absorbe ni emite se dice que se encuentra en su estado fundamental.
Pero si se le proporciona una cierta cantidad de energía pasa a su primer estado excitado o a los sucesivos estados excitados,si la energía es lo bastante grande.
Pero un átomo excitado es inestable y tiene en seguida que recobrar su estado fundamental de forma espontánea.
En el caso del sodio excitado por la absorción de energía del fotón láser incidente,la recuperación del estado fundamental se hace en un tiempo muy breve: 16 nanosegundos (milmillonésimas de segundo).
Y esa desexcitación se hace emitiendo otro fotón en una dirección aleatoria.
El átomo,una vez recuperado su estado fundamental,puede ya reabsorber de nuevo otro fotón,y así sucesivamente.
La repetición de este proceso acaba por frenar progresivamente la marcha del chorro de átomos ; cada choque con un fotón supone una disminución de velocidad de unos cuantos centímetros por segundo.
En teoría,pues,al cabo de treinta o cuarenta mil choques,el átomo acaba por inmovilizarse.
Ahora bien,¿qué distancia necesita recorrer hasta entonces? Por supuesto,dependerá de la cadencia a la cual se producirán los choques ; el recinto en el que se lleva a cabo la experiencia no puede ser muy grande,porque en él debe reinar un vacío casi absoluto,y eso resulta técnicamente difícil,sobre todo en grandes tamaños.
Ello implica necesariamente una frecuencia muy alta de choques.
Y por eso el láser de frenado no sólo debe tener la frecuencia adecuada (para poder ser absorbidos sus fotones por los átomos de sodio),sino que además debe tener potencia suficiente.
Por ejemplo,si se consiguiera un choque cada 32 nanosegundos,los átomos se pararían al cabo de una milésima de segundo,habiendo recorrido hasta entonces sólo 20 centímetros,lo cual es muy razonable.
Pero en la práctica,las cosas no son tan simples.
Como los átomos se ven frenados con los primeros choques,su pérdida de velocidad va a afectar a la concordancia de frecuencias que existían inicialmente.
Al frenarse,los átomos ven cambiar aparentemente la frecuencia de los fotones (es el mismo efecto Doppler que nos hace escuchar el pitido de una máquina de tren muy agudo cuando se acerca y más grave cuando se aleja).
Y entonces dejan de absorber fotones y,por tanto,ya dejan de ser frenados.
Para solucionar este inconveniente habría que variar la frecuencia de los fotones paralelamente a la disminución de velocidad de los átomos.
Parece difícil,pero existen dos vías para hacerlo.
La primera consiste en hacer variar la frecuencia del láser,utilizando un cristal especialmente escogido en función de sus propiedades electroópticas (una corriente eléctrica modulada sobre la velocidad de los átomos hace variar el índice de refracción del cristal y,por tanto,la frecuencia del rayo que le atraviesa).
La segunda vía consistiría en hacer variar no la frecuencia de los fotones,sino la de los átomos,a base de un solenoide que creara un campo magnético variable.
Los investigadores de los laboratorios Bell escogieron la primera de estas dos vías.
Su experimento se desarrolló de la siguiente forma: en primer lugar,los átomos fueron frenados en un factor diez ; es decir,pasaron de 200 a sólo 20 metros por segundo.
Para frenarlos aún más (podríamos decir también para enfriarlos,ya que la disminución de energía cinética - - velocidad - - supone una disminución de temperatura) se hace penetrar al haz de átomos en una zona bautizada por Ashkin como pegamento óptico.
Una zona luminosa en la que,como su nombre indica,los átomos se van a quedar como adheridos a la luz.
Ello se consigue mediante tres pares de láser,en los que cada uno de los seis láseres está frente a su pareja,alineados en los tres ejes (longitud,anchura y altura) y convergiendo en una zona esférica de un centímetro de diámetro.
Los átomos sufren,en esa zona,tal cantidad de choques en todas las direcciones que llegan a enfriarse a sólo 0,00024 grados Kelvin.
¡Un récord de frío en los gases! A esa temperatura,los átomos están ya casi inmóviles.
Sólo queda encerrarlos en una jaula óptica.
Los científicos americanos utilizaron para ello otro láser.
La escalada permanente y el esfuerzo que implica han moldeado un corazón enorme y unos miembros extraordinariamente musculados.
Pero su secreto principal se esconde en sus pezuñas y suelas plantares.
Los pies de las cabras constan de dos partes principales: la pezuña,de borde duro y puntiagudo,actúa como la clavija del escalador ; es decir,entra en cualquier resquicio y se clava en el más pequeño de los resaltes,y la planta,que está formada por blandas almohadillas que evitan el deslizamiento cuando el rumiante desciende laderas empinadas o placas de hielo o nieve.
Así dotadas,las cabras trepan,saltan y corren por cualquier superficie.
Ya he mencionado que las montesinas tienen un corazón enorme,pero también sus pulmones son grandes y su sangre cuenta con más glóbulos rojos que la de sus congéneres de llanura.
Se trata de una adaptación a la menor cantidad de oxígeno disponible en las alturas.
Pero si tener poco con que llenar los pulmones es un problema,todavía lo es mayor el no contar con abundantes alimentos.
Las cabras salvajes son austeras y muy versátiles a la hora de nutrirse.
No hay un producto vegetal que no les aproveche,desde la corteza de un árbol al liquen más oculto.
Por si ese recurso no fuera suficiente,estos animales entran de lleno en la categoría de los escasísimos mamíferos parcialmente migradores ; son capaces de descender a los valles en invierno cuando la nieve cubre las cimas.
Esculpidas,pues,a golpes de hambre,frío y vértigo,las cabras montesas demuestran una vez más que una buena definición de lo vivo es su ansia por ocuparlo todo,por llegar a los rincones más insospechados con un mínimo de posibilidades.
Tal vez la diversidad genética no responda a más reglas que el desaforado impulso colonizador de la materia organizada en vida.
Para intentar explicar la monumental proeza de las cabras,en verdad no caben más ni menos argumentos,si bien,vaya por delante,que casi siempre,sobre todo cuanto estoy sumergido en plena naturaleza,dejo pasar primero a mis sentimientos.
La razón ya viene después con sus rebajas.
Disgresión tal vez innecesaria,pero que reproduce el hilo de los acontecimientos externos & internos que acompañaron mi primer encuentro con las monteses.
La pelea que contemplo enfrenta a dos machos.
Son de igual tamaño y sus cornamentas,como liras petrificadas,deben tener casi un metro de longitud.
La forma de estas defensas,obviamente,evita el derramamiento de sangre.
La pendencia,por tanto,debe resolverse por agotamiento ; ganará el más resistente.
Ya no lo veré,pero el desenlace no es otro que el mantenimiento de una determinada posición jerárquica entre los demás machos.
Este privilegio alcanzado tiene repercusión inmediata en el apareamiento.
Los líderes cubren a más hembras que los vencidos.
Un viejo y drástico sistema de asegurar descendencias aptas,ya que sólo los más fuertes llegan a la cópula.
Pero seguramente lo que más habrá llamado la atención de los lectores no avezados es la fecha de estas observaciones.
Pues bien,incluso se trata de un calendario,éste que describo,un tanto prematuro,ya que la mayoría de las cabras suelen quedar preñadas en diciembre y enero.
De ninguna otra forma los partos coincidirán con finales de abril y mayo ; es decir,con el mejor momento del año para sacar adelante a los recentales.
Los amoríos otoñales son,por tanto,frecuentes entre los grandes mamíferos de nuestra fauna,sujeta al imperio de las estaciones climáticas.
Y como si la naturaleza estuviera siguiendo los caminos de mi pensamiento,los machos trasponen y se ocultan.
Continúo subiendo hasta el lugar que ellos convirtieron en arena de gladiadores,me asomo con evidente poca prudencia,pues ya sólo alcanzo a ver un rebaño de casi 40 cabras que se precipitan ladera abajo.
Me quedo solo con un rumor de cascos.
Se van los señores de esta sierra,pero me queda toda la vieja Castilla delante,como un incierto oleaje.
Constituye la verdadera batalla contra la propia enfermedad valvular y la única manera de llegar a evitar la operación.
Antes de adentrarnos en el mecanismo de estas válvulas artificiales,conviene realizar un breve repaso de la estructura y función cardíaca.
Hay que considerar al corazón como un verdadero motor,capaz de expulsar la sangre hacia todo el sistema de arterias encargadas de distribuirla por el organismo,a cada una de las células,para nutrirlas y oxigenarlas.
Para poder realizar esta función cuenta el corazón con cuatro cavidades dispuestas en línea.
El corazón derecho,compuesto por la aurícula derecha y el ventrículo derecho,es el encargado de recibir la sangre venosa de todo el organismo y de expulsarla hacia los pulmones para poder oxigenarla y desprender allí el anhídrido carbónico,producto de desecho del metabolismo celular.
Desde allí,la sangre ya oxigenada llega a las cavidades izquierdas del corazón,desde donde será expulsada hacia la gran arteria aorta y posteriormente distribuida por todo el organismo.
Todo este juego de corrientes y contracorrientes necesita generar una fuerza,una presión suficiente para vencer las resistencias del sistema.
En condiciones normales,ello obliga al ventrículo izquierdo a alcanzar 120 o 140 mm / Hg,pero en algunas circunstancias puede llegar a 200 mm / Hg.
En cambio,el ventrículo derecho,gracias a una especial disposición de los vasos del pulmón,es un sistema de baja presión ; le bastan 50 mm / Hg para lograr sus objetivos.
Para alcanzar estas presiones de trabajo es imprescindible que los ventrículos y las aurículas queden,durante un momento del ciclo cardíaco,totalmente cerrados.
Este cierre hermético sólo puede ser conseguido gracias a un sistema de compuertas,de válvulas,suficientemente poderoso,pero flexible,que permita abrir y cerrar el paso a la sangre setenta veces por minuto.
Durante una fase de contracción del corazón,las válvulas situadas entre las aurículas y los ventrículos y las colocadas entre los ventrículos y las arterias de salida de los mismos han de permanecer totalmente cerradas.
Es entonces cuando el poderoso músculo cardíaco se contrae para generar las presiones necesarias para expulsar la sangre hacia delante.
Si el sistema de válvulas falla,se producen escapes de presión y de sangre o bien se obliga a los ventrículos y aurículas a realizar mucho mayor esfuerzo para alcanzar la presión necesaria.
Todo ello lleva,invariablemente,a un deterioro del músculo cardíaco que,en unos años,desemboca en una incapacidad para funcionar adecuadamente.
Este fracaso lleva al paciente a una situación peligrosa,limitante y,a veces,mortal.
Existen,pues,cuatro válvulas cardíacas.
Dos auriculoventricularesN: la válvula tricúspide,que separa el ventrículo y la aurícula derecha,está formada,como su nombre indica,por tres valvas que se cierran y abren sincrónicamente,y la válvula mitral,que separa la aurícula y el ventrículo izquierdo,formada por dos velos de diferente tamaño y recorrido.
Es la válvula que con más frecuencia se ve afectada por las lesiones de la fiebre reumática y,por ello,una de las más interesantes para la cirugía cardíaca.
Las válvulas que separan los ventrículos de los grandes vasos reciben el nombre de válvulas sigmoideas,aórtica la izquierda y pulmonar la derecha.
Son estructuras más pequeñas que las auriculoventricularesN,a modo de nidos de golondrinas ; al cerrarse,se unen en el centro del vaso,tapándolo completamente.
La sigmoidea pulmonar es una válvula curiosamente resistente a las infecciones y por ello raramente sufre degeneraciones reumáticas y pocas veces necesita ser recambiada por esta causa.
Pero la sigmoidea aórtica es una válvula sensible a las alteraciones,tanto de tipo congénito como a las producidas por la fiebre reumática.
Naturalmente,hasta llegar a la situación actual,donde colocar una prótesis artificial es una intervención quirúrgica que puede durar unas cuatro o cinco horas,a corazón abierto y con circulación extracorpórea,han sido necesarios gigantescos avances técnicos,quirúrgicos y médicos.
En tan sólo veinticinco años,la revolución ha llegado casi inesperadamente a este campo de la medicina.
Curiosamente el corazón,que tan sólo dista de la piel unos pocos centímetros,era una víscera inalcanzable para el cirujano.
Una víscera temida y lejana,a pesar de su proximidad.
Hasta 1896 nadie jamás había osado aproximarse a él ; pero en ese año se realizó una sutura por una herida de arma blanca en el corazón.
La intervención fue un verdadero éxito y abrió la vía de nuevas técnicas quirúrgicas.
Durante el principio del actual siglo ya se conocían las lesiones que la fiebre reumática producía en las válvulas cardíacas ; pero el médico veía,impotente,los síntomas y el deterioro que se producía en estos pacientes sin poder hacer casi nada por ellos,sin encontrar el camino para llegar hasta las válvulas enfermas.
En el año 1925,el doctor Southar logra por primera vez llegar hasta la válvula mitral,en una técnica realizada con el corazón latiendo,introduce un dedo por la aurícula izquierda y dilata la válvula mitral que se encontraba estrechada (estenosada) por causa de las lesiones reumáticas.
La operación es un verdadero éxito,pero por diversas razones técnicas el procedimiento no llega a extenderse lo suficiente.
Habrían de pasar veintitrés años más para que otros dos cirujanos americanos,los doctores Bailey y Brock,el día 19 de febrero de 1948,realizaran esa misma operación de forma reglada.
Esta cirugía se realizaba con el corazón latiendo,es decir,en condiciones fisiológicas,y estaba llena de complicaciones,lo cual producía lógicamente una gran mortalidad.
Los resultados fueron considerados,no obstante,alentadores,pero distaban mucho de ser una solución perfecta y asequible.
Al realizarse la técnica a ciegas,es decir,sin poder ver la válvula lesionada,la corrección sólo podía ser parcial,basada en la experiencia del cirujano que introducía su dedo enguantado o ciertos dilatadores metálicos especialmente diseñados.
Todo ello hacía que las correcciones fueran imperfectas y que produjesen desgarros y trombos en el interior de las cavidades cardíacas ; pero,desde luego,fue el primer camino de esperanza para los miles de enfermos que,en aquellos años,no tenían cura.
Durante los años cincuenta comienza a desarrollarse una técnica que permitirá el paso al verdadero recambio valvular,la circulación extracorpórea.
Es necesario poder abrir el corazón,llegar a su interior y visualizar las válvulas si se pretende reparar el daño de manera directa.
Y Para ello es necesario dejar el corazón exangue,sin sangre en su interior,para pararlo y permitir al cirujano entrar en su interior.
Durante el tiempo que dura la intervención es necesario sustituir la función de la bomba,del motor cardíaco,y a la vez oxigenar la sangre para impedir el daño en cada una de las células del organismo.
Un verdadero reto que hubiera puesto los pelos de punta a los cirujanos y anestesistas de principios de siglo.
Básicamente,el sistema era el siguiente: en primer lugar,se procedía a la derivación de la circulación para puentear al corazón.
Mediante gruesos tubos se recogía la sangre de las venas cavas,que llevan la sangre venosa hasta el corazón derecho,y se desviaba hacia un oxigenador de membrana,donde era puesta en contacto con oxígeno puro,para ser después devuelta hacia la circulación general a través de un nuevo sistema de tubos conectados a las arterias de las piernas.
De esta manera,la sangre podía seguir fluyendo,rica en oxígeno,pero dejando al corazón apartado de la circulación general,libre de su cometido y sin sangre en el interior.
Naturalmente,era necesario mantener la sangre incoagulable dentro del sistema extracorpóreo ; ello se conseguía con la droga más capaz para impedirlo,la heparina,y mediante la inclusión de un pequeño rotor en el sistema que,por simple compresión alternativa de los tubos,permitía el avance de la sangre sin burbujas de aire en su interior.
Al contrario del juego de la confesión policial con el que empezamos este artículo,el juego de los gallinas permite todo tipo de manipulaciones,simulaciones y mixtificaciones.
Los conductores pueden intentar influirse o impresionarse mutuamente jugando con los nervios del adversario o especulando con su fuerza de carácter.
Uno puede fingir al comienzo embriaguez y tirar ostensiblemente su volante por la ventanilla para hacer creer al otro que no cambiará de dirección.
En suma,ambos pueden introducir lo irracional allí donde es de esperar sobre todo lo racional,por no decir lo razonable (de hecho,se trataba de una tesis del doctor Kissinger,que pretendía que,para engañar a los soviéticos,había que dejarles creer que se era capaz de actuar irreflexivamente).
Pronto se observó que,aunque la teoría de los juegos proporcionaba un medio de apreciar los problemas estratégicos de una forma simple,resultaba en exceso somera como para abarcar cuestiones tan complejas como las tensiones entre soviéticos y norteamericanos.
Una representación simple como el juego de los gallinas tiene un carácter demasiado general y rígido como para describir una situación de crisis internacional,donde es difícil prever las reacciones del adversario ; éste puede cometer errores de juicio (hipótesis que no toma en consideración la teoría de los juegos,que presupone que cada adversario se comporta de manera racional,incluso cuando trata de hacer creer lo contrario).
Pero también porque la importancia del resultado puede modificarse con el tiempo,y un determinado objetivo,prioritario en un momento dado,puede resultar secundario en una nueva coyuntura.
En suma,la teoría de los juegos,al menos tal y como era concebida en los años sesenta,era demasiado reductora como para ser aplicable a cuestiones de estrategia ; y,como mucho,sólo podía proporcionar un marco y unas ideas generales.
Steven J. Brams ha tratado de remediar esos defectos introduciendo dos innovaciones capitales en la utilización tradicional de la teoría de los juegos.
Por un lado,emplea la noción de probabilidad allí donde sólo había antes elecciones bien determinadas ; por otro,sustituye el esquema clásico rígido,en el que los participantes seleccionan de una vez por todas su estrategia,por un esquema secuencial,es decir,evolutivo,en el que puedan modificar en cualquier momento su elección inicial.
Un ejemplo estudiado por el propio Brams ilustra su procedimiento ; se refiere al conflicto árabe-israelí de 1973.
El 6 de octubre de ese año,mientras los israelitas celebraban su fiesta religiosa del Yon Kippur,las fuerzas egipcias atacaron por sorpresa el frente israelí del canal de Suez,mientras que,al mismo tiempo,las fuerzas sirias iniciaron la ofensiva en el frente del Golán.
Bajo ese doble choque,el ejército israelí empezó por retroceder.
Pero se recuperó rápidamente ; desde el 11 de octubre lanzó una contraofensiva en el Golán y,cuatro días más tarde,doblegó a las fuerzas egipcias que habían atravesado el canal de Suez.
En pocos días,el tercer ejército egipcio se vio rodeado,totalmente aislado de su retaguardia,sin víveres ni municiones.
Ante la gravedad de la situación,el Consejo de Seguridad de la ONU adoptó,el 22 de octubre,una resolución exigiendo el alto el fuego inmediato.
Pero Israel no parecía tener prisas en seguir dicha resolución,pensando sólo en rematar su victoria.
¿Cómo explicar que se haya optado por la solución A? Para Brams no se pueden comprender las auténticas motivaciones de unos y otros si se contenta uno con la representación matricial que acabamos de ver.
Es preciso recurrir a la representación secuencial,única capaz de describir el proceso intelectual que llevó a las dos partes interesadas a preferir el compromiso a cualquier otra solución.
Analizando ese esquema secuencial,se ve que si los Estados Unidos no aceptan el compromiso (USA 3 - URSS 3),la Unión Soviética,para evitar caer en la peor solución,no podrá menos que pasar a la confrontación directa con los norteamericanos es decir,pasar a USA 2 - URSS 2. En cuyo caso,los Estados Unidos deberán,o bien aceptar la confrontación,o bien dejar manos libres a los soviéticos,lo que resulta ser para ellos la peor solución (USA 1 - URSS 4).
Remontando la secuencia de abajo arriba se observa que,tanto para la URSS como para USA,la mejor solución es el compromiso (USA 3 URSS 3).
La teoría de los juegos,revisada y corregida por el profesor Steven J. Brams,permite una mejor aproximación a los problemas estratégicos,que ya no son tratados de forma estadística,sino que se inscriben en un contexto evolutivo y dinámico más acorde con la realidad.
¿Quiere esto decir que el nuevo método es capaz de resolver todos los problemas relativos a la disuasión nuclear? Ciertamente no,y por diversas razones.
Para empezar,porque sean cuales fueren los refinamientos que se añadan,la teoría de los juegos nunca podrá tomar en cuenta la percepción que un político,un estado mayor o un Gobierno en crisis tienen de una situación dada,el modo en que presienten las reacciones adversas y,por tanto,la influencia de éstas en sus propias decisiones.
Además,porque hay demasiados parámetros psicológicos que no son reducibles a una formulación matemática.
Y,por último,porque la lógica de la disuasión consiste precisamente en desafiar a la lógica ; en tales condiciones,sería cuando menos temerario tratar de introducir en un sistema racional algo que,por naturaleza,es completamente irracional.
En efecto,¿no hay que haber renunciado al sano juicio para pensar en ejecutar la amenaza nuclear,esto es,aceptar la destrucción propia y ajena,incluso la de toda la humanidad? Y la disuasión funda toda su credibilidad precisamente en esta hipótesis de su aplicación... En definitiva,la teoría de los juegos sólo puede proporcionar un marco de razonamiento lógico que podría,eventualmente,ayudar a tomar una decisión.
Pero no ciertamente una modelización fiel de la realidad.
Porque,¿cómo racionalizar lo irracional? La teoría de los juegos no da la respuesta.
Los que no creen en la disuasión nuclear verán en ella un argumento más para considerar que se trata de un sinsentido.
Y los que creen,por el contrario,se verán ratificados en su opinión,puesto que para ellos es precisamente la desmesura de la amenaza nuclear lo que le da toda su fuerza.
Se les llama,por esa razón,seres autótrofos,lo que significa que fabrican sus propios alimentos.
A los animales se les llama heterótrofos porque sólo se nutren de sustancias elaboradas por otros seres vivos,los vegetales.
La mayoría de animales son herbívoros o fitófagos,es decir,comedores de plantas.
Pero algunos de ellos comen otros animales que viven de plantas.
Y otros grupos comen animales que a su vez han comido otros animales que anteriormente se alimentaron de plantas... Y así,en cualquiera de los casos,todas las cadenas tróficas convergen en lo mismo ; todos los tejidos animales son fabricados a partir de una base vegetal.
Las plantas,por tanto,son los únicos seres capaces de producir energía partiendo de sus propios tejidos ; e,inversamente,pueden fabricar sus tejidos partiendo de la energía solar.
Gracias a esa capacidad,los vegetales constituyen el inagotable almacén alimenticio del planeta.
Pero,en realidad,las plantas no pueden formar sus tejidos de la nada.
¿De dónde toman,pues,sus alimentos? ¿Con qué materia prima fabrican sus propios tejidos? Descubrir los fenómenos vitales que realizan las plantas no fue tarea fácil.
La evidencia parecía demostrar que los vegetales obtenían del suelo algún tipo de alimento.
Pero ningún suelo,por fértil que fuese,podía mantener ningún tipo de vida vegetal si no le era suministrada de algún modo una mínima cantidad del agua requerida por la planta.
Parecía,pues,evidente que la tierra y el agua eran los dos elementos básicos que hacían crecer a los vegetales.
Jan Baptista van Helmont,un alquimista del siglo XVII,quiso comprobarlo experimentalmente.
Para ello trasplantó un sauce joven a una maceta llena de tierra,pesando antes el sauce y la tierra por separado.
Después de cinco años,el sauce había aumentado siete kilos y medio,mientras que la tierra sólo había perdido sesenta gramos.
El suelo,por tanto,había contribuido en algo al crecimiento de la planta,aunque en muy pequeña cantidad ; este hecho fue despreciado por Van Helmont,quien argumentó entonces que las plantas obtenían su alimento del agua,y no del suelo,como se había creído hasta entonces.
Ningún alquimista había tenido en cuenta el aire en sus experimentos.
Pero fue precisamente Van Helmont quien primero se fijó en la existencia de un gas,que era,sin él saberlo,un factor primordial en el entramado fisiológico de la nutrición de las plantas.
Era el dióxido de carbono.
En 1772,Daniel Rutherford observó que una vela ardiendo en un depósito cerrado con aire se apagaba al cabo de un rato,por lo que dedujo que además de dióxido de carbono existía otro gas.
Por esa época se sabía que,al retener un animal en un recinto herméticamente cerrado,el aire se corrompía y el animal moría asfixiado.
Un nuevo paso hacia adelante fue dado por Joseph Priestley,en 1774,al descubrir un gas que hacía arder una vela con inusitada brillantez.
Un año después,Lavoissier le dio el nombre de oxígeno al gas de Priestley (procedente de dos palabras griegas que se traducen como productor de ácido y ázoe) al que descubriera Rutherford (sin vida) porque hacía morir a los animales que lo respiraban.
Más tarde al ázoe se le cambió el nombre por el de nitrógeno (productor de nitro).
Fue el químico holandés Jan Ingen-House quien descubrió que eran sólo las partes verdes de las plantas las que podían mejorar el aire,mediante el proceso contrario al que realizan los animales ; éstos consumen oxígeno y desechan dióxido de carbono,como hoy sabemos perfectamente.
Presintió además que ese fenómeno ocurría siempre en presencia de la luz del sol.
Por otro lado,también observó que las flores y otras partes no verdes de las plantas,así como las hojas verdes en la oscuridad,no producían oxígeno,sino que lo utilizaban para respirar del mismo modo que los animales.
Definitivamente,los trabajos de Théodore de Saussure (1804) permitieron deducir la fórmula exacta de lo que ocurre en las partes verdes de las plantas mediante la influencia de la luz solar.
La combinación de seis moléculas de dióxido de carbono con seis de agua,en presencia de energía luminosa,da lugar,en las plantas,a la formación de una molécula de azúcar y seis moléculas de oxígeno.
La fórmula resultaba ser justamente la contraria de la que resume el proceso respiratorio animal y vegetal.
Con los descubrimientos de Inge-Housz quedó demostrado que la producción de alimentos y de oxígeno por las plantas son parte del mismo proceso,y que a partir de sustancias simples del suelo,del agua y del aire,pueden fabricar sustancias complejas.
Este fenómeno fue bautizado por los químicos con la palabra griega síntesis,que significa juntar.
Pero como el proceso sólo podía realizarse con la influencia de la luz solar,fue finalmente denominado fotosíntesis (juntar por medio de la luz).
Preguntarse cómo se verifica realmente la fotosíntesis sería tanto como preguntar sobre el comienzo misterioso de la vida en la Tierra.
Hoy en día se conocen dos principios importantes.
En primer lugar,se sabe que el efecto directo de la luz solar no se realiza sobre el dióxido de carbono,sino sobre el agua,rompiendo sus moléculas.
Esto es posible gracias a la presencia de la clorofila,un pigmento que se encuentra en las hojas y tallos verdes de las plantas y que libera oxígeno y acumula energía en forma de un compuesto inestable llamado ATP (adenosintrifosfato).
Por medio de la energía química acumulada en este compuesto,los niveles de dióxido se reducen para formar azúcares.
Esto fue demostrado por el bioquímico Calvin,en la Universidad de California.
Calvin añadió dióxido de carbono radiactivo en forma gaseosa a algas verdes,y pudo,de ese modo,delimitar el camino seguido por los átomos de carbono ; descubrió así que se habían instalado en el interior de las células,formando parte de las de azúcar.
Si se combina,finalmente,la fórmula de la fotosíntesis con la de la respiración,se formará un amplio ciclo,en el cual todo vuelve a ser lo que anteriormente fue.
La vida en la Tierra resulta,pues,una continuidad indefinida.
El ciclo es impulsado por la luz solar para convertir la energía luminosa en energía química.
Como todo tipo de vida tiene su origen en la energía química,se deduce que el Sol es el responsable de toda la vida en la Tierra.
Las plantas son,día a día,las encargadas de mantener el ciclo de la vida mediante la fotosíntesis,un fenómeno que sólo ellas pueden realizar y del que nos aprovechamos y nutrimos,directa o indirectamente,todos los demás seres vivos.
Tanto en sentido propio como figurado,el ´rtico y el Antártico son realmente dos polos opuestos ; no sólo por encontrarse el primero en pleno norte y el segundo en pleno sur,sino,sobre todo,porque uno de ellos es un mar helado rodeado de tierras,mientras que el otro es un vasto continente aislado en medio de los océanos.
En torno al polo norte viven diversas poblaciones,mientras que en el polo sur no hay vida terrestre.
Como punto estratégico de nuestro planeta,el ´rtico es una zona militarizada que se reparten diversas naciones,mientras que el Antártico,casi exclusivamente consagrado a la ciencia,es una zona desmilitarizada que se rige,como la Luna,por un tratado internacional ; hasta nueva orden,no pertenece a nadie.
También existen un polo magnético sur y un polo magnético norte ; pero resulta que,para enredar más las cosas,el polo magnético sur está en el polo norte,y el polo magnético norte está en el polo sur... Aunque no siempre ha sido así,ya que de vez en cuando,y sin que se sepa por qué,el campo magnético de la Tierra se invierte... Se ha podido constatar que estas anomalías magnéticas tienen duraciones aleatorias,y que la última inversión se produjo hace un centenar de miles de años.
No es,pues,imposible que el polo magnético norte regrese un día al norte y el polo magnético sur al polo sur geográfico,con lo que todo volvería a su sitio y la aguja de la brújula indicaría el sur en vez del norte.
Mientras que en el ´rtico existieron antiguas civilizaciones,las tierras antárticas se han resistido durante mucho tiempo al hombre.
Disimuladas en medio de los océanos,a gran distancia del resto de los continentes,antes de nada fue preciso descubrirlas.
La superficie del planeta no siempre ha presentado el mismo aspecto que hoy: hace 250 millones de años,Laurasia,en el norte,unía América del Norte con Europa y Asia ; el continente de Gondwana,por su parte,enlazaba América del Sur,Africa,India y Australia (que entonces estaba pegada a la Antártida).
Cuando se produjo la primera de las dos dislocaciones que hubo de sufrir ese supercontinente,América del Sur y Africa se separaron primero ; 125 millones de años más tarde se separaron la India y Australia.
La India salió tan rápido a la deriva que llegó a alcanzar Asia en unos 50 millones de años,y el violento choque de ambos continentes dio lugar a la cadena del Himalaya (típica cadena de colisión).
Los fragmentos de ese Gondwana roto se repartieron por la superficie del globo ; la Antártida se puso también a derivar,hasta encajarse en el polo sur,relegada a las antípodas y convirtiéndose en el más frío,ventoso e inaccesible de los siete continentes ; el casquete glacial que cubrió su superficie,hace unos 14 millones de años,no tuvo ya oportunidad de moverse.
Durante centenares de miles de años,el clima de la Tierra fue más frío que el actual.
Unos cuantos grados de recalentamiento bastaron para provocar la fusión de la casi totalidad de los hielos,provocando una subida general del nivel de los océanos (en algunos lugares se han constatado hasta 120 metros).
Sólo la Antártida conservó entonces su casquete de hielo.
Desde hace casi 10.000 años,la Tierra atraviesa una fase de recalentamiento,y los hielos permanentes sólo cubren hoy 18 5 millones de kilómetros cuadrados de los que 16,5 pertenecen a la Antártida (más del 90 por 100 de todo el planeta).
Esto no es de extrañar si se considera el clima particularmente rudo del continente austral: durante seis meses (de marzo a septiembre),el Sol suele estar detrás del horizonte ; las temperaturas medias de los meses más cálidos (diciembre y enero) oscilan entre - 10 y - 35 grados centígrados,y las de los más fríos entre - 25 y - 70 °C.
Aunque es cierto que la temperatura media en la costa es de - 15 °C,lo cierto es que ha llegado a registrarse,en 1978,el récord invernal de - 89,2 °C en la altiplanicie (a 3.500 metros de altitud).
Solemos decir que la Tierra tiene un clima templado en comparación con Marte,cuya temperatura media medida en el suelo no es más que de - 27 octubre -... Por otra parte,la Antártida presenta más de un rasgo común con el planeta rojo.
Como él,es un desierto helado de clima frío,seco y ventoso.
Las precipitaciones de nieve son muy raras y en las regiones centrales sólo cae una media de 300 milímetros anuales.
El aire frío se desplaza del interior hacia las costas y provoca vientos llamados catabáticos,que levantan la nieve y pueden alcanzar los 320 km / hora ; terribles ventiscas que se parecen a las tempestades de polvo observadas en Marte.
Además,en torno a la Antártida,los vientos del oeste soplan de forma continua sin hallar jamás obstáculo ; vistos desde el espacio,esos vientos permanentes toman la forma de un gigantesco torbellino,semejante al observado por encima del polo norte de Marte y que,sin duda,obedece a mecanismos análogos.
El estudio de los terrenos revela también similitudes entre Marte y la Antártida.
En torno a los dos casquetes polares del planeta rojo,los terrenos están estratificados en capas paralelas y horizontales (o casi),alternativamente claras y oscuras,según estén constituidas de nieve,escarcha o hielo,o bien con mezcla de partículas minerales.
Pues bien,estas coloraciones las encontramos también en los depósitos antárticos,a los que las tormentas mixtas aportan nieve y arena.
En el plano de la vida vegetal y animal se observan también ciertas analogías.
Durante mucho tiempo se creyó que Marte podía estar habitado,pero las sondas Viking y Mariner demostraron,con sus exploraciones,que el planeta era,en realidad,estéril.
Con pocas diferencias,la Antártida presenta el mismo carácter desértico: en las regiones costeras,la flora se limita a algunos musgos y líquenes y la fauna a algunos acáridos y mosquitos ; pero en cuanto nos internamos en las tierras del interior,a sólo 500 metros del litoral,estamos ya en un continente muerto.
Existe,no obstante,una diferencia fundamental entre Marte y la Antártida: mientras que en Marte es hielo lo que permanece confinado en el subsuelo,lo que rodea a las estériles tierras australes son mares.
Y eso es importante,porque el agua es fuente de vida.
No existen mamíferos ni pájaros terrestres,pero la fauna marina es abundante: más de 30 millones de focas retozan en el hielo,y todo tipo de pájaros marinos acuden allí a reza la plena libertad de la investigación científica.
El acceso a la totalidad de las estaciones está abierto a todos los observadores debidamente acreditados.
Soviéticos y americanos conviven distendidamente.
Desde el momento de la firma,los signatarios se consultan regularmente.
Polonia se une muy pronto al grupo consultor,y ocho nuevos Estados ratifican el tratado.
Las investigaciones tienen,pues,vía libre.
Los archivos climáticos de la Tierra,atesorados en las capas de hielo del Antártico y que abarcan un período de 200.000 años (alcanza a las grandes glaciaciones cuaternarias),van siendo examinados poco a poco.
Su interpretación permitirá conocer mejor las variaciones del pasado y determinar los diversos parámetros que actúan sobre el clima y aprender la lección para el futuro... Pero la actividad humana crea grandes perturbaciones en la atmósfera terrestre (ver CONOCER,n. o 46,noviembre 1986," Los profetas del verano carbónico ".
Hace poco,al analizar muestras de hielo tomadas del Antártico,un equipo francés ha podido determinar el valor preindustrial de concentración de gas carbónico (260 - 270 ppm en volumen) y constatar así la diferencia con la concentración actual (340 ppmv).
En casos así,hay que dar la señal de alarma,ya que el aumento de CO2 en la atmósfera podría acentuar,según algunos,el efecto invernadero y recalentaría así el clima general del planeta,con las desastrosas consecuencias previsibles.
En cualquier caso,vigilar los polos es como tomarle el pulso a la Tierra... Para el astrónomo,el Antártico es un lugar privilegiado.
Como las noches duran casi seis meses,se puede observar una estrella sin tregua alguna,seguir sus variaciones de brillo y sus pulsaciones,aunque estén muy espaciadas en el tiempo ; también puede determinarse mejor el carácter lunático de algunos astros,como las estrellas variables,las estrellas pulsantes y otros objetos celestes caprichosos.
Es también un lugar ideal para observar el Sol,puesto que la luz solar se prolonga igualmente por seis meses: sin perderlo de vista,se puede ver sus oscilaciones de período amplio y acechar sus imprevisibles erupciones.
No hay ninguna necesidad de levantar la cabeza para ver objetos preciosos,en ciertos lugares,el suelo está salpicado de meteoritos.
Desde que un equipo japonés encontró el primero,en 1970,las colecciones mundiales de meteoritos se han doblado y su número es ya de 8.000.
La mayoría proceden del cinturón de asteroides o son restos de cometas,pero algunos han resultado ser fragmentos desgajados de la superficie de la Luna o incluso de Marte.
Como refugio de pájaros marinos,el Antártico constituye para el biólogo,por último,un ecosistema original que depende no de la tierra,sino del mar.
Las cadenas alimentarias empiezan aquí por las algas y el fitopláncton ; los invertebrados y los vertebrados marinos que se alimentan de ellos son entonces presa de pájaros que traen a tierra materia orgánica,sobre todo bajo forma de defecaciones: al contrario que en el sistema clásico,las raras plantas terrestres y los invertebrados que se encuentran en la Antártida dependen de ese aporte marino.
El Tratado del Antártico nada dice sobre la explotación eventual de sus recursos,pero ya está planteado el problema de saber de quién dependerán.
En un hermoso impulso de generosidad,los doce primeros signatarios proponían que fueran herencia común de toda la humanidad.
¿Pero qué será de todas esas nobles intenciones cuando vuelva a discutirse el tratado en 1991,en vísperas del siglo XXI? PARECÙA una epidemia entre científicos del más alto nivel.
Primero fueron varios astrónomos americanos que veían,hace dos años,un cuásar doble en el objeto celeste denominado 2.016 + 112,a la derecha de la estrella Altair,en la constelación del ´guila.
Luego fueron,ya el año pasado,cinco astrónomos franceses del observatorio del Meudon que sólo veían un objeto celeste en los tres cuásares repertoriados en el catálogo Palomar bajo el número de identificación 1.115 + 080.
Por último,en la pasada primavera,un equipo de investigadores de la Universidad americana de Princeton observó,en el sector de Virgo,dos imágenes anormalmente separadas correspondientes a un único objeto.
Todos estos espejismos cósmicos son conformes a las teorías de la relatividad general de Einstein.
Lo que ocurre es que no siempre es fácil darse cuenta de ello ; y muchos científicos se dejan engañar por tan extraño fenómeno.
Conviene recordar que la luz,según Einstein,puede sufrir las consecuencias de las fuerzas gravitatorias.
De ahí la eventualidad de que un astrónomo,al mirar hacia un astro muy lejano,no lo vea allí donde se encuentra sino al lado ; simplemente porque un objeto de la categoría de los pesos pesados se ha interpuesto,desviando con su gravitación al curso rectilíneo de la luz.
Más aún ; si ese objeto superpesado tiene un tamaño muy pequeño (y hay muchos así),es posible que lo que se observe sea una imagen doble o triple ; porque en tal caso los rayos luminosos del astro podrían pasar a uno y otro lado del objeto pesado,y por eso se le puede observar como objeto doble.
Y además,más grueso de lo que es en realidad ; el objeto celeste superpesado actúa,realmente,como una lente óptica.
En este caso,como una lente gravitatoria.
Hasta ahora ya han sido señalados cinco casos de falsas imágenes dobles o triples ; los tres últimos son los citados al principio de este artículo.
En todos estos casos,los ilusionados astrónomos tuvieron que aceptar que habían visto doble,engañados por invisibles objetos muy pesados que se interponen entre el astro observado y ellos.
Pero,¿qué objetos podrían ser éstos? En tres de los cinco casos citados,se trataba nada menos que de galaxias muy lejanas.
Es más,este verano un equipo de astrónomos americanos señalaba la existencia de una cuarta lente gravitatoria,en un artículo publicado por la revista británica Nature.
Nadie se asombró ya ; es más,los científicos están ideando ahora nuevas formas de averiguar con mayor exactitud el tamaño del Universo,basándose en la existencia y características de estas lentes gravitatorias lejanas.
Nada parece oponerse,en efecto,a la utilización de estas lentes - - ¡y qué lentes! - - para llegar aún más lejos que los telescopios espaciales y otros minúsculos (a escala cósmica) instrumentos de origen humano.
Porque las tales lentes gravitatorias parecen muy indicadas para ayudarnos a responder a la pregunta esencial: ¿Dónde se esconde la masa oculta del Universo? (véase,al respecto,el artículo " La masa oculta del Universo ",en el número 43 de CONOCER de agosto pasado).
Como se recordará,los astrónomos parecen convencidos hoy día de que el 90 por 100 de la masa del Universo es invisible.
Los espejismos cósmicos provocados por las lentes gravitatorias implican,necesariamente,la existencia de una enorme masa invisible que se interpone entre el cuásar observado y nosotros.
Y así,al revelarnos dónde se encuentran estas masas ocultas,las lentes gravitatorias podrían revelar indirecta pero infaliblemente dónde se encuentra la masa oculta del Universo.
El caso del espejismo observado en Princeton en la pasada primavera parecía sumamente prometedor: nadie había podido verla,pero la masa oculta debía ser enorme.
Las dos imágenes del cuásar desdoblado estaban separadas por un ángulo de 2,5 minutos de arco,es decir,treinta veces más alejadas entre sí que todas las que hasta la fecha habían sido observadas.
Según los cálculos,el oculto objeto celeste responsable de la desviación de los rayos luminosos debía pesar tanto como mil galaxias ordinarias juntas.
¿Cómo es posible que tan considerable masa hubiera podido pasar,hasta ahora,desapercibida? La cosa parecía sospechosa,sobre todo considerando la cantidad de telescopios ópticos,radiotelescopios y satélites astronómicos que escudriñaría el Universo con agudeza cada vez mayor.
La única solución estribaba en un agujero negro,esa especie de cajón de sastre cósmico en el que cabe casi todo.
Pero debería ser gigantesco,aunque,eso sí,perfectamente invisible.
Los agujeros negros comienzan siendo de pequeño tamaño,ya que son cadáveres de grandes estrellas que explotaron al terminárseles su combustible nuclear,y posteriormente se contrajeron sobre sí mismas.
DESPUÉS de la traumática explosión del transbordador norteamericano,lo cierto es que los programas espaciales de la NASA han quedado momentáneamente paralizados.
Y cada vez son más numerosas las personas que cuestionan el enorme gasto que las investigaciones espaciales conllevan (véase al respecto el artículo " El colapso de la NASA " en el número 42 de CONOCER,julio de 1986).
Por su parte,y dentro de su modestia no exenta de competitividad,la Agencia Europea del Espacio sufrió,poco después,un nuevo accidente en su cohete lanza satélites Ariane III.
Un accidente que ha impedido nuevos lanzamientos y que ha retrasado todo el programa Ariane ; según las más recientes previsiones,el próximo lanzamiento europeo se realizará a finales del actual invierno.
¿Y los rusos? Podría decirse,con toda propiedad,que sin novedad en el frente.
Después del acontecimiento de la unión entre la nave rusa y una nave americana (Soyuz-Apollo,en julio de 1975),los ingenieros soviéticos optaron por un camino diferente del de los americanos.
En lugar de embarcarse en un programa de nave espacial de ida y vuelta,al estilo del shuffle,intensificaron sus trabajos con vistas a obtener estaciones orbitales semipermanentes,abastecidas más o menos regularmente por naves pequeñas del tipo Soyuz.
Así,los rusos cuentan con el récord de estancia prolongada de cosmonautas en el espacio,lo cual les confiere un significativo avance de cara al establecimiento de misiones científicas o militares casi permanentes en órbita alrededor de la Tierra y,eventualmente,en la Luna o incluso en Marte.
Y ya que hablamos de Marte,conviene decir que existe la intención,todavía no plasmada en ningún acuerdo concreto,de realizar una misión conjunta USA-URSS al planeta rojo.
La delantera que les llevan los soviéticos a los americanos de cara a la larga estancia de los astronautas fuera de la Tierra,tanto en el viaje de ida y vuelta como en la visita a Marte,podría verse compensada por la posible superioridad de la NASA en la cuestión de los motores y cohetes propulsores ; aunque en este terreno es muy probable que la tecnología rusa haya avanzado mucho más de lo que se piensa.
Pero si Marte aún está lejos,no sólo en el espacio,sino también en el tiempo,lo cierto es que los rusos siguen avanzando sin desmayo en la vía de la colonización permanente del espacio próximo a nuestro planeta.
Tras los múltiples ensayos de las estaciones orbitales Saliut,pequeñas e incómodas,pero capaces de albergar cosmonautas ininterrumpidamente durante muchos meses,en febrero de este año 1986 fue lanzada la estación orbital MIR,palabra rusa que significa paz.
A diferencia de la estación Saliut - 7,lanzada en 1971 y todavía utilizable,MIR constituye una novedad en la cosmonáutica soviética.
Su diseño es completamente nuevo,y los ingenieros han tenido en cuenta todas las observaciones que,durante años,fueron aportando los diferentes cosmonautas que vivieron durante meses en las estaciones Saliut.
En particular,Saliut - 7,en la que se batió el actual récord de permanencia en el espacio con doscientos treinta y ocho días (prácticamente nueve meses),a pesar de ser la más perfeccionada resultaba ya muy vieja y sumamente incómoda,además de requerir numerosas maniobras de pilotaje manual que fatigaban inútilmente a sus tripulantes.
En cambio,MIR es la primera estación espacial soviética de la llamada tercera generación,con sofisticados sistemas electrónicos que garantizan el pilotaje automático de la estación,de los diferentes sistemas de a bordo y de la mayor parte de los experimentos científicos.
La alimentación en energía ha sido notablemente incrementada,la conexión con Tierra se ha hecho mucho más fiable gracias a la utilización (reconocida por primera vez por los soviéticos) de una red de satélites intermedios y,sobre todo,la habitabilidad y el confort han sido netamente mejorados.
Cada cosmonauta tiene su propio rincón: un pequeño camarote con una mesa,un asiento y una litera.
Sólo hay una ducha para todos,pero hay varios servicios higiénicos y numerosos aparatos para el mantenimiento físico: tapiz rodante para andar,bicicleta ergonómica,aparatos de gimnasia... La temperatura interior ha sido fijada en 28 grados,tres más que en Saliut,y se mantendrá constante en toda la nave gracias a un sistema permanente de ventilación.
La estación dispondrá de numerosas ventanas circulares,una de ellas en el suelo,con el fin de poder observar la Tierra.
Igual que Saliut - 7,MIR consta de tres compartimientos: dos grandes salas de trabajo y,al otro lado,de las de entrada y salida,un compartimento auxiliar.
La estación,y esto también es novedad,puede recibir simultáneamente la visita de hasta seis naves Soyuz,y a ella se puede unir,en caso necesario,la enorme estación espacial automática y no tripulada Modulny.
Las naves Soyuz y Modulny se enganchan a las piezas de ensamblaje que se encuentran situadas en el eje longitudinal de MIR,por delante y por detrás de la estación.
Un brazo telemanipulador se ocupará de realizar la fijación definitiva de los módulos Soyuz o Modulny.
Estos módulos,y muy especialmente los automáticos Modulny (las naves Soyuz serán sobre todo vehículos de ida y vuelta,auténticos autobuses de carga y pasajeros) son totalmente automáticos,ya que disponen de alimentación autónoma y pueden maniobrar en el espacio dentro de ciertos límites,pero con entera libertad de movimientos.
Cada uno tiene una vocación principal: astronomía,estudio de materiales,investigación biomédica,etcétera.
Los soviéticos todavía no han proporcionado informes exactos sobre ellos,pero al parecer se asemejan a las naves Cosmos 1686 y ciertas versiones modificadas de las naves Soyuz.
Por lo que respecta a las estaciones Saliut,ya pasadas de moda,nada impide que puedan unirse a la moderna MIR.
Aunque los ingenieros no parecen considerar que ello tenga demasiado interés,salvo en caso de emergencias.
Con un cargo Progress,cuatro módulos Modulny y una nave Soyuz,la estación MIR podría llegar a pesar 150 toneladas,con una tripulación total de doce hombres que,en caso de emergencia,podrían ser bastante más.
Como elemento comparativo,habría que añadir que la vieja Saliut - 7 pesa 35 toneladas y sólo podía albergar una tripulación de tres hombres.
Después de múltiples pruebas teledirigidas desde Tierra,la estación MIR fue ocupada por los astronautas Leonid Kizim y Vladimir Soloviev a finales de marzo de 1986,que viajaron a bordo de un vehículo Soyuz T - 15.
Su trabajo consistía en activar todos los sistemas científicos y de mantenimiento de la nave con vistas a su posterior ocupación,en un programa que durará varios años,por diferentes tripulaciones especializadas.
La primera ciudad espacial habitada ha comenzado su singladura.
Y aunque más que ciudad sea un simple villorrio,lo cierto es que se trata de considerable avance en la conquista del espacio por la humanidad.
La Luna y,sobre todo,Marte,esperan.
TODO empieza cuando un ejército de minúsculos espermatozoides corre enloquecido a la caza de un recién nacido óvulo.
Al fin,uno sólo de los miles de pretendientes llega a conquistar ese objetivo.
Y ahí,precisamente ahí,comienza la construcción de un nuevo ser.
Pero,¿cuál es el escenario en el que se desarrolla toda esta función? Es un escenario complicado.
Hay muchas zonas y cada una tiene un trabajo que desempeñar.
Es un mecanismo,por lo demás,que sólo posee la mujer.
Su localización es conocida por todos: el vientre femenino,el útero,un extraordinario órgano cuya flexibilidad le permite pasar del tamaño de una pera en situación normal al de una sandía cuando se acerca el final del embarazo.
Antes del embarazo,la cavidad uterina,de forma triangular,apenas puede contener una cucharada de agua.
Sin embargo,cuando el parto se aproxima,puede dar cabida y sostener a un bebé de aproximadamente cuatro kilos,una placenta de medio kilo y dos litros de líquido amniótico.
Para que ello sea posible,el útero quintuplica su longitud y multiplica su peso por cuarenta.
El 90 % de sus tejidos son músculos potentes que harán uso de su fuerza cuando se trate de expulsar hacia la vida a ese organismo que ha tenido dentro durante nueve meses.
A cada lado del útero,como dos apéndices,se encuentran las trompas de Falopio,de unos doce centímetros de longitud y paredes elásticas,relativamente gruesas y también muy flexibles.
Las trompas cumplen una función muy curiosa: son un poco el maestro de ceremonias en el momento crucial,que será el de la unión entre el espermatozoide y el óvulo.
Cada una de ellas termina en unos delicados y pequeños brazos,llamados cimbrias,cuya sensibilidad les permite conocer en qué momento va a desprenderse el óvulo del ovario.
En ese instante - - la ovulación - - las cimbrias rodean al óvulo,lo abrazan y,mediante una especie de succión,lo aspiran hacia el interior del útero.
Pero antes,y si ha habido un contacto sexual reciente,en cuanto el joven óvulo inicie su viaje por la trompa se encontrará con los espermatozoides del varón,que han salido ya en su carrera enloquecida.
En forma de almendra,los ovarios se diferencian en su estructura y función según la edad,el número de partos y la fase del ciclo menstrual.
No hace falta decir que es en ellos donde se producen los óvulos,del mismo modo que los espermatozoides se originan en los testículos.
Durante la vida genital,los ovarios - - dos,situados cada uno en los laterales de la cavidad peritoneal,ligeramente por detrás de las trompas uterinas - - miden aproximadamente tres centímetros de largo por dos de ancho y uno de grueso,pero más tarde se atrofian.
Por una parte,se encuentran unidos al útero por un ligamento de unos cinco centímetros de longitud,y,por la otra,se comunican con las trompas de Falopio a través de las ya citadas cimbrias.
Cada ovario contiene desde el nacimiento entre 40.000 y 400.000 óvulos inmaduros,de modo que una niña recién nacida cuenta ya,en potencia,con todos los óvulos que irá desprendiendo,mes a mes,a lo largo de su vida reproductora.
Parece ser que los ovarios se alternan en cada menstruación,en una carrera de relevos que permite el descanso de uno de ellos en el intervalo de veintiocho días ; pero esta alternancia no es constante ni obligatoria,como demuestra el hecho de que las mujeres que tienen un solo ovario cumplen la misma cadencia menstrual.
Los ovarios no se limitan a la producción de óvulos.
Son también glándulas endocrinas que segregan las hormonas conocidas con el nombre de estrógenos y progesterona,que ayudarán al desarrollo de la mujer,harán madurar uno a uno los óvulos y,finalmente,contribuirán a que el embarazo sea posible.
La placenta es una barrera,ya hemos dicho,pero su función de filtro no resulta infranqueable para determinados elementos nocivos.
La placenta selecciona las sustancias nutritivas que circulan en la sangre de la madre de una manera muy sencilla: aminoácidos (proteínas),monosacáridos como la glucosa (hidratos de carbono),ácidos grasos (grasas),agua,vitaminas hidrosolubles y sustancias minerales.
Algunas de estas sustancias pasan directamente en solución hasta el feto por simple difusión,mientras que otras se desdoblan en elementos aún más sencillos (de bajo peso molecular) por la acción de enzimas presentes en la placenta.
Estas últimas llegan hasta el feto a través del cordón umbilical.
La formación del nuevo ser pasa por dos etapas claramente delimitadas: el desarrollo embrionario (hasta los tres meses),en el que quedan perfiladas todas las características del niño,y el desarrollo fetal (de los tres a los nueve meses),período en el que se consolidan y aumentan progresivamente de tamaño las estructuras corporales.
Al mismo tiempo que el trofoblasto se desarrolla formando la placenta,se producen cambios en la masa interna de células,que se agrupan en tres capas: ectodermo,mesodermo y endodermo ; cada una de ellas tendrá una misión específica que cumplir.
Las células ectodérmicas,las más externas,formarán el sistema nervioso,muchas estructuras epiteliales (la piel,el pelo,las uñas) y bastantes glándulas endocrinas.
Las endodérmicas formarán el tubo digestivo,las vías respiratorias,las gónadas y los órganos accesorios de la digestión.
Por último,las mesodérmicas (capa celular situada entre las dos anteriores) formarán el corazón,los vasos sanguíneos,todo el tejido conjuntivo,incluyendo los huesos,la sangre,los músculos,las membranas de revestimiento y los órganos urinarios y de reproducción.
A estas tres capas de células,de las que derivarán todos los órganos y tejidos del nuevo ser,se las conoce en conjunto como disco embrionario.
Alrededor del decimoctavo día,el disco embrionario cambia de forma,alargándose hasta tomar un aspecto parecido al de la suela de un zapato.
De un extremo a otro de esta suela se formará un canal,del que se originarán el sistema nervioso,las vértebras y las costillas.
A medida que pasan los días,la vesícula amniótica (una de las dos cavidades,junto con el saco vitelino,formadas en los primeros días de implantación del blastocito en el útero),que era pequeña,aumenta progresivamente de tamaño y,tres semanas más tarde rodeará totalmente el disco embrionario.
Dentro del útero,el feto permanece bañado o,mejor dicho,sumergido en el líquido amniótico,compuesto en un 99 % de agua.
El resto de su composición son proteínas,glucosa y diversas sales minerales,además de urea,ya que el feto expulsa orina en este líquido.
Sus funciones son muy precisas: permite el crecimiento y libre movimiento del feto en el útero,actúa como amortiguador de golpes y mantiene constante la temperatura.
El líquido amniótico lo produce el amnios,una membrana que reviste la cavidad interior del útero y que permanece adherida al corion,la otra membrana fetal que se comunica y continúa en la placenta.
La producción de líquido amniótico es constante,estimándose que se renueva totalmente cada tres horas.
El feto lo traga y lo dirige por el cordón umbilical hacia la placenta.
DEBE quedar muy claro que la naturaleza ha dispuesto el mejor alimento para un recién nacido de la especie humana: la leche materna.
La leche de la madre es la más fácil de digerir.
Los niños criados a pecho son menos propensos a trastornos digestivos,y es una leche adecuada a todos los bebés.
No es ni demasiado fuerte,ni demasiado acuosa,ni poco apropiada.
Tampoco necesita preparación alguna y se encuentra a la temperatura idónea.
Por si fuera poco,la leche materna va cambiando en cada toma,de manera que completa la alimentación.
En los primeros minutos de la mamada,la leche tiene menos grasa,como si fuera preparando el organismo del bebé para recibir después la leche más consistente en los últimos minutos.
El porcentaje de grasa va cediendo de acuerdo con la secreción.
La lactancia mixta se basa en complementar la leche materna con otro tipo de leches,de forma que el niño obtenga la cantidad de alimento necesario por dos vías: la de su madre,con las ventajas de protección que obtiene,y la artificial.
Pero suele ocurrir que el niño encuentra menos dificultad en la succión del biberón que en la del pecho materno,con lo que poco a poco va rechazando el pecho y prefiriendo la lactancia artificial.
No debe confundirse esta lactancia mixta con la complementaria,a partir de los tres meses.
Ha habido muchas modas en torno a la alimentación infantil.
Incluso hubo una época en que la lactancia artificial era considerada como la gran liberación de la mujer con el descubrimiento de unos determinados tipos de leches similares a la materna.
Hoy,todos los especialistas coinciden en señalar que,mientras sea posible,no hay duda de que la madre debe dar el pecho.
La lactancia artificial queda reservada por si,por alguna razón sustancial,el niño no mama.
El sistema de vida y la incorporación de la mujer al mundo del trabajo,así como las investigaciones en torno a la alimentación infantil,han facilitado bastante las cosas para este tipo de alimentación.
Todo aquello que se introduce en la dieta del niño después de su primera etapa láctea,ya sea materna o artificial,constituye la alimentación complementaria.
La base de la alimentación del niño es la leche,pero a partir del cuarto mes se le van añadiendo otras cosas,normalmente cereales.
Para la primera lactancia de fórmula hay dos alternativas: utilizar la misma leche durante toda la lactancia,o,si se prefiere,comenzar con una leche de iniciación y al cuarto o quinto mes seguir con una de continuación.
Hasta el sexto o séptimo mes,las funciones orgánicas del niño están normalizadas.
Se pueden utilizar ya cereales con gluten y se comienzan a introducir otros nutrientes,como las carnes y las verduras.
A esta edad pueden iniciarse las tomas de purés de carnes y verduras,con un aumento progresivo.
Cada nuevo alimento que se introduzca en la dieta,hay que recordar que debe cumplir la regla de las tres " pes " (poco a poco,paulatina y progresivamente).
Desde su nacimiento,más aún,desde el mismo instante de la fecundación,el niño crece,aumenta de tamaño.
El número de años que va a durar este proceso es ciertamente variable,pero lo normal es que en la niña dure hasta los quince o dieciséis años,mientras en el varón se prolonga un poco más,hasta los diecisiete o dieciocho anos.
Todos los tejidos y órganos que componen nuestra anatomía humana van a aumentar de tamaño y a desarrollarse progresivamente,pero lo que entendemos por crecimiento,esto es,el aumento de estatura,se lo deseemos a nuestra parte ósea.
Durante la formación del niño en el vientre de la madre,gran parte de sus huesos,en especial los largos están enteramente constituidos por cartílago.
A partir del nacimiento,este cartílago se va osificando progresivamente ; pero mientras dura la etapa de crecimiento persistirán en los extremos de los huesos largos unas zonas de conjunción,que son las que van a permitir el aumento de tamaño de la pieza ósea hasta su definitiva osificación.
Ahora bien,¿por qué,si su material óseo es idéntico,unas personas son altas y otras bajas? La razón hay que buscarla en una serie de condicionantes relativos tanto al propio individuo o al grupo étnico en el que nace (factores intrínsecos) como al ambiente en el que se desarrolla (factores extrínsecos).
Los factores intrínsecos son los que de una forma más directa actúan sobre el crecimiento infantil.
Podemos citar los principales.
La herencia es el conjunto de factores que se transmiten a partir de los genes de ambos padres.
Es,sin duda,el elemento más decisivo en la futura constitución del niño.
De padres altos,suelen nacer hijos altos.
La raza influye sobre el tamaño definitivo,pero no sobre la velocidad del crecimiento.
Es el denominado crecimiento secular,y en él están muy presentes las condiciones ambientales y de nutrición (históricas) del grupo étnico en el que ha nacido.
El sexo también actúa en el crecimiento infantil ; se sabe que en general la niña mide y pesa menos que el varón en el momento de nacer.
También su velocidad de crecimiento en la pubertad va a ser menor en la niña y,además,terminará antes.
Por lo que respecta a las hormonas neuroendocrinas,en determinados lugares del organismo se producen estas sustancias químicas,hormonas,que van a favorecer o retrasar el crecimiento.
La más importante es la hormona tiroidea (tiroxina) ; es segregada por el tiroides,glándula localizada en la zona inferior del cuello.
Es necesaria para la síntesis de proteínas y normal maduración del cerebro,pero también interviene en la longitud y maduración del hueso.
Actúa principalmente durante los seis primeros meses de vida.
También hay que destacar la hormona de crecimiento,producida por la hipófisis.
Influye predominantemente a partir del primer año de vida.
Finalmente,las hormonas gonadales (estrógenos,testosterona),producidas respectivamente por el ovario y el testículo.
Ambas influyen poderosamente durante el crecimiento que se registra en la pubertad.
Un último factor de crecimiento lo constituye la insulina.
Esta sustancia la segrega el páncreas,y su función mas importante es la de incrementar el transporte de glucosa y aminoácidos a través de la membrana celular plasmática.
Entre los factores extrínsecos se agrupan los elementos extremos al niño,los cuales actúan también de un modo fundamental sobre su crecimiento y desarrollo,como,por ejemplo,la alimentación.
GRACIAS al notable progreso de la microcirugía y a la introducción de medicamentos inductores de la ovulación,muchas de las antiguas causas de esterilidad pueden ser hoy corregidas.
En los casos extremos es posible ya acudir a otros recursos.
Se estima que la esterilidad afecta todavía del 10 al 12 % de las parejas,incluyéndose en ellas a las que han efectuado con regularidad el coito durante dos años sin que de ello se haya derivado un embarazo.
El proceso normal de la ovulación,de la fecundación y de la anidación del huevo en el útero puede estar perturbado por numerosas razones que afecten exclusivamente al organismo femenino: mecánicas - - cuando las vías por las que transcurre el óvulo (trompas de Falopio) están alteradas - -,hormonales - - cuando es la producción misma de las células sexuales lo que falla por lesiones o mal estado del endometrio (superficie interna del útero) - - o por obturación del cuello uterino que impide la ascensión de los espermatozoides.
Es difícil resumir estadísticamente el porcentaje de casos de esterilidad femenina motivados por cada una de estas anomalías (las más importantes,pues la lista podría ampliarse a las esterilidades surgidas tras la adopción de prácticas anticonceptivas,la infertilidad de origen psicológico,las idiopáticas o de origen desconocido,etcétera).
No obstante,se suele estimar como acertada la siguiente relación: Esterilidad endocrina (u hormonal): 20 - 35 %.
Esterilidad por lesión en las trompas: 20 - 25 %.
Esterilidad por anomalías en el endometrio: 20 - 25 %.
Esterilidad uterina: 23 %.
Esterilidad debida al cuello uterino: 5 - 7 %.
Como es lógico,el médico necesita conocer la causa exacta de la infertilidad,como paso previo para dar con el método adecuado que resuelva el problema.
A este fin se emplean hoy tres tipos de pruebas,no necesariamente complementarias en todos los casos.
La curva de la temperatura permite diagnosticar alteraciones en la ovulación o en el asentamiento del óvulo en el interior del útero,que pueden estar causadas tanto por un mal estado del endometrio como por una deficiente producción hormonal.
La mujer debe comprobar todas las mañanas,antes de levantarse,su temperatura rectal durante los dos meses que suceden a la última menstruación.
El test de Huhner requiere de la pareja la práctica del coito entre seis y veinte horas antes del examen.
Este consiste en el estudio de la mucosidad excretada por el cuello uterino,lo que arroja luz tanto sobre su permeabilidad al semen como sobre el número y la movilidad de los espermatozoides.
Finalmente,se analiza el estado de las trompas.
Consiste en comprobar si las trompas de Falopio están o no saturadas,impidiendo el paso del óvulo desde el ovario hasta el útero.
Para ello se utilizan habitualmente dos procedimientos: la inyección de un producto radio-opaco por el cuello del útero,para ver si es capaz de pasar a la cavidad abdominal a través de las trompas,o la introducción de un pequeño tubo a nivel del ombligo (la paciente está anestesiada) mediante el que se ve el interior de la cavidad abdominal,así como el estado del útero,las trompas y los ovarios.
Por lo que respecta a la infertilidad masculina,durante mucho tiempo los hombres se han negado a aceptar que ellos pudieran ser la causa de la esterilidad de la pareja,retrasando así las investigaciones que permitieran el diagnóstico,primero,y el descubrimiento de medidas terapéuticas,después.
Los primeros estudios del esperma se remontan tan sólo a los años cincuenta en los Estados Unidos,de la mano del doctor MacLeod.
Y hoy se sabe que aproximadamente el 50 % de las parejas con problemas de esterilidad tienen su responsabilidad en el varón.
La prueba conocida con el nombre de test de Huhner ya vimos que sirve para comprobar la calidad de los espermatozoides a su paso por el cuello del útero.
Y es ésta precisamente una de las causas principales de infertilidad en el varón: los espermatozoides ; o no hay bastantes o no tienen la suficiente movilidad y fuerza como para ascender por el útero y llegar a las trompas en busca del óvulo.
Exámenes hormonales y biopsias testiculares permiten evaluar también la cantidad de esperma fabricado,y las exploraciones del tracto genital pueden poner en evidencia una obstrucción del mismo que aconseJe una intervención quirúrgica.
El líquido seminal contiene productos químicos excretados por las diferentes glándulas del tracto genital (fosfatos ácidos,de origen prostático,o fructosa,procedente de la vesícula seminal),y la comprobación de las dosis en que están presentes puede alertar al andrólogo sobre el estado de las vías genitales.
(Si ginecólogo proviene del griego gine,que significa mujer,la palabra andrólogo tiene el mismo origen: andros significa hombre.
) La más absoluta de las esterilidades masculinas es la azoospermia ; es decir,la ausencia total de espermatozoides en el líquido seminal.
Afecta a un 8 por 100 de los hombres no fértiles.
Las causas son múltiples y casi todas complejas,por lo que su remedio resulta hoy difícil.
Téngase en cuenta que la causa más frecuente - - el hipogonadismo periférico de origen testicular - - está asociada muchas veces a anomalías cromosómicas,cuando no a factores hormonales.
El hipogonadismo puede ser debido también a infecciones (en concreto,las paperas),a radiaciones,al efecto de ciertas quimioterapias e incluso a la acción de determinados pesticidas.
El problema se complica cuando las estadísticas recientes confirman que alrededor de un 30 % de los problemas de esperma son de origen desconocido.
Finalmente,no hay que desdeñar,como causa de infertilidad en el hombre,los procesos inmunológicos que alteran la barrera hemo-testicular y que son consecuencia generalmente de infecciones o intervenciones quirúrgicas.
Porque falsificar un holograma resulta,desde luego,mucho más difícil que falsificar un número o una cartulina.
Queda el método tecnológico,que se interesa sobre todo por la banda magnética del dorso de la tarjeta ; el cajero lee los datos grabados en esta cinta,auténtica llave electrónica del Sésamo bancario.
Y a esta llave se dirigen,preferentemente,los ataques de los nuevos piratas electrónicos.
Los cajeros automáticos funcionan según dos sistemas: online y offline.
En el primer caso,se encuentran en conexión permanente con el ordenador de un gran centro informático.
Los offline son autónomos durante buena parte del tiempo,y sólo intercambian información con el ordenador central cada 24 horas.
Esta diferencia resulta esencial para los que desean piratear de alguna forma el sistema ; el offline es mucho más vulnerable,lógicamente,que el online.
Veamos con detalle el recorrido de una tarjeta bancaria en un cajero automático offline: nada más introducirla en el sistema,éste ya toma conocimiento de los datos grabados en su banda magnética.
El aparato consta de un microordenador que verifica el período de validez de la tarjeta y calcula,con ayuda de un algoritmo presente en su memoria (generalmente un simple disquete) el código secreto del titular,así como la suma máxima que puede ser concedida en función de lo solicitado y de la fecha de la última extracción.
Aquí nos encontramos con un primer punto débil.
Un sistema offline,por la capacidad limitada de su disquete,no puede memorizar más allá de 2.000 números de tarjetas prohibidas.
Estas son aquéllas que han sido anuladas por mal uso o exceso de morosidad en sus propietarios.
Lo cual resulta muy escaso en relación al número de tarjetas anuladas a cada momento,que debe rondar las 100.000.
En su intercambio de información con el ordenador central,el cajero offline sólo conocerá los 2.000 números mas candentes,ignorando olímpicamente la existencia de los otros 98.000.
Esta noción de número candente corresponde a un factor de probabilidad,desde el punto de vista geográfico ; así,si usted pierde la tarjeta o se la roban,una vez comunicado el hecho,todos los cajeros automáticos de su ciudad incluirán el dato como número candente,pero es muy poco probable que en otras ciudades también puedan hacerlo ; y allí podrá ser utilizada sin problemas.
Claro que,para ello,hay que conocer el código confidencial ; lo cual,como luego veremos,no es tan difícil.
Desde luego,el ordenador central se enterará de esta extracción ilícita y avisará a los cajeros de la región en que se cometió el delito.
Pero entonces,el astuto ladrón podrá desplazarse a otra ciudad y volver a empezar.
Sin poder superar los límites diarios,semanales o mensuales,claro ; pero la operación puede repetirse en muy diversas poblaciones durante meses... Volvamos a nuestra operación habitual ante un cajero automático.
Cuando el aparato,después del primer control,no encuentra en su memoria ninguna razón para negarnos la operación (durante ese intervalo de tiempo nos avisa con un cartelito de " Espere,por favor "),entonces solicita del usuario su número clave,que debe ser tecleado correctamente.
El microordenador compara ese número al resultado del tratamiento algorítmico de una cifra registrada en la tarjeta.
Porque,y esto suele ignorarse,la banda magnética de la tarjeta no contiene el número secreto que cada usuario conoce ; si así fuera,cualquier aficionado a la electrónica podría leerlo sin problemas.
Sólo contiene una serie de cifras que nada tienen q! le ver con el código secreto real ; esta serie es luego descifrada por el microordenador con su algoritmo,a base de una serie de operaciones preestablecidas que convierten las cifras grabadas en un resultado final que,éste sí,resulta ser el código secreto que nosotros tecleamos en el cajero.
A pesar de ciertas alarmas que ocasionalmente se han producido,lo cierto es que esta fórmula del algoritmo ha resultado inviolable hasta ahora.
Pero no cabe excluir la posibilidad de que un especialista superdotado sea capaz algún día de hacerle confesar al microordenador su combinación secreta.
Por lo visto,esto ha ocurrido ya en los Estados Unidos,pero no aún en Europa.
Por otra parte,el algoritmo,por complejo que sea,es necesariamente conocido por muchas personas,aunque sólo sean las que lo han elaborado y las que lo aplican al programar las máquinas.
Y,por mucha honradez que se les suponga,es sabido que todo secreto compartido por varias personas siempre corre el riesgo de ser conocido por alguien ajeno al grupo.
En todo caso,los piratas de la VISA y demás tarjetas bancarias lo tienen mucho más fácil ; la negligencia de los usuarios les allana notablemente el camino.
En efecto,a pesar de todos los avisos recomendando prudencia,muchas personas apuntan su código personal y secreto en algún papel o agenda que suelen llevar consigo,incluso en el mismo billetero en el que guardan la tarjeta.
Y como ésta suele ser robada conjuntamente con el billetero,al ladrón le resulta muy sencillo averiguar cuál es el número.
Los distribuidores online realizan exactamente las mismas fases de tratamiento que los offline,pero se encuentran permanentemente conectados con el centro informático a través de una red de transmisiones especializada y de gran capacidad.
En cambio,en sus intercambios diarios con el centro informático,los sistemas offline pasan por las líneas telefónicas ordinarias.
Los cajeros automáticos online saben,pues,en cada operación,si el usuario tiene su tarjeta anulada ; le da igual que haya 100.000 o varios millones ; recordemos,en cambio,que en el sistema offline sólo se controlaba la ilegalidad de 2.000 tarjetas.
Se elimina así la posibilidad del ladrón que,cambiando de ciudad,podía pasarse meses sacando dinero con una tarjeta anulada.
Y,además,el sistema le permite al usuario legal no sólo disponer de dinero,sino también consultar el estado de su cuenta bancaria y efectuar numerosas operaciones que quedarán instantáneamente registradas en el ordenador central.
Todo son ventajas ; y aunque estos sistemas resultan algo más caros,es obvio que son mucho más seguros y ofrecen un mejor servicio al cliente.
Se puede estimar que en la actualidad,la tercera parte de los cajeros automáticos españoles funcionan ya con el sistema online.
Con todo,la seguridad absoluta tampoco reside en el sistema online.
Aunque estos delitos de piratería electrónica pasan muchas veces inadvertidos,hace unos meses ocurrió en Francia un acontecimiento del que se han podido extraer consecuencias importantes de cara a la seguridad de los cajeros automáticos.
El truco utilizado por la banda de piratas no podía ser más sencillo: el cajero debe grabar los datos de cada operación en la banda magnética de la tarjeta.
Bastaría con producir un gran número de copias de esta tarjeta,antes de ser utilizada,y realizar la extracción máxima permitida con cada una de las copias en distintos cajeros.
Para confundir al centro informático,la operación debe ser realizada con perfecta sincronización cronológica,montando una actuación simultánea en muchos sitios a la vez.
La operación realizada por los ladrones franceses se inició con la apertura de una cuenta bancaria,generosamente provista de fondos.
El banco,encantado con el nuevo cliente,no puso reparo alguno ante la solicitud de una tarjeta.
De este original,perfectamente legal,se obtuvieron numerosas copias ; lo cual no resulta sencillo,pero es posible.
Sobre todo si,como en el caso que estamos comentando,se contaba con la complicidad de uno de los empleados del banco encargado del mantenimiento de los cajeros automáticos.
Los ladrones obtuvieron así un millar de tarjetas.
Luego se dirigieron a distintos cajeros,en los que utilizaron a la misma hora distintas tarjetas,una tras otra.
El ordenador central tardó cierto tiempo en reaccionar ; algo raro estaba pasando: las tarjetas utilizadas eran válidas,el código era exacto,el crédito máximo por extracción se respetaba escrupulosamente.
TRAS la pausa y el silencio,brotan los síntomas.
De momento,sólo son perceptibles por quienes hemos estado aguardándolos desde hace meses.
Pero basta la tímida entonación del primer canto de un mirlo o la hinchazón de las yemas de alisos y sauces para cerciorarnos de que la señal de partida ha sido dada.
Como todos los años,recorro las orillas de la gavia que baja de los neveros.
Viene más alborotada y no ha llovido.
Pronto compruebo que el mismo aumento de temperatura que ha iniciado el deshielo pone en marcha a los inquilinos del torrente.
Algunos,como las ranas,están despertando del letargo que los mantuvo sepultados en los limos del fondo desde finales de noviembre.
Torpes todavía,los jóvenes,metamorfoseados el verano anterior,preceden a sus mayores.
Aquí y allá sus cabezas emergen entre la pelambrera de ranúnculos que,como otro testimonio del inicio de la primavera,nacen en estos arroyos montunos.
Esta vez,en cualquier caso,pretendo oír y no ver.
No hace mucho leí que el croar de estos anfibios fue la primera modalidad de comunicación sonora entre los vertebrados.
Esto quiere decir que hace unos 300 millones de años se moduló la primera,y lógicamente rudimentaria,canción de amor entre los animales superiores.
Para escucharla y describirla he llegado hasta el gran meandro donde,por estar las aguas amansadas,se concentran las ranas para su reencuentro con la vida.
Nada más sencillo y placentero que la contemplación de una vieja ceremonia.
Sobre todo cuando no implica,como sucede con tantas otras especies,el esconderse a la perfección y usar óptica adecuada.
Para observar a las ranas en celo basta una cierta inmovilidad.
Aguantan sin sorprenderse proximidades de tres o cuatro metros,lo que me sitúa en una indiscutible primera fila.
Desde tan privilegiada posición se comprueba que la mecánica de los sonidos de las ranas apenas tiene nada que ver con la del resto de los vertebrados.
Ante todo,porque se produce con la boca y los orificios nasales completamente cerrados.
Esto no quiere decir que no se utilice el paso del aire para provocar una vibración (como hacen las aves,los mamíferos y algunos reptiles),sino que se emplea,una y otra vez,el mismo aire contenido en los pulmones.
Tengamos en cuenta que los anfibios pueden respirar a través de la piel ; es decir,no precisan,como nosotros,introducir aire en su aparato respiratorio de forma continua y regular.
En cambio,el potencial terapéutico de estos anticuerpos monoclonales producidos por hibridomas todavía no ha sido explotado,y apenas se está esbozando.
La primera autorización de uno de estos anticuerpos como agente curativo en medicina ha tenido lugar en los Estados Unidos el pasado verano ; se trata de un producto que se utilizará en los trasplantes de riñón para impedir el rechazo.
Otros anticuerpos monoclonales están siendo ensayados actualmente,sobre todo contra ciertas formas de cáncer.
La fábrica de células de Missouri también trabaja en el tema de las vacunas ; pero se trata de vacunas revolucionarias.
Desde Jenner y Pasteur,se vacuna inoculando microbios no virulentos que desencadenan la reacción defensiva contra la infección de ese mismo microbio activo.
Los microbios no virulentos suelen ser bacterias o virus muertos,o de vitalidad atenuada mediante procedimientos químicos o físicos.
Pero ahora se abre una vía inédita ante los investigadores: vacunar con sustancias que exigen una respuesta inmunológica del organismo pero sin correr el más mínimo riesgo de infección o de reacción negativa.
La primera de estas vacunas ya ha sido comercializada en Estados Unidos a finales del verano pasado: una vacuna contra la hepatitis viral B,afección inflamatoria del hígado que puede llegar a ser muy grave,sobre todo,si degenera en cáncer,cosa relativamente frecuente.
Esta primera vacuna de ingeniería genética será seguida,probablemente,por otras vacunas inmunizadoras sin efectos secundarios.
Toda una revolución en este campo.
En teoría,las células humanas cultivadas invitro deberían ser capaces de producir cualquier sustancia cuya carencia afecta a nuestra salud: proteínas sanguíneas que les faltan a los hemofílicos,hormonas como la insulina,de la que son deficitarios los diabéticos,inhibidores del crecimiento cuya ausencia favorecería ciertos cánceres,enzimas diversos,imprescindibles para el buen funcionamiento del organismo... La evidente vocación industrial de Invitrón como proveedor de productos celulares no impide,antes al contrario,una intensa actividad de investigación fundamental.
Sobre todo,para comprender mejor aún el funcionamiento de las células a nivel molecular ; porque cabe esperar el descubrimiento de moléculas biológicas aún desconocidas y que posean nuevas virtudes terapéuticas o profilácticas frente a las más de 3.000 enfermedades genéticas censadas.
Hasta ahora,las principales herramientas de la ingeniería genética eran diversos organismos unicelulares simples (células procariotas,es decir,bacterias,levaduras...) ; la posibilidad de utilizar células más complejas (eucariotas) de seres tan complejos como los mamíferos abre unas perspectivas todavía difíciles de evaluar pero,con mucho,más prometedoras.
Claro que el cultivo a gran escala de este tipo de células plantea problemas muy delicados.
Primero,porque hay que disociarlas del tejido al que pertenecen.
Después,porque para que se reproduzcan hay que instalarlas en un cultivo muy complejo,auténtico cóctel de los más diversos componentes (a veces más de 50) dosificados con la máxima precisión: sales,glucosa,aminoácidos,hormonas,enzimas,factores de crecimiento,vitaminas,elementos diversos del suero sanguíneo... Por otra parte,las células de mamíferos son muy vulnerables,ya que no se benefician del sistema inmunológico que protege al organismo completo del que proceden.
Una sola bacteria,un solo virus pueden arruinar todo un cultivo trabajosamente puesto en pie durante meses.
De ahí la esterilización rigurosa de la fábrica a la que aludíamos al principio ; los enemigos acechan: micoplasmas,parvovirus... Por eso,a veces se introducen antibióticos en los cultivos,con el consiguiente riesgo: el desarrollo de bacterias resistentes a ellos.
Los reactores biológicos utilizados no sólo deben ser perfectamente herméticos y estériles,sino que también deben cumplir numerosos requisitos físicos y químicos muy especiales: niveles muy concretos de acidez,temperatura,oxígeno,gas carbónico,agua... Las células de los mamíferos plantean,además,un problema particular: para reproducirse suelen exigir un soporte físico.
La multiplicación en pequeñas probetas o pocillos no tiene sentido a escala industrial,y por eso la empresa química Monsanto,de la que depende Invitrón,ha desarrollado un sistema de cultivo celular en soporte sólido informatizado y automático: el reactor de percusión quimiostática.
Una de las principales ventajas del sistema es la elevada concentración de células en relación con el volumen del reactor: unos 30 millones de células por mililitro de cultivo,diez veces más que en los dispositivos clásicos (aunque diez veces menos que la concentración media en los organismos vivos).
Al final de la cadena de producción,la fábrica dispone de sistemas de purificación muy perfeccionados gracias a los cuales se obtienen producciones diarias de hasta varios gramos del producto deseado ; una cifra considerable si se tiene en cuenta que muchas de estas biomoléculas se utilizan en dosis que se miden por millonésimas de gramo.
Invitrón no es la única empresa que aspira a fabricar en serie células humanas con el fin de obtener de ellas moléculas de inestimable valor.
HOY ha cobrado todo su sentido simbólico la conformación redonda de las monedas como expresión física del dinero.
Desde que las telecomunicaciones y la cibernética maridaron,el mundo financiero ha explotado al máximo las posibilidades que planteaba esta unión,enlazando las principales Bolsas de todo el mundo.
Si se estableciese el inicio de las operaciones en Tokio,estas irían siguiendo la evolución solar y la Bolsa de Japón enlazaría con las de Hong-Kong,Singapur y Melbourne,cuyos cierres prácticamente coinciden con la apertura de los mercados europeos,que a su vez solapan su última hora de actividad con el inicio de las operaciones en Wall Street.
De esta forma,el dinero desmaterializado circula continuamente durante las veinticuatro horas del día,a través de los circuitos más complejos de comunicación,a velocidad vertiginosa.
En consecuencia,lo que se ha comprado en Tokio puede ser vendido en Singapur,volver a ser adquirido en Londres o Madrid y ser recolocado en Nueva York.
Estas operaciones se hacen con divisas,deuda pública y acciones,siempre de países o compañías fuertes,sin que sea necesaria ni la existencia física de los títulos bursátiles ni de las monedas que se compran y venden.
Las transacciones se realizan a través de un sistema de anotaciones que se encargan de vigilar las autoridades de cada una de las Bolsas.
Pero,por si todo esto no resultase suficientemente complicado,ha comenzado a desarrollarse la negociación de lo que se llama en la jerga bursátil productos derivados que son los futuros y las opciones.
Con estas fórmulas de negociación ya ni siquiera se negocia una divisa concreta o una acción,sino el compromiso de comprarla o venderla en un plazo determinado.
En definitiva,el dinero se ha rearfimado como una mercancía a la que los últimos avances de la técnica permiten exprimir a conciencia.
Un dolar puede valer mas o menos pesetas no sólo en función de como haya evolucionado la economía española,sino también de cuál haya sido el comportamiento de la capacidad de generar riqueza por parte de Estados Unidos.
Esa relación,en cada momento es lo que se conoce como paridad.
Este mecanismo de relación bilateral tampoco resulta demasiado difícil de comprender,y de hecho estuvo en el origen de una buena parte de los conflictos bélicos que han salpicado la historia.
Si un país había tenido buenas cosechas y su riqueza se había incrementado sustancialmente,solía incurrir en la tentación expansionista y con sus excedentes armaba un ejército con el que incorporar nuevos territorios.
También podía darse el caso contrario,cuando un pueblo hambriento se lanzaba a la conquista violenta de la despensa de un vecino opulento.
Evidentemente,entonces se empleaba una medida tan inmediata como era el estómago para la tasación de la riqueza.
Pero en la medida en que las relaciones sociales se fueron ampliando comenzaron a aparecer los acuerdos y tratados entre distintos pueblos con carácter fundamentalmente defensivo.
A partir de entonces ya había que recomendar cautela a los estómagos chirriantes,si los afanes confiscatorios afectaban n vecino que tuviese sellado algún acuerdo con una potencia bélica de la época.
Se iniciaban así las relaciones multilaterales con los consiguientes quebraderos de cabeza que dieron lugar al género humano.
Este tipo de relaciones constituyen un nuevo peldaño en la dificultad de comprensión de los mecanismos que existen para fijar precios o paridades de unas monedas frente a otras.
Mientras las comunicaciones fueron cosa de las sillas de posta y las diligencias,las relaciones económicas entre los distintos paleoestados se complicaron pero eran controlables,sobre todo para los inventores de los lobbies,la inquieta comunidad hebrea,que se había hecho con el monopolio de la actividad comercial desde la baja Edad Media.
Sin embargo,con la aparición del teléfono todo se complica.
Las posibilidades que semejante medio de comunicación dieron al comercio vinieron a trastocar todas las reglas hasta entonces establecidas.
Los acuerdos verbales se realizaban antes de que el contrato llegara a las manos de los protagonistas de cualquier acuerdo comercial,a pesar de que la falta de un soporte escrito dotaba a aquellas transacciones de un riesgo notable.
Cualquiera de las dos partes podía alegar desconocimiento de lo acordado verbalmente y negarse a cumplirlo sin que hubiese forma legal de impedirlo.
De cualquier forma,el nuevo medio de comunicación era tan ventajoso para los negocios que pronto se extendió el uso de su precursor,el telégrafo,como medio para confirmar los acuerdos mercantiles.
Así fue como el comercio,y con él su hasta entonces inseparable compañero,el dinero,comenzaron a perder su condición física para convertirse en entes que viajaban a través de hilos de cobre.
Ya no hacía falta esperar a que llegase a puerto un vapor cargado de algodón para vender la mercancía.
Desde que era despachada,e incluso durante el viaje,embarcación y carga podían cambiar de dueño varias veces,sin que los compradores hubieran llegado a tomar posesión física ni del buque,ni de su contenido.
AL mismo tiempo,la máquina de vapor daba lugar a la revolución industrial.
Estamos en pleno siglo XIX.
Ya se podía producir mucho más y a menor precio.
Pero para montar una fábrica hacía falta un capital mayor que el necesario para instalar uno de esos talleres artesanales que consolidaron a la burguesía.
Y es aquí donde aparecen las sociedades por acciones.
Un número indeterminado de ahorradores anónimos decidía invertir su dinero en esta o aquella industria confiados en que iban a multiplicarlo en breve plazo.
Para que el proceso tuviera credibilidad había que dotarlo de algunos mecanismos propios que ahuyentaran a los amigos de lo ajeno.
Florecieron las Bolsas en términos muy similares a los que hoy entendemos,como mercados organizados donde se garantizaba la seguridad jurídica de lo que se negociaba.
Es decir que aquellos trozos de papel impreso que se compraban y vendían,efectivamente se correspondían con una participación en la empresa deseada.
Broker.
Intermediario que actúa a comisión,por cuenta de terceros,sin asumir riesgos propios.
Creador de mercado.
Es un intermediario autorizado por una bolsa para realizar operaciones por su propia cuenta.
Su función es posibilitar que siempre haya liquidez,es decir partidas a la compra y la venta.
Divisa.
Es la moneda aceptada comunmente en una transacción internacional y puede materializarse en billetes de banco,cheques bancarios,letras de cambio e incluso pagarés.
Futuros.
Es un contrato que compromete al comprador a adquirir un determinado activo financiero,a un precio previamente pactado y en una fecha también establecida con antelación.
Interbancario.
Mercado en el que los bancos se ceden dinero a plazos muy cortos (entre uno y tres días).
LIBOR (London Interest Banking OHered Rate).
Siglas correspondientes al tipo de interés en el mercado interbancario de Londres.
Mercado de capitales.
Es el formado por los activos financieros negociables emitidos a largo plazo,tanto sean de renta fija (deuda pública o bonos y obligaciones) o de renta variable (acciones de compañías mercantiles) Opciones.
Es un contrato que concede a su comprador el derecho,pero no la obligación,a comprar un activo financiero en una fecha y a un precio previamente fijados.
Durante todo este largo proceso de siglos,ese dinero,que nació como un puro sustituto de la mercancía física,fue adquiriendo un notable protagonismo y por tanto consiguió ser valorado como tal,con independencia de lo que representara.
Pasamos el 90 por ciento del tiempo entre cuatro paredes y quizá no somos conscientes de la influencia que los distintos elementos con los que convivimos tienen sobre nuestra salud.
Está claro que el microambiente interior de la casa nunca es neutral,pero se pueden tomar algunas medidas para que sea un lugar más saludable AL mismo tiempo que aumenta día a día la conciencia para preservar el medio ambiente,se está cayendo en la cuenta de que también es necesario vigilar ese microambiente en el que pasamos la mayor parte del día: los edificios públicos y privados.
La descripción,hace unos pocos años,del síndrome del edificio enfermo ha significado una primera seríal de alarma sobre los inadvertidos riesgos que pueden anidar en el centro de trabajo o en la propia casa.
La Organización Mundial de la Salud define el síndrome del edificio enfermo como un cúmulo de síntomas y signos físicos tales como irritaciones de garganta y nariz,picor de piel y ojos,dolores de cabeza y postulares,cansancio precoz o excesivo,alergias,dificultades de concentración y otras molestias relacionadas con el ambiente laboral.
En general,se admite que un edificio está enfermo cuando al menos una de cada cinco personas que trabajan en él padezca algunos de estos trastornos.
Del mismo modo,cabría hablar de casas sanas y de casas enfermas refiriéndose al microambiente doméstico.
Y esto no depende tanto de la edad del edificio como de sus materiales,orientación,iluminación,la presencia de campos electromagnéticos creados por los aparatos eléctricos,el microclima creado por la contaminación urbana o industrial,el aislamiento,la ventilación,los materiales de alfombras,muebles y demás elementos de la decoración,las pinturas empleadas y un sinfín de factores más.
Obviamente,unos son más fácil de modificar que otros,pero en cualquier caso lo importante es estar informados sobre sus respectivos peligros.
En un centro de trabajo,las medidas más habituales para tratar el síndrome del edificio enfermo son la revisión de ese nido de gérmenes,sustancias alergizantes y corrientes de aire que es el sistema de aire acondicionado.
Otras medidas consisten en colocar pantallas protectoras en los ordenadores y sillas más ergonómicas,adaptadas al cuerpo humano.
En el propio domicilio hay que tener presente,como criterio general,que cuanto más ecológica,armoniosa y natural sea la casa,existe menos peligro de causar esta clase de trastornos.
La ventilación exterior,la iluminación natural y un buen aislamiento térmico y acústico son requisitos básicos de todo hogar saludable.
También hace falta mantener una humedad relativa de entre el 35 y el 60 por ciento (pueden utilizarse humidificadores del aire para contrarrestar la sequedad que produce la calefacción) y una temperatura entre los 18 y los 24 grados centígrados.
Además,hay que evitar al máximo la presencia de todo tipo de contaminantes químicos.
Algunos pueden estar presentes en los materiales de construcción,como el radio y el radón (en el hormigón,ladrillos...),los formaldehídos (en el aislamiento de la espuma de las paredes huecas y en algunos yesos y escayolas) o el asbesto (en algunos materiales de aislamiento).
Los productos de limpieza,insecticidas,y ambientadores pueden contener también irritantes de la piel,vapores tóxicos y hasta cancerígenos,por lo que es aconsejable emplear el menor número de estos productos y asegurarse de que los que se utilicen sean naturales y seguros.
Muchas pinturas y barnices,así como maderas y muebles tratados,también pueden desprender vapores tóxicos.
Es especialmente irritante el formaldehído o formol,un compuesto orgánico volátil muy empleado como agente adhesivo en los productos de la madera (especialmente en los tableros de aglomerado) y plástico,en moquetas y hasta en ropa de cama y de vestir.
Son tantas las sustancias químicas presentes en el hogar que parece recomendable utilizar utensilios y elementos decorativos naturales.
Asimismo,hay que limitar el número de aparatos eléctricos,sobre todo en el dormitorio,para evitar los efectos de sus campos electromagnéticos.
Para crear un hogar saludable hay que considerar también desde el colorido y la decoración hasta la ubicación del dormitorio en la habitación más tranquila,entre otros factores.
Pero quizá lo más importante sea la sensibilidad individual,tanto a una sustancia química como a los aproximadamente diez kilos de polvo que se acumulan al año en cada casa.
Está claro que no se trata ni de volver a las cañas y el barro ni de vivir en un ambiente plástico,cosido de cables y cargado de vapores tóxicos,sino de guiarse por un sentido común bien informado y pedir que se estudie más a fondo qué es eso de la casa saludable.
A falta de aparatos de medida más fiables,el olfato es el mejor instrumento para detectar las sustancias químicas desagradables,algunas de las cuales son,además,tóxicas.
Antes de incorporar cualquier nuevo producto al hogar,ya sea una alfombra,una mesa o un ambientador,conviene olerlo previamente.
Muchas alergias,irritaciones de la piel y ojos e incluso alguna pérdida del olfato no existirían si se evitara la exposición continuada a los malos olores.
Sin duda,el olfato debe incorporarse al diseño de interiores.
Esto es precisamente lo que está haciendo un grupo de expertos oledores de la Comunidad Europea para diagnosticar,a base de oler alfombras y cortinas,el síndrome del edificio enfermo.
Y cualquiera puede hacerlo en su propia casa,sustitullendo progresivamente todos aquellos materiales o elementos que no le huelan bien.
MICROBIOS son,por definición,todos los organismos invisibles a simple vista,aunque hoy se prefiere la palabra microorganismo.
Como quiera que antes de su descubrimiento todas las cosas vivas eran o animales o plantas,con los microorganismos se quiso hacer lo mismo.
Pero estos diminutos seres forman un mundo aparte,pues ni los hongos son propiamente plantas ni los protozoos animalillos.
El mundo microbiano no es fácil de clasificar,pero se admiten dos grandes grupos: los eucariotas o protistas,donde se incluyen mohos,hongos,protozoarios y algas ; y los procariotas o bacterias.
Aparte están los virus,ya en el límite de lo vivo.
Los microorganismos que habitan en el cuerpo humano son,en su mayoría,bacterias.
Tres son sus formas más típicas: esférica (cocos),como los estafilococos de la piel (Staphylococcus epidermidis) o los estreptococos,de bastón (bacilos),como el lactobacilo de Doderlein,de la vagina ; y de sacacorchos o espiroquetas.
Pero también hay otros microrganismos,desde hongos y levaduras,como los de la especie Cándida,que habitan en la boca o los del género Trichophyton que provocan el pie de atleta,hasta los virus,que suelen estar de paso provocando enfermedades como el catarro común,aunque otros se instalan sin que haya forma de expulsarlos,como ocurre con el del sida.
La presencia de muchos de estos diminutos huéspedes es tan habitual que se habla de una flora microbiana residente,por oposición a la transitoria o que está de paso.
Los microorganismos residentes,bacterias en su mayoría,son comensales fijos,y se consideran tan de casa que una de sus principales funciones es la de impedir que se asienten los gérmenes que vienen de paso,potencialmente peligrosos.
Esta flora transitoria está compuesta por microorganismos que se hospedan en la piel o en las mucosas (de la boca o de la vagina,por ejemplo) durante horas,días o a lo sumo semanas.
Los microbios transeuntes provienen del ambiente,en general no son patógenos y no llegan a asentarse por sí mismos.
Sólo cuando la flora residente sufre daños,los microorganismos que estaban de paso pueden aprovechar la situación para proliferar y asentarse,llegando a producir enfermedades.
La flora vaginal es especialmente protectora,está compuesta por microorganismos fijos que permanecen de forma constante en un lugar determinado,aunque con la edad puede haber algún cambio en el tipo de inquilinos.
Así,por ejemplo,en la vagina se instala al poco de nacer el lactobacilo de Doderlein,que es la bacteria predominante durante las primeras semanas de vida,mientras el pH es ácido.
Al hacerse neutro,la flora residente se compone básicamente de una mezcla de cocos (estreptococos anaerobios y hemolíticos del grupo B,principalmente) y bacilos.
Esta flora mixta dura hasta la pubertad,cuando el Ph se torna de nuevo ácido y reaparece en grandes cantidades el lactobacilo de Doderlein.
Con la menopausia,vuelven a disminuir los lactobacilos en beneficio de la flora mixta.
Como los gérmenes residentes son comensales fijos con su plato garantizado,el que prosperen en un sitio depende fundamentalmente de la existencia o no de su alimento,así como de factores fisiológicos como la temperatura,la humedad o la presencia de sustancias inhibidoras segregadas por el cuerpo.
La simbiosis es lo suficientemente perfecta como para garantizar que estos gérmenes residentes no sólo no sean perjudiciales,sino que en algunos casos favorezcan la salud.
Así,por ejemplo,algunas bacterias del intestino sintetizan la vitamina K y ayudan a la absorción de los nutrientes.
Sin embargo,en ciertas condiciones la flora residente puede causar enfermedades.
Por ejemplo,cuando estos microorganismos son removidos violentamente de su lugar habitual y pasan a la sangre.
Esto es lo que puede ocurrir,sin ir más lejos,con el Streptococo viridans,uno de los residentes habituales de la boca y la faringe,que en una extracción dental o en una operación de amígdalas puede pasar de forma masiva a la sangre y afectar las válvulas cardíacas.
Otro tanto ocurre con los bacteroides intestinales cuando,como consecuencia de un traumatismo,salen en masa a la cavidad abdominal y pueden llegar a provocar graves infecciones.
Por eso,a estos gérmenes residentes,normalmente inocuos y beneficiosos,se les considera en algunas ocasiones oportunistas.
A menudo,el cuerpo humano está además habitado por una auténtica fauna microscópica,verdaderos animalillos con fauces y garras.
La mitad de la población,por ejemplo,posee una colonia de ácaros que vive en las pestañas,en los folículos pilosos o en las glándulas sebáceas.
Estas diminutas arañas,de unos 0,3 milímetros de longitud,son translúcidas,tienen cuatro pares de patas achaparradas unidas al tórax y,aunque normalmente son inofensivas,pueden provocar descamación.
Otros inquilinos ocasionales como las ladillas o los piojos tienen un aspecto al microscopio igualmente amenazador.
Gusanos que llegan a medir 10 metros,colonizan nuestro cuerpo Los residentes de mayor tamaño que pueden colonizar nuestro cuerpo son,sin duda,algunos parientes de los gusanos.
Según la Organización Mundial de la Salud,la cuarta parte de la población mundial está parasitada por el Ascaris 1 umbricoides,un gusano que vive en el intestino delgado y sólo da señales de vida cuando... ¡asoma la cabeza por la boca o la nariz!.
Otros muchos millones de personas están colonizadas por distintas tenias,entre las cuales la más impresionante por su tamaño es la Taenia saginata o gusano de la carne,que llega a medir hasta diez metros,aunque es más inofensiva de lo que aparenta.
Tampoco es rara la solitaria o Taenia solium,causante de la cisticercosis porcina,aunque sí más grave.
Otros gusanos que parasitan a centenares de millones de personas son los tricocéfalos y los anquilostomas,que como la mayoría de los parásitos intestinales abundan sobre todo en áreas subdesarrolladas.
En los países industrializados quizá los inquilinos más habituales sean los oxiuros o lombrices infantiles,unos gusanillos blancos que no suelen ser muy perjudiciales y rara vez producen algo más que una simple irritación anal.
En su conjunto,las criaturas que nos habitan,por muy terrible que pueda ser su aspecto al microscopio,son mayoritariamente inofensivas.
Por suerte,casi todas ellas son invisibles a nuestros ojos.
Así que cuando se vean dragones y serpientes ascendiendo por un brazo o descolgándose por un hombro,lo más probable es que simplemente se trate de un tatuaje.
AUNQUE cualquier alteración de la flora residente se restituye con rapidez,conviene cuidarla para evitar la aparición de otros gérmenes no tan benéficos.
En el tubo digestivo,lo principal es evitar la administración indiscriminada y sin prescripción médica de antibióticos que destruyen la flora intestinal y crean resistencias.
Como la dieta tiene una influencia notable en la composición de la flora intestinal,no hay sólo que evitar la ingesta de productos contaminados con gérmenes patógenos,sino también el abuso de algunos quesos ricos en bacterias muy distintas a las del intestino.
En la piel,ni el lavado,ni el baño,ni la sudoración abundante pueden eliminar o alterar significativamente la flora local,pero para una mejor higiene y salud dermatológica es preferible utilizar jabones con un Ph fisiológico y evitar los lavados intensivos de zonas que,como la vagina,poseen una flora especialmente protectora.
LAS inundaciones catastróficas en el País Vasco y en numerosas regiones mediterráneas en 1982 y 1983 y las lluvias torrenciales del mes de noviembre de 1984 en Cataluña y en Valencia tienen un responsable fácilmente identificado ya por la opinión pública: la gota fría.
Un fenómeno atmosférico todavía mal conocido y difícilmente previsible que todos los otoños tiene en jaque a los meteorólogos españoles.
Las lluvias torrenciales en las regiones mediterráneas españolas no son ninguna novedad en otoño.
Raro es el año en que no se producen inundaciones y riadas en Cataluña,Valencia,Murcia o Andalucía.
La causa es bien conocida: la llegada de aire frío sobre un mar cálido,que provoca la formación de enormes sistemas nubosos con lluvias muy intensas.
Sin embargo,últimamente se ha puesto de moda un término hasta ahora reservado al lenguaje científico más especializado: la gota fría.
Un fenómeno al que se le achacan males sin cuento,y que conserva cierto misterio por la dificultad que suele ofrecer su localización preventiva,antes de que sus temibles efectos se ejerzan sobre valles,campos y ciudades.
La gota fría tiene en realidad un tamaño considerable,con bastantes centenares de kilómetros cuadrados de superficie ; consiste en una masa de aire bastante más frío de lo normal,que gravita en altura asociada a una depresión de los altos niveles atmosféricos,sobre una zona de la superficie en la que no necesariamente existe borrasca.
Esta masa fría,a modo de gran burbuja aislada del aire polar situado mucho más al norte,tiende a descender hacia el suelo,por su mayor densidad.
En este descenso provoca una auténtica subversión de la atmósfera,inestabilizando notablemente el tiempo.
Los movimientos verticales (aire frío descendente,aire cálido y húmedo ascendente) facilitan la formación de gigantescas nubes de desarrollo vertical,los cumulonimbos,capaces de descargar a veces varios centenares de litros por metro cuadrado en una hora.
En las regiones mediterráneas españolas,la aparición de una gota fría suele dar lugar a lluvias torrenciales casi siempre.
Veamos con un poco más de detalle lo que realmente ocurre.
La sola presencia de una de estas gotas frías en los altos niveles de la atmósfera puede no bastar para producir una catástrofe.
Hacen falta otros factores desfavorables conjugados: por una parte,aire inestable en superficie,y por otra una gran capacidad de evaporación del mar,como base esencial de la humedad que posteriormente alimentará a las nubes.
El mar Mediterráneo es un mar muy cálido en otoño.
Ello es debido a que,por inercia térmica (el agua se enfría,y se calienta,mucho más despacio que la tierra o el aire),la acumulación del calor de verano todavía perdura en otoño,hasta el punto de que en octubre puede estar más caliente el agua que en julio,aunque la temperatura del aire pueda ser de diez grados menos.
Un mar caliente facilita enormemente la evaporación.
Si,además,le llega aire frío (procedente del Atlántico,o en el caso de una gota fría,procedente de los altos niveles de la atmósfera),la consecuencia es inmediata: una intensísima evaporación y la formación de enormes nubes.
Cuando además de todo ello existen condiciones propicias para la inestabilidad (por ejemplo,una borrasca en superficie centrada en el sureste de la península),entonces la situación se convierte en casi explosiva: las nubes tienen su base cerca del suelo,a unos trescientos metros,pero pueden alcanzar en su cima más de doce kilómetros.
Dentro de ellas se producen violentas corrientes de vientos ascendentes y descendentes ; y por debajo de ellas descargan,a veces con intenso aparato eléctrico,chaparrones intensísimos,que en pocos minutos convierten en torrentes desbordados lo que hasta entonces eran resecos cauces sin apenas agua.
La gota fría es,pues,una condición necesaria,pero no suficiente.
De hecho,se trata de un fenómeno muy frecuente en primavera y en otoño,pero también se da en verano ; muchas tormentas estivales de difícil predicción suelen ir asociadas a pequeñas gotas frías en altura,no detectadas por los sistemas de observación más modernos.
Pero en otoño,y muy especialmente sobre el cálido Mediterráneo,la gota fría es,por así decirlo,el detonante de una situación potencialmente explosiva.
Un detonante que,como es fácil de recordar,se produce con cierta frecuencia en nuestro país.
En el País Vasco este tipo de situaciones se producen de forma muy diferente,y de hecho allí son frecuentes los temporales atlánticos pero,en cambio,son rarísimos los fenómenos convectivos asociados a gotas frías.
No obstante,el año pasado también se produjo un fenómeno que podría asemejarse a los que habitualmente aparecen en el Mediterráneo ; en ello influyó sin duda el hecho de que el Cantábrico,normalmente un mar frío,tuviese temperaturas anormalmente altas,pero ello no explica por si sólo la violencia de la lluvia.
Otros factores atmosféricos se conjugaron para ocasionar la catástrofe,y bien se puede afirmar que se trató de una casualidad muy poco probable,pero precisamente por ello aún más peligrosa.
La detección de las gotas frías no es sencilla.
En primer lugar,por su reducido tamaño: aunque ya hemos visto que puede ocupar centenares de kilómetros cuadrados,a escala planetaria se trata de la mínima expresión de una borrasca.
De ahí el nombre,algo despectivo,de gota fría ; una gota sin duda,frente a la inmensidad de la atmósfera,pero sumamente peligrosa por sus efectos.
Una segunda dificultad para su detección estriba en la rapidez con que se forma y evoluciona.
Y,en tercer lugar,habría que precisar que estos fenómenos,al no producirse en superficie,ofrecen mayores problemas de localización y estudio porque no existen observatorios flotantes en la alta atmósfera.
En efecto,hasta hace pocos años,la única posibilidad de conocer la atmósfera superior estribaba en el lanzamiento de radiosondas.
Se trata de globos que arrastran en su ascenso a una pequeña emisora que,automáticamente,mide los valores de temperatura,presión,humedad y viento y los transmite a tierra de forma continua hasta que el globo,a una cierta altura (entre 15 y 20 kilómetros) explota,cayendo todo el conjunto al suelo.
Se trata de un método caro,porque en cada lanzamiento se pierde normalmente todo el material ; la emisora de radiosonda lleva un paracaídas para amortiguar la caída y la dirección postal del centro que lo lanzó para que sea devuelto por quien lo encuentre,pero lo cierto es que se recuperan muy pocos.
Lo cual significa que no se pueden hacer muchos radiosondeos de este tipo,porque resultaría prohibitivo,y además los pocos que se hacen son lanzados desde lugares tan distantes como Madrid,La Coruña o Palma de Mallorca,por ejemplo,entre los cuales muy bien puede colarse una gota fría sin que nadie llegue a detectarla.
Muy recientemente,los satélites artificiales nos ofrecen no sólo fotografías de los sistemas nubosos,sino auténticas radiografías de lo que acontece en la atmósfera: temperatura en vertical,desplazamiento,contenido de vapor de agua,etcétera.
Todos estos datos son susceptibles de ser tratados mediante ordenador,obteniéndose así imágenes muy completas del estado atmosférico a cada instante.
LAS fotografías muestran por primera vez en el mundo cómo era el planeta Tierra en diferentes momentos del pasado,gracias a la ayuda de la informática.
La deriva de los continentes es el nombre de una teoría que afirma que las placas se mueven lentamente las unas contra las otras,y que lo vienen haciendo desde el más remoto pasado de la corteza terrestre.
Esta teoría había sido propuesta por el geólogo,meteórologo y explorador Alfred Wegener a principios de siglo,y sólo fue tomada en serio a partir de la década de los sesenta.
Para comprender el movimiento de los continentes,es importante conocer cuál es la estructura de nuestro planeta.
En general,los primeros cien kilómetros de la corteza forman una especie de concha rígida,dividida en diferentes fragmentos o placas.
Más abajo,las rocas siguen siendo sólidas,pero su elevada temperatura,creciente con la profundidad,hace que su comportamiento sea semiviscoso.
Los fragmentos de corteza,las llamadas placas tectónicas,son bastante delgados en espesor respecto a su extensión superficial.
Por ejemplo,la llamada placa del Pacífico mide varios millares de kilómetros de longitud,pero su espesor medio apenas llega a los cien kilómetros.
Teóricamente,las placas son rígidas y se mueven de forma global.
Estos movimientos pueden ser de tres tipos: las placas se acercan unas a otras,o bien se alejan entre sí o finalmente se deslizan unas a lo largo de otras.
Cuando dos placas chocan entre sí,una de ellas cabalga a la otra.
Esta se ve forzada,así,a hundirse en el manto interior viscoso y caliente.
Los continentes no sufren este efecto,llamado subducción,que sí se produce,en cambio,en las partes oceánicas de las placas tectónicas.
Los márgenes de las placas en los que este fenómeno aparece nos ofrecen una especie de trincheras en las que la profundidad marina alcanza importantes valores: son las fosas tectónicas.
Buen ejemplo de este tipo de fosas nos lo ofrece la zona oeste de los Andes,en la que el suelo del Océano Pacífico se desliza bajo los Andes a razón de unos cuantos centímetros por año.
El movimiento genera calor,y éste es evacuado hacia arriba en forma de volcanes,con los que la cadena montañosa crece.
Cuando dos continentes colisionan,la subducción se detiene ; así ocurrió con el Himalaya,al unirse Asia y la India.
Cuando la unión tuvo lugar,un continente más amplio reemplazó a los que anteriormente estaban separados.
Como la Tierra ni se expansiona ni se contrae,lo cierto es que las áreas consumidas deben ser reemplazadas por nuevas áreas.
Estas se forman precisamente allí donde dos placas tectónicas se alejan una de otra: la nueva superficie aparece bajo el mar,mediante toda una hilera de volcanes submarinos que conforman una cadena montañosa en el centro de los océanos,llamada rift.
Uno de los mejores ejemplos de este sistema lo constituye la isla de Islandia,que crece en longitud unos dos centímetros anuales y que no existía hace tan sólo 90 millones de años.
Por entonces,Groenlandia estaba unida a América y a Europa ; la creación del Atlántico norte rompió en varios pedazos lo que antes era un enorme continente único.
También ocurrió,más recientemente,algo parecido con el Golfo de Adén y el Mar Rojo,cuya aparición separó Arabia de Africa,antes unidos en un único continente.
Finalmente,el tercer tipo de movimiento entre placas lleva a una de ellas no a chocar con otra sino a deslizarse tangencialmente a ella.
Así ocurre,por ejemplo,a lo largo de la falla de San Andrés,en California.
La ciudad de Los ´ngeles pertenece ahora a la placa del Pacífico y se desliza hacia el norte,con una velocidad de unos seis centímetros por año,respecto al resto del continente norteamericano.
Por supuesto,estos movimientos de las placas tectónicas que se alejan,o bien se acercan chocando,o bien se deslizan en permanente roce,provocan terremotos y temblores de tierra de muy diversa intensidad.
Generalmente,los más violentos se dan en el caso del deslizamiento por rozamiento y en el choque entre placas.
Lo que todavía no ha conseguido elucidar la ciencia es el porqué de esta deriva de las placas tectónicas.
Las rocas del fondo del océano son más cálidas y pesadas que las rocas del interior de la Tierra.
Una vez que comienzan a hundirse,en principio continúan haciéndolo ; y así,una fuerza importante que explicaría los movimientos tectónicos obedecería a la presión ejercida por antiguas y ya enfriadas rocas del fondo oceánico sumergidas en el manto interior.
No conocemos ningún fondo oceánico que tenga una antigüedad mayor de 200 millones de años,excepto algunas zonas aisladas en áreas de colisión y que han resurgido desde el Interior.
Esto significa que los más antiguos fondos oceánicos han desaparecido.
Lo cual nos lleva a pensar que el ciclo de reacción y destrucción de los fondos oceánicos lleva funcionando desde mucho antes que esos 200 millones de años,quizá incluso desde la propia formación de la Tierra como planeta,hace 4.500 millones de años.
Las fotografías muestran el trabajo final de la computadora,trazando mapas en dos tipos de proyección cartográfica.
La primera de ellas en proyección cilíndrica,y la segunda en proyección ortográfica.
Esta intenta mostrar el planeta tal y como se vería desde el Espacio exterior ; como es imposible representar así todo el planeta,el trabajo se refiere sólo a la zona que incluye América,Africa y buena parte de Eurasia,con el Atlántico como centro.
La proyección cilíndrica,menos ajustada a las dimensiones reales cuanto más nos alejamos hacia los polos,nos ofrece en cambio una visión conjunta de toda la superficie terrestre.
Esta serie de gráficos realizados por computadora sólo cubre la novena parte de la historia geológica de la Tierra,es decir,los 500 millones últimos de sus 4.500 millones de años de existencia.
Es posible imaginar que movimientos similares se produjeron anteriormente,pero necesitamos conocer muchos más datos de los que ahora tenemos para poder siquiera imaginar cómo fueron tales movimientos.
En todo caso,este trabajo nos lleva a dos conclusiones sumamente importantes: los continentes se unieron y desunieron en el pasado y además sufrieron importantes cambios de latitud.
La trascendencia de estas conclusiones puede ser fácilmente comprobada con un ejemplo.
Hoy día,Australia es un continente aislado en el que se han desarrollado una fauna y una flora que le son características.
Pero si un continente se unió a otro en el pasado,se produjo entonces la mezcla de sus respectivos animales y plantas,dando lugar a importantes cambios evolutivos posteriores.
Lo cual tiene una tremenda importancia para el estudio biológico de la evolución de las especies,al poder retroceder en el tiempo según se observan los distintos cambios estructurales en los fósiles que se estudian hoy día,pero que pertenecen a épocas en las que los continentes no eran lo que actualmente son.
Los cambios de latitud de las placas continentales hacen que los sedimentos que se depositan cambien con el tiempo.
Por ejemplo,hace 35 millones de años la mayor parte de Europa y Norteamérica se encontraba en latitudes tropicales,en las que predominaban selvas y bosques similares a los que hoy se dan en la Amazonia o en el Africa ecuatorial.
Los restos de este vegetación abundante y gigantesca fueron sedimentándose subterráneamente,y hoy día existen bajo forma de carbón.
Por otra parte,hace 250 millones de años,estas áreas viajaron a través de regiones subtropicales similares a las que hoy ocupa,por ejemplo,el desierto del Sahara.
En este tránsito de unos cuantos millones de años de duración,los sedimentos vegetales se posaron sobre arenas y lechos salinos.
La arena y la sal han formado así excelentes trampas en las que los hidrocarburos líquidos y gaseosos quedaron aprisionados y aislados del exterior.
Hoy día las prospecciones petrolíferas proporcionan a este petróleo y a este gas natural la primera posibilidad de escape hacia el exterior,después de un aprisionamiento de más de 200 millones de años.
Para terminar,vamos a explicar someramente cómo hemos enfocado el trabajo para que la computadora pudiera ofrecernos el resultado final en forma de mapas.
EL largo y complejo proceso de dar vida a un nuevo tipo de avión se inicia a partir de dos posibles alternativas en cuanto a su uso: civil o militar,con las diferencias básicas que ello comporta.
En el primer caso existen ciertas facilidades,puesto que se conocen de antemano la velocidad de crucero a la que volará el aparato más del 80 por 100 de su tiempo,así como la altura óptima de vuelo.
Esto sólo despeja muchas incógnitas al equipo creador y,al mismo tiempo,elimina muchas servidumbres.
Como contrapartida,las exigencias económicas (de consumo de combustible,espacios de tiempo entre revisiones,etcétera) suelen ser muy fuertes,puesto que normalmente el aparato entrará en un campo de durísima competencia y el posible comprador se fijará en ellas prioritariamente,estableciendo comparaciones implacables y definitorias.
Este aspecto monetario que antes quedaba en segundo plano entre las Fuerzas Aéreas,más preocupadas por conseguir prestaciones altas,está cobrando importancia decisiva.
Como explicaba recientemente Lee Begin,jefe.
de diseños de Northrop,se tiende cada vez más a enfocar los proyectos militares como estudio global ; éste incluye datos que van desde las horas de vuelo hasta cuidadosos estudios de financiación y contrapartidas económicas que pueden abarcar sectores completamente extraños (turismo,agricultura...).
El caso del F - 18 FACA,precisamente fruto del equipo de Begin,es un buen ejemplo de complicación en estos aspectos.
Si nos centramos en el sector militar,con su grado adicional de sofisticación,el primer paso consiste en definir las características operacionales del futuro avión.
Estas aclaran aspectos básicos de la misión principal.
Por ejemplo,qué carga militar debe llevar,a qué distancias,altura y velocidades,así como el ambiente hostil en que debe realizarlas y algunas características aparentemente accesorias.
Se entiende claramente que la carga de equipos electrónicos embarcados será mucho mayor en un clima adverso,como podría ser el de Europa central,que en el norte de Africa donde rara vez aparecen las nieblas y nubes.
Del mismo modo,la amenaza antiaérea mayor o menor influirá en esos equipos para avisar y,eventualmente,ayudar al piloto o pilotos.
Las características aparentemente accesorias citadas antes podrían ser la disponibilidad de pistas largas o cortas,o la exigencia del despegue y aterrizaje verticales.
Cuando los suecos o los suizos desean utilizar la red de autopistas como posibles aeródromos,están obligando al diseñador a elegir determinadas relaciones entre peso y empuje de motores,así como a utilizar ayudas aerodinámicas suplementarias como podrían ser unos alerones delanteros especiales (tipo canard).
Debemos insistir más en la cuestión del ambiente operacional,porque suele influir incluso en el número de reactores.
Si estudiamos los aviones de la marina americana,observaremos que suelen ser birreactores,en lugar de tener un solo reactor.
No se trata de ningún capricho pues,aunque dispongamos del mismo empuje total,en el primer caso un fallo de reactor permite al avión seguir en vuelo con el otro,mientras que en el segundo,y más aún en vuelos sobre el mar,el fallo significa la pérdida del aparato y,muchas veces,la del piloto.
El ambiente húmedo y corrosivo determina exigencias suplementarias que deben ser tenidas muy en cuenta.
En términos generales este primer grupo de exigencias operacionales elimina muchos posibles caminos,marcando ya los denominados parámetros básicos.
Estos parámetros básicos,simplificándolos al máximo con el objeto de hacer fácil su comprensión,son los siguientes: forma alar,número de reactores y tipos de entrada de aire para su alimentación,estabilidad y controles,cargas militares con su situación y posición de los timones de cola.
Veamos ahora su influencia individual,sin perder de vista que existen relaciones recíprocas que nos marcan muy de cerca.
Así tenemos el caso de que un ala en delta no sólo influye en la carga alar (kilogramos de peso por metro cuadrado de ala),decisiva a su vez en la maniobrabilidad en vuelo ; también impone aumentos de peso y,por lo tanto,exige mayor empuje de los motores.
Por lo que a las alas respecta,debido a fenómenos de compresión del aire a velocidades cercanas al número de Mach (aproximadamente,1.200 Km / h a baja altura),los ingenieros aeronáuticos descubrieron a mediados de los años cuarenta que el aflechamiento retrasaba la aparición de dichos problemas.
Por ello,normalmente los aparatos supersónicos suelen tener el borde de las alas inclinado hacia atrás,en lugar de disponerlas rectas como era usual.
El mayor o menor ángulo dependerá del tiempo que el aparato vuele a altas velocidades,mientras que la superficie total,el tamaño,proporcionará diferente grado de agilidad.
LA técnica aeronáutica está viviendo desde hace algo más de un decenio una revolución silenciosa: se trata de la utilización de ordenadores electrónicos para estudiar y calcular la aerodinámica de un avión,con lo cual se suple en gran parte el trabajo que realizaban hasta fechas no muy lejanas los tuneles aerodinámicos.
La consecuencia de este cambio espectacular reside en la reducción de tiempo,personas y costos que permite el ordenador.
El ingeniero aeronáutico dispone ahora de medios de proceso para llegar a resultados que eran inesperados hace sólo unos pocos años.
La concepción,los ensayos y la fabricación ayudados por un ordenador son,desde hace tiempo,una realidad diaria.
La mejora continua de estos procesos ha conducido al desarrollo de un sistema interactivo tridimensional de concepción por ordenador,que satisface las necesidades de la mayor parte de las industrias aeronáuticas.
El sistema desarrollado por Avions Marcel Dassault-Breguet Aviation ha sido comercializado por la filial Dassault Systemes e IBM Corp. Varias industrias,como Boeing,Rockwell y Lockheed,en los Estados Unidos,lo han adoptado,lo cual demuestra la calidad alcanzada por la industria de ordenadores muy especializados en Europa.
Dassault-Breguet ha empleado el nuevo sistema de concepción por ordenador para diseñar sus aviones más modernos,tales como el Mirage 2000 y el Mirage 4000.
En el caso del 2000,su aplicación permitió efectuar los análisis estáticos y dinámicos,desarrollar los modelos de estructuras que sirven para el cálculo de las cargas por el método de los elementos acabados y realizar los estudios de reducción de la resistencia aerodinámica.
El resultado de todo ello fue la definición de alas mayores y más complejas que las del Mirage III con un peso igual.
El nuevo sistema tiene también aplicaciones en otro tipo de actividades industriales ; por ejemplo,en el campo de la industria del petróleo,de la construcción e incluso en medicina.
SIEMPRE que nos enfrentamos a un problema de estética surge la polémica.
Gordo u obeso,flaco o esmirriado... Las modas actuales parecen estar en contra de los gorditos.
Bañadores,playas,pantalones ajustados y otros sistemas de lucimiento corporal han puesto de manifiesto muchas gorduras indeseadas y más o menos ocultas.
En tiempos pasados el vestuario parecía favorecer de alguna manera a estos obesos: túnicas,miriñaques,cancanes y blusones permitían ocultar el exceso de carnes.
De lo que no cabe duda es que los gustos han cambiado ; las modelos que hoy llenan las revistas de modas tendrían bien poca aceptación en tiempos de Rubens,e incluso a principios de nuestro siglo XX.
Lo curioso es que no ha ocurrido así con la belleza masculina ; los hombres altos y delgados,a lo James Stewart o Gary Cooper,han sido siempre los preferidos de las damas.
En todo caso,parece sensato pensar que la obesidad es un mal de las civilizaciones opulentas,donde la oferta de los alimentos y el escaso trabajo físico requerido para la obtención del pan nuestro de cada día están claramente descompensados.
Y,en todo caso,no se trata de un estricto problema de estética.
La medicina sabe ya desde hace muchos años que la obesidad es uno de los principales factores de riesgo,es decir favorecedor de diversas enfermedades graves.
Sobre todo,y por un efecto de potenciación mutua,si además viene acompañada por hipertensión arterial,consumo de tabaco y otros hábitos o dolencias potencialmente peligrosos.
Antes de seguir adelante,quizá sería bueno que definiéramos lo que es un obeso.
La respuesta parece fácil,pero lo cierto es que no existe unanimidad al respecto.
Se han ideado muchos métodos científicos para definir la obesidad: medición de los pliegues de grasa acumulada en el tejido,utilización de marcadores radiactivos,etcétera.
Pero probablemente la manera más sencilla,y quizá una de las más correctas,estriba en comparar la edad,la talla y el sexo en tablas elaboradas estadísticamente,que nos proporcionan los pesos ideales.
Sobre estos valores predictivos,un aumento de 9 a 10 kilos se considera ya obesidad.
Realmente sería erróneo considerar al obeso como una persona enferma en el sentido médico de la palabra.
Desde luego,algunos gordos lo son por causa de enfermedades ; muchas insuficiencias de glándulas de secreción interna (diabetes,hipogonadismo,síndrome de Cushing,hipotiroidismo) pueden producir obesidad.
Pero se puede afirmar que la mayoría de los obesos sufren un desajuste entre la alimentación y la actividad física.
Médicamente,decimos que la obesidad es un trastorno metabólico definido por la acumulación de grasa,almacenada en el tejido adiposo.
Por lo tanto,tejido adiposo no es sinónimo de obesidad,ya que forma parte de nuestro organismo de manera normal.
En una persona media,de unos setenta kilos de peso,existen unos diez kilos de tejido adiposo ; en una persona obesa la proporción de este tejido es muy superior y puede superar el 50 %.
La distribución del tejido adiposo presenta diferencias personales marcadas y,aunque no conocemos con exactitud los mecanismos que marcan estas diferencias,sabemos de la influencia de determinadas glándulas como! a hipófisis,las glándulas sexuales,el tiroides... Así,en las mujeres el tejido adiposo tiende a distribuirse en la parte inferior del tronco y en los muslos,mientras que en el hombre predomina en el abdomen,quedando libres las extremidades.
Existen muchas teorías para tratar de explicar la obesidad.
Se ha hablado de personas que digieren mejor los alimentos y que engordarían más que otras.
Esta teoría nunca ha sido probada científicamente: a igualdad de alimentos y consumo de energía,hay pocas diferencias para la asimilación de calorías.
En cambio,hay otro punto mucho más interesante que sí ha sido demostrado científicamente: la diferencia entre el obeso infantil,es decir,el gordito que se va haciendo desde la infancia,y el obeso en la edad adulta.
En el adulto,el número de células que componen el tejido adiposo se ha podido medir: unas 4 x 101 ° (40.000 millones).
En el obeso en edad adulta muchas de estas células aparecen rellenas de grasa,pero su número total es más o menos el mismo que en el adulto no obeso.
En cambio,en el gordo infantil no sólo encontrarnos estas células adiposas rellenas de grasa,sino que su número total es muy superior.
Lo cual nos lleva a insistir sobre la necesidad de prevenir la obesidad desde los primeros años,mediante una educación alimenticia correcta.
El hombre,como todo ser vivo,tiende a su propio equilibrio interno.
Para vivir necesita consumir una determinada cantidad de energía en su actividad física y en sus funciones vitales.
A mayor trabajo físico,mayor consumo de energía.
Y para reponer esta energía contamos con dos fuentes: los alimentos y el oxígeno.
El segundo sirve para quemar los primeros,haciéndolos asimilables para las distintas células del cuerpo.
Para medir la cantidad de energía consumida recurrimos a estudios indirectos,midiendo la cantidad de oxígeno que consumimos en cada actividad física: correr,nadar,andar,reposar... El oxígeno consumido estará en proporción directa con las energías consumidas,habitualmente expresadas en calorías.
Midiendo las calorías proporcionadas por cada alimento,podemos elaborar una tabla de necesidades energéticas en función de nuestra actividad.
Así,por ejemplo,para consumir las calorías proporcionadas por dos manzanas medianas (200 Kcal) necesitamos realizar una carrera de unos doce minutos.
Para gastar las calorías de dos cucharaditas de mantequilla,precisamos nadar treinta minutos.
Para consumir las calorías proporcionadas por 3 / 4 de litro de cerveza necesitaríamos catorce horas sentados ante el televisor... Conociendo estos mecanismos,se comprueba que adelgazar es realmente sencillo.
- Bastará adecuar nuestra ingesta de alimentos por debajo de las calorías que consumimos en el ejercicio diario ; el resto de las que necesitaremos las obtendrá el cuerpo quemando la grasa almacenada,haciéndonos perder peso.
Por desgracia,conseguir que un obeso coma menos y además haga más,ejercicio es tarea digna de Hércules.
En la mayoría de los casos hay que apoyarse en medidas médicas o psicológicas que ayuden a modificar unos hábitos muy arraigados.
Los esfuerzos dietéticos deben ir orientados desde la infancia a comer mejor.
Que no significa comer más,sino comer lo justo,es decir,lo más adecuado a nuestra habitual actividad física.
Quizá se trate de una empresa casi heroica,pero merece la pena.
Por motivos estéticos,desde luego,pero también y sobre todo por motivos médicos.
Eliminar uno de los factores de riesgo con los que nos obsequia la vida moderna no es cosa despreciable.
La mayoría de los fracasos en muchos regímenes para adelgazar estriban en habernos planteado metas demasiado difíciles de alcanzar o en no haber contado con las ayudas necesarias para superar tan difícil prueba.
Quizá la foto de Gary Cooper o de Jane Birkin en el comedor de nuestra casa puedan ser un estímulo suficientemente fuerte para prescindir de ese segundo plato de paella que,más por gula que otra cosa,nos apetece tomar.
En todo caso,estamos ante un problema de equilibrio vital entre lo que se come y lo que se gasta.
Y ese equilibrio es el que hay que restablecer.
¡El inversamente proporcional al cuadrado de la distancia que los separa ; los estudiantes de Física conocen bien esta sencilla formulación de las Leyes de Coulomb (electromagnetismo) y Newton (gravitación).
Pero estas fuerzas ofrecen también enormes diferencias.
Principalmente,el hecho de que las ondas de gravedad son ni más ni menos que una expresión de la famosa distorsión en el espacio-tiempo,que no tiene explicación sin la Teoría General de la Relatividad.
Otra diferencia sustancial entre el electromagnetismo y la gravedad deriva del hecho que la electricidad puede atraer y repeler (electricidad positiva y negativa),mientras que la gravitación es una fuerza que sólo se ejerce en el sentido de la atracción.
Desde luego,la existencia fugaz de antimateria podría dar lugar a una repulsión gravitatoria,pero no hay que olvidar que la antimateria se une inmediatamente a la materia convirtiéndose ambas en energía ; es el caso del electrón y su antipartícula,el positrón,de cuya unión nace un muy energético fotón gamma,es decir,una radiación electromagnética muy penetrante.
Vistas desde el punto de vista cuantitativo,las fuerzas electromagnética y gravitatoria son enormemente diferentes: el electromagnetismo es una fuerza casi infinitamente superior a la gravedad.
El orden de magnitud es de 104 ° veces mayor,un 1 seguido de nada menos que 40 ceros... Esa es la proporción entre ambas fuerzas,y eso explica que la gravitación sólo aparezca en la relación entre grandes concentraciones de materia (planetas,estrellas,galaxias),pero en cambio sea una fuerza prácticamente inexistente en nuestro mundo habitual y,desde luego,despreciable a nivel atómico o subatómico.
Recordemos,de pasada,que a nivel de los átomos,la fuerza imperante,por así decirlo,es precisamente la electromagnética,que retiene a los electrones en órbita alrededor del núcleo ; una disposición que recuerda a la de los planetas alrededor del Sol en virtud de la fuerza gravitatoria.
La tercera fuerza básica del Universo se ejerce entre algunas partículas que componen el núcleo atómico y es mucho más débil que la electromagnética.
Es la fuerza nuclear débil.
Finalmente,la cuarta fuerza aparece entre los neutrones y protones del núcleo,se denomina nuclear fuerte y es 120 veces mayor que la electromagnética.
Como muchos de nuestros lectores saben ya,los esfuerzos de la física actual se orientan hacia la unificación de todas estas fuerzas,que era el sueño de Einstein al final de su vida.
Ya en 1979 el Nobel de Física a Glashow,Weimberg y Salam premiaba hallazgos teóricos esenciales en la unificación del electromagnetismo y la fuerza nuclear débil,y el Nobel de 1984 a Carlo della Rubbia y a Van der Meer premia la comprobación experimental de esa teoría.
Por lo que hoy día se puede afirmar que la fuerza electromagnética y la nuclear débil son una sola,llamada electrodébil.
Los físicos tardarán mucho en unificar esta nueva fuerza electrodébil con la nuclear fuerte,porque para ello se necesitan enormes cantidades de energía y aparatos tan sofisticados y potentes que tardaremos mucho en poderlos diseñar y utilizar Pero a nivel teórico nadie duda de su unificación.
Por su parte,la cuarta fuerza,la gravedad,plantea un problema totalmente inverso: su extremada debilidad (recordemos que es 104 ° veces menor que el electromagnetismo) impide siquiera medir su partícula asociada o su onda característica.
De ahí la trascendencia del experimento que se empieza a realizar ahora en Stanford,ya que antes que nada conviene saber si existe,y cómo es,esa onda de gravedad o esa partícula desconocida,el gravitón.
Ya hemos visto anteriormente que la única analogía entre la gravedad y el electromagnetismo estriba en la ley del inverso del cuadrado de las distancias.
Se trata de una analogía importantísima,que sugiere sin duda la posibilidad de una misma esencia,por muy diferentes en intensidad que sean una y otra.
Esta es la razón por la que los físicos piensan en la posibilidad de alcanzar algún día la unificación global de los cuatro grandes campos de fuerzas comentados más arriba.
Veamos un poco más despacio esta importante analogía.
Del electromagnetismo se sabía,desde el principio,que una carga eléctrica en movimiento generaba un campo magnético.
El físico escocés James Clerk Maxwell estableció matemáticamente la relación entre ambos campos de fuerza y la onda común,bautizada,lógicamente,con el nombre de onda electromagnética.
La ecuación de Maxwell recibió un espaldarazo total con los experimentos que diez años más tarde realizara Hertz,demostrando que la luz,las ondas de radio y otras ondas eran distintas manifestaciones de esas ondas electromagnéticas.
Hoy día,el campo de las ondas electromagnéticas es bien conocido y explotado: radio,televisión,infrarrojos,luz visible,ultravioleta,rayos X,rayos gamma... Einstein,siempre él,intuyó que existía una relación entre la gravedad y los fenómenos electromagnéticos que había descrito Maxwell.
En 1916,la Teoría General de la Relatividad establecía un paralelismo entre la carga eléctrica y la masa de un cuerpo,y afirmaba que toda masa,por el solo hecho de existir,generaba una distorsión en el espacio circundante.
Esta distorsión venía a ser algo así como el equivalente,en gravitación,del campo eléctrico.
Einstein llegó a dar unas ecuaciones que especificaban la relación existente entre la distorsión en el espacio-tiempo y la masa gravitatoria existente.
Estas ecuaciones guardaban cierta analogía con las de Maxwell del campo electromagnético,pero su base física no era la electricidad ni el magnetismo,sino,y aquí reside lo extraordinario,las alteraciones en la base misma de la existencia: ¡nada menos que ondulaciones en el espacio-tiempo! Unas ondulaciones que se desplazarían a la velocidad de la luz y que fueron bautizadas,claro está,con el nombre de ondas de gravedad.
Sin embargo,las ecuaciones de Maxwell tuvieron confirmación experimental pocos años después,y hoy día las aplicaciones prácticas son casi infinitas.
En cambio,pese a haber transcurrido casi setenta años,nadie ha conseguido encontrar todavía las famosas ondas de gravedad,o su partícula asociada a la que se ha bautizado,quizá prematuramente porque a lo mejor no existe,con el nombre de gravitón.
La clave de todo ello reside en el hecho,ya comentado de la extremada debilidad de la fuerza gravitatoria que dificulta,hasta extremos inverosímiles,la detección de su onda elemental.
La única posibilidad consiste,quizá,en captar ondas de gravedad procedentes del Espacio y generadas por masas inmensas,por lo que tales ondas tendrían una intensidad extraordinaria.
Fuentes idóneas de estas ondas de gravedad muy intensas son,por ejemplo,las estrellas de neutrones y,sobre todo,los fantásticos agujeros negros en los que se concentran masas casi infinitas en: dimensiones sumamente reducidas.
En particular,interesa a estos efectos cierto momento de su evolución: el colapso.
Debido a la enorme cantidad de materia acumulada por unidad de volumen,la densidad en un agujero negro puede ser superior a varios miles de millones de toneladas por centímetro cúbico.
¿RECUERDA el lector haber visto líquenes sobre las piedras de los edificios de la Gran Vía de Madrid o de Bilbao,o en las Ramblas barcelonesas? Aun en el caso de que estuviese acostumbrado a verlos en el campo,no sería posible que los reconociese en las columnas de las Cortes o en el Barrio Chino.
Y ello es debido a que estos extraños,desconocidos e incomprendidos seres tienen un gusto exquisito por el aire puro y los lugares no contaminados.
Estos vecinos nuestros,discretos e ignorados,se encuentran amenazados por la civilización humana.
Los agentes contaminantes,particularmente el dióxido de azufre (SO2),primero los esterilizan y después los matan.
Precisamente,la máxima utilidad de los líquenes hoy se encuentra relacionada con su finísima sensibilidad ante los contaminantes atmosféricos.
Según el nivel de destrucción en que se hallen,se puede averiguar el grado de polución de un determinado lugar.
Como la sensibilidad a estos gases varía de unas especies a otras,observando los que crecen en las ramas de los árboles,por ejemplo,se llega a conocer la cantidad de contaminación en un sitio (aunque no en un momento concreto) y su efecto prolongado a lo largo del tiempo.
El lento metabolismo y crecimiento de los líquenes motiva la acumulación en sus organismos de sustancias tóxicas que acaban causando su desaparición.
Pero vamos a intentar en primer lugar descorrer levemente el velo que oculta el misterio de los que muy posiblemente son los seres vivos más asombrosos del planeta.
Los líquenes son esas manchas de muy variados colores que parecen formar parte de las rocas,esas cintas e hilos verde-azulados o cenicientos que cuelgan de las ramas de los árboles.
Aunque aparentan ser elementos absolutamente pasivos,cuya existencia se limita a la mera supervivencia,y ni siquiera parecen crecer,son el resultado de una unión extraordinaria y poseen sofisticadísimos sistemas de adaptación al medio ambiente.
Imaginemos dos seres completamente distintos,un alga microscópica,muy semejante a una bacteria,y un hongo ; ambos están separados dentro de la escala evolutiva del reino vegetal - - hasta tal punto que muchos científicos piensan que los hongos constituirían un reino aparte,diferente de plantas y animales - -.
Imaginemos,pues,que en cierto momento decidiesen unir sus vidas de un modo mucho más íntimo que el de cualquier matrimonio,hasta el punto de mezclar sus tejidos,perder su identidad propia y dar lugar a otro ser,distinto por completo de ellos mismos y perfectamente capacitado para sobrevivir.
Hongo y alga obtienen de su simbiosis una serie de mutuos beneficios.
Para comprender cuáles son estas ventajas no tenemos más que observar los lugares en que habitan los líquenes: los vemos sobre las heladas piedras de las montañas,en las desguarnecidas parameras que los vientos fuertes hacen casi inhabitables,en las gélidas tundras árticas,o incluso sobre los hielos polares.
No podrían allí vivir ni el hongo ni el alga por separado ; el primero no encontraría la materia orgánica que necesita para nutrirse,pues a diferencia de los demás vegetales carece de clorofila y no puede realizar la fotosíntesis ; y la segunda se deshidrataría rápidamente por el efecto combinado del intenso frío y de la fuerte radiación solar.
Sin embargo,unidos en el liquen,el alga proporciona al hongo la materia orgánica,azúcares sobre todo,que éste necesita,y el hongo forma con sus filamentos una trama protectora alrededor de las células del alga.
De este modo son capaces de soportar temperaturas cercanas a - 190 °C y largos períodos,incluso de años,de desecación prácticamente total.
Pero ambos componentes conservan aún su personalidad propia.
De modo que si artificialmente proporcionamos a uno de ellos las condiciones óptimas para su desarrollo,abandonará la simbiosis y proliferará hasta hacer desaparecer al otro.
El aspecto y forma de los líquenes es asombrosamente variable.
A grosso modo se pueden distinguir dos grandes clases de organización interna de los líquenes: un primer tipo muy simple,propio de especies muy dependientes de la humedad,incapaces de soportar condiciones de fuerte desecación ; con un aspecto exterior gelatinoso y blando,son reconocibles a través de un microscopio en un corte transversal,porque no poseen ningún tipo de estructuración interna y los filamentos del hongo y las células del alga se mezclan sin ningún orden aparente.
El segundo tipo es ya más complejo ; se distingue una capa superior más dura y protectora formada por tejidos del hongo,una capa central en que predomina el alga y otra inferior,similar a la superior o provista de cortas raicillas que sirven para sujetar el liquen a las superficies en que vive.
Este segundo modo es el más perfeccionado y el que presenta una mayor cantidad de variaciones.
Corresponden a él la gran mayoría de los líquenes que vemos habitualmente,ya que los gelatinosos pasan inadvertidos con mucha facilidad.
De este tipos son,por ejemplo,los líquenes crustáceos,que viven íntimamente adheridos a la superficie de las rocas y resultan prácticamente imposibles de separar de ellas.
Pueden llegar a ser muy abundantes,con lo que en ocasiones dan un colorido característico a montañas enteras,como es el caso del hizocarpon geographicum.
Debido a su lentísimo crecimiento,muchas veces inferior al milímetro anual,se cuentan entre los seres vivos más viejos del planeta.
A algunos ejemplares se les calcula unos 4.000 años de vida.
Los denominados foliáceos no están tan fuertemente adheridos al sustrato,sino que presentan solamente algunas raicillas de anclaje,que no les sirven en absoluto para absorber sales minerales,como en las plantas superiores,sino sólo como sujección.
Suelen tener aspecto de láminas aplanadas semejantes a hojas.
En otras ocasiones son como pequeñas escamitas que viven sobre la tierra se denominan entonces escuamulosos.
Otro de los tipos más familiares es el de los líquenes fruticulosos,que son los de cuerpo ramificado,de sección aplanada o cilíndrica.
Viven colgando de la corteza de los árboles,a la que se sujetan mediante un órgano de fijación basal que actúa de modo comparable a una ventosa.
Todos estos líquenes poseen una gran variedad de estructuras anatómicas,cuya función en muchos casos se ignora o al menos no se conoce con certeza.
Más adelante hablaremos de algunas de las relacionadas con la reproducción ;.
Vamos a mencionar ahora solamente dos de las más curiosas y bastante comunes,aunque en ocasiones no fáciles de detectar: los isidios y los cefalodios.
Los primeros semejan pequeñas vejiguitas que aparecen sobre la superficie del liquen,compuestas por los mismos elementos e igualmente organizados que en éste.
Se piensa que su función podría ser la de ampliar la superficie libre que recibe la luz del sol y,por tanto,aumentar la eficacia y rendimiento de la fotosíntesis.
Los cefalodios,en cambio,son a modo de quistes que crecen tanto en el interior como sobre la superficie.
Aparecen en aquellas especies cuya alga componente pertenece al grupo de las algas verdes (Clorofitas).
En el cefalodio el alga es una cianofita (alga verde-azulada),anatómica y filogenéticamente muy distanciada de la anterior.
Se ha discutido mucho sobre las posibles funciones de estas estructuras ; en principio se creyó que podría tratarse de elementos patógenos,algo así como tumores ; pero hoy en día los botánicos más bien se inclinan a opinar que son adaptaciones para la obtención y asimilación del nitrógeno de la atmósfera,capacidad que poseen las cianofitas pero no las clorofitas.
Muy raramente se encuentran viviendo libremente en la naturaleza las especies de hongos que aparecen en los líquenes.
Un automóvil de tipo medio,circulando en zona urbana,arroja a la atmósfera por medio de sus gases de escape unos siete gramos de plomo por cada 100 kilómetros recorridos.
No parece mucho,sobre todo si la emisión se produce en espacios abiertos ; pero,en las grandes ciudades de tráfico muy congestionado,la concentración de emisiones nocivas alcanza en total más de una tonelada diaria de plomo.
Cantidad que,en este caso,ya puede ser considerada como muy preocupante.
Los efectos del plomo sobre el organismo son mal conocidos,aunque una ingestión excesiva puede provocar la grave enfermedad conocida como saturnismo.
No parece que,por mucho plomo que expulsen los gases de escape,su inhalación en el aire urbano llegue a producir masivamente esta enfermedad ; pero lo cierto es que se conoce mal el efecto acumulativo que sobre el cuerpo humano tiene la inhalación casi permanente de pequeñas cantidades de este metal.
Respirar un ambiente con gases de plomo hora tras hora,día tras día,año tras año,puede suponer un peligro que,aun sin cuantificar exactamente,ha provocado ya las primeras medidas de alerta en los países más avanzados.
En los Estados Unidos hace ya años que existe una normativa legal que prohíbe el plomo en las gasolinas,aunque el proceso de adaptación a esta nueva situación todavía no ha terminado.
Otros países,como Alemania,quieren imponer una legislación similar en breve plazo ; y,a no tardar mucho,la propia Comunidad Económica Europea impondrá sus limitaciones.
La gasolina sin plomo será un hecho dentro de pocos años,aunque ello suponga un importante esfuerzo a la industria automovilística y,sobre todo,a las refinerías productoras de combustible.
Pero,¿por qué la gasolina contiene plomo? Este metal pesado y con escaso interés metalúrgico parece totalmente ajeno al mundo de los combustibles.
¿Cuál es su función? La respuesta viene dada por la necesidad de evitar,en el interior de los cilindros,que la gasolina se queme antes de tiempo,explote antes de lo que le corresponde,reduciendo por esa causa su efectividad a la hora de empujar al pistón.
Esta capacidad de no explosión,también llamada capacidad antidetonante,se mide mediante el llamado Número de Octano N. O. Cuanto más bajo es este valor más fácilmente detona la mezcla de gasolina y aire que entra en los cilindros ; es decir,más fácilmente se produce la ignición espontánea de la mezcla al ser comprimida en el pistón.
Una ignición adelantada no deseable,ya que cuando la bujía emite su chispa ya no queda casi gasolina por quemar,perdiendo así su efectividad.
Se comprende que cuanto más se comprime la mezcla de gasolina y aire,más fácilmente aparece la detonación adelantada y mayor número de octano se requiere del combustible.
Las gasolinas obtenidas en las refinerías poseen en origen un número de octano bastante bajo,comprendido entre 50 y 70.
Esta gasolina sólo es utilizable en motores cuya relación de compresión es menor de 7 a 1,es decir motores en los que la mezcla de gasolina y aire no queda demasiado comprimida.
Este tipo de motores hace ya tiempo que no se fabrican en la actualidad,los modernos moto res de gasolina tienen índices de compresión elevados,generalmente superiores a 9 a 1,lo cual requiere una gasolina con alto poder antidetonante,de por lo menos 95 N. O. Para que la gasolina tenga este número de octano se ha venido utilizando tradicionalmente un método barato eficaz: añadirle una serie de compuestos de plomo (fundamentalmente tetraetilo y tetrametilo de plomo que reducen el poder detonante de la gasolina comprimida.
En los años cincuenta y sesenta muchas gasolineras disponían de un surtidor complementario,en el que figuraba genéricamente el rótulo " Plomo " ; los coches cuyos motores requerían un combustible más antidetonante añadían,en este surtidor,una cantidad complementaria de plomo a la gasolina de bajo número de octanos.
Hoy día,toda la gasolina que utilizamos en nuestros automóviles contiene ya de origen esta mezcla.
Además de estos productos de plomo,las refinerías modernas le añaden a la gasolina otros compuestos,como el dibromuro y dicloruro de etileno,que forman productos residuales a base de plomo que son gaseosos a las temperaturas de trabajo del motor.
Con ello se consigue,al menos en teoría,que el plomo de la gasolina llegue al medio ambiente en forma gaseosa y no depositándose directamente bajo forma metálica en pequeñísimos granos suspendidos en el aire.
En todo caso,tales compuestos gaseosos de plomo son probablemente tan nocivos como el plomo metálico ; y por otra parte,la combustión nunca es perfecta dentro de los cilindros,por lo que parte del tetraetilo y tetrametilo de plomo pasa directamente al aire que respiramos.
Los dos esquemas nos indican lo que sucede dentro de un cilindro cuando se produce una explosión normal y otra anómala.
A la izquierda podemos apreciar una explosión correcta,inflamándose la mezcla de gasolina y aire con la chispa de la bujía.
El empuje de los gases contra el pistón se produce de forma continua,ejerciendo un esfuerzo de varias toneladas en un tiempo muy corto.
En la detonación anómala (a la derecha),la ignición de la mezcla se inicia espontáneamente,antes de que salte la chispa de la bujía ; esto ocurre cuando la compresión es superior al limite de autoencendido de la gasolina.
La efectividad de esta explosión incontrolada es mucho menor,y además se ejerce a destiempo,por lo que el motor pierde mucha potencia y puede llegar a averiarse gravemente.
UN grupo de biotecnólogos españoles está creando una planta de producción industrial de hormona del crecimiento,sustancia absolutamente indispensable para el normal desarrollo físico del cuerpo humano.
Según el director científico del proyecto,el endocrinólogo Jesús Tresguerres," la disponibilidad en un futuro próximo de esta hormona,obtenida por ingeniería genética,permitirá a todas las personas alcanzar estaturas normales ".
Es no sólo el fin del enanismo en su más cruda realidad,sino del insuficiente crecimiento de muchas personas que,en la edad adulta,resultan mucho menos altas de lo que podrían ser.
Son casi un millar,como hemos visto,los niños españoles que padecen enanismo hipofisario.
Una enfermedad que les condena a quedarse con la talla de un niño durante toda su vida,aunque el resto de sus funciones físicas e intelectuales se desarrollen normalmente.
Además de estos casos extremos,existen muchos miles de personas que,por desarreglos hormonales durante su época de crecimiento,se quedan bajitos para toda la vida cuando en realidad podrían tener tallas normales.
En nuestro país,el tema del enanismo ha sido frecuentemente inspiración de un cierto tipo de humor negro,cuando no gratuitamente cruel.
Ahí están el bombero torero o los enanos del circo,sin olvidarnos de los bajitos de Gila.
En el cine,en todo tipo de espectáculos o incluso en la mendicidad,los enanos eran,son,fenómenos de la naturaleza sin otra alternativa que la de ser físicamente inferiores a los demás para toda la vida.
Afortunadamente,hoy ya conocemos las causas del problema,e incluso es posible remediarlo en un muy reducido número de casos.
La esperanza de cara al futuro inmediato reside en la ingeniería genética,cuyas técnicas podrán permitirnos obtener,masivamente y a costes abordables,lo que hoy todavía resulta prohibitivo y muy escaso: el producto responsable del crecimiento,una hormona producida por la hipófisis cerebral y denominada científicamente somatotropina o STH.
Esta hormona del crecimiento es una proteína.
Es decir,una larga molécula química compuesta,como todas las proteínas,por aminoácidos.
Nada menos que 192 unidades de aminoácidos se unen en un orden genéticamente programado cada vez que fabricamos una molécula de la hormona.
Después,esa larga hilera se dispone en el espacio hasta adquirir su configuración tridimensional definitiva y que la convierte en una sustancia eficaz.
Su efecto consistirá en estimular el crecimiento de los huesos,músculos y tejidos durante varios años,hasta el final de la adolescencia.
Crecer no es una alternativa de sí o no ; no es como tener los ojos azules o castaños.
La talla,el tamaño del cuerpo humano,es una característica que debe ir de la mano de un equilibrio global del organismo.
Creciendo,nuestro cuerpo se prepara para la vida que nos va a ser propia,la vida normal de nuestra raza humana.
No tenemos el largo cuello de las jirafas porque no vivimos de ramonear en las hojas de los más altos árboles.
Nuestra vida,nuestra forma de comer,amar,dormir o usar la inteligencia ha condicionado,en miles de años de evolución,la talla normal de la especie Homo sapiens.
El cerebro es el órgano que armoniza y dirige nuestro funcionamiento vital.
También controla cuándo deben ocurrir ciertos procesos,cómo deben desarrollarse y dónde son necesarios.
En el proceso del crecimiento,el cerebro elabora la orden: " hay que crecer ".
La unidad cerebral correspondiente,el hipotálamo,envía instrucciones precisas para la fabricación de la hormona adecuada a la glándula especializada,que los médicos denominan hipófisis.
Las instrucciones precisas que emanan del hipotálamo consisten en el envío de un mensajero químico,el factor de liberación o HGRF,hacia la hipófisis.
Esta fabrica la hormona,que posteriormente llegará,a través del torrente sanguíneo,hasta el hígado.
Allí nuevas sustancias,las somatomedinas lograrán que la hormona del crecimiento cumpla por fin su objetivo: el desarrollo de los huesos,los músculos y los diversos tejidos orgánicos.
Cuando un niño no crece (caso del enanismo) o crece muy poco (caso de las futuras personas de muy baja estatura),el fallo debe encontrarse en uno de los puntos de ese proceso que acabamos de resumir ; en particular,que el hipotálamo no envíe correctamente sus instrucciones a la hipófisis o bien que ésta no las cumplimente y no fabrique la hormona o la fabrique en muy escasa cantidad.
Mucho más raro es que los tejidos no respondan a la llegada de la hormona a través de la sangre.
La hormona del crecimiento,STH,no es como la aspirina.
Los farmacólogos no pueden,por ahora,fabricarla en sus laboratorios.
La única fuente posible de obtención es la propia hipófisis humana,extraída de personas recién fallecidas.
Y esta obtención resulta muy costosa y proporciona muy pequeñas cantidades de producto aprovechable.
Cada hipófisis humana proporciona unas 12 unidades internacionales de STH ; esta dosis sólo sirve para el tratamiento de una semana,cuando un niño enano necesitaría prolongarlo durante muchos años.
Además,esas 12 unidades cuestan muchísimo dinero y resultan prácticamente inabordables en la inmensa mayoría de los casos.
Sólo para los casos más graves de enanismo,nuestro país necesitaría unas 125.000 dosis anuales.
Los presupuestos de la Seguridad Social sólo pueden abordar la importación de unas 45.000,por lo que el déficit actual es de 80.000 dosis.
Y esto sólo en el caso de enanos propiamente dichos.
Si contáramos los miles y miles de casos en los que un tratamiento con STH podría hacer crecer a los que se van a quedar con estaturas inferiores a lo normal,el déficit se convertiría en astronómico.
La solución al problema está ya aquí,en España,en manos de científicos del Centro de Biología Molecular y de la Universidad de León,encargados por el Ministerio de Educación y Ciencia de la investigación por ingeniería genética de la hormona y su factor de liberación.
Mientras tanto,el Ministerio de Sanidad y Consumo está racionalizando la distribución.
EL día 8 de enero de 1985 un equipo de científicos descubrió,mediante un telescopio de infrarrojos,un misterioso objeto extraordinariamente energético en el centro de la Vía Láctea.
La noticia no sólo conmocionó a la opinión pública,sino que,en medios científicos,fue tomada como la primera prueba visual de algo que ya se suponía que existía: una especie de gran sumidero gravitatorio en el centro de la espiral galáctica,una especie de inmenso agujero negro que engulle la materia próxima.
Y es que en los últimos años se han ido sucediendo los descubrimientos en torno a nuestra Galaxia,hasta el punto de hacer variar considerablemente la concepción habitual que de ella tenían los científicos bien entrado el siglo XX.
La Vía Láctea que conocemos,o más bien que creíamos conocer,es ya considerada como sólo la parte central de un conjunto mucho más amplio,una especie de inmensa esfera espacial cuyo diámetro podría sobrepasar los 300.000 años luz,y cuya masa total alcanzaría la cifra,realmente fantástica,de dos billones de veces la masa de nuestro Sol.
Pero antes de llegar a tan sensacionales descubrimientos,lo cierto es que la Vía Láctea fue objeto de numerosas,y a veces pintorescas,interpretaciones.
En la antigüedad,los griegos la consideraban como la huella indeleble de las hazañas extraconyugales de Zeus ; Hércules,uno de los bastardos del dios de los dioses,habría mordido glotonamente el pecho de la divina Hera cuando ésta le estaba dando de mamar,derramándose la leche por la herida y quedando como una auténtica vía láctea celeste.
Nuestra galaxia,hoy día,sigue siendo conocida como Vía Láctea,o como Galaxia,así,con mayúscula.
Nombres sinónimos,por otra parte,ya que en griego galaxias kyklos significa... vía láctea.
Con el telescopio de Galileo se pudo comprobar que los amores extraconyugales de Zeus no tenían nada que ver con el tema: la Vía Láctea estaba formada por miríadas de estrellas.
Y,siglos después,gracias al ingenio desplegado por los astrónomos,hemos llegado a saber que la Tierra,girando alrededor del Sol ocupa un lugar más bien alejado del centro de ese conjunto de miles de millones de estrellas llamado Vía Láctea.
Esta tiene la forma de un disco plano de unos 100.000 añosluz de diámetro,con una especie de bulbo central ; como una especie de platillo volante,pero rodeado de un halo esferoidal mucho mayor,aunque muy poco denso,y que contiene unos cuantos millones de estrellas viejas y solitarias y unos agrupamientos estelares igualmente seniles,los cúmulos globulares.
El Sol,con su enjambre planetario,se encuentra a unos 28.000 años-luz del centro galáctico,que vemos en la Tierra en dirección de la constelación de Sagitario.
Habría que saber ahora de qué está hecho el cuerpo de la Galaxia,el disco galáctico en sí.
En contraste con el halo diáfano,este disco es más bien macizo,ya que está constituido por todo tipo de estrellas: astros apenas menos viejos que los que pueblan el halo,estrellas en la flor de la edad (como por ejemplo el Sol),estrellas jóvenes e incluso bebés-estrellas recién nacidas.
De cualquier forma,en este cóctel astral predominan las estrellas de edad avanzada,enriquecidas en elementos pesados por las generaciones anteriores.
Las estrellas nacen de la materia interestelar,que,en el origen,estaba constituida por hidrógeno,helio y rastros de elementos ligeros como el deuterio o el litio.
Pero,con el tiempo,esta materia va enriqueciéndose en elementos pesados producidos por reacciones termonucleares en el vientre de las estrellas (citemos sobre todo el carbono,el oxígeno,el neón y el hierro).
Estos elementos son luego expulsados hacia el espacio bajo forma de vientos estelares o cuando las estrellas explotan en supernovas al llegar al término de su vida.
Al mezclarse entonces con el gas de origen,cada nueva generación de estrellas cuenta desde su nacimiento con una cantidad cada vez más importante de productos sintetizados por las generaciones que le han precidido.
Ahora bien,como hemos visto,las estrellas del halo son de la primera generación y,por tanto,al no beneficiarse de ninguna herencia,son pobres en elementos pesados y pertenecen a una clase desfavorecida,llamada población 11.
Por el contrario,las estrellas del disco,en su mayoría de la segunda generación o incluso de generaciones ulteriores,contienen un porcentaje más elevado de metales.
Por esta razón,llevan una vida un poco diferente y pertenecen a la clase de las privilegiadas,o población 1. De hecho,las estrellas no se reparten uniformemente en el disco.
Como en toda galaxia que se respete,su densidad aumenta de los bordes hacia el centro.
Además,no son los únicos habitantes del disco,aunque representan el 89,3 por 100 de su masa ; el espacio entre los astros no está vacío,sino lleno de un gas distribuido muy irregularmente y mezclado con pequeños granos de polvo.
Este gas (10 por 100 de la masa de la Galaxia) y el polvo que lo acompaña (7 por 100 de la masas del gas) forman un disco aún más plano que el de las estrellas,que aparece como una banda absorbente que corta en dos la Vía Láctea.
Este polvo debería impedirnos ver los objetos situados a lo lejos en el plano galáctico.
Por suerte,sus indeseables efectos sólo afectan a la luz visible y a la ultravioleta y muy poco al resto del espectro electromagnético ; esto lo que permite observar la Galaxia en otras longitudes de onda (ondas radio,infrarrojos,rayos X y gamma) y deducir los detalles de su estructura comparando las diferentes observaciones.
Más de la mitad de las galaxias que pueblan el Universo presentan una estructura en espiral.
Con frecuencia tienen un aspecto general muy distinto,pero se puede seguir en todas ellas,desde las regiones del centro hacia la periferia,el dibujo de una,dos o más espirales más o menos regulares,llamadas brazos galácticos.
EXISTEN muchos casos de animales que,a falta de una madre biológica,siguen a otros seres de especies distintas a las suyas,incluso a objetos en movimiento.
Fue Konrad Lorenz (premio Nobel de fisiología y medicina en 1973) el que explicó este fenómeno,al que denominó impronta o fijación.
También recibe el nombre de troquelado o impregnación.
Lorenz cuenta en su libro El anillo del rey Salomón que incubó huevos de gansos silvestres,con la colaboración de una gansa doméstica y una pava.
Cuando estaban a punto de nacer,colocó los huevos en una incubadora,a fin de poder estudiar los nacimientos más de cerca.
Una vez que el primer pollito se liberó trabajosamente del cascarón,Lorenz lo puso bajo un cojín,que suplía el cálido vientre de la madre gansa.
Mientras observaba con expectación las reacciones del recién nacido,cometió una equivocación: pronunció unas palabras,al tiempo que realizaba unos movimientos.
Cuando el gansito ya podía correr,Lorenz lo llevó de nuevo con la gansa,que cuidaba de los otros pequeños.
Pero el gansito no quería quedarse con la madre y corría desesperadamente,piando,tras el profesor.
Este comprendió enseguida que el animalito le había tomado por su propia madre.
Al premio Nobel no le quedó otro remedio que adoptarlo.
Es relativamente corriente que algunos animales tomen como madre a lo primero que ven al nacer,siempre y cuando ese objeto se mueva y emita sonidos rítmicos: una persona,otro animal.
una cortadora de césped... El fenómeno de la fijación sólo es posible dentro de un período sensitivo,el único susceptible de quedar impregnado.
Si durante ese tiempo,el animalito no puede fijar nada o a nadie,sufrirá trastornos emocionales y de conducta que le afectarán toda la vida.
Defienden las nidadas aun a costa de los mayores riesgos.
El macho de halcón vuela infatigablemente sus territorios de caza atacando a sus presas las veces que sea preciso para que no falte la comida a sus pollos.
La hembra no se separa ni un momento del nido,centrada en cuidar,dar de comer y vigilar el desarrollo de sus hijos ; los alimenta delicadamente uno por uno y,entre ceba y ceba,monta guardia en la atalaya más alta del cantil.
¡Pobre del intruso que ose acercarse por allí! Zorros,cuervos,ginetas y águilas conocen muy bien el zumbido inesperado y casi suicida de una hembra de halcón.
Paradójicamente,cuando las crías ya han aprendido a cazar,son los propios padres los que las expulsan del territorio.
Los gansos,a su vez,profesan también un gran cariño a sus pequeños.
Ambos progenitores comparten la dirección de la bandada.
Unos férreos lazos de unión mantienen las estructuras familiares,que sólo se rompen en la primavera siguiente,cuando los padres deben entregarse a nuevas tareas reproductoras.
Durante el tiempo que permanecen con sus padres,los gansitos se desarrollan normalmente y aprenden a defenderse de sus enemigos naturales y a buscar alimento.
Pero si por algún motivo los pierden o se les separa de ellos,notan tanto su ausencia que,en ocasiones,llegan a morir a causa de la sensación de abandono aunque no les falte alimento.
Los gansos,igual que las gallinas distinguen los polluelos propios de los extraños.
Si se les intenta introducir un pollo de otra familia,lo reconocen en seguida,y con silbidos,gestos amenazadores (cuello bajo y estirado) y picotazos lo expulsan de su enclave.
Sin embargo,esto no quiere decir que conozcan personalmente a sus pequeños,pues si se les quita alguno sin que ellos lo vean no sienten ninguna impresión ; no obstante,si se les arrebatan todos,se muestran desconcertados al no poder satisfacer sus instintos maternales.
Los mamíferos,de la misma manera que las aves,nacen muy desvalidos e incapacitados para enfrentarse al mundo hostil que les espera.
Tienen los ojos cerrados y precisan la leche materna y cuidados especiales para poder sobrevivir.
Mientras dependen de sus padres,sufren cambios fisiológicos importantes: sustituyen los dientes de leche por los definitivos y van adquiriendo la capacidad de mantener constante su temperatura corporal.
Durante ese período de contacto,los hijos adquieren y empiezan a poner en práctica ajustes de conducta que les serán imprescindibles para desenvolverse de adultos.
Esto constituye una necesidad importante cuando se llega a los últimos escalones evolutivos,o sea,los grandes mamíferos y los primates.
El comportamiento social de los grandes primates depende de los primeros años de su vida,especialmente de la relación sostenida con la madre.
El contacto materno constituye una condición sine qua non para establecer los diversos tipos de conducta que exige el perfecto desarrollo de las comunidades de hábitos sociales.
El hombre no es una excepción a esta norma.
A nadie le extrañan los problemas de adaptación que sufren los jóvenes cuya infancia han vivido lejos de los cuidados de la familia.
Animales que enloquecen cuando se quedan sin crías,especies que hacen de la vigilancia y el cuidado de sus hijos el tema más fundamental de su vida,grupos que no muestran ningún interés por sus pequeños... Como vemos,no se puede generalizar con respecto al instinto maternal de los animales.
Cada especie es un mundo y,dentro de ellas,se establecen también excepciones.
La electrificación creciente de los hogares y la multiplicación de conducciones eléctricas de todo tipo han ido creando un nuevo mundo inundado de radiaciones electromagnéticas cuyo efecto sobre el organismo es todavía poco conocido.
Un grupo de investigadores españoles está realizando una serie de estudios sobre el tema,con el fin de averiguar los efectos biológicos de estos campos energéticos en los animales y,por supuesto,en el hombre.
Según el doctor José Manuel Rodríguez Delgado,jefe del Departamento de Investigación del Centro Ramón y Cajal de Madrid y responsable de estos estudios,el programa multidisciplinar sobre bioelectromagnetismo pretende delimitar los efectos beneficiosos de estas radiaciones y sus posibles influencias negativas.
Todo ello referido a una gran variedad de procesos biológicos que incluye el crecimiento de los huesos,la genética,la embriogénesis,las infecciones,la regeneración de ciertos nervios y las funciones cerebrales vivos se ven sometidos a diversas radiaciones electromagnéticas de origen natural ; la mayor parte provienen del Espacio exterior,fundamentalmente del Sol.
El magnetismo terrestre,utilizado desde hace siglos para orientación geográfica gracias a la brújula,es asimismo una fuente de radiación magnética natural,lo mismo que las descargas eléctricas de las tormentas.
Pero el mundo civilizado actual ha creado multitud de sistemas eléctricos que nos sumergen en toda clase de campos magnéticos a los que nunca hubiéramos estado sometidos de forma natural: líneas de alta tensión,electrodomésticos de todo tipo,centrales de producción de energía eléctrica,transformadores,etcétera.
Es indudable que los seres vivos deben notar,de algún modo,estas radiaciones invisibles,aunque parecía probable,a priori,que su efecto fuese relativamente inapreciable frente a otros factores ambientales más potentes.
Por otra parte,hace ya algún tiempo que se sabe,y no sólo está ya científicamente comprobado sino que se utiliza a nivel clínico,que ciertas radiaciones magnéticas aceleran los procesos de soldadura de huesos en numerosos casos de fracturas.
Y aquí es donde interviene el doctor Rodríguez Delgado,pionero de los estudios de comportamiento cerebral mediante estímulos eléctricos.
Este investigador español se planteó como objetivo el análisis de distintos parámetros de las radiaciones electromagnéticas cuando eran aplicadas a ciertos animales.
Todo ello con el fin de comprobar los efectos biológicos de estas radiaciones y su posible utilización médica ; y,en caso de resultar dañinas bajo ciertas condiciones,para conocer y prevenir mejor este nuevo tipo de contaminación ambiental.
Para estos trabajos,se utilizan nuevos instrumentos para medir campos electromagnéticos en tejidos vivos y en el ambiente,y para generar señales con parámetros ajustables a voluntad.
La microminiaturización de los estimuladores ha permitido,asimismo,alcanzar con precisión las zonas cerebrales deseadas en los animales de experimentación.
Con este instrumental como herramienta,y siguiendo una línea de investigación pluridisciplinar,el equipo de Rodríguez Delgado ha llegado a conclusiones sorprendentes y,en algunos casos,de gran interés médico.
Sobre todo en el campo de la influencia genética,celular y embriológica del electromagnetismo,en el campo de las infecciones bacterianas,en el estudio de la soldadura de huesos rotos y en la modificación de la actividad neuronal y cerebral.
Respecto a la contaminación electromagnética del medio ambiente,una de las conclusiones más importantes reside en el hecho de que las grandes centrales de producción de energía eléctrica emiten mucha menos radiación que muchos pequeños aparatos electrodomésticos de esos que todos tenemos en casa.
A lo mejor la solución estriba simplemente en un mejor blindaje de protección... Veamos con un poco más de detalle cada una de estas líneas de investigación.
En primer lugar,para estudiar los posibles efectos genéticos de las radiaciones electromagnéticas,el doctor Eduardo Ramírez utilizó un pequeño insecto bien conocido en los ambientes científicos por su rapidez de reproducción y la sencillez relativa de su dotación genética (ocho cromosomas),la mosca de la fruta o Drosophila melanogaster ; con esta mosca se pueden estudiar varias generaciones seguidas en sólo unos pocos días.
Pues bien,los ojos rojos y redondos constituyen un carácter propio de los machos,ligado al cromosoma X. Al someter a los machos durante setenta horas a campos magnéticos pulsantes con parámetros similares a los usados en las terapias favorecedores de soldaduras óseas,pudo observarse que estas radiaciones tenían un efecto letal sobre dicho cromosoma en un porcentaje significativo de casos.
El estudio de 6.000 cromosomas X permitió deducir un efecto mutagénico del electromagnetismo no muy grave,pero sí apreciable.
Esta conclusión por sí sola podría no ser demasiado importante.
No obstante,experimentando con huevos de pollo en incubación,la doctora Jocelyn Leal consiguió evidencias de un potente efecto de inhibición en la embriogénesis (desarrollo del embrión) cuando se aplicaba una radiación magnética de 100 herzios de frecuencia y 1,2 microtesla de intensidad.
Un tal campo magnético es físicamente poco intenso y no cabe duda de que en la vida diaria estamos sometidos en ocasiones a energías similares o mayores.
Conviene recordar que la unidad de campo magnético es el tesla o weber por metro cuadrado ; se trata del valor del campo en un punto en el que,al moverse una carga positiva de un culombio a la velocidad de un metro por segundo,se engendra una fuerza de un newton.
También se usa el gauss,que equivale a la diezmilésima parte de un tesla.
Como es lógico,un microtesla,millonésima parte de un tesla,es una unidad muy pequeña de campo magnético.
El riesgo embriogenético,puesto de manifiesto en el desarrollo de los huevos de pollo,y el posible efecto mutagénico,observado en el caso del cromosoma X de la mosca de la fruta,ponen de manifiesto la posibilidad de un riesgo que conviene conocer y evitar.
No existe,por ahora,evidencia de que estos riesgos sean de similar importancia para el género humano,pero estos trabajos pueden servir como punto de partida para posteriores estudios en ese sentido,y sobre todo para intentar elaborar toda una política de prevención,reforzando los blindajes de los aparatos caseros.
susceptibles de crear campos electromagnéticos importantes.
En este sentido,otra de las líneas 100 de la que hay cerca de la Tierra.
La sonda Mariner 9,que fue la primera en ponerse en órbita alrededor de Marte,en 1971,contaba con cuatro paneles solares que totalizaban ocho metros cuadrados.
Las aproximadamente 38.000 células fotovoltaicas de estos paneles proporcionaban 800 vatios en las proximidades de la Tierra,pero sólo 400 cerca de Marte,y no habrían producido más de 10 a la altura de Saturno.
¡Para obtener 1 kilovatio a la distancia de este último se requerirían dos millones de células solares y 300 m2 de paneles portadores! En la Luna,donde la noche dura dos semanas,tampoco era cuestión de alimentar mediante fotopilas,incluso recargando baterías,a los aparatos científicos de las estaciones ALSEP (Apollo Lunar Scientific Experiment Package) depositados por las misiones Apollo.
Estas estaciones,así como las sondas Viking colocadas en Marte,o las Pioneer y las Voyager que navegan a través del Sistema Solar lejano,fueron,pues,dotadas de generadores eléctricos de un tipo completamente distinto.
Estos generadores radioisotópicos obtienen su energía de la conversión nucleotérmica.
Esto significa que convierten en electricidad,mediante diodos especiales llamados termoiónicos,el calor desprendido por la desintegración lenta de unas decenas de gramos de materia radiactiva,plutonio y estroncio esencialmente.
Los americanos ya experimentaron hace tiempo con estos generadores,puesto que empezaron por equipar con ellos a sus satélites de navegación Transit,al principio de la década de los sesenta.
Estos generadores,bautizados con el nombre de SNAP (System for Nuclear Auxiliary Power),son ligeros (unos 10 kilos),poco embarazosos (menos de 50 cm. Los más grandes) y de una gran seguridad de funcionamiento en razón del reducido número de piezas que los componen.
Sin embargo,sus potencias son débiles: tres vatios sólo en el SNAP 3 del primer satélite Transit (junio de 1961),veinticinco vatios en los SNAP 11 de las sondas lunares Surveyor,setenta vatios en los SNAP 27 de las estaciones lunares ALSEP y ciento treinta vatios en las sondas Voyager,que llevaban tres generadores cada una.
Para potencias superiores se necesitan verdaderos reactores nucleares,es decir,minicentrales atómicas móviles.
El rendimiento,inferior a dos vatios por kilo de peso total en los SNAP,supera los tres vatios / kg. en los reactores.
También en esto los americanos fueron los primeros: enviaron el SNAP 10 - A,satelizado a título experimental,en abril de 1965 ; su potencia era de 650 vatios.
Pero esta experiencia no supuso la generalización de los reactores nucleares en órbita ; acaso sólo los posean los nuevos satélites militares de vigilancia oceánica SSU y OPS.
Por el contrario,se sabe con certeza que los soviéticos utilizan reactores nucleares para alimentar el potente radar que equipa a los Cosmos de vigilancia oceánica.
Estos satélites fueron equipados,a partir de 1967,con reactores del tipo Romachka,que proporcionan un kilovatio de potencia.
Luego,a partir de 1977,fueron reemplazados por un nuevo modelo,el Topaze,que ofrece no menos de diez kilovatios.
Esta es precisamente la gama de potencia que pretenden conseguir los americanos con su futuro reactor SP - 100.
El espacio circunterrestre se atesta,pues,progresivamente de materias radiactivas: plutonio 238,estroncio 90 y curio 242 en los SNAP,y uranio enriquecido al 90 por 100 en los Romachka y los Topaze soviéticos.
Por supuesto,no es cuestión de volver a traer estos aparatos a la Tierra,so pena de incrementar la polución radiactiva.
La técnica consiste en colocarlos en órbitas lo bastante altas como para que su caída,bajo los efectos del inevitable frenado atmosférico,no se produzca antes de varios siglos.
Los elementos radiactivos habrán tenido tiempo de perder intensidad,habida cuenta del período de los elementos utilizados.
Conviene recordar que el período o vida media es el tiempo que emplea un elemento radiactivo en ver reducida su actividad a la mitad: 90 años para el plutonio,por ejemplo.
Los Transit americanos circulan a una altura de entre 800 y 1.200 km ; los SSU,entre 1.100 y 1.400 ; y los Cosmos,entre 900 y 1.000 km. Los satélites soviéticos,al contrario de los americanos,son colocados primero en una órbita baja,lo que permite al radar barrer los océanos con mayor precisión.
Después,al término de un tiempo de servicio que no supera nunca los cinco meses,se pone en marcha un motor-cohete incorporado que los expide a una órbita mucho más elevada,que les asegura una supervivencia espacial del orden de los cinco siglos.
La radiactividad del reactor se ve entonces reducida a la millonésima parte de su valor inicial.
En cuanto a los generadores radioisotópicos de las estaciones ALSEP emplazadas en los mares lunares,no molestan a nadie ; lo mismo ocurre con los de las sondas Surveyor y Viking,o los de las Pioneer y Voyager,que,siguiendo el camino del Pioneer 10,abandonarán el Sistema Solar dentro de unos años en dirección a las estrellas.
En un futuro más o menos próximo,el átomo también será utilizado en la propulsión de los cohetes.
Los cohetes de propergoles químicos han sido,en efecto,descartados para las futuras misiones espaciales humanas a través del Sistema Solar.
Para expedir naves espaciales de varias decenas o incluso centenares de toneladas hacia Marte,por ejemplo,serían precisos,para salir de la Tierra,artefactos monstruosos,de imposible construcción.
El cohete gigante Saturno V,que llevó hombres a la Luna durante el programa Apollo,o el misterioso lanzador que los soviéticos parecen estar a punto de probar,tras quince años de contrariedades,para transportar las piezas de su futura gran estación espacial,representan el tope máximo posible en este campo: entre 3.000 y 5.000 toneladas en el despegue,para satelizar unas 150 toneladas en órbita baja.
Un aprovechamiento de sólo un 3 a un 5 por 100.
Desde el punto de vista del rendimiento,la lanzadera espacial es de lo mejor que se ha hecho en materia de propulsión química.
La conjunción criogénica de oxígeno e hidrógeno líquidos que utiliza permite,en efecto,satelizar 100 toneladas con un peso en el despegue de 2.000 toneladas,es decir,el 5 por 100.
No será posible ir más allá,puesto que el abanico de los propergoles utilizables ya ha sido completamente explorado ; sólo la sustitución del oxígeno por el flúor permitiría mejorar un poco el rendimiento,pero los inconvenientes que plantea su utilización (toxicidad de los residuos de combustión,dificultades de manipulación) no compensan en absoluto la débil mejoría que podría obtenerse.
Un motor-cohete nuclear permitiría triplicar las capacidades de un cohete químico,al triplicar también la velocidad de eyección.
El principio de su funcionamiento es muy simple: gracias al calor que desprende,el corazón de un reactor nuclear de uranio enriquecido calienta un fluido que es,posteriormente,eyectado por una tobera.
Mientras que en un cohete químico es la materia propulsora (propergol) la fuente de energía,aquí las dos están disociadas y,por tanto,hay libertad de elección en la materia eyectada.
Bastará con elegir aquélla que tenga una masa molecular más débil,ya que la velocidad de eyección es inversamente proporcional a la raíz cuadrada de esta masa molecular.
El propulsor ideal para un reactor nuclear es,pues,el hidrógeno de masa molecular 2. En un cohete químico,este hidrógeno debe ser asociado al oxígeno,como concurrente.
sin lo cual no podría arder en el vacío.
La masa molecular del conjunto alcanza entonces 18.
La relación entre la raíz cuadrada de 2 y la de 18 es exactamente 3 ; he aquí por qué,utilizando también hidrógeno,un propulsor nuclear permitirá triplicar la velocidad de eyección.
Pero no todo son ventajas en el cohete nuclear,ya que el reactor y los blindajes son muy pesados (y son indispensables en el caso de una nave espacial habitada).
La mala relación de masa nos hará perder,pues,inevitablemente,lo que habíamos ganado con la velocidad de eyección.
Además,la reserva de energía casi inagotable del reactor no es tan ventajosa como parece,ya que sólo es útil cuando hay materia que eyectar.
Entre dos fases propulsoras los productos de fisión continúan desintegrándose,desprendiendo calor,y es preciso seguir enfriando el reactor para evitar que se deteriore,por lo que el consumo de fluido de refrigeración deberá ser tenido en cuenta.
En un plano puramente técnico,hay que considerar también que para obtener las velocidades de eyección antes anunciadas hay que calentar el hidrógeno a unos 3.000 oc.
Por tanto,conviene utilizar para el reactor materiales que soporten tales temperaturas.
Sólo los carburos de tántalo y de zirconio,aliados a materiales compuestos refractarios,podrían convenir,pero en cualquier caso no permitirían sobrepagar - los 2.000 oc.
En estas condiciones,la velocidad de eyección se quedaría en unos 9 km / s.,y el rendimiento,en definitiva,no seria ya el triple,sino sólo el doble que el de los propulsores químicos.
Para conseguir algo mejor habría que renunciar a los reactores nucleares clásicos,que transfieren su calor al flujo gaseoso que los atraviesa,y hacer un reactor cuyo propio corazón fuese gaseoso,y en el que los productos de fisión calentasen directamente el gas a eyectar.
Pero entonces,¿cómo impedir que el gas se llevara consigo los materiales fisibles? Quizá animándolo con un movimiento de rotación creador de una fuerza centrífuga.
Incluso se ha llegado a estudiar la idea de hacer explotar en cadena una serie de bombas atómicas colocadas en la parte trasera del vehículo espacial: una pantalla absorbería la energía emitida para propulsar la nave por reacción... Idea digna de la mejor ciencia-ficción.
Y sin embargo,los americanos han estudiado tal proyecto,clasificado como Secreto de Defensa,bajo el nombre de código Orión.
La idea fue emitida en 1955 por el doctor Stanislas Ulam,del célebre laboratorio de Los Alamos,donde precisamente fue estudiada la primera bomba atómica americana.
Tres años más tarde,la firma General Dynamics obtenía un primer contrato de un millón de dólares para los estudios de viabilidad.
Otros dos créditos fueron otorgados hasta agosto de 1959,con una media de 100.000 dólares mensuales.
En 1963,la NASA se asoció al proyecto y pidió a la General Dynamics...,¡que se orientara hacia la posibilidad de propulsar así una nave espacial habitada! Pero el proyecto Orión llegó a su término en octubre de ese mismo año de 1963,tras la firma del acuerdo que prohibía las experiencias nucleares en el Espacio.
EL riesgo de padecer una hepatitis ha aumentado considerablemente en los últimos años.
En realidad,resulta muy difícil realizar un estudio de la incidencia de esta enfermedad en una población determinada,debido a que el germen causal es siempre un virus difícil de aislar y de confirmar.
Hasta hace poco conocíamos dos tipos de hepatitis,la A y la B,ocasionadas por virus diferentes.
Ambas formas de enfermedad viral varían no sólo por el agente responsable,sino también por las manifestaciones clínicas,sus modos de contagio y su pronóstico.
Recientemente se han podido describir otras clases de hepatitis que,pomposamente,han recibido el nombre de hepatitis no-A no-B,lo que demuestra la ignorancia que se tiene con respecto al virus que las produce.
Los primeros síntomas de esta enfermedad son bastante parecidos cuando se trata de los dos tipos de virus conocidos,aunque con pequeñas diferencias que pueden poner en guardia al médico.
Su comienzo es insidioso y,a veces,de diagnóstico complejo.
Aparece anorexia (pérdida de apetito),astenia (cansancio físico intenso),náuseas,vómitos,dolores articulares,musculares y de cabeza,tos,faringitis... Es decir,el cuadro de una infección viral generalizada,por lo que puede prestarse a confusiones.
No hay un tratamiento específico contra esta enfermedad.
La ingeniería genética ha logrado vacunas contra la hepatitis B,de gran ayuda para las personas expuestas a contagio.
El signo principal de esta afección es la ictericia,o sea,la coloración de las conjuntivas y de la piel de un tono amarillo-verdoso.
Este cambio de color surge cuando la bilirrubina,fabricada por el hígado,pasa al suero a causa de la propia lesión del hígado y alcanza valores superiores a los 2,5 miligramos por cada 100 c. c. de suero.
Normalmente,antes de esta pigmentación se presenta un cambio de color en la orina.
La fiebre,cuando aparece,no suele ser muy elevada.
Como vemos,estos virus causan una enfermedad sistémica,es decir,que afecta a todos los órganos del cuerpo,especialmente al hígado.
Sobre el virus A no se saben demasiadas cosas.
Se trata de un virus de ácido ribonucléico (RNA),que pertenece a una de las razas más antiguas de virus: los enterovirus.
A través del microscopio electrónico se ha podido localizar en las heces de los enfermos ; también mediante sofisticadas técnicas como el radioinmunoensayo o la fijación de complemento,se han encontrado los anticuerpos que fabrica el organismo contra el propio virus.
Sobre el virus B se conocen muchos más datos.
Se trata de una infección única para la especie humana,que no se puede transmitir a los animales.
Gracias al microscopio electrónico se han podido comprobar muchos aspectos de la morfología de este temible agente en el suero de los enfermos.
Hace algunos años se localizó el antígeno propio de estos virus,antígeno Australia,en los sueros de los enfermos.
Hoy se puede medir e identificar fácilmente en el laboratorio.
El virus productor de la hepatitis B está compuesto por el ácido dexosirribonucléico (DNA) esencialmente diferente del RNA.
Tradicionalmente,ambos virus se han diferenciado por su forma de producir el contagio.
Se creía que la hepatitis tipo A se transmitía por contagio directo - - por vía fecal u oral - - y la de tipo B mediante el suero,o sea,a través de inyecciones o transfusiones de sangre.
Sin embargo,estos datos no bastan para diferenciar un tipo de hepatitis de otro.
Para una correcta clasificación de la hepatitis hay que determinar en el laboratorio las diferentes clases de anticuerpos.
La medición del antígeno Australia en el suero del paciente permite establecer el tipaje de la enfermedad,el estudio epidemiológico y la determinación de si se trata de una dolencia profesional o no.
El suero que contiene el antígeno Australia es altamente contagioso.
Bastaría una mínima cantidad de sangre para extender la afección.
ltimamente se ha encontrado esta clase de antígeno no sólo en el suero,sino también en el líquido seminal,en las lágrimas,en la saliva,en la leche materna...,es decir,en casi todas las secreciones externas.
Por tanto,se ha podido precisar que los contagios del virus B se efectúan por otros muchos canales,incluido el sexual.
Los bancos de sangre tienen,por este motivo,un cuidado escrupuloso.
A pesar de ello,se calcula que de cada 100 personas que reciben una transfusión de sangre de 0,3 a 10 pacientes pueden adquirir una hepatitis B. En los países occidentales son portadores del antígeno Australia entre el 0,1 y el 1 por 100 de la población,aunque no muestren signos de la enfermedad.
En Africa y Asia,la proporción se cifra entre un 3 y un 30 por 100.
¿Cómo se trata esta dolencia? La verdad descorazonadora es que no existe un tratamiento específico contra ella.
En los casos de hepatitis A,parece aconsejable la inoculación de gammaglobulina sérica,inmune a los contactos más cercanos al paciente.
En cuanto a la hepatitis B,la reciente aparición de vacunas concretas contra esta enfermedad están siendo de gran ayuda para las personas especialmente expuestas a esta clase de contagio.
El virus de la hepatitis B se transmite a través de las inyecciones y transfusiones de sangre y por medio de casi todas las secreciones externas.
las algas constituyen más del 50 por 100 de la materia viva vegetal del Planeta.
Sin embargo,se trata de uno de los grupos de plantas peor estudiados,a pesar de que constituyen ya una importante fracción de los recursos alimenticios humanos,y pueden ser,en un futuro no demasiado lejano,la solución al hambre en el mundo.
Bajo el nombre de algas se agrupan toda una serie de vegetales que sólo tienen en común su necesidad de un alto grado de humedad para sobrevivir.
Además,todas las algas,y actualmente se han descrito unas 25.000 especies,disponen de una estructura orgánica muy sencilla.
Y,naturalmente,como vegetales que son,poseen la capacidad de producir materia orgánica a partir de las sales minerales,el anhídrido carbónico,la clorofila y la energía luminosa del Sol.
El proceso de fotosíntesis mediante el cual los vegetales fabrican materia orgánica,que se traduce en un crecimiento de su organismo o en creación de descendencia,produce oxígeno como material de desecho.
En el caso de las algas,este gas vital no sólo oxigena el medio acuático en el que viven,sino que también acaba escapándose a la atmósfera,contribuyendo de forma muy importante a la reoxigenación del aire del Planeta.
Porque,no lo olvidemos,las algas constituyen más de la mitad de las especies vegetales de todo el mundo.
A su transcendencia actual como elementos oxigenadores de la atmósfera y como alimento humano,faceta todavía en embrión y susceptible de desarrollos espectaculares,las algas unen un pasado científicamente apasionante y,desde luego,absolutamente básico para comprender la evolución de la vida en la Tierra.
Los primeros organismos autótrofos,es decir,capaces de alimentarse y sobrevivir por sí mismos,que aparecieron en los mares terrestres hace unos 3.000 millones de años,fueron algas.
Unas algas azules microscópicas capaces de sintetizar la materia disuelta produciendo,como residuo indeseable,el oxígeno que,más tarde,posibilitaría la vida terrestre y aérea tal y como hoy la conocemos.
Más adelante,casi 2.000 millones de años más tarde,a partir de un grupo de algas,las clorofíceas,se originaron todas las plantas superiores,de las que ha derivado el mundo vegetal actual.
Con toda esta carga histórica a sus espaldas,el mundo de las algas constituye,hoy día,un grupo sumamente heterogéneo,como si esos miles de millones de años de vida no hubiesen hecho mella en su vitalidad.
Existen algas monocelulares,algas pluricelulares y de tamaño casi gigante e incluso un paso intermedio constituido por colonias de algas celulares independientes.
A nivel macroscópico,y dejando de lado las algas microscópicas,existen algas de sólo un centímetro junto a inmensas plantas marinas del Pacífico que llegan a alcanzar los 400 metros de longitud.
Las algas del litoral español no llegan a tanto ; los tamaños mayores los alcanzan ciertas especies atlánticas que rondan los cuatro metros de longitud.
Muchas de estas plantas de gran tamaño pueden dar la impresión de tener el organismo estructurado en raíces,tallo y hojas,como auténticos árboles submarinos.
Pero se trata tan sólo de una apariencia ; las algas son vegetales inferiores,talófitos,y carecen de vasos conductores de sustancias,condición sine qua non para considerarlas como seres diferenciados y biológicamente superiores.
Quizá esta inferioridad de las algas es la que les ha permitido,paradójicamente,sobrevivir y proliferar durante tantos millones de años... No sólo existen tamaños muy diversos en el mundo de estos vegetales marinos.
También muestran coloraciones sumamente variadas,según la diferente composición de sus blastos,orgánulos celulareS que contienen los pigmentos fotosintéticos de los vegetales.
El resultado es una enorme variedad de algas de color verde,pardo,azul o rojo,en las que los diversos pigmentos de esos colores captan una gama mayor de longitudes de ondas luminosas,transmitiendo esa energía a la verde clorofila con el fin de optimizar el proceso fotosintético.
Gracias a su coloración se ha conseguido clasificar a las algas en sólo cuatro grandes grupos: clorofíceas (algas verdes),cianofíceas (algas azules),feofíceas (algas pardas) y rodofíceas (algas rojas).
En este último grupo existen algunas especies capaces de depositar en su superficie externa grandes cantidades de carbonato cálcico (CO3Ca),lo que les confiere una peculiar apariencia pétrea,sin perder,por ello,su coloración.
Y para terminar con la heterogeneidad de las algas,no hay más remedio que aludir al hábitat en que viven.
Un hábitat que puede ser de lo más variado,ya que va desde el agua dulce hasta el agua salada,con temperaturas por debajo de cero o por encima del nivel de ebullición ; incluso son capaces de vivir en la nieve,en la arena,en las rocas,dentro de ciertas plantas o en el interior de otros organismos a los que pueden colorear.
En todo caso,la fracción más importante la constituyen las especies marinas,que pueden agruparse en dos grandes unidades: las planctónicas,que se encuentran en suspensión en el agua,y las bentónicas,que viven fijas al sustrato o a otros organismos del fondo marino.
De todas estas algas marinas depende la casi total producción de materia orgánica que entra en la cadena trófica marina ; lo cual equivale a decir que de las algas marinas pueden depender,para su subsistencia,todas las demás especies vivas del océano,animales o vegetales.
Incluso si,como afirman algunos autores,las algas planctónicas producen el 80 por 100 del oxígeno atmosférico,cabría afirmar que hasta la vida terrestre depende de ellas.
Desde el punto de vista humano,y prescindiendo de los muchos detalles científicos de interés apuntados hasta aquí,es evidente que la importancia de las algas,como productoras de oxígeno,cobra especial importancia en esta época de grandes poluciones,marinas y atmosféricas.
Toda una amenaza que puede limitar gravemente su existencia al reducir la luminosidad solar a causa de las basuras aéreas o el aporte de su alimento por envenenamiento del mar.
Por otra parte,la humanidad ha aprendido a utilizar las algas en su provecho,y es probable que en algas resida,en gran medida,la clave para solucionar el problema del hambre en el mundo.
En los países occidentales el consumo humano de algas todavía resulta exótico,pero lo mismo en América del Sur que en los países asiáticos la dieta utiliza hasta 40 especies diferentes de algas en sopas,ensaladas y guisos e incluso elaborando conservas en salazón o,recientemente,en lata.
Numerosas poblaciones costeras,en Europa como en el resto del mundo,han utilizado algas como abono natural.
EN 1986 la informática habrá entrado en todos los colegios de enseñanza pública,si se cumplen las previsiones del Proyecto Atenea.
Dicho proyecto implica no sólo una revolución en la enseñanza española,sino también un reto para la industria electrónica e informática nacional,que elaborará un producto hardware y software adecuado.
El Estado va a invertir en este programa 2.000 millones de pesetas.
El Proyecto Atenea es un plan piloto para introducir las nuevas tecnologías en la enseñanza no universitaria,o sea,en la EGB,BUP,FP y COU.
Su objetivo consiste en sentar unas bases flexibles que permitan con el análisis de experiencias anteriores en este campo,adaptarse de forma rápida y eficaz a los cambios tecnológicos y pedagógicos que depara el futuro.
Tres son los grandes problemas que han de abordarse en este proyecto: la formación del profesorado la selección del software educativo y la instrumentación de la docencia mediante el ordenador.
Como es evidente,el personal docente no está preparado técnicamente para enfrentarse a este reto.
Lógicamente,la formación del profesorado se va a llevar a cabo en cascada sobre todo teniendo en cuenta que éste procederá de distintas áreas y ramas.
Nadie mejor que el maestro para comprender y estimular la aplicación del ordenador a los métodos pedagógicos.
La preparación inicial será breve aunque suficiente para afrontar ciertos temas generales,dos lenguajes de programación (LOGO y Basic) y el manejo de programas de aplicación.
Con respecto a la creación del software,se trata de un tema que se encuentra en una situación precaria y confusa.
No hay respuesta generalizada del sector industrial,aunque sí de empresas que ya tienen realizados paquetes.
Entre los integrantes de la comisión que han elaborado el Proyecto Atenea,se encuentra la Fundación para el Desarrollo de las Comunicaciones (Fundesco),entidad con la que el Ministerio de Educación y Ciencia (MEC) estudia la metodología de aplicación de los contenidos educativos a través de la informática.
El director del Departamento de Aplicaciones de Fundesco,Jesús Galván,considera que la elección de los programas que el MEC necesita quedará determinada a través de concurso público y todavía está por decidirse cuál será el sistema de selección de las ofertas.
En este sentido,Luis Felipe Paradela,subdirector general de Organización y Automatización del MEC,afirma que " es una posibilidad muy remota que el Ministerio cree una empresa de software " defendiéndose de esta manera de la agresividad comercial de editoriales,que ya se han adjudicado la distribución comercial de programas educativos importados de otros países.
Añadió que se reconoce la posibilidad de traducir lo que ya está hecho en otros países,pero esta traducción,de llevarse a cabo,se haría de las técnicas y no de los contenidos.
EL Ministerio de Industria ha sido encargado de seleccionar,mediante concurso público,los programas y los equipos que necesita el Proyecto Atenea No obstante,los responsables de la decisión consideran que no han logrado una respuesta industrial que satisfaga las necesidades y cumpla el requisito de ser un producto totalmente nacional.
La única empresa que ha presentado un proyecto completo de informática educativa no universitaria ha sido IBM España,en colaboración con la Escuela Técnica Superior de Ingenieros Industriales de la Universidad Politécnica de Madrid.
Para iniciar este programa,dicha compañía mantuvo contactos con el Ministerio de Educación y Ciencia,con la Generalitat de Cataluña y con diversos centros privados ; concretamente,contaron con la ayuda de 21 colegios que prepararon un proyecto de enseñanza de informática.
El plan propuesto por IBM consta de tres fases,dos de las cuales ya se han llevado a cabo.
En la fase I se formó a dos profesores de cada una de las escuelas elegidas.
En la fase ll estos profesores,ya preparados,impartieron un curso a otros compañeros de sus centros,especializados en áreas de idiomas,ciencias humanas,etcétera.
En la fase III van a trabajar conjuntamente todo el personal docente ya preparado.
Estudiarán el programa de Informática ideal para los escolares ; evaluarán la adecuación de las distintas metodologías y programas,tanto por niveles como por materias ; aplicarán los conceptos fundamentales de la Inteligencia Artificial a la educación y a los distintos lenguajes de programación ; y,finalmente,tienen el proyecto de diseñar un aula informática.
Los profesores implicados en este plan tienen a su disposición,en la Escuela de Ingenieros Industriales de Madrid,diversos paquetes Integrados para la resolución de los problemas que les surjan.
Este dibujo ha sido realizado mediante ordenador.
Una de las características del aula Informática reside en que prevé un lenguaje para la realización de diseño gráfico.
EN el silencio de la noche o a pleno sol,por encima de las montañas o siguiendo el perímetro costero,solos o acompañados,los pájaros recorren anualmente miles de kilómetros.
Los modos y variedades de la migración resultan inmensos y no exclusivos de los pájaros,pero tienen un claro denominador común: son uno de los fenómenos más atractivos y espectaculares de cuantos acaecen en la naturaleza.
Casi todo el mundo cree que las cigüeñas parten todos los veranos y retornan por San Blas.
Curiosamente este último dato,rubricado por uno de los refranes más conocidos,no deja de ser radicalmente incierto,pues son numerosas las hogareñas zancudas que se posan en nuestros campanarios en diciembre y enero.
Pero la cronología de la llegada o de la partida de las aves apenas revela una mínima parte de los secretos de la migración.
Porque: ¿Qué justifica el calendario migratorio?,¿qué rutas siguen los grandes voladores?,¿a qué alturas viajan?,¿durante cuánto tiempo? Y,sobre todo,¿qué sistemas de orientación siguen? Preguntas tan apasionantes,que llevan despertando el interés de millares de investigadores desde hace casi un siglo.
Gran parte de los trabajos encaminados a la resolución de ese abanico de interrogantes están directamente relacionados con el anillamiento científico de aves.
Marcar pájaros es,hoy,una práctica relativamente usual,que conlleva la captura de aves vivas a las que se coloca una tira redondeada de metal en la que están impresas una dirección,ciertas siglas y una numeración.
Este sencillo y eficaz sistema permite,siempre y cuando quien encuentre un ave anillada se tome la molestia de notificar su hallazgo,conocer muchos aspectos de las migraciones.
Por ejemplo: la longevidad,áreas de reproducción e invernada,parte de las líneas de vuelo seguidas e incluso la velocidad de desplazamiento.
Tengamos en cuenta,a este último respecto,que con cierta frecuencia algunas aves son capturadas a considerable distancia del lugar donde fueron anilladas y a las pocas horas o días de ser liberadas.
Entre otros muchos ejemplos,podemos citar el del chorlito dorado,que emplea sólo cuarenta y ocho horas en atravesar el Pacífico desde Alaska hasta Hawai.
Es decir,que recorre 3.000 km. a una velocidad de unos 90 km / hora.
La misma,curiosamente,que se estima puede alcanzar el atún,pues se ha comprobado,también mediante marcas colocadas en las aletas de algunos ejemplares,que estos peces llegan a atravesar el Atlántico norte en sólo cuatro jornadas.
Pero hay más técnicas para el estudio de las migraciones.
De cara a la investigación de los sistemas de orientación,se ha recurrido a la más moderna tecnología.
Ciertos pájaros que viajan preferentemente durante la noche,por ejemplo,son introducidos en jaulas,y éstas,a su vez,en el interior de grandes planetarios,con lo que las aves cautivas pueden ver una bóveda celeste estrellada.
Pues bien,así se ha podido comprobar que muchos de los más intrépidos voladores se orientan,precisamente,como los primitivos navegantes: siguiendo las estrellas.
ltimamente se han conseguido radiotransmisores tan reducidos que permiten ser transportados por ciertas especies migrantes sin mermar sus facultades.
Lógicamente con las ondas de radio se conoce,en cualquier momento,la posición del emisor.
A nadie se le pueden ocultar los espectaculares resultados que se obtienen con esas radiografías de los vuelos de las aves viajeras.
Con tales experimentos y otros muchos,realizados en su mayor parte durante los últimos veinte años,se ha logrado comprobar que,al menos,una decena de mecanismos fisiológicos y perceptivos se ponen en juego,aislada o conjuntamente,para indicar a los pájaros el camino a seguir.
Nos referimos a las brújulas internas ; es decir,a la medición de los campos magnéticos ; al cálculo de los desplazamientos del sol y las principales constelaciones en el firmamento ; a la memoria topográfica,lo que equivale a la capacidad de leer en el paisaje las sendas a seguir ; a los barómetros de enorme precisión y pronóstico anticipado ; a la captación de las variaciones en la luz polarizada ; a la distinción de los infrasonidos que emanan,como los olores,de las distintas regiones.
Pero a estas capacidades sensoriales hay que sumar otras todavía más incomprensibles,como las que corresponderían al higrómetro,al termómetro y hasta al voltimetro.
En suma,que en el cerebro de los modestos pájaros hay un laboratorio capaz de medir la luz,el sonido,el olor,la temperatura,los campos eléctricos y magnéticos y la humedad,tanto del lugar por el que viajan,como de regiones alejadas.
Pero no sólo migran las aves,por mucho que sean las acaparadoras de nuestra atención.
Viajan muchos peces,como el salmón o la anguila,desde los océanos hasta los ríos.
No son raras las especies en este grupo zoológico que invierten años enteros en sus migraciones.
Se desplazan también todos los mamíferos marinos.
Algunas ballenas,por ejemplo,recorren varias veces el mundo a lo largo de su vida.
En tierra firme unas cuantas especies de murciélagos y recorren continentes enteros.
Asimismo,los mamiferos terrestres de la tundra y la alta montaña cambian significativamente de lugar siguiendo el vaivén de las estaciones climáticas.
El cerebro de un pájaro es capaz de medir la luz,la temperatura,los campos eléctricos y magnéticos.
Son,en cualquier caso,los insectos quienes sorprenden definitivamente con su capacidad de desplazamiento.
Libélulas,mariposas,algunos dípteros y hasta escarabajos recorren centenares y miles de kilómetros por rutas todavía más desconocidas que las de los pájaros y guiados por sistemas aún ingnorados.
Sin olvidar,claro,a todos los polizontes ; es decir,a un gran número de animales parásitos que viajan sobre los verdaderos migrantes.
Ni a los migrantes forzosos o involuntarios que son transportados por los vientos o las aguas a lugares distantes de sus moradas habituales.
En todos los grupos,pues,y siguiendo todas las estrategias imaginables,se produce el apasionante fenómeno de las migraciones.
Sólo nos queda entender la función evolutiva de tal comportamiento.
Tal vez la mejor explicación resida en esos íntimos impulsos que la materia,organizada en vida,siente por ocupar todo el espacio disponible y en el momento en que se hace asequible.
Ciertamente,de no existir la capacidad de viajar,más de la mitad de las tierras emergidas y de los mares estarían desprovistos,durante todo el año,de vida.
Las latitudes extremas están pobladas sólo durante una parte del año y exclusivamente por migrantes.
Y no son pocos: nada menos que una tercera parte de las aves existentes son migradoras,es decir,cerca de 3.000 especies.
Gracias a los buscadores del eterno verano,el planeta Tierra cuenta con vida en sus rincones más inhóspitos.
Esas rutas invisibles que siguen los viajeros merecerían,en consecuencia,figurar en todos nuestros mapas y gozar de respeto.
El viejo sueño de los alquimistas,transmutar un elemento en otro,fue conseguido por primera vez hace medio siglo.
Poco después,en 1943,la bomba atómica... Las travesuras de los átomos,que los físicos contemplaban asombrados desde finales de siglo pasado,comenzaron a ser comprendidas y,luego,parcialmente dominadas.
A finales del siglo XIX se creía que los átomos eran los elementos más pequeños de cuantos constituyen la materia y que de sus combinaciones surgían todas las sustancias inanimadas o vivientes.
El origen de esta idea se remonta al siglo 11 antes de Cristo,cuando el filósofo griego Demócrito afirmó que toda la materia se reducía a la asociación de pequeñísimas partículas que bautizó como átomos (en griego,irrompible,indivisible).
Naturalmente,Demócrito no aportó prueba alguna de su representación atómica del mundo material.
Pero el extraordinario desarrollo de la química en los siglos XVIII y XIX comenzó a mostrar lo ajustado de semejante concepción,hasta entonces puramente ideal.
Aunque los tales átomos,que tan perfectamente se ajustaban a la noción de reacciones químicas entre los cuerpos,no tenían nada de indestructibles,como lo probó el descubrimiento de la radiactividad.
Un descubrimiento que reveló,ante la sorpresa de muchos científicos,que los átomos eran mucho más traviesos y complicados de lo que Lavoisier y sus colegas habían supuesto.
El primer paso hacia el estudio de la radiactividad lo dio en 1895 el físico alemán Rontgen (1845 - 1923),estudiando un fenómeno bien distinto: los rayos catódicos.
Diez años antes,éstos habían sido observados (ver dibujo 1) con la lámpara de Crookes,un tubo de vacío con dos electrodos conectados a una corriente de alta tensión.
La emisión que iba del cátodo,electrodo negativo,al ánodo,electrodo positivo,pudo ser registrada porque provocaba la fluorescencia del tubo de vidrio y del gas que había en el interior (los tubos de vacío de aquella época no eran demasiado perfectos,como puede suponerse).
Esos rayos catódicos (o sea,procedentes del cátodo) impresionaban además una placa fotográfica colocada al lado del tubo.
Rontgen se propuso averiguar su naturaleza y sus propiedades,ya que algunos científicos pensaban que se trataba de luz de muy corta longitud de onda,y otros opinaban que se trataba de partículas cargadas de electricidad.
En 1897,el inglés Joseph John Thomson (1856 - 1940) demostró que la segunda hipótesis era la verdadera,y que los rayos catódicos eran partículas negativas,que él denominó electrones.
Incluso llegó a pensar en un modelo de átomo en el que estos electrones negativos se encontrasen impregnando una masa positiva ; lo cual,en aquella época,sonaba a herejía,puesto que el átomo era considerado como indivisible.
Pero no anticipemos,y volvamos a Rontgen,en noviembre de 1986.
Para no distraer su atención con la fluorescencia,cubrió su instrumental con un espeso papel negro para no dejar pasar ninguna luz (ver dibujo 2).
Entonces volvió a poner su tubo bajo tensión y se quedó estupefacto al observar que,pese a sus precauciones,ciertos cuerpos situados lejos de la lámpara de Crookes comenzaban a brillar.
Dado que ni la luz ultravioleta ni los colores visibles atraviesan el papel negro,tuvo que admitir que se encontraba ante una nueva radiación desconocida,que bautizó como rayos X. El tubo de rayos catódicos emitía,pues,una radiación invisible pero penetrante,capaz de atravesar,como pudo comprobar Rontgen,no sólo el papel y el vidrio,sino también la madera,el cartón,las manos... Estos descubrimientos provocaron una viva sensación en el mundo científico,y Rontgen recibió por ellos el Nobel de física en 1901.
Sin embargo,nada se sabía realmente de la naturaleza exacta de los misteriosos rayos X. La cuestión fue resuelta por el físico francés Henri Becquerel (1852,1908),gracias a sus estudios sobre las sustancias que,expuestas a la luz solar y trasladadas inmediatamente a un lugar oscuro,se volvían luminosas,devolviendo poco a poco la energía recibida.
Para relacionar los rayos X con esta fosforescencia de algunos materiales expuso al Sol sales de uranio colocadas sobre una placa de aluminio.
Debajo de ellas situó una placa fotográfica perfectamente protegida de la luz por un espeso papel negro (ver dibujo 3).
El pálido sol de febrero de 1896 fue aparentemente capaz de producir rayos X,que impresionaron débilmente la placa fotográfica.
Pocos días después,Becquerel guardó en un cajón el dispositivo con las sales de uranio,porque vinieron unos días de tiempo gris y lluvioso.
Cuando,ya en marzo,volvió a lucir el sol,antes de reiniciar sus experimentos tuvo la genial intuición de revelar la placa fotográfica que había estado guardada en el cajón de su mesa (ver dibujo 4).
Su asombro no tuvo límites cuando vio que la placa mostraba una impresión mucho más potente que la que se producía por el efecto fosforescente debido al Sol.
Lo cual demostraba que el propio uranio emitía rayos X,sin ninguna necesidad de exponerlo a la luz solar ; en realidad,Becquerel creía que eran rayos X (y por eso bautizó a las sales responsables como uranio X),pero acababa de descubrir,fortuitamente,la radiactividad natural.
Estos rayos uránicos,que Becquerel tomaba erróneamente por rayos X,parecían poseer,entre sus propiedades más interesantes,la capacidad de ionizar el aire próximo.
Becquerel lo puso de relieve colocando una fuente de uranio entre dos placas metálicas separadas por aire y conectadas a los dos polos de una bacteria (ver dibujo 5).
Con un electrómetro medidor del paso de una corriente eléctrica pudo constatar la presencia de una tal corriente entre las dos placas.
Este descubrimiento les sería muy útil,más tarde,a otros científicos para poder determinar el grado de actividad de otras sustancias radiactivas,simplemente midiendo la cantidad de electricidad que producían al ionizar el aire circundante.
María Curie (1867 - 1934) fue uno de estos investigadores.
Aconsejada por su marido,Pierre Curie (18591906),decidió estudiar para su tesis doctoral el fenómeno descubierto por Becquerel.
Y,muy particularmente,si el uranio era el único elemento que emitía los misteriosos rayos.
Gracias a un electrómetro muy preciso,puesto a punto por Pierre Curie en función del fenómeno denominado piezoelectricidad (pequeñas deformaciones de cristales de cuarzo por débiles cargas eléctricas),María estudió diversos compuestos.
Aunque es difícil aplicar fórmulas a parámetros biológicos,se puede decir que la cifra ideal de colesterol en sangre sería aquélla que añade un gramo a la cifra de la propia edad ; por ejemplo,si se tienen cincuenta años el colesterol debiera rondar la cifra de 1,50 gramos por litro.
En los Estados Unidos,los consejos dietéticos aplicados a gran escala en los últimos quince años (disminución del consumo de grasas animales fundamentalmente) han supuesto una disminución media de la colesterolemia del 4 al 7 por 100.
Un régimen adecuado actúa,siempre en pequeña proporción,de la siguiente forma: el intestino absorbe el colesterol de los alimentos que lo contienen (yema de huevo,leche entera,hígado,sesos,grasas animales) ; pero además,nuestro propio organismo sintetiza colesterol y,al mismo tiempo,va eliminando el sobrante.
El mecanismo conjunto de la absorción,la síntesis y la eliminación se va regulando de forma más o menos automática ; por eso,el efecto del régimen,que sólo actúa en la absorción,es limitado.
Y ello es afortunado,porque el exceso de colesterol es malo pero su ausencia sería dramática.
Se trata de una molécula fundamental,sin la cual no podríamos vivir: lo mismo la encontramos en las membranas celulares que como precursora de las hormonas esteroides (sexuales y corticosurrenales) o de los ácidos biliares que nos permiten la digestión de los cuerpos grasos.
En contra de lo que a veces se lee o se dice,no hay colesterol bueno y malo,sino simplemente hay colesterol útil y exceso de colesterol ; también aquí la dosis hace el veneno,aunque influye mucho el lugar en el que se localiza el exceso.
Pues bien,el efecto benéfico de la manzana actúa sobre estos dos parámetros,reduciendo la dosis,y además en los lugares adecuados.
En primer lugar,la manzana,como todo alimento vegetal,no aporta colesterol ; ésta es una molécula exclusiva del reino animal,cuyo nombre exacto es hidroxi - 3 - colésteno 5 - 6,lo cual indica una concentración del radical hidróxilo (OH) en el carbono 3 de su núcleo,con doble enlace entre los carbonos 5 y 6 (véase la fórmula del colesterol en la página de al lado).
Generalmente,el colesterol se presenta bajo una forma esterificada,es decir,unido a un ácido graso en el área del grupo OH de su molécula (este ácido graso suele ser oleico o linoleico).
El organismo prefiere esta forma cuando se trata de almacenar o transportar colesterol,pero cuando lo utiliza o lo intercambia entonces lo libera del ácido graso dando lugar a un cristalito de colesterol libre.
TODAS nuestras células pueden fabricar colesterol (ladrillos azules),pero es,sobre todo en el intestino y en el hígado donde esta síntesis es más activa (1),bajo la dependencia de una enzima: HMGCoA reductasa (r).
El colesterol recién formado puede ser almacenado (2),bajo forma esterificada (ladrillos marrones),gracias a la actividad de otra enzima: la ACAT (A).
Cuando hay que utilizarlo,o hacerlo salir o entrar en una célula,una tercera enzima,la esterasa (E),desesterifica y libera la molécula de colesterol (3).
El colesterol es transportado hacia los tejidos periféricos que tienen necesidad de él.
Este transporte en la sangre lo realizan las lipoproteínas LDL (camiones LDL) (4).
Aquí,dos tercios del colesterol están bajo forma esterificada,lo que aumenta la capacidad de transporte de las lipoproteínas.
En la membrana de las células de los tejidos,un receptor específico recibe a la LDL,y entra con ella en la célula,donde se desintegra (5).
La célula,avisada así de la llegada de colesterol,frena la síntesis de nuevos receptores.
De esta manera la entrada de las LDL queda limitada,y si por una u otra razón hay demasiadas en la sangre,la colesterolemia aumenta (6).
El colesterol liberado por la entrada de las LDL en la célula es almacenado,o utilizado de inmediato,por ejemplo para la renovación del colesterol de las membranas celulares (7 y 8).
Mientras tanto,la enzima (R) que sintetiza el colesterol descansa (9).
Del " viejo " colesterol (o excedente de colesterol) se encargan las HDL (10).
Estos otros transportadores de grasas reconducen el colesterol al hígado ; una enzima de la sangre,la LCAT (L) favorece este drenaje esterificando el colesterol en las HDL,lo que les permite llevar una mayor cantidad (11).
Una vez en el hígado,las HDL descargan el colesterol.
Este puede ser reutilizado (2 y 4) o eliminado en la bilis (12).
Puede ser excretado en su forma habitual o bien tras haber sido transformado en ácidos biliares (bolitas en el dibujo) por medio de una enzima,la 7 - x-hidroxilasa (H).
La bilis se vierte en el intestino,donde los ácidos biliares son reabsorbidos (13),salvo una pequeña cantidad que es expulsada.
El colesterol,al que viene a añadirse el de la alimentación,es en su mayor parte reabsorbido en la linfa ; a caballo de los quilomicrones,otro tipo de transportadores,llega al hígado y a la circulación general (14).
Una mínima parte queda en el bolo alimenticio que fermenta en el tubo digestivo y acaba siendo expulsado en las heces.
Los efectos de la manzana.
Han sido demostrados en una raza especial de hámsters que acumulan espontáneamente colesterol con la edad,así como en cobayas y conejos convertidos en hipercolesterolémicosN.
En el hombre,algunos de estos efectos son sólo hipótesis,por el momento,pero hipótesis muy probables.
Los principales efectos de un consumo regular y prolongado de manzanas están resumidos en el interior de las siluetas verdes de esta fruta en nuestro dibujo.
Aunque sea insoluble en agua,el colesterol libre tiene un polo hidrófilo por su concentración de OH,mientras que la forma esterificada es totalmente hidrófoba.
De todos modos,para los médicos y sus enfermos la partícula grasa a tener en cuenta es la lipoproteína,forma química mediante la cual las grasas circulan por la sangre al salir del intestino y del hígado (donde fueron ingeridas o sintetizadas,respectivamente).
Para circular en el medio acuoso sanguíneo necesitan un medio de transporte,porque no olvidemos que son insolubles.
Este medio de transporte son las apoproteínas,sintetizadas lógicamente también en el intestino y en el hígado.
Las diferentes lipoproteínas se definen según la naturaleza y la cantidad de las aproproteínas que les sirven de medio de transporte y según la naturaleza y la cantidad de las grasas que ellas mismas almacenan.
La densidad y el tamaño de las lipoproteínas varían en función de estos factores.
Si son ricas en proteínas,serán pequeñas y pesadas: se las llama HDL (High Density Lipoproteins,lipoproteínas de alta densidad).
Si tienen más grasas y menos proteínas,serán más ligeras y se llamarán VLDL (Very Low Density Lipoproteins,lipoproteínas de muy baja densidad) y LDL (Low...,lipoproteínas de baja densidad).
Incluso existe un cuarto grupo,los quilomicrones,aún más grandes y ligeros que las precedentes,y que transportan las grasas de origen alimenticio,desapareciendo no más de seis horas después de la comida.
En esta clasificación de las lipoproteínas según su densidad se ha basado el mito del buen y mal colesterol: los buenos de la película podrían ser las HDL y los malos las LDL y VLDL.
En la práctica,la acumulación de LDL en la sangre es peligrosa ; por su parte,algunas HDL son más aptas que otras para recuperar el colesterol de las células y reconducirlo al hígado,proceso éste sumamente beneficioso puesto que sólo el hígado es capaz de evacuar el colesterol de la circulación general,excretándolo por la bilis.
EN la actualidad,ningún especialista pondría en duda que los ancestros remotos de las aves son los reptiles.
Las estructuras óseas de los restos fósiles descubiertos confirman sobradamente esta hipótesis.
Pero ¿qué cambios evolutivos sufrieron estos animales antes de convertirse en dueños del espacio aéreo? Evidentemente,sería necesaria la conjunción de varias modificaciones a lo largo de miles de años.
Por una parte,la aparición de la locomoción bípeda,que posibilitó conservar libres las extremidades anteriores.
Por otra,la reducción del peso de sus huesos,que llegaron a ser rígidos,huecos y neumatizados,así como la transformación posterior de algunos de ellos,principalmente los de las manos y el esternón.
Y también fue imprescindible la homeotermia o constancia en la temperatura interna del cuerpo,cualquiera que sea la del medio ambiente ; sin esta última característica no hubiera sido posible la adaptación al vuelo,ya que exige un consumo energético que sólo pueden afrontar los animales de sangre caliente.
Las plumas surgieron a partir de las escamas reptilianas,aunque se desconoce a ciencia cierta si antes o después de los mecanismos de la homeotermia.
Curiosamente,en un principio no tenían como objeto favorecer el vuelo,sino evitar la dispersión de calor.
Se trata de una fórmula distinta a la empleada por los mamíferos,que se revistieron de pelo,pero con una finalidad semejante.
La capacidad para mantener constante la temperatura corporal constituye un gran paso evolutivo,pues significa la independencia del medio y,con ello,la posibilidad de colonizar nuevos ecosistemas.
Por supuesto que los reptiles primitivos,antes de convertirse en aves,hubieron de desarrollar un modo especial de ventilación y unos sacos aéreos.
Según los últimos estudios paleontológicos,los reptiles más próximos a las aves fueron los tecodontos (Euparkeria capensis).
Estos animales,que vivieron hace 225 millones de años,en el período Triásico,ya poseían caracteres como la homeotermia y el bipedalismo.
Sus restos han sido hallados en sedimentos del sur de Africa.
No obstante desde el tecodonto hasta el primer ser volador hubieron de pasar nada menos que 65 millones de años.
Para rellenar este vacío en el conocimiento del tema fueron necesarios nuevos descubrimientos.
En las canteras de Solenhofen,en Baviera,se encontraron cuatro ejemplares de esqueletos de tipo aviano ; el fino grano de la piedra permitió la conservación de detalles notables,como el de la impresión de las plumas.
Con todo ello se llegó a deducir que se trataba de un ave primitiva,ya que,además,el estudio de su anatomía reveló que se trataba también de un reptil.
Estos esqueletos fueron bautizados con el nombre de Archaeopteryx (ala arcaica) litographica (se extrajeron de una cantera de calizas explotada para litografías).
Dicha cantera que data de la Era Secundaria,hace 160 millones de años,ha posibilitado el acercamiento a la investigación del origen de las aves.
Resulta difícil reconstruir la historia completa de este grupo animal,puesto que el número de hallazgos es todavía muy escaso.
Aparte de los ya descritos,existen restos de un pequeño reptil bípedo y con plumas,de hace 180 millones de años y procedente de Rhodesia,que podría ser un aspirante a ese ser proavis ; también hay fósiles de un pájaro nadador y buceador,de hace 130 millones de años,cuyas alas.
demasiado reducidas,le debieron resultar ineficaces para volar ; y finalmente hay que hablar del Icthiomis,muy parecido a las gaviotas actuales,pero provisto de dentadura.
El Archaeopteryx se considera,no obstante,el eslabón intermedio entre reptiles y aves.
Se cree que tenía el tamaño de un cuervo,una envergadura alar de unos 58 centímetros y un esqueleto que pesaba 200 gramos.
Posiblemente se alimentara de insectos,gusanos o bayas.
Este pájaro ancestral guarda un enorme parecido tanto con las aves modernas como con los reptiles.
Los científicos han especulado mucho con respecto a su forma de vida.
Sus hipótesis han servido de punto de partida para establecer las diversas teorías sobre la aparición del mecanismo de vuelo.
Una de ellas es la teoría corredora,según la cual el Archaeopteryx sería un bípedo corredor,a juzgar por el estudio de las extremidades posteriores y la pelvis.
Procedería del tecodonto o del dinosaurio,cuyos miembros anteriores se habrían convertido en alas para adaptarse al vuelo,por una necesidad de impulso en la carrera.
La teoría del predador parte de la idea de que se trataba de un predador muy activo,que empleaba manos y brazos para atrapar sus presas.
En este caso,sus brazos se convertirían en alas no sólo para adaptarse al vuelo,sino también a fin de poderse elevar para atrapar los insectos voladores.
La teoría trepadora considera a este animal como un trepador.
La presencia de las alas estaría justificada en cuanto que actuarían de paracaídas al saltar de árbol en árbol ; esta suposición queda apoyada por los tres dedos libres de cada ala,que servirían para agarrarse a las ramas al final del salto.
Los defensores de esta corriente explican que,una vez el ave-reptil se convirtiera en bípedo,pasaría a ser arbóreo,luego saltador,más tarde paracaidista y,finalmente,planeador y volador.
A pesar de todo,se ignora con exactitud cómo se inició el vuelo en realidad.
Parece cierto que la anatomía del Archaeopteryx revela que no se trataba de un gran volador,dada la ausencia de quilla en el esternón.
LOS distintos grupos científicos que nacieron en 1962 con la fugaz pertenencia de España al mayor centro de investigación nuclear de Europa,fueron adquiriendo con el tiempo madurez y prestigio.
Desde el pasado año,nuestro país vuelve a ser miembro de pleno derecho del CERN (Centro Europeo de Investigación Nuclear),un organismo que actualmente se encuentra a la cabeza de la física mundial,como lo atestigua el reciente Nobel a los investigadores Rubbia y Van der Meer.
Un premio Nobel a un trabajo en cuyos experimentos iniciales Intervinieron físicos españoles de forma decisiva.
La importancia de nuestros científicos en esta rama del conocimiento humano tiene,sin embargo,escasa resonancia pública en España.
Probablemente porque se trata de una materia muy difícil de comprender y que conlleva esa mala prensa anexa a todo lo que suena a nuclear.
En todo caso,la física de partículas o física subnuclear,también llamada física de altas energías por necesitar precisamente la utilización de considerable energía en sus trabajos,no tiene otro objetivo que averiguar la estructura íntima de la materia.
Es decir,conocer cómo son los ladrillos fundamentales de todo lo que nos rodea,y qué fuerzas permiten su unión arquitectónica.
. Todo ello,a escalas de dimensión realmente inimaginable: 10 - 18 metros,es decir,0,000000000000000001 metros (en total,18 ceros).
Ese es el tamaño del electrón y,probablemente,de los quarks,de los que luego hablaremos.
Para comprender la importancia que tienen estas investigaciones,es necesario partir de conocimientos supuestamente generales: la existencia de moléculas y átomos como base fundamental de toda la materia,viva y inerte,que nos rodea.
Los átomos,recordémoslo también,están formados por un núcleo central,en el que se encuentra concentrada prácticamente toda su masa,y por una corteza de electrones que se mueven a su alrededor,a distintos niveles,como un cortejo planetario alrededor del Sol.
En este siglo se conoció la estructura de los componentes del núcleo atómico: el protón y el neutrón.
Y con esto quedaba clara la estructura atómica: protones y neutrones en el núcleo,y los electrones,mucho más ligeros,en la llamada corteza exterior.
Muy pocas personas saben más acerca de los constituyentes fundamentales de la materia.
Y,sin embargo,esta estructura del átomo a base de electrones,protones y neutrones constituye ahora poco menos que la prehistoria de la física de partículas.
En los últimos treinta años se ha producido una auténtica revolución científica,todavía no concluida,que ha cuestionado de forma total la estructura arquitectónica de las partículas elementales y de las fuerzas que las unen,también llamadas interacciones.
En estos años,los físicos han pasado de manejar dimensiones de 10 - 15 metros,que es la dimensión de un protón o un neutrón,a manejar tamaños mil veces menores,o sea,10 - 18,que es el tamaño de un electrón y de los quarks,esas nuevas partículas de las que están formados los neutrones y protones.
Para poder observar este mundo de dimensiones tan reducidas no es posible utilizar la luz con la que vemos los objetos de nuestro mundo actual ; la longitud de onda de la radiación incidente tiene que ser menor que el objeto que se quiere estudiar.
Y a escalas tan diminutas sólo es posible utilizar partículas a gran velocidad,es decir,con gran energía.
Como toda partícula tiene una onda asociada,según la mecánica cuántica,la longitud de esta onda será tanto menor cuanta más energía le comuniquemos a la partícula.
Es decir,que si aceleramos mucho un electrón,por ejemplo,es posible que la longitud de onda asociada sea suficientemente pequeña como para producir interacciones,es decir,algún tipo de efecto,que posteriormente podamos detectar y medir.
De ahí la necesidad de utilizar enormes aceleradores de partículas,que requieren gran energía.
De ahí,asimismo,la denominación de física de altas energías aplicada a esta rama de la física.
Cuando se producen interacciones,o sea,choques entre partículas muy aceleradas,se obtienen toda una serie de acontecimientos ; por ejemplo,la aparición de nuevas partículas,cuyo análisis puede aportar valiosos datos para comprender mejor lo que ocurre a esa escala de tamaños.
La sorpresa con que los físicos han ido descubriendo hasta más de dos centenares de nuevas partículas les ha estimulado a elaborar teorías que agrupasen todo este universo nuevo de forma coherente.
¡Qué lejos estamos ya de aquella simplicidad sugerida por la estructura atómica a base de protón,neutrón y electrón! Agrupando las partículas nuevas y viejas según una serie de características denominadas números cuánticos,se llegó a una clasificación según la cual todo parecía deberse a la presencia de estados fundamentales,con tres partículas básicas y sus correspondientes antipartículas.
En la década de los setenta ya nadie dudaba de la existencia de estas tres partículas básicas,y se les llamó quarks.
Los tres primeros fueron bautizados como up (arriba),down (abajo) y strange (extraño).
Posteriormente,desde 1975 hasta ahora,se han descubierto otros tres nuevos quarks: charm (quark con encanto),bottom o quark con belleza y top o quark con verdad.
El primero,descubierto en 1975,el segundo en 1979 y el tercero en 1984.
Toda la materia conocida,incluidas las nuevas partículas de vida muy efímera que había ido apareciendo en las colisiones producidas en los grandes aceleradores,está compuesta,por tanto,por sólo un puñado de partículas básicas: los seis quarks ya citados,y los seis leptones,o partículas elementales complementarias: el electrón y su neutrino,el muón y su neutrino y el tau y su neutrino.
A la dimensión de 10 - 18 metros ésta es la estructura básica de las partículas elementales,a base de 6 leptones y 6 quarks.
Pero además se sabe que estas 12 partículas están agrupadas en tres grandes sistemas.
El primero consta del electrón y su neutrino y los quarks up y down ; con estas cuatro partículas se ha formado todo el Universo visible.
toda la materia estable que conocemos.
Un segundo grupo,formado por partículas de vida efímera (milésimas,hasta trillonésimas de segundo),incluye a quarks strange y encanto,y al mut y su neutrino ; este grupo tiene una vida media cortísima,pero posee,la materia con él formada fuese estable,características semejantes mundo estable que conocemos,aquí que con masas mayores (el mut tiene mayor masa que el electrón Finalmente,el tercer mundo,formad por los quarks con belleza y verdad y el tau y su neutrino,tiene una vid aún más efímera,y mayor masa que los anteriores.
¿Queda todo explicado con esta teoría? La respuesta afirmativa sería presuntuosa,porque cada vez so mayores las sorpresas que depara los aceleradores de partículas,más potentes.
Por otra parte,no sólo importa conocer cómo son las partículas elementales,sino que tienen aún mayor interés saber cuáles so las fuerzas que las mantienen unidas.
Es decir,no basta con saber de qué están hechos los ladrillos,sin que necesitamos saber cómo es el cemento que los une.
La manifestación de estas fuerzas o interacciones entre partículas se estudia a través de unas partículas intermedias,lo bosones,por ejemplo.
Precisamente por haber descubierto los bosones \ / y ZO en el CERN,bosones esenciales para unificar las energías.
Como la temperatura de su cuerpo depende de la del ambiente,cuando el invierno es frío entran en un estado de aletargamiento e inactividad,para despertar luego con los primeros calores de la primavera.
La rana marrón (Rana temporaria) sale de la hibernación en marzo,fecha en la que comienza su ciclo biológico y su actividad reproductora.
Son los machos los que toman la iniciativa,buscando unas aguas adecuadas para el futuro desove.
Desde ellas,empiezan a croar para atraer a las hembras y,cuando éstas acuden,se colocan sobre su espalda,obligándoles a la puesta mediante una ligera presión con sus patas posteriores.
Una vez expulsados los huevos,y siempre dentro del agua,los machos los fecundan descargando sobre ellos el líquido seminal,con lo cual finalizan su labor en la reproducción y vuelven a tierra para reanudar su vida habitual.
Lo mismo harán las hembras,aunque un cierto tiempo después,ya que primero tienen que recuperarse del notable esfuerzo realizado.
Cada huevo está envuelto en una cápsula gelatinosa y tiene un tamaño insólitamente grande en comparación con los de otras especies animales.
Basta pensar en los casi dos milímetros que llega a tener su diámetro,veinte veces más grande que en el caso humano,por ejemplo.
Una hembra pone entre 2.800 y 4.000 huevos ; en conjunto,esta masa tiene el tamaño de un tomate,aunque poco a poco se va hinchando,según va absorbiendo agua,hasta alcanzar las dimensiones de un coco pocas horas después.
El huevo es el punto de partida para la producción de un nuevo individuo.
Su desarrollo depende de la temperatura ambiente ; si ésta oscila alrededor de los 15 °C,a las dos o tres horas de la puesta tiene lugar la primera división de la célula.
Como resultado de dicha división se producen dos células semiesféricas,que se volverán a fragmentar en cuatro,luego en ocho y así sucesivamente.
En este proceso de segmentación celular el tamaño global del huevo no cambia,por lo que,lógicamente,las células se hacen cada vez más y más pequeñas: mientras que sus núcleos se multiplican y mantienen la dotación genética,los citoplasmas se van reduciendo en cada división para repartirse entre las nuevas células hijas.
Veinticuatro horas más tarde,su número ha llegado a 5.000 y su tamaño diminuto le da al huevo el aspecto de una mora.
Por esta razón dicha fase del desarrollo celular es conocida como fase de mórula.
Mientras continúa la división,las células interiores migran hacia el exterior del huevo de manera que forman una pelota vacía,llamada blástula,que dará lugar,gradualmente,a tres secciones.
En la siguiente etapa se forman las capas embrionarias,que constituyen una copa llamada gástrula.
Esta se compone del ectodermo,que origina el sistema nervioso y la piel ; el endodermo (tracto y órganos digestivos ; y el mesodermo que da lugar a la musculatura,el esqueleto,aparato circulatorio y riñones.
Todo este proceso de desarrollo al que estamos asistiendo puede ser observado a simple vista a partir del cuarto o quinto día.
Dentro de la gelatina envolvente podemos ver cómo el huevo va cambiando de forma y haciéndose más grande hasta diferenciar claramente la cabeza,el cuerpo y la cola de la larva.
Dos días más tarde se produce la eclosión y la larva sale del huevo.
Pero este instante del nacimiento no es más que el principio del largo camino que conducirá a través del desarrollo larvario,primero,y de la metamorfosis,después,hasta la forma adulta y madura del animal.
En este momento,la larva mide aproximadamente seis milímetros y su cuerpo tiene forma ovoidal,con la cola larga y comprimida.
A ambos lados de la cabeza presenta dos apéndices,las branquias externas,gracias a las cuales el renacuajo filtrará oxígeno para respirar.
En conjunto su estructura es tan débil y delicada que se vería irremediablemente arrastrada por la corriente en el agua si no fuera por la presencia,debajo de la boca,de un órgano de succión en forma de U que le permite agarrarse a las rocas o a las plantas.
Como es fácilmente comprensible la alimentación de la larva en esta fase es muy importante para su posterior desarrollo.
Su alimento principal lo constituyen las algas,que son arrancadas de las rocas gracias a los pequeños dientes que presenta en las mandíbulas.
A lo largo de su crecimiento,el renacuajo ampliará su dieta,incluyendo ya otro tipo de plantas e incluso materia animal.
A cada lado de la cola se van desarrollando las patas traseras y,unos pocos días después,el cuerpo sufrirá un cambio importante: los ojos se hacen más prominentes,la boca se ensancha y la forma ovalada del cuerpo se hace más angulosa.
Al mismo tiempo,comienzan a crecer las patas anteriores.
A partir de este momento se inicia la etapa más crítica de la metamorfosis.
La larva deja de alimentarse porque sus órganos internos se están ajustando gradualmente a su futura vida en tierra.
Por su parte,las branquias son sustituidas por pulmones por lo que la larva tiene ya que salir continuamente a la superficie para respirar.
Cuando las patas traseras están completamente desarrolladas,la cola comienza a reabsorberse por la acción de las enzimas de sus células,que adquieren proteínas.
TAL vez Televisión Española ya no es lo que era desde que se dejaron de anunciar bebidas alcohólicas en los interminables descansos del telefilm americano.
Ahora nuestras noches televisivas son un poco más sobrias,como si una ley seca a la española se hubiera adueñado de nuestras costumbres ; pero los protagonistas de las películas siguen demostrándonos lo bien que se liga con una copa en las manos.
Nuestro país no sólo ostenta el título de mayor productor de bebidas alcohólicas de Europa,sino también el de mayor consumo alcohólico por habitante y año del continente.
La Organización Mundial de la Salud considera el alcohol una droga que crea dependencia.
Sin caer en dramatismos sobre los peligros que acarrea,es necesario realizar una toma de conciencia ante el abuso de este tipo de licores.
De los diversos tipos de alcohol existentes,nos interesa especialmente el etanol o alcohol etílico,ingrediente activo de la mayor parte de las bebidas alcohólicas de nuestra civilización: ginebra,vino guisqui... Todas ellas contienen además otros compuestos que les confieren su olor y sabor característicos,pero no producen los efectos del primero.
Adentrémonos en los fenómenos que ocurren al ingerir este producto.
El alcohol se absorbe rápidamente por las mucosas del estómago y del intestino delgado,de modo que a los cuatro o seis minutos ya se detecta su presencia en la sangre y en hora y media se alcanzan los niveles de concentración más elevados.
El alcohol circula libremente por la sangre,disuelto en el plasma,y se distribuye por todos nuestros órganos.
En el hígado comienza el metabolismo,por medio del cual se convierte en una sustancia química denominada acetaldehido,verdadera responsable de gran parte de las alteraciones que sufre el bebedor.
Aunque resulta difícil saber a qué velocidad se metaboliza el alcohol,sí parece confirmado que en los alcohólicos crónicos el proceso se realiza mucho más rápidamente.
Pero,¿qué es el alcoholismo? En términos médicos se define como una adicción al alcohol ; desde un punto de vista menos científico se podría considerar como un padecimiento crónico,caracterizado por un trastorno de la conducta,que lleva al individuo a ingerir bebidas alcohólicas sobrepasando las costumbres sociales.
Su adicción interfiere en sus relaciones interpersonales,en su salud y,naturalmente,en su situación económica.
En el bebedor no habitual,se ha logrado establecer una escala entre la cantidad de alcohol en la sangre y los efectos que causa.
Con unos 30 miligramos por 100 centímetros cúbicos de sangre aparece una ligera euforia o alegría descontrolada.
Con 50 miligramos,surge la dificultad para coordinar los movimientos fijos y el lenguaje.
Si la cantidad asciende a 300,las alteraciones en la marcha y la coordinación son muy importantes.
Superada esta cifra,entra en escena la anestesia,que puede llegar a ser mortal.
En el caso de la persona alcohólica,la adaptación a la bebida trastoca estas escalas,conduciéndole a situaciones más peligrosas.
Precisa mayores cantidades de alcohol para alcanzar la euforia y,por tanto,dosis más cercanas a las de mortalidad.
El alcohol afecta a todos los órganos del cuerpo.
El corazón,el hígado o el riñón suelen ver alteradas sus funciones,llegando a ocasionar cuadros degenerativos irreversibles,como la miocardiopatía del bebedor de cerveza,la degeneración grasa del hígado o las también degenerativas cirrosis o nefropatías alcohólicas.
Pero donde las manifestaciones se hacen más llamativas es en el sistema nervioso central.
Al contrario de lo que vulgarmente se piensa,el alcohol es un gran depresor del sistema nervioso y no un estimulante.
Si bien en una primera fase de su ingesta produce locuacidad,agitación o agresividad,su verdadero efecto consiste en deprimir los centros nerviosos subcorticales,encargados de regular las respuestas cerebrales.
Por este motivo,los primeros síntomas de intoxicación aguda se ponen de manifiesto en la dificultad para la coordinación,la posición de pie,el habla y los movimientos delicados (como el de los ojos).
También sufren deterioro funciones como la atención,el aprendizaje,la memoria,el razonamiento y la capacidad de juicio.
Estamos,por supuesto,describiendo el caso de una gran borrachera.
El alcohólico crónico ve alteradas todas estas capacidades de manera permanente.
El aparato digestivo y el sistema nervioso serán sus órganos más afectados,pero padece además un nuevo problema: la abstinencia,situación dramática que tantas veces ha servido de temática para el cine y la literatura.
El temblor,las alucinaciones visuales o auditivas,los ataques convulsivos y el delirium tremens se presentan de forma conjunta en estos enfermos.
No podemos finalizar este artículo sin hablar del síndrome alcohólico fetal,que ha protagonizado un reciente congreso en Madrid,con la asistencia de los mejores especialistas mundiales.
Durante el embarazo,el consumo prolongado de alcohol por parte de la gestante puede ocasionar trastornos irreversibles en el feto,incluso alteraciones en la formación de las células del nuevo ser.
Es lo que en términos médicos se llama teratogenias.
Todos los extremos son malos.
Las excesivas propagandas antialcohol,a veces demasiado alarmistas,parecen contraproducentes en algunos casos.
Tampoco dan buenos resultados las actitudes permisivas y frívolas.
El primer paso para lograr la moderación consiste en concienciar a la sociedad de que la ingesta de alcohol no es condición sine qua non para relacionarse y de que la alegría no depende de su consumo,sino de algo mucho más personal.
LAS reacciones amorosas de muchos animales no se encuentran nada lejos de las de los humanos.
La moderna etiología,de hecho,ha descubierto y evidenciado,con millares de experimentos y observaciones,la llamada atracción instintiva.
Gran parte de las especies zoológicas se unen a un solo individuo.
No obstante,en el mundo animal existen también centenares de miles de seres que no precisan contactar con otro de sus mismas características genéticas para asegurar la descendencia.
El amor como garantía de continuidad carece de sentido para muchos peces,moluscos,gusanos,equinodermos,esponjas y corales.
Hay,todavía,otra categoría más que escapa a las dos posiciones extremas que hemos descrito.
Se trata de los organismos unicelulares que se multiplican,desde la unidad,por simples,repetidas y hasta interminables biparticiones: amebas,infusorios,vorticelas,ciliados...,entre los que no parece darse el más mínimo indicio de sexualidad,aunque no conviene negarlo de forma absoluta.
Lo verdaderamente apasionante de la vida de estos animales reside en que,en teoría,son seres inmortales.
Porque cualquier protozoo proviene de otro idéntico que no muere sino que simplemente recarga baterías dando origen a un nuevo ser.
Infinidad de estirpes de animales unicelulares no han interrumpido este proceso desde hace centenares de millones de años.
Son formas de vida que no envejecen,ni mueren mientras quede uno solo igual,exactamente igual,al primero de todos ellos.
El individuo nace con la aparición de organismos de más de una célula,en el transcurso de la complejidad morfológica que supuso el fenómeno.
Y con el individuo surge el amor,o los amores,en todas sus incontables formas ; pero las protagonistas del amor son siempre dos células diminutas,una macho y otra hembra,que se buscan desesperadamente para iniciar la aventura de la vida.
El espermatozoide,un minúsculo látigo que se agita,persigue siempre hacer diana en el óvulo.
Del triunfo de esta operación depende la supervivencia de cada especie.
Por este motivo,la naturaleza ha inventado un sinfín de estrategias encaminadas a que este objetivo se cumpla.
No podemos olvidar que algunos animales,especialmente la mayoría de los pobladores del océano (incluidos los vertebrados,como los peces),no necesitan poner en contacto parte alguna de su cuerpo para asegurar la fecundación.
Esta se lleva a cabo en el agua.
Machos y hembras liberan esperma y óvulos al mismo tiempo y en tales cantidades,que siempre se fertilizan un número suficiente de huevos.
La sincronía se alcanza a menudo gracias a la acción de sustancias químicas que,al ser difundidas en el líquido,estimulan los procesos hormonales de los dos sexos.
Otra de las grandes categorías dentro de las modalidades de la reproducción es el hermafroditismo.
La concurrencia en un mismo cuerpo de los elementos femeninos y masculinos garantizan la continuidad.
Pero algunos animales tan familiares como el caracol,la lombriz de tierra o la sanguijuela deben copular con otro ser hermafrodita de su misma especie para de esta forma procrear.
Dentro de la escala evolutiva,la separación de lo masculino y femenino en cuerpos diferentes corresponde a un estadio posterior.
En la actualidad,todavía se desconoce el significado de este sesgo evolutivo,en principio gratuito para la economía de la naturaleza.
Y sorprende aún más cuando se descubre que algunos vertebrados cuentan con poblaciones de un solo sexo y que se reproducen por partenogénesis (la hembra tiene descendencia sin intervención del macho),fenómeno más sorprendente que la fecundación.
En este obligado resumen de las formas de amar no podemos obviar a los animales que matan al cónyuge mientras se está produciendo la cúpula.
No nos estamos refiriendo únicamente a la mantis y a las arañas,sino también a un mamífero de todos conocidos como el tigre,que no pocas veces resulta devorado tras copular con su pareja.
Tiranías igualmente sobrecogedoras se establecen entre algunos peces abisales: una vez aferrados a la dermis de sus hembras,los machos se atrofian,se reducen y pierden todos sus órganos excepto los sexuales.
La brutalidad con el cónyuge es el modus operandi de la mayoría de los reptiles,anfibios,no pocos mamíferos...,incluidos algunos humanos.
¿Se confunden el amor y el hambre? Sin embargo,la actividad amatoria también posee su lado amable.
En infinidad de vertebrados,la búsqueda de la unión de los sexos se asocia al desarrollo de unas relaciones previas,destinadas a la conquista del otro sexo.
Así se desarrollan las ceremonias y los cortejos,el vistoso coqueteo... Todo culmina en el acto más hermoso y gratificante de cuantos puedan ser emprendidos en el seno de la biosfera.
Vivir es,en definitiva y ante todo,deseo de seguir viviendo.
Se ama precisamente para asegurar que la vida prosiga y,para ello,se emplean muy diversas formas.
El porqué todavía lo ignoramos y,tal vez por esta razón,el amor nos resulta imprescindible,aunque en muchas ocasiones se olvide que es el motor real de la vida.
El ser vivo más corpulento que existe en la Tierra se llama General Sherman y pesa 2.145 toneladas.
El más alto mide 108 metros y se llama Howard Libbey.
El más viejo vive todavía en el estado norteamericano de Nevada y vio su primera luz casi 3.000 años antes de nacer Jesucristo.
Por si alguien todavía no lo ha adivinado,esos seres vivos son árboles ; los dos primeros,sequoias y el tercero,un pino.
Los árboles,gracias a la solidez y rigidez de sus troncos de madera,consiguen mantenerse erguidos durante períodos inigualados por ningún otro ser vivo ; y alcanzan tamaños que,asimismo,superan a todos los demás organismos vivientes.
Sin embargo,aún los árboles más corpulentos poseen una composición íntima de sus fibras vegetales que nos lleva rápidamente desde lo gigantesco a lo microscópico.
Y ése es el camino que ha seguido el fotógrafo norteamericano James Bell,en su laboratorio de Allston,en Massachussets.
Sus microfotografías de distintas maderas constituyen un material científico de primer orden ; al mismo tiempo,se trata,sin duda,de una forma exótica,pero de innegable valor plástico,de enseñarnos lo que es la madera por dentro.
Las imágenes que mostramos en estas páginas han sido obtenidas proyectando la luz de una lámpara de tungsteno de 400 watios de potencia a través de un filtro polarizador,un microscopio y un segundo filtro polarizador.
La madera había sido previamente cortada en finísimas bandas prácticamente transparentes con ayuda de un microtomo,modificado adecuadamente para tan insólito uso.
El bello cromatismo de estas fotografías no es totalmente artificial ; desde luego,la madera cortada en láminas tan finas ofrece una coloración muy neutra,casi imperceptible ; pero las diferentes tonalidades impresionadas por la cámara tienen su origen en la estructura orgánica de los distintos componentes celulares de la madera y en las desviaciones que sufre la luz al pasar por los filtros polarizadores.
No son,pues,colores artificiales ; simplemente distintos e insólitos.
La mayor parte de las imágenes muestran un complejo pero ordenado entramado de finísimos tubos,las traqueidas,que son los encargados de llevar las vitales sales disueltas en agua a las células de toda la planta,desde la más profunda de las raíces hasta la hoja más elevada.
El material básico lo constituye la celulosa,cuyas fibras se cimentan entre sí gracias a la lignina ; este proceso le proporciona a la madera de los troncos su vigor y resistencia y permite la existencia de gigantes que pesan cientos de toneladas y miden más de cien metros.
Por cierto,que esta celulosa puede ser prensada para obtener combustibles de todo tipo,carbón incluido,o bien destilada para obtener perfumes,alcohol y otras sustancias igualmente útiles ; y si se la reduce a la forma de pulpa,podemos fabricar con ella las múltiples variedades de papel,que nuestra vida diaria ha hecho indispensables para casi cualquier actividad humana.
Durante muchos años fue un misterio la forma en que la savia vital ascendía por los tubos internos de la madera,sobre todo en los grandes árboles.
En un principio se pensaba que la evaporación del líquido en la superficie de la hoja creaba una especie de vacío que succionaba hacia arriba la nueva savia.
Pero aunque este vacío hubiese sido perfecto,la acción de la presión atmosférica,como si de un auténtico barómetro vegetal se hubiese tratado,sólo sería capaz de elevar el agua hasta una altura aproximada de unos diez metros.
Por encima de esta altura la savia se vería imposibilitada en su ascenso ; algo así le ocurre al mercurio mucho más denso que el agua,cuando en su columna barométrica no es capaz de ascender más allá de 760 milímetros,aunque por encima exista un vacío casi perfecto.
La existencia de millones de árboles cuya copa supera los diez metros de altura demuestra bien a las claras que esa primera explicación no sirve.
Lo que ocurre en realidad es que cada tubo,cada vaso,cada traqueida operan como auténticas bombas de presión.
En cada piso de la planta,a cada nivel de ascenso de la savia,las células del entorno aspiran progresivamente con más fuerza,añadiendo al líquido distintas sustancias químicas que incrementan su cohesión.
El resultado conjunto de todos estos esfuerzos a escala celular y microtubular es que toneladas de savia suben constantemente desde las raíces hasta las hojas,literalmente aspiradas por estos finísimos capilares vegetales.
La aparición de las traqueidas,cuyo diámetro no rebasa el milímetro,en la evolución de las plantas hace muchos millones de años constituyó,sin duda,un hito fundamental.
La fina estructura tubular de los grandes árboles actuales no sólo les permite elevar los líquidos nutritivos desde las raíces hasta las hojas más altas,sino que también posibilita,gracias a la unión rígida pero vital entre la celulosa y la lignina,la existencia de troncos gruesos y resistentes capaces de soportar árboles muy altos y pesados.
Los árboles de hoja caduca y las plantas con flores constituyen,dentro del reino vegetal,los seres más evolucionados.
En ellos,el sistema tubular de transporte de savia incluye vasos principales y múltiples ramificaciones de tamaño progresivamente menor.
Pero los árboles de hoja perenne,como los sequoias y los pinos,de los cuales hemos citado al principio los más notables representantes,todavía transportan la savia hasta sus células más altas,a más de cien metros por encima del suelo,a través del sistema de traqueidas.
Y ello constituye,probablemente,un prodigio hidráulico más llamativo aún que la mera existencia de estos vegetales seculares y gigantescos.
EL maravilloso mundo mineral está comenzando a interesar cada vez más a un amplio sector de la sociedad,que encuentra en él un campo nuevo para sus aficiones intelectuales,ecológicas,de coleccionismo o de decoración.
Traemos por ello a las páginas de CONOCER una nueva sección que acercará a los lectores una de las riquezas menos conocidas de nuestro planeta.
Dentro de este amplio mundo de los minerales,el aficionado descubre,al poco de haberse introducido en él,unas formaciones cuyo origen y variedad le sorprenden siempre: las geodas.
Vamos a tratar de descubrir el origen de estos minerales extraños,con su enorme diversidad de formas,y la muy peculiar atracción que ejercen sobre el estudioso que penetra en su interior con la misma ilusión con que un espeleólogo entra en una gruta.
Una geoda es una cavidad de una roca recubierta de minerales perfectamente cristalizados.
Las hay de todas las formas y tamaños: esféricas o amigdaloides,casi totalmente huecas o prácticamente macizas,desde medio centímetro a diez metros de diámetro.
..
Ya en 1775 Guillermo Browles,en su obra " Introducción a la Historia Natural y a la Geografía Física en España " hablaba de las geodas en un exquisito texto que transcribimos: El mundo está lleno de piedras redondeadas de todas figuras y naturalezas.
Se hallan en los valles,en la tierra a una gran profundidad,y sobre los cerros y montañas más altas de nuestro globo.
Yo he visto diamantes redondeados cubiertos de una ligera corteza ; zafiros y topacios orientales redondeados,y cornalinas de Levante redondeadas y gruesas como un huevo con cáscara... El estudio de la formación de una geoda nos permite explicar las frases de este texto de Browles: el porqué de las morfologías distintas,de las variedades de cristales,de las localizaciones tan dispersas... Para ello tenemos que remontarnos unos cuantos millones de años.
Imaginemos una burbuja dentro del material que,más tarde,compondrá una roca.
Esa burbuja podría muy bien ser,por ejemplo,una bolsa de agua en el interior de una roca sedimentaria actual o un simple hueco,una oclusión,en el basalto de una roca volcánica cuando era lava fundida.
En el caso de las rocas sedimentarias,imaginemos el fango del fondo marino que comienza a endurecerse dando origen,muy lentamente,a lo que más adelante será una roca sedimentaria.
Si en algunos de los puntos de ese fango marino quedan atrapadas bolsas rellenas de agua,un agua que contiene disueltas multitud de sales minerales,tendremos las condiciones necesarias para que,con el paso del tiempo,la burbuja se convierta en geoda.
En el otro caso,durante el proceso de enfriamiento de la lava de un volcán,que se convertirá mucho más tarde en roca basáltica,pueden aparecer grandes burbujas huecas en el seno de la masa semifluida.
En ellas penetra el agua de las lluvias,que llevan disueltas distintos óxidos de silicio,manganeso,hierro y otros elementos.
En ambos casos,el lento discurrir del tiempo geológico origina la progresiva evaporación del agua en estas bolsas sedimentarias o volcánicas.
Las sales y óxidos disueltos se van depositando lentamente en las paredes interiores de las burbujas.
Los silicatos,por ejemplo,lo hacen en forma de agregados microcristalinos,y dan lugar a un crecimiento concéntrico de la pared en forma de bandas,constituyendo así las ágatas.
En otros casos,aparecen en algunos puntos del interior de las bolsas pequeños cristales de cuarzo que,en su crecimiento,van tapizando las paredes ; en determinadas ocasiones,ese tapiz se colorea suavemente debido a la presencia del óxido de hierro,adquiriendo una brillante tonalidad azulada: tenemos una amatista... Los cristales de cuarzo,de amatista,de calcita y otros van recubriendo la superficie interior de la burbuja inicial a medida que el agua va desapareciendo.
El basalto se meteoriza por la acción de los elementos exteriores,y las partes duras de la matriz de ágata se conservan,encerrando en su interior las más caprichosas y bellas formas cristalográficas.
Exteriormente.
observamos unas extrañas piedras esféricas o ligeramente abombadas.
Interiormente,están huecas.
Si las sopesamos,nos parecen mucho más ligeras de lo que su aspecto pudiera indicar.
Y si las abrimos,entonces nos revela su maravilloso mundo interior ; estamos ante una geoda,cuya belleza puede ser capaz de llevarla hasta lugares de honor en los museos.
En la génesis de las geodas que acabamos de repasar someramente,hemos hecho abstracción,lógicamente,de las múltiples diferenciaciones que pueden aparecer a lo largo de tan prolongado y complejo proceso (procedencia y origen de las rocas matrices,naturaleza y composición de las sales minerales disueltas en el agua,edad geológica,etc.).
Sobre la matriz de ágata que forma la envuelta interior de la geoda suelen aparecer,descendiendo hacia el hueco central,espléndidas estalactitas y estalagmitas de calcedonia de la más traslúcida fosforescencia.
A veces,estas formaciones muestran,en sentido transversal,una matriz de ágata en la que han crecido cristales absolutamente perfectos de amatista azulada ; las bandas de ágata está coloreadas de azul,verde,rojo y hasta amarillo brillante,y sobre ellas emergen los cristales de amatista.
El mundo de las geodas no ha revelado todavía todos sus secretos.
Existen aún numerosas formaciones por descubrir ; un aliciente más para aficionarse a su estudio y observación.
Como,además,podemos encontrarlas de forma inesperada en medio del campo,su estudio se puede convertir en una nueva forma de aventura.
La espeleología de esas cuevas supone,sin lugar a dudas una ilusión al alcance de todos aquéllos que buscan en la naturaleza la aventura,la sorpresa,la belleza... y la ciencia.
LA mayor parte de las geodas son variedades de cuarzo (Si02).
La coloración adquirida se debe a la presencia de diversos elementos metálicos.
Puede homogeneizarse la coloración por medio del calor.
Su principal aplicación es la decoración y ornamentación ; los mejores cristales se emplean en joyería.
Los mayores ejemplares aparecen,en España,en las minas de La Unión (Murcia),en las cortadas de esfalerita (SZn) y greenalita (mineral de hierro).
Pueden encontrarse buenas geodas de calcita en los pedregales de las llanuras segovianas,y excelentes amatistas en Cáceres,Vich,serranía de Córdoba (Hinojosa) y Las Escullas (Almería).
El más formidable emplazamiento de geodas del mundo se encuentra en la frontera brasileño-uruguaya,cerca de Ribera y Artigas.
Otros emplazamientos de alta calidad son los profundos yacimientos de basalto de Sierra del Mar,en el estado brasileño de Río Grande del Sur,donde apareció una geoda de 35 toneladas y 100 metros cúbicos de volumen.
También merecen mención las arenas del desierto argelino,la Auvernia francesa,los desiertos mejicanos zacatecas,la región checoslovaca de I Trebic,los Urales rusos,los estados norteamericanos de Montana,Carolina y Maine,y numerosos puntos de Madagascar / la India,Sri Lanka,Burma,Namibia y Japón.
LA búsqueda del lugar exacto en que se asienta la memoria función base de toda actividad mental,ha sido objeto de múltiples e infructuosas investigaciones.
Se había llegado a admitir que residía en todo el cerebro y en ninguna zona en concreto.
Pero ahora,un equipo de la universidad de Stanford (California),dirigido por el profesor Thompson,acaba de aportar la primera prueba experimental: las neuronas de la memoria se localizan en el cerebelo.
Tener una buena memoria constituye un factor de éxito en la vida.
Su disminución se ha convertido en sinónimo de envejecimiento.
Para abrir una puerta o para conducir un coche,ponemos en práctica una serie de gestos que hemos aprendido y que hemos retenido.
Lo mismo sucede cuando,ante la presencia de un objeto,un vaso,por ejemplo,lo identificamos como tal,diferenciándolo del resto de las cosas que conocemos.
Pero,¿cómo es posible que una célula guarde un recuerdo?,¿cómo puede ésta quedar marcada por un aprendizaje previo? Determinar la localización de las funciones del espíritu representa un viejo sueño que se remonta a la más alta antigüedad.
Los egipcios asignaban al corazón y al hígado un papel esencial en el desarrollo de las emociones.
Platón,al dividir el alma en tres componentes - - deseo,pasión y razón - - hacía del razonamiento un atributo del cerebro.
Y Franz Gall (1758 - 1828),médico alemán,llegó a la conclusión de que las diferentes facultades del hombre están repartidas en regiones específicas del cerebro ; creía,erróneamente por supuesto,que cuanto más desarrollada está una facultad,más voluminosa es el área afectada.
Medio siglo más tarde,el cirujano francés Paul Broca situaba la afasia (pérdida del habla a consecuencia de una lesión cerebral) en la tercera circunvolución izquierda.
Este descubrimiento,totalmente riguroso,abrió las puertas a una serie de localizaciones relativas al lenguaje: zona de lectura,zona de expresión verbal,zona de escritura,etcétera.
Pero los progresos de la neurocirugía no tardaron en someter a revisión la idea de la localización.
En efecto,tras las extirpaciones de zonas extensas del córtex (tumores,abcesos...) se observaron recuperaciones funcionales en ocasiones totales,principalmente si el sujeto era joven.
Hacia 1930,el neurofisiólogo estadounidense Karl S. Lashey negó que dicha localización existiese ; argumentaba que si un grupo de células encargados de una función desaparecía,otros conjuntos tomaban el relevo instantáneamente.
No obstante,esta puesta en tela de juicio no afectaba a la memoria,ya que no se había encontrado una zona cuya extirpación produjera amnesia.
En compensación,se constató que la estimulación eléctrica del lóbulo occipital (zona de la visión) desencadenaba la aparición de imágenes ya vividas ; de ello se dedujo que había una memoria visual en esa parte del cerebro.
Se comprobó también que la estimulación de las áreas auditivas provocaba el resurgimiento de músicas o de frases ya escuchadas.
Entonces se llegó a la conclusión de que la memoria residía en todo el córtex cerebral.
Sin embargo,posteriormente,al realizar la autopsia en cerebros de personas afectadas por la demencia senil,se descubrieron alteraciones en los tubérculos mamilares,situados en la base del cerebro,detrás de la hipófisis.
¿Se asentaba aquí la memoria? Se comprobó también que las lesiones en la circunvolución del hipocampo,en el lóbulo temporal,ocasionaban asimismo disfunciones de la memoria.
¿Había,pues,una segunda zona de residencia? No exactamente.
La circunvolución del hipocampo y los tubérculos mamilares forman parte del sistema límbico,encargado de las emociones y ubicado en el centro del cerebro.
El lóbulo límbico entra en acción cuando nos enfrentamos ante algo desconocido y prepara al cerebro para convertir lo ignorado en aprendido.
Allí se emplaza el aprendizaje en sí,pero no la recuperación de los recuerdos.
Esto se comprueba en las personas que sufren amnesia senil: les resulta muy difícil memorizar datos nuevos,aunque recuerdan exactamente informaciones muy antiguas.
A principios de siglo,el neurofisiólogo ruso Iván Paulov,estudiando el reflejo condicionado,dedujo que la memoria es un asunto del córtex cerebral,sin una ubicación precisa.
En la actualidad,la mayoría de los especialistas concuerdan en reconocer que no existe un centro único y concreto de la memoria.
La memoria visual,auditiva o la del razonamiento hacen intervenir,probablemente,zonas cerebrales diferentes,que activan los lóbulos frontal,parietal,occipital y temporal.
Pero,dentro de estas regiones funcionales,todas las neuronas participan en la memoria?,¿hay en ellas,por el contrario,neuronas o circuitos especializados? Ha sido el profesor estadounidense R. Thompson el primero en responder a estas preguntas con pruebas científicas: efectivamente,existen neuronas y circuitos encargados específicamente de la memoria,pero no pertenecen al córtex,sino al cerebelo.
Al menos así ocurre en el caso de los conejos,animales que ha utilizado en sus experimentos.
Las neuronas que Thompson ha descubierto intervienen en la memorización de una tarea motriz.
Es posible que en otros puntos del cerebro haya estructuras que memoricen un material diferente.
Esto no obsta para que el descubrimiento sea revolucionario,ya que saca a la luz todas las sutilezas anatómicas de un mecanismo que podría ser el de todo el acto de memorización.
Es ciertamente difícil emitir cualquier juicio futurista sobre la medicina.
Las predicciones tienen un porcentaje de error,en razón directa al tiempo que se intenta cubrir.
Limitándonos estrictamente al campo de los logros científicos,es posible prever para los próximos cinco años cuáles son las directrices por las que se desarrollará la investigación médica ; pero aventurar los avances de diez o quince años me parece un ejercicio inútil,pues es totalmente aleatorio su resultado.
Pero no puede contemplarse el futuro de la medicina pensando estrictamente en los logros científicos.
La medicina es una ciencia profundamente social,con connotaciones evidentemente políticas.
Decía Virchow (1821 - 1902) que la política no es otra cosa que la medicina a gran escala ".
Su futuro como ciencia social es probablemente más importante para el usuario de hoy que el análisis de los posibles adelantos científicos.
Lo ideal sería conseguir el perfecto hermanamiento entre técnica y humanismo.
Sin embargo,existen condicionamientos actuales que pueden constituir un notable obstáculo al desarrollo de una medicina futura que sea desmasificadora,eficaz,humanizada y al mismo tiempo altamente tecnificada.
No debemos olvidar que el proceso histórico-médico está escrito siempre por dos grandes protagonistas: el enfermo y el cuerpo sanitario.
Entiendo por tal a cualquier trabajador de la medicina que entre en contacto con el paciente.
Es obvio que en nuestro país se dan una serie de peculiares circunstancias que hacen más dificultosa la evolución hacia una única medicina igual para todos: la buena... La insuficiente preparación de los estudiantes de medicina,condicionada a su vez a las restricciones económicas universitarias,lo cual acarrea una dudosa capacitación para el ejercicio práctico,da lugar a un inicio de andadura que es altamente frustrante para el joven médico.
La opción a ser un 8 - 10 % de los afortunados seleccionados en el examen MIR es,aparentemente,el único camino legal,lo cual no implica que no represente una injusticia social.
Para los que no consiguen plaza,el camino es arduo,pero no lo es menos para los que la obtienen tras el examen.
Es posible que no hayan escogido la especialidad que deseaban y,en cualquier caso,al terminar sus años de aprendizaje especializado,de nuevo surgirá una gran incógnita.
Se ha podido comprobar que el médico que está angustiado por su futuro no se integra en la sociedad en la que vive,no se interesa por sus problemas.
En resumen,no se humaniza.
Se buscará el anonimato de los grandes hospitales,donde sanitarios y pacientes parecen habitar mundos distintos.
Largo sería hablar de todas las premisas que obligan a considerar el presente de la medicina española de forma harto sombría y aducir que el futuro,si lo queremos floreciente,tiene que exigir un notable esfuerzo.
Cuando leo que los conflictos laborales hospitalarios enfrentan en muchas ocasiones a los representantes de los diversos estamentos,me viene a la memoria el reglamento que se propuso para dirigir un manicomio de Valencia,nada menos que en el año 1409: " Deben ser diez ciudadanos,mercaderes o de similar condición,pero que no pueden serlo presbíteros,caballeros dignificados con generosidad,juristas o notarios ; y no porque cada una de estas clases no merezca las mayores preeminencias y honores,sino porque dicha obra debe ser totalmente laica y de hombres llanos en lo tocante a categoría,jurisdicción y toda clase de actos y no de los mencionados estamentos... Documento extraordinario,pues apunta dos condiciones que sería loable tuvieran los consejos rectores de todo hospital: - - la independencia política,- - el control por parte del colectivo al que va destinado el trabajo del hospital.
Si la historia de la humanidad,según Toffler,se puede catalogar en diferentes olas sociales,y la segunda,denominada también industrial,se basaba en el músculo y era masificadora y deshumanizadora,ojalá la tercera,como dice el propio Toffler,se base en el trabajo intelectual,aliviado por sofisticados aparatos,que volverán obsoletos nuestros actuales métodos e incluso instituciones,permitiendo mayor tiempo al hombre para ser verdaderamente humano.
En medicina ello representaría un futuro de gran tecnicismo,al servicio de una atención médica altamente humanizada también.
Aunque suene a canto voluntarista,quiero pensar y creer que así será,para bien de sanitarios y de los usuarios de la sanidad.
LA península Ibérica ocupa una posición geográfica de puente para las aves migratorias,que en sus periódicos itinerarios utilizan nuestras zonas húmedas como etapas para el reposo y la alimentación.
También las especies sedentarias necesitan para vivir de hábitats cuya relativa escasez confiere a su protección una indiscutible prioridad.
Prioridad que no sólo lo es para una política de defensa del medio ambiente,sino económica,porque el natural desarrollo de las actividades humanas en las áreas de humedales está íntimamente ligada a la calidad del recurso agua.
Entre las zonas húmedas españolas podemos citar por su relevancia las marismas del Guadalquivir,el delta del Ebro,la albufera de Valencia,las lagunas de Castilla-La Mancha,las Rías Bajas gallegas,la albufera de Alcudia,en Mallorca ; las lagunas de Gallocanta y Fuentepiedra,en Andalucía ; las salinas del Mar Menor,en Murcia,etcétera.
Algunas de estas áreas tienen una especial protección legislativa como reservas integrales de interés científico,parques nacionales o parques naturales.
Por otra parte,España ha ratificado el convenio de Ramsar,que obliga a la protección de humedales de importancia internacional como hábitat de aves acuáticas.
Sin embargo,una serie de amenazas se ciernen sobre los humedales: asentamientos humanos,extracciones de áridos,contaminación industrial y urbana,desecación del agua para otros aprovechamientos,explotaciones de minas,navegación a motor,disminución de la vegetación,etcétera.
Reviste especial importancia para la protección de las zonas húmedas la calidad de las aguas subterráneas.
Ello se debe a que el agua constituye un recurso unitario,renovable a través del ciclo hidrológico y que conserva a efectos prácticos una magnitud casi constante dentro de cada una de las cuencas hidrográficas españolas.
La contaminación de los acuíferos subterráneos se debe en gran parte a las actividades de la agricultura intensiva,que utiliza fertilizantes,pesticidas y herbicidas que,infiltrándose en el subsuelo,son un constante problema para la utilización posterior del agua.
Otras vías de entrada de contaminantes,como los líquidos residuales urbanos sin depurar,agravan la situación junto con el hecho de que,una vez contaminadas,las aguas subterráneas son mucho más difíciles de limpiar que las superficiales.
Por tanto,es aún más preciso el establecimiento de medidas preventivas de la contaminación.
Por otra parte,los aspectos cualitativos de las aguas están en conexión íntima con los cuantitativos,pues cuando depuramos un agua estamos incrementando las posibilidades de su utilización futura ; y a la inversa,cuando optimizamos su utilización estamos defendiendo la calidad del agua y su integración en los ciclos ecológicos esenciales.
Por tanto,si a la contaminación de las aguas superficiales y de los acuíferos unimos la sobreexplotación de estos últimos,podemos llegar al panorama desolador en que se encuentra nuestro Parque Nacional de Dairniel,donde el elevado número de pozos instalados en las zonas de influencia del parque han producido la disminución de los caudales de los ríos que recorren la zona,e incluso la interrupción o secado de algunos como el Záncara,Ciguela o Córcoles.
Lo que ha desembocado en una pobreza palustre que obliga a huir a las aves acuáticas que utilizan desde tiempos seculares nuestras zonas húmedas.
Todos estos aspectos relativos al control de la calidad junto a la cantidad del agua,la posibilidad de proteger y restaurar eficazmente las riberas de los cauces y lagos,así como de desarrollar los mecanismos de prevención de la contaminación,se contemplan en la nueva Ley de Aguas,que marcará,sin duda,un hito trascendental para la protección legal del Medio Ambiente.
NECESITAMOS una nueva escuela.
Necesitamos una nueva formación.
Estamos educando a nuestros hijos para un mundo que no será el suyo,perpetuando unos contenidos de enseñanza muy lejanos a la realidad.
Nuestros hijos saben que,en efecto,Lorca y Totana están en Murcia,y que el Ebro nace en Fontibre y que España limita al Sur con el continente africano.
Pero no saben mirar a su compañero de al lado,saben poco de eso que se llama convivencia.
Los más actuales conocen la letra de las canciones de Stevie Wonder,y prefieren la hamburguesa al bocadillo de queso.
Y están en este mundo.
En el mismo que nosotros,ya adultos.
Y si ahora todos,ellos y nosotros,nos preguntáramos cómo es ese mundo,¿qué diríamos? Nos daría vergüenza responder.
Pero no sabríamos hacer siquiera fuego.
No sabríamos hacer nada de nada.
Seguramente no podríamos ni comer.
Pero,eso sí,podríamos recitar incluso las recetas más sofisticadas ; y teniendo el paladar acostumbrado a exquisiteces y rarezas,nos moriríamos de hambre,conformándonos con el recuerdo del sabor de los más apetecidos manjares.
Honestamente hablando,¿no es absurdo? Pues bien,a esta circunstancia que,efectivamente,hemos llevado exageradamente al extremo,estamos llegando ya.
Y sirva de ejemplo que hoy la civilización - - mientras unos se empeñan en saberse las fechas de la historia y otros la retahíla de títulos de un autor literario - - va por otros derroteros.
Va por el software y el hardware,por ejemplo.
Prueba evidente es que el adulto siente ante el avance incontenible de la informática una especie de temor inconfesado.
Y no cabe preguntarle por qué.
La respuesta,aunque no la diga,es que él se siente impotente ante esas maquinitas que lo hacen todo.
Luego se convencerá a sí mismo de que es el hombre quien gobierna y que la máquina,sin el hombre,no hace nada.
El problema grave es que se habla " del hombre.
Y sabe muy bien que en esa acepción no se encuentra él ; esto empieza a ser cosa de unos pocos.
Como Si ellos,los que saben de esa nueva ciencia,fueran los dominadores.
¿Una exageración? Es posible... Pero el ciudadano medio tiene tendencia a ver lo científico como algo esotérico.
Porque a él le enseñan una cosa,y el mundo que ve le muestra otra.
Porque advierte que eso que le enseñan no sirve,mientras que esa ciencia aplicada le vigila hasta sus más mínimos ingresos.
He ahí la gran labor.
El hombre sabe de la Luna y de Marte,pero se ignora a sí mismo.
Conoce el problema de Afganistán,pero no sabe para qué vale su bazo (que es,para él,mucho más importante).
Sabe el nombre de los Nobel de literatura,pero le teme al teclado de un ordenador.
Conoce la biografía de los demás,y apenas sabe cómo se llaman sus vecinos... Admira la aventura científica del Espacio,y nadie le ha dicho que ese bolígrafo que utiliza se consiguió gracias a eso.
O que los paneles solares que quiere poner su comunidad de vecinos para calentar el agua... también son un logro de la NASA.
¿Verdad que sería importante poner al alcance de todos lo que la ciencia hace,y dice,y proyecta,y piensa? Sería maravilloso lograr que,como un juego,nuestros hijos conocieran el mundo en que se mueven.
Y supieran de pronto,aislados y so! os,iniciar una nueva civilización... Eso es divulgar.
Esa es su importancia.
Esa es la dedicación: que si la escuela tradicional sigue enseñando los afluentes del Tajo y el nombre de todas las obras de Lope de Vega,esa otra escuela vital,la de la lectura,la de la radio,las revistas,la televisión y la prensa,pueda llevar esa idea del mundo que se me antoja mucho más real.
Porque,además,sería,por la propia dinámica del mundo,una formación continuada y permanente.
Apasionante y hermosa.
Y sería muy simple.
Para iniciarla sólo basta preguntar por qué,cómo... ¿Cómo se hace esta revista? ¿Por qué se ven las letras? ¿Cómo se llegan a distinguir los colores? ¿Por qué las palabras significar lo mismo? ¿Cómo y por qué es capaz usted de recordar? O mucho más simple: ahora,cuando le llamen por teléfono,pregúntese cómo funciona.
O la nevera.
O la luz... (No siga.
Por hoy ya está bien.
Si por lo menos le ha " picado " un poco la curiosidad,me alegro.
Ya es de los nuestros.
) Y tengo para mí que si todos divulgáramos un poco,todos sabríamos un poco más.
Por eso: divulga,que algo queda.

