La aproximación no socava la confianza de Weingarten en sus resultados.
" La coincidencia entre la aproximación de valencia y los datos observacionales indica,en cierto sentido,que la teoría completa concuerda con los experimentos ",afirma.
Pero sí coinciden todos en que estos cálculos avalan que el empleo de ordenadores en una demostración es legítimo.
" Hemos llevado a cabo un verdadero cambio cualitativo ",medita Weingarten.
La novedad estriba en que se acepte que haya incertidumbre " experimental " en el cálculo mismo de las consecuencias observables de una teoría física fundamental.
" No es ",dice," sino física teórica experimental ".
Diagnóstico cósmico: Cúmulos de galaxias.
No sólo para los médicos son los rayos X un medio inapreciable con el que descubrir estructuras ocultas.
El año pasado Trevor Ponman y sus colaboradores,de la Universidad de Birmingham,anunciaron que las observaciones de rayos X del gas caliente del cúmulo de galaxias de Coma indican que la distribución de la masa del cúmulo es sorprendentemente complicada,como una aglomeración de bultos.
" Lo que apoya que los cúmulos han crecido gracias a la agregación de grupos de galaxias,y que el proceso aún está ocurriendo ",dice Ponman.
Este descubrimiento es de especial interés,ya que el cúmulo de Coma,situado a una distancia de 300 millones de años luz,en la constelación Coma Berenices,es el más cercano de los cúmulos abundantes en galaxias y uno de los mejor estudiados.
Desde entonces,Simon D. M. White,del Instituto de Astronomía de la Universidad de Cambridge,y sus colaboradores han llevado más allá los descubrimientos de Ponman.
Con datos del Roentgen Satellite (ROSAT),el grupo de White ha logrado una imagen de rayos X del cúmulo de Coma con un detalle sin precedentes.
White describe su trabajo como " arqueología de rayos X ",ya que le permite reconstruir el proceso por el que se formó el cúmulo de Coma.
" Se ven claramente los remanentes de subagregados anteriores ",dice White.
Las extensiones del cúmulo de la figura adjunta,visibles en la parte inferior derecha,son gas caliente que envuelve galaxias gigantes ; éstas fueron alguna vez,probablemente,los objetos dominantes de sus propios cúmulos,más pequeños,antes de que se fundiesen con el cúmulo de Coma.
La estructura observada de éste casa con las ideas dominantes relativas al origen de la estructura cósmica,según las cuales los vastos cúmulos de galaxias se formaron por la captura y absorción de masas más pequeñas.
Los modelos alternativos,donde los cúmulos similares al de Coma se originan de golpe,cada vez parecen menos atractivos a la vista de los datos actuales,observa Simon White.
De todas formas,no todo es felicidad para los teóricos.
Las observaciones de rayos X de cúmulos galácticos permiten calcular su masa total y determinar en qué proporción están compuestos de materia ordinaria (" materia bariónica ") ; el resto tiene que ser de la misteriosa materia oscura.
White ha hallado que,en las partes más internas del cúmulo de Coma,del 11 al 35 por ciento de la masa es materia ordinaria.
Los modelos cosmológicos más aceptados predicen que la fracción de materia ordinaria tiene que ser mucho más pequeña," aproximadamente en un factor cinco ",afirma.
" En mi opinión,se trata de una discordancia importante.
" Entonces,¿dónde se esconde toda la materia oscura? Un grupo encabezado por John S. Mulchaey,del Instituto Científico del Telescopio Espacial,de Baltimore,a partir de otros datos del ROSAT,sostiene que podría estar encerrada en cúmulos mucho más pequeños que el de Coma,lo que,para Ponman,es un poco " escurrir el bulto ".
White señala que el fallo podría estar en los datos de rayos X o,si no,en que sean incorrectas nuestras ideas acerca de cómo se formaron los cúmulos de galaxias,cuán denso es el universo,o,incluso,acerca de cómo nació éste.
El origen del petróleo Formación de roca madre En 1988,químicos de la Escuela Nacional Superior de Química de París y microscopistas de la Escuela Normal Superior de la capital gala descubrieron,gracias al microscopio electrónico de transmisión,una gran cantidad de estructuras muy delgadas y laminares en la materia orgánica de diversos esquistos bituminosos y roca madre.
Estas " ultralaminae " se parecían a las finas paredes externas de las microalgas verdes,en especial las del género Scenedesmus.
Se observó que estas paredes se componen de macromoléculas resistentes a los ataques químicos ; surgió entonces la pregunta de si la roca madre petrolífera no se produciría,al menos en ciertos casos,por acumulación de macromoléculas biológicas conservadas a pesar de cientos de millones de años de enterramiento.
Esta hipótesis se oponía al mecanismo admitido clásicamente de degradación-recondensN: la materia orgánica habría sufrido ataques bacterianos y químicos en las capas superiores de los sedimentos,de manera que las macromoléculas biológicas se habrían despolimerizado y después degradado ; sólo una pequeña porción habría escapado a la destrucción debido a reacciones de recondensación que habrían ido constituyendo,al azar,geomacromoléculas complejas cada vez menos solubles ; éstas,a su vez,habrían formado el querógeno (la parte orgánica de las rocas,transformada progresivamente en petróleo a lo largo del enterramiento) de la roca madre y los esquistos.
Para comprobar su hipótesis de la " conservación selectiva ",C. Largeau y sus colegas han comparado las estructuras químicas de las ultraláminas fósiles y de las paredes delgadas externas de microalgas modernas.
Como los compuestos presentes en estas dos estructuras tienen una gran estabilidad química,las han estudiado por pirólisis,es decir,mediante la degradación térmica por calentamiento en ausencia de oxígeno.
El análisis de los productos de pirólisis de las paredes delgadas de Scenedesmus quadricauda y otras microalgas actuales ha revelado,en primer lugar,una red compuesta de largas cadenas hidrocarbonadas (encadenamiento de grupos CH2).
Al igual que los productos de pirólisis de la roca madre y de los esquistos que contienen ultraláminas,las macromoléculas resistentes de las algas tenían la particularidad de liberar,en el momento de su pirólisis,N-alquilnitrilos (moléculas orgánicas lineales terminadas por el grupo - C =N): esta semejanza química confirma que las ultraláminas fósiles provienen de la conservación de paredes externas de algas Scenedesmus u otras microalgas de morfología análoga.
¿Por qué las macromoléculas de las delgadas paredes externas de las microalgas son tan resistentes químicamente,a pesar de la fragilidad de algunos de sus enlaces (ésteres - COO - o amidas - CONH2)? Porque estas funciones están protegidas por la red de largas cadenas hidrocarbonadas,que impiden que los reactivos de rotura alcancen las zonas más frágiles: las macromoléculas permanecen intactas a lo largo de su fosilización y pasan a través de los tiempos geológicos sin degradación.
Por el contrario,la protección por la red es ineficaz contra las degradaciones térmicas: los nitrilos identificados en los productos de pirólisis,por ejemplo,provienen de la rotura térmica de los enlaces de amidas.
Estos trabajos,una parte de los cuales se ha efectuado en colaboración con la sociedad Elf Aquitaine,han sacado a luz una nueva familia de macromoléculas biológicas y determinado las causas de su excepcional resistencia.
Y han desembocado en el descubrimiento de un nuevo mecanismo de fosilización,el de conservación de macromoléculas resistentes.
El análisis en curso de otros grupos de microorganismos indica que este mecanismo es general.
Tras el descubrimiento de la primera proteína con dedos de cinc,por Aaron Klug y su equipo,se han multiplicado los ejemplos de proteínas que tienen características similares a las de aquéllas.
Se sabe ahora que existen al menos tres grandes grupos de proteínas con dominios estructurados que se unen específicamente al ADN y en cuya constitución desempeñan un papel esencial los átomos de cinc.
En primer lugar están las proteínas con dedos de cinc " clásicos ",como la TFIIA y Zi98.
Esas proteínas tienen numerosos dominios en forma de dedos de cinc (entre 2 y 37),cada uno de los cuales contiene dos cisteínas y dos histidinas,en combinación las cuatro con un átomo de cinc.
La segunda clase es la de los receptores nucleares de hormonas,que contienen siempre dos (y sólo dos) estructuras que forman complejos con el cinc.
El primer dominio asegura la fijación específica al ADN,mientras que el segundo estabiliza la unión.
En este caso son cuatro cisteínas las que se unen a un átomo de cinc.
Esos dominios están formados por dos hélices alfa: una entra en contacto con el ADN,y la otra,que se entrecruza con la primera,la obliga a mantener ese contacto.
Bert Vallee,de la Universidad de Harvard,ha propuesto que los dominios de fijación al ADN de los receptores nucleares reciban el nombre de zinc twists.
Es decir,bucles,o entrelazados,o tirabuzones,de cinc.
Por último,más recientemente se han identificado en ciertas proteínas de levaduras,como la Gal4,dominios de fijación con seis cisteínas que fijan dos átomos de cinc.
Su estructura tridimensional es distinta,y se ha propuesto que este nuevo tipo de dominios de unión al ADN se denomine zinc cluster (grupos o racimos de cinc).
Estos tres tipos de estructuras no tienen un antecesor común ; se deben a una convergencia evolutiva a escala molecular.
Se puede comparar esta convergencia evolutiva con la que se da entre las alas de los pájaros y las de los murciélagos.
Esas estructuras se parecen mucho,porque sin duda representan la mejor solución para que un animal pueda volar,¡pero los murciélagos no son pájaros! A escala molecular,es posible imaginar que la naturaleza ha encontrado un manera cómoda de fabricar dominios proteínicos capaces de unirse específicamente al ADN,en los que el cinc sirve para estabilizar la estructura tridimensional de la proteína.
Otros tipos de dominios proteínicos dotados de actividad catalítica también se estabilizan mediante combinaciones con cinc,pero no reconocen el ADN.
El cinc no confiere a la molécula propiedades específicas,sino que estabiliza ciertas configuraciones.
Sin duda,los diferentes dominios de unión (dedos,bucles y racimos de cinc) han adquirido de forma independiente su capacidad de reconocer el ADN.
Los biólogos confiesan su fascinación por los procesos que se desarrollan durante la división celular.
En particular por uno de ellos: el movimiento de los cromosomas.
Tiene éste por cometido distribuir el material genético durante la mitosis de las células eucariotas.
Se asegura con ello que dicho material se mantenga constante para cada célula,generación tras generación.
Debido a ese mecanismo de control,el hombre posee 46 cromosomas en todas y cada una de sus células,el caballo 64,el maíz 20,el algodón 52 y la estrella de mar 36.
Durante la meiosis,las células germinales reducen,en cambio,el número de sus cromosomas a la mitad,asegurando así que,tras la fecundación,el cigoto albergará el juego completo de cromosomas.
Tres son las estructuras fundamentales comprometidas en la mitosis o división celular: centrosoma,huso mitótico y cromosomas.
De estos últimos y en especial de una formación clave de los mismos,involucrada en el proceso mitótico,vamos a ocuparnos aquí.
A partir de los años treinta,se han venido usando indistintamente los términos centrómero y cinetocoro para designar una misma región de los cromosomas.
Pero conviene no confundirlos.
Hemos de reservar el vocablo centrómero para aludir a la constricción primaria de los cromosomas,es decir,al lugar de intersección de las dos cromátidas (brazos del cromosoma) ; el cinetocoro designa el lugar de unión de los microtúbulos del huso mitótico a los cromosomas.
El cinetocoro es,pues,la zona específica de la región centromérica donde se genera,a través de la interacción con las fibras del huso,la fuerza que permite el avance de las cromátidas de cada cromosoma hacia los polos.
La región centromérica posibilita así,por un lado,la unión al huso a través del cinetocoro y,por otro,la unión de las dos cromátidas a través de la heterocromatina asociada al centrómero.
Aunque ambas regiones operan de manera conjunta y son necesarias para la segregación normal de los cromosomas durante la mitosis,cinetocoro y centrómero muestran diferencias morfológicas y bioquímicas.
Ambas pueden estudiarse mediante la microscopía electrónica y la aplicación de los sistemas de vídeo a la biología celular junto con las técnicas de identificación y aislamiento de proteínas y clonación de ADN.
En la mayoría de los organismos estudiados,los cinetocoros se hallan circunscritos a un lugar,a la constricción primaria del cromosoma.
El cinetocoro de los eucariotas superiores ofrece una estructura trilaminar localizada en la región centromérica.
Observado al microscopio electrónico,distinguimos una zona densa u obscura (la más alejada del cromosoma),otra zona intermedia menos densa y,por último,una tercera densa u obscura,adyacente ésta a la cromatina centromérica.
La capa externa posee de 40 a 60 nanómetros de espesor,la intermedia de 25 a 30 nm y la interna de 40 a 60 nm.
Aparecen fibras delgadas de cromatina que enlazan la lámina externa del cinetocoro con la interna centromérica.
Esa lámina externa se une,durante la mitosis,a los microtúbulos del huso mitótico,unión que se produce a través de ciertas fibras de cromatina que constituyen la corona ; se supone que en la corona acontece la unión específica de la tubulina (principal proteína de los microtúbulos).
Los microtúbulos desempeñan un papel principal en la función del cinetocoro.
Se trata de estructuras dinámicas constituidas por 13 profilamentos cuyo proceso de formación y destrucción dentro de una célula eucariota se halla sometido a una regulación temporal.
Entre las funciones que ejercen los microtúbulos durante las distintas fases del ciclo celular destaca la de facilitar el ensamblaje correcto del huso mitótico.
Los microtúbulos son estructuras polares compuestas,en buena parte,por dos proteínas emparentadas: alfa y beta tubulina,que forman el heterodímero de tubulina.
Por su carácter polar,una punta del microtúbulo crece más deprisa (extremos positivo) que la otra (extremo negativo) mediante la adición de nuevos dímeros de tubulina.
Durante el proceso de la división nuclear mitótica,los microtúbulos arrancan de los extremos negativos en los polos del huso,o centrosoma,y progresan hacia el citoplasma mediante la adición de subunidades de tubulina al extremo positivo.
(Berl R. Oakley,de la Universidad de Ohio,acaba de describir una tercera tubulina,la gamma,que se aloja exclusivamente en los polos del uso y no en los microtúbulos ; parece,pues,hallarse implicada en la capacidad de nucleación de los centrosomas y en el establecimiento de la polaridad de los microtúbulos.
) La estructura laminar del cinetocoro puede variar su posición en el interior del cromosoma ; el centrómero,y el cinetocoro con él,puede quedar en el centro del cromosoma (cromosoma metacéntrico),más cerca de un extremo (cromosoma acrocéntrico) o en un extremo del cromosoma (telocéntrico).
Organismos tan dispares como algas,peces,aves,plantas y mamíferos presentan la estructura laminar del cinetocoro descrita.
Pero existen otros modelos de organización del cinetocoro.
En levaduras,hongos y algunos insectos,son muy pocos los microtúbulos que se unen a cada cromosoma y no se observa región de unión,la típica estructura trilaminar.
(En el caso de la levadura Saccharomyces cerevisiae sólo se da un microtúbulo por cinetocoro.
) En determinadas plantas superiores y ciertos artrópodos,los microtúbulos no se unen a una constricción primaria,sino que lo hacen al cromosoma por entero.
Se trata de los llamados cinetocoros difusos,no localizados.
En los dinoflagelados,el cinetocoro se halla embebido en la propia envoltura del núcleo celular,con los microtúbulos que provienen del citoplasma unidos a la lámina externa y,unida a la capa interna,la cromatina nuclear.
¿De qué modo se relaciona la estructura trilaminar del cinetocoro con la organización del ADN de la región centromérica? De acuerdo con Jerome B. Rattner,de la Universidad de Calgary,las láminas están constituidas por fibras de cromatina que arrancan del centrómero,se prolongan lejos del mismo y se pliegan para volver hacia el centrómero de nuevo.
El hacinamiento de las fibras de cromatina a cierta distancia del centrómero crearía la lámina externa del cinetocoro,y éste vendría a ser una continuación de la nucleoproteína centromérica con una organización especial que originaría la estructura trilaminar observada por microscopía electrónica.
En los organismos inferiores que carecen de la estructura trilaminar,pensemos en S. cerevisiae,aparece difusa la distinción entre cinetocoro y centrómero.
En esos casos,los microtúbulos se unen directamente a las fibras de cromatina centroméricas,aunque en el ejemplo particular de la levadura mencionada la cromatina se uniría a un solo microtúbulo.
La caracterización de los componentes moleculares de la región centromérica y su estudio en la escala filogenética nos ayudan a conocer la evolución del centrómero indiviso primitivo (levadura) hacia la estructura trilaminar (mamíferos).
En los organismos superiores,sabemos que cada cromosoma se une,durante el proceso de mitosis,a varios microtúbulos ; la estructura molecular y la organización del cinetocoro ayudan a explicar ese fenómeno.
El cinetocoro de las células de mamífero consta de múltiples unidades de cromatina que se organizan originando la estructura trilaminar mencionada.
Las unidades de cromatina son,a su vez,complejos de ácido nucleico y proteína,cada uno dotado de capacidad de unión a los microtúbulos del huso mitótico.
El número de microtúbulos que se anclan en el cinetocoro depende,en cada especie,del número de complejos de ácido nucleico y proteína que posea.
Durante la división celular,el cinetocoro,como parte que es del cromosoma,ha de duplicarse y separarse con las cromátidas.
Para estudiar ese proceso de replicación se ha recurrido a técnicas de inmunofluorescencia indirecta,mediante el empleo de anticuerpos encontrados en el suero de pacientes con ciertas enfermedades autoinmunes de tipo reumatoide,fundamentalmente el sindrome de CREST.
A comienzo de los años ochenta,en el laboratorio de Eng Tan en la Jolla,California,se descubrió que esos sueros humanos teñían,en cultivos celulares,determinada parte del cromosoma asociada a la región centromérica.
Se trataba del cinetocoro.
Vióse,además,que éste se hallaba asociado a cada cromátida en la fase Gl del ciclo celular,antes de la duplicación del ADN.
En el laboratorio de Bill Brinkley,en Houston,se demostró que,tras la replicación del ADN en la fase G2 del ciclo celular,cada cromosoma presentaba dos cinetocoros asociados al material genético.
El lenguaje del cuerpo.
Expresión corporal y ordenadores.
Quién no se despereza,bosteza,sueña o dormita ante la pantalla del ordenador? Pero éste nada responde a nuestras muecas,o,al menos,así venía siendo: ahora,un neurólogo convertido en empresario está construyendo unos dispositivos gracias a los cuales se puede hacer música a partir de la acción de los músculos,desplazar un cursor con la mirada o lanzar en un videojuego una nave al hiperespacio con un simple acto de la mente.
Hugh S. Lusted creó la empresa BioControl Systems en Palo Alto,California,persiguiendo un sueño de su adolescencia: construir una máquina con la que se pudieran controlar las cosas directamente con la mente.
No era una idea original: abunda en las obras de ciencia-ficción.
Pero Lusted la materializa por medio de unas máquinas que detectan y descodifican las señales eléctricas de los músculos,el globo ocular y el cerebro de manera que estos órganos puedan actuar sobre los ordenadores.
BioControl nació gracias al interés por la música que Lusted y Benjamin Knapp,ingeniero eléctrico de la Universidad Estatal de San José,compartían.
Se conocieron en 1987,mientras tomaban parte en un proyecto de implantación coclear.
Lusted era por entonces neurofisiólogo de la Facultad de Medicina de la Universidad de Stanford.
" Construíamos un oído biónico ; implantábamos electrodos en voluntarios sordos para estimular directamente su nervio auditivo ",recuerda Lusted.
Ambos científicos se dedicaban.
terminada la jornada,a preparar los circuitos y el soporte lógico necesario para controlar instrumentos musicales electrónicos con la información extraída de los electromiogramas,es decir,de los registros de las descargas eléctricas que las fibras musculares emiten al contraerse.
Al cabo de unos meses ya tenían un prototipo operativo.
Mediante electrodos ceñidos a sus brazos,podían tocar violines invisibles moviendo en el aire un arco imaginario: al doblar uno de los brazos el tono subía o bajaba ; la tensión en el otro variaba la intensidad del sonido.
Animados por los resultados,Lusted y Knapp aplicaron sus técnicas de procesamiento a los electro-oculogramas (EOG) que producen los ojos,con la esperanza de obtener un ratón de ordenador que no hubiese que manejar con las manos.
" El globo ocular es una batería,un dipolo ",explica Lusted.
" La retina,al ser metabólicamente activa,es negativa con respecto a la córnea,y es posible detectar el movimiento de ese dipolo dentro de un campo eléctrico ".
Y no sólo el movimiento,sino también la profundidad.
Como los ojos convergen cuando enfocan algo,el EOG sirve para el control bidimensional y tridimensional de cursores,tal vez incluso de robots.
Además," como la señal del EOG se hace más intensa cuando aumenta la luminosidad,el nivel de luz da información acerca de qué mira el usuario ",añade Lusted.
Sin embargo,las miradas han resultado ser más furtivas que los brazos.
" Siempre se había creído que el EOG no podría ser,a causa de la deriva que presentan las señales,útil ",dice Lusted," pero se trataba sólo de un problema de electrodos y de electrónica " que BioControl dice haber resuelto mediante sensores de hidrogel,producidos por la firma Alternative Design Systems,de Nueva Jersey.
Lusted compara la textura de los sensores reutilizables a la de las lentillas: " son ligeramente adhesivos,pero no dejan residuos.
" Unas bandas de Velcro puestas alrededor de una extremidad o de la frente mantienen los electrodos en su sitio.
Una vez habían logrado que músculos y ojos estuviesen bajo - - o más bien en el - - control,Knapp y Lusted se dispusieron a llegar a lo más alto: las ondas cerebrales.
Los mismos sensores en forma de cinta alrededor de la cabeza que leen el movimiento de los ojos pueden captar también un electroencefalograma ; descifran,en tal caso,las ondas de potencial variable que emiten las neuronas del cerebro cuando se activan.
" En principio,podrían darse las luces con pensar la palabra ' encender ' ; en esa dirección nos encaminamos ",dice Lusted," pero todavía estemos muy lejos del reconocimiento de patrones de ese tipo.
" BioMuse (" muse " significa cavilar),sacado al mercado por BioControl en octubre como su producto de bandera,puede,de hecho,encender o apagar luces - - o cualquier otra cosa - - en respuesta a ondas cerebrales (no a pensamientos concretos).
" Detecta la transición entre la actividad de onda alfa y la de onda beta del cerebro ",aclara Lusted.
" Cada vez que se cierran los ojos se pasa a un breve período de ondas alfa ; esto es así en la mayoría de las personas.
Practicando un poco,se pueden generar ráfagas alfa sin más que pensar en el negro ".
Por medio de una pastilla de proceso digital de señal producida por Texas Instruments,BioMuse incorpora en una sola caja todos los algoritmos diseñados por BioControl.
Puede así descodificar las señales transmitidas simultáneamente por ocho sensores como máximo,lo que basta para conectar al aparato las extremidades,ojos y cerebro,sobrando aún un canal para el reconocimiento de la voz.
Aunque Lusted concibe un sinfín de aplicaciones de este dispositivo - - cuya licencia de uso por tres años cuesta 10.000 dólares - -,la mayor parte de sus actuales clientes lo utilizan para la investigación.
Hay,sin embargo,excepciones.
El verano pasado,Atau Tanaka,alumno posgraduado de música en Stanford,compuso y estrenó un solo de BioMuse de 2 minutos de duración que tituló " Kagami " (Espejo),porque " su música es reflejo directo de los elementos del cuerpo ".
Ciñéndose dos brazaletes con equipo radiotransmisor,Tanaka controlaba al danzar el timbre,tono y posición acústica de sonidos de campanas,tambores y de un instrumento de viento australiano,el " didgeridoo ".
Más común es el caso de Volvo,que para fines de investigación adquirió dos unidades BioMuse.
Estudian con ellas,en simuladores de automóvil,la fatiga y el estrés de los conductores.
" También ",dice Lusted," quieren utilizar el controlador ocular para unas pantallas que estarían en la parte superior del parabrisas.
La idea es que se mantenga la mirada alta,que no haya que bajarla para ver el salpicadero.
Por seguridad.
" BioControl comprende que el mercado de la investigación no basta para asegurar su supervivencia a largo plazo ; atempera,por eso,su tecnología de manera que sea de utilidad en unos cuantos sectores lucrativos.
Lusted explica que dispone ya de los prototipos de cuatro productos,derivados todos ellos de BioMuse,que espera poner a la venta,aunque sea a pequeña escala,en las próximas Navidades.
El más sencillo,denominado BrainMan,es un conmutador controlado por onda cerebral para videojuegos.
" No es más que un juguete,y no queremos alardear de ello ",dice Lusted," pero con él hacemos llegar al mercado el mensaje de que esta tecnología es viable y puede merecer la pena económicamente ".
Otros productos en preparación son un controlador de videojuegos por seguimiento visual (con él se dispara encajando las mandíbulas) y un dispositivo de control muscular para instrumentos musicales.
Producidos en grandes series,no costarían más de 150 dólares.
Pero esto es mucho suponer.
Los gigantes de la industria de videojuegos,Sega y Nintendo,no han acogido muy bien las proposiciones de BioControl.
" Sólo les interesa la mejora lineal de lo que ahora tienen ",se lamenta Lusted," y nuestros controladores no son mejores o más rápidos,sino diferentes ".
Los planes de BioControl no podrán salir adelante si no se asocian con otras firmas para la fabricación y distribución,pues el capital de la empresa,fundada por siete inversores particulares,no es lo que se dice abundante.
Antes de que Lusted abandonase la universidad el año pasado,ni siquiera tenía la empresa un presidente con plena dedicación.
" Mi beca de investigación expiró,y por fin pude pagarme a mí mismo un salario.
" Para su cuarto producto,el Enabler (" Capacitador "),BioControl cuenta con un aliado entusiasta,David J. Warner,del Centro Médico de la Universidad de Loma Linda,quien ha sabido utilizar las posibilidades de seguimiento muscular y visual de BioMuse - - que Enabler incorpora en una placa de expansión para ordenadores personales - - como medio auxiliar de diagnóstico y tratamiento de pacientes discapacitados.
" Ando todo el día con verdaderas maravillas ",dice," pero aún así el BioMuse es una de las técnicas de interfaz más interesantes que he conocido ".
Gracias a Enabler,pueden manejar ordenadores incluso personas que sufren parálisis muy graves.
Warner lo ha probado con tres pacientes.
Cristal,tetrapléjica de 18 meses de edad,ni siquiera es capaz de respirar por sí misma.
Sin embargo,a los pocos minutos de ceñirse la cinta a la cabeza aprendió a desplazar una imagen que representaba un rostro sonriente por la pantalla de un ordenador sin más que mover los ojos.
Tetrapléjicos de más edad emplearon,en combinación con programas,ya existentes,que completan palabras y frases,un dispositivo de seguimiento visual para escribir.
Warner conectó también un adolescente parapléjico a BioMuse.
Tras colocar electrodos en los brazos disfuncionales del chico," le proporcionamos un sonido de guitarra de rock ",dice Warner," y se convirtió en un Jimi Hendrix durante 10 minutos,hasta el punto de que acabó sudando.
Este era un paciente del que no podíamos conseguir que hiciese nada ; su motivación creció grandemente.
" Pero quizá fue aún mayor el efecto que esta técnica tuvo en Dennis,un hombre de 38 años al que un ataque cardiaco dejó sin riego cerebral durante diez minutos,tras lo cual quedó en un estado vegetativo que,en opinión de los médicos,era permanente ; es decir,quedó - - comenta Warner - - convertido " en un cuerpo unido a un tronco nervioso ".
Movido por la intuición,Warner solicitó y obtuvo permiso para conectar un BioMuse a Dennis.
Localizaron en el hombro un músculo relativamente tranquilo,al que adhirieron unos electrodos ; ejecutadas las pruebas básicas," le di una orden verbal específica: que alzase el hombro.
Al cabo de unos segundos ",dice Warner," pudimos comprobar claramente que existían órdenes de contracción del músculo de magnitud superior a cualquier ruido aleatorio.
Fui entonces al neurólogo,y le dije que todavía había alguien en ese cuerpo,que tenía que modificar su diagnóstico.
Las posibilidades de rehabilitación son muy escasas,pero no es lo mismo un estado vegetativo persistente - - lo que quiere decir que en ese cuerpo no mora ya nadie - - que una hipoxia cerebral aguda - - que todavía hay alguien dentro,pero en un estado de gran deterioro - - ; la diferencia es nada menos que la diferencia entre que haya aún un alma y que haya ya desaparecido del todo.
Las enfermeras saben ahora que Dennis de alguna manera las percibe.
Sea como sea,hemos aumentado cualquier posibilidad que pueda haber de mejorar su condición.
" Un solo caso conmovedor no basta para que la Administración de Alimentos y Medicamentos estadounidense apruebe el uso médico regular de BioMuse.
Warner admite que deberían reunirse varias personas cualificadas para estudiar a fondo el producto,lo cual,por otra parte,requiere unos fondos que hasta ahora han escaseado.
Sin embargo,en unos días en que el orden de cosas concerniente al cuidado de la salud se encuentra sometido a reconsideración,es posible,señala,que una técnica que " podría reducir costes gracias a la automatización de la terapia " vaya siendo cada vez más respaldada.
En todo caso,ocurra lo que ocurra,lo cierto es que nos promete nuevos juguetes en verdad fascinantes.
Lógica borrosa Anticongelante para aviones Cuando,por efecto del hielo,la nieve o la escarcha,se forma una capa de hielo en los planos de sustentación o en los alerones de un avión,sus condiciones aerodinámicas pueden empeorar hasta el punto de d que podría llegar a caer.
Por esta f razón,todos los aeropuertos tienen a punto vehículos especiales de deshielo: poco antes de la salida del avión,e un técnico retira del fuselaje el hielo y la nieve con una mezcla caliente a alta presión de agua y anticongelante.
La mezcla depende de las condiciones climáticas,de la temperatura exterior que haga,de cuál sea el grado de congelación.
Para evitar que antes del despegue las alas se congelen de nuevo,se rocía sobre el avión una capa protectora de anticongelante concentrado: el llamado ADF 2 (del inglés Anti-deicing fluid,fluido anticongelante),hoy muy usado,que protege durante unos 20 minutos.
Como el tiempo acucia,hay que elaborar la mezcla rápidamente y con extrema precisión.
Una dosificación deficiente,que reduzca la acción anticongelante del producto,pondrá en peligro la seguridad del avión ; por otra parte,el ADF 2 es caro,y delicada su aplicación.
Las particulares condiciones del anticongelante convierten en extraordinariamente difícil el problema de la dosificación.
La presión,el calor,el esfuerzo mecánico - - debido a la estrechez y sinuosidad de los tubos,o a que haya válvulas abiertas sólo a medias - - pueden cortar las largas cadenas moleculares del ADF y anular su eficacia.
De ahí que se aspiren el agua y el ADF de sus respectivos depósitos por medio de bombas de espiral excéntrica,y entonces se mezclen.
El técnico quiere estar en condiciones de determinar la mezcla y el flujo que rocía.
Debe haber un sistema regulador que actúe de tal manera sobre la acción de las bombas que la proporción de la mezcla se mantenga constante.
Con ese fin,dos aparatos medidores registran el flujo momentáneo del agua o del ADF antes de que se produzca la mezcla ; se mide,además,la presión en la conducción.
Ahora bien,la existencia de fenómenos no lineales dificulta la regulación.
El motor del vehículo acciona las bombas mediante un ciclo hidráulico en el que unas válvulas de ajuste regulan el rendimiento de las bombas - - que no será,sin embargo,proporcional a la disposición de las válvulas - -.
La viscosidad del anticongelante,y con ella la resistencia que éste opone a la bomba,depende de la temperatura de manera no lineal.
Los medidores inductivos del flujo trabajan con un gran retraso.
El proceso anticongelante de la casa Schroder recurría hasta ahora al método de regulación proporcional integral (PI).
En él,los reguladores basan su acción en la desviación del flujo respecto de su valor correcto.
Una regulación fina permite que se mantenga la cantidad deseada de un litro por minuto.
Si el valor registrado se desvía demasiado de ésta (más de tres litros por minuto),se pasa a una regulación menos fina que rápidamente reconduce el funcionamiento de la bomba.
A esta regulación bipartita del flujo se añade además una regulación PI de la presión de inyección que,cuando ésta llega a los siete bar,se activa e impide que sobrepase los diez bar.
Para que el cambio de un regulador a otro no ocurra con excesiva frecuencia,la transición se efectúa con retraso,sujeta a histéresis.
Desarrollamos esta idea de forma heurística ; no era posible elaborar un modelo matemático completo del proceso sin que hubiese que realizar un esfuerzo exagerado.
La regulación sufre desde un principio el lastre de que no todos los estados internos del sistema son medibles,lo que tiene por consecuencia que el principio de compensación de la no linealidad del sistema mediante cambios bruscos de unos reguladores lineales a otros está condenado a no cumplir su tarea de manera perfecta.
La existencia de estos problemas derivados de la no linealidad recomendó la utilización de técnicas borrosas.
La experiencia con el método PI nos ofrecía una serie de reglas condicionales de la forma si-entonces ; se tenía a mano,pues,la descripción verbal de una estrategia adecuada.
Desarrollamos el regulador borroso en un ordenador personal conectado en serie al controlador del sistema anticongelante que intercambiaba las magnitudes de medida y situación.
Una vez hubimos preparado un regulador satisfactorio,procedimos a instalarlo en forma de tabla en el control de memoria programable del sistema.
El controlador borroso nos ha proporcionado mejores resultados que el método PI.
Lo desarrollamos e instalamos en un tiempo notablemente más breve que el necesario para hacer lo propio con un regulador PI.
Nos ahorramos,sobre todo,la complicada alternancia de reguladores,puesto que el borroso muestra un buen comportamiento oscilatorio y de regulación fina que se debe a su no linealidad intrínseca.
Es poco sensible a los cambios de peso o de cantidad inyectada,así como al desgaste.
Además,el regulador borroso tiene en cuenta,al contrario que el algoritmo PI,distintas magnitudes de medida a la vez ; en este caso,de presión y de flujo.
Como las reglas de un sistema borroso sólo simulan el comportamiento del regulador en sus líneas más generales,el establecerlas adecuadamente no es por lo común demasiado difícil.
Sin embargo,para la bondad del regulador borroso es de importancia decisiva la correcta elección de los elementos lógicos en que se basa - - disposición y forma de las funciones de pertenencia,operadores,procedimientos de inferencia y de desemborronamiento - -,puesto que la interpolación entre las reglas sueltas viene dada por ellos.
En parte,esa elección es,sin duda,muy laboriosa,pero aún así resulta mucho más sencilla que la introducción de magnitudes en la regla PI.
Con las soluciones borrosas que hemos ido obteniendo es posible una cómoda y rápida optimización informatizada.
La eficacia del nuevo sistema de mezcla ha sido comprobada tanto en numerosos experimentos como en la práctica.
La demostración de su utilidad es,pues,al mismo tiempo la prueba de que las técnicas de control borrosas también son aplicables a problemas en los que anden en juego la seguridad y,por tanto,vidas humanas.
(Heiko Knappe.
) Conocimiento por reflexión sísmica: Proyecto ECORS-Pirineos.
Se publicó recientemente el informe final del Proyecto Hispano-Francés ECORS para el estudio de la corteza terrestre de los Pirineos.
Debía realizar un perfil de sísmica de reflexión vertical profunda a través de la cadena montañosa y de estudios complementarios de apoyo.
La sísmica de reflexión vertical profunda es una técnica de alta resolución que permite obtener una imagen de los distintos reflectores de la corteza terrestre a lo largo de un perfil o línea.
Se trata,en esencia,del examen de la propagación de las ondas sísmicas que,generadas de forma artificial en la superficie,se recogen en ella después de penetrar en la zona de interés.
Los estudios de apoyo han consistido en perfiles de sísmica de gran ángulo y gravimétricos que atravesaban la misma zona y zonas próximas a las del perfil principal.
La sísmica de gran ángulo,de menor resolución que la de reflexión vertical permite obtener información sobre la localización en profundidad de los reflectores de la corteza.
A partir de los métodos gravimétricos se descubre la distribución de densidades en el interior de la corteza.
El proyecto se llevó a cabo,tras un año de preparativos,durante los otoños de 1985 y 1986.
Las instituciones participantes fueron,por parte española,la Comisión Interministerial de Ciencia y Tecnología,el Instituto Geográfico Nacional,el Instituto Tecnológico Geominero de España y REPSOL-exploración,y,por parte francesa,el Institut Français du Pétrole,el Institut National des Sciences de l'Univers,la Société Nationale Elf-Aquitaine y la compañía ESSO.
El perfil ECORS supuso,en su momento,una primicia mundial.
Por vez primera,una cadena montañosa era atravesada,desde sus extremos no deformados,por un perfil de sísmica de reflexión profunda.
Los primeros resultados aparecieron a finales de 1987.
Desde entonces y hasta la actualidad los expertos se han dedicado a analizar e interpretar los datos recogidos.
El proyecto ECORS-Pirineos se planteó para obtener una relación entre la estructura profunda de la cadena conocida hasta ese momento a partir de los datos de sísmica de reflexión de gran ángulo,y las principales características geológicas observadas en la superficie.
Esta relación permitiría aclarar las pautas evolutivas de la cadena pirenaica,formada en la orogenia alpina,que tuvo lugar en el período de tiempo comprendido desde hace 70 millones de años (m. a.) hasta hace 20 m. a. Interesaba conocer la prolongación en profundidad de los rasgos de la geología superficial y determinar su importancia en la evolución de la cadena.
El Proyecto ECORS-Pirineos se proponía,sobre todo,desentrañar la geometría profunda de la Falla Nordpirenaica (F. N. P.) y conocer las posibles diferencias entre las cortezas a ambos lados de ésta.
Esta falla,situada en el eje de la cadena,en la denominada Zona Axial,formada por materiales paleozoicos (- 580 m. a. - 290 m. a.),es a su vez el eje de la disposición en forma de abanico de materiales mesozoicos (- 248 m. a. - 65 m. a.) y paleozoicos.
Esta geometría divergente de los materiales,junto con el hecho de que su vertical geográfica coincide con un salto brusco de 15 km en el grosor de la corteza,hacían de la F. N. P. el límite de las placas Ibérica y Europea.
El perfil ECORS-Pirineos,de 250 km de longitud,atraviesa en dirección N-S la cadena pirenaica desde la Cuenca de Aquitania,cerca de Toulouse (Francia),hasta la Cuenca del Ebro,cerca de Balaguer (España).
La imagen sísmica obtenida permite resolver algunos de los problemas planteados hasta entonces,pero abre,a su vez,nuevas incógnitas.
Aunque la calidad sísmica de la línea es buena,en el centro de ésta,coincidiendo con la Zona Axial y la máxima topografía de la cadena,pierde resolución ; por culpa de ello,la estructura en esa zona no queda inequívocamente determinada.
Las acciones de apoyo,que aportaron información adicional sobre la distribución espacial de los materiales,fueron eliminando algunos modelos y favorecieron la creación de una imagen coherente de la estructura pirenaica.
La conclusión a que se llega es que la geometría y estructura actual de los Pirineos resulta del acercamiento de las placas Ibérica y Europea durante el Terciario.
Como consecuencia del acercamiento,los materiales mesozoicos y su zócalo (paleozoico) fueron comprimidos del orden de unos 100 km en la parte sur y 70 km en la parte norte,hasta formar los Pirineos.
Los materiales se dispusieron en la superficie,a ambos lados de la Falla Nordpirenaica,bajo una geometría en abanico,divergente en la parte superior.
Existe asimetría entre las placas Ibérica y Europea.
En la primera,la discontinuidad corteza-manto situada a unos 35 km de profundidad se hace más profunda desde el sur hacia el norte,para terminar por debajo de la Zona Axial en una estructura en cuña que alcanza,justo al norte de la traza en superficie de la F. N. P,una profundidad máxima de 60 km. La corteza europea,situada al norte del perfil,con 30 km de espesor,queda interrumpida hacia el sur a la altura de la F. N. P. Estos resultados sobre la geometría de superficie y profundidad,consecuencia del acercamiento de las placas,son aceptados por la mayoría de los expertos.
Los modelos discrepantes se basan en el lugar de emplazamiento de la corteza de la placa Ibérica,que se habría consumido en el proceso de convergencia.
La discrepancia entre los diversos modelos arranca de las hipótesis dispares sobre la situación y características de las cortezas en un estadio anterior a la convergencia de la cadena y de si este material fue o no engullido por el manto,se encuentra bajo la Zona Axial en la corteza o está distribuido en la corteza Ibérica más allá de la región pirenaica.
En resumen,si bien el Proyecto ha resuelto muchos de los problemas planteados sobre los Pirineos,queda aun un largo camino por recorrer en lo concerniente a la evolución seguida por la cadena.
Nadie debe pensar en abordarlo aisladamente.
La integración y comparación de los resultados obtenidos en el marco del Proyecto con los obtenidos de estudios similares en otras cadenas montañosas ayudará sin duda a despejar no pocas incógnitas.
(E. Suriñach,del departamento de geología dinámica,geofísica y paleontología,de la Universidad de Barcelona.
) Lavoisier y Cauchy: Entre escombros y polvo.
Cerca de 350 cartas de Lavoisier,cuyo rastro se había perdido hace bastantes años,acaban de unirse a los escritos guardados en los archivos de la Academia de Ciencias de París.
Lavoisier murió en el cadalso el 8 de mayo de 1794,víctima de la condena que recayó sobre el cuerpo de recaudadores (los " fermiers généraux ").
En 1791,la " Ferme générale ",sociedad encargada de recaudar los impuestos especiales sobre el tabaco y la sal - - la " gabelle " - -,queda abolida ; todos sus miembros,detenidos el 24 de diciembre de 1793.
Llevados ante el tribunal revolucionario,son condenados y ejecutados.
Lavoisier solicita,y se le niega,un plazo de gracia para concluir algunos trabajos.
Como comentaría Lagrange,costó un minuto segar su cabeza,pero habrían de pasar cien años antes de que apareciera otra de su talla,si es que ha aparecido.
A su muerte,Marie Anne Pierrette Paulze,su esposa,reunió escritos y correspondencia.
Lavoisier había desempeñado múltiples cargos oficiales: administrador de la " Régie des poudres et salpetres " (la administración de pólvoras y salitres)," fermier général ",miembro de la Academia de Ciencias y algún cargo público más.
Mostró interés por los asuntos sociales y educativos.
Sin hijos,Marie Anne Paulze legó los documentos a una sobrina,esposa de Léon de Chazelles.
En 1846,una vez clasificados,catalogados y numerados todos los documentos,Léon de Chazelles donó a la Academia de Ciencias la obra científica de Lavoisier,los escritos relacionados con su vida administrativa,sus reflexiones sobre la instrucción pública y parte de su correspondencia.
Se reservó la correspondencia privada y los documentos familiares.
Poco después,Jean Baptiste Dumas,secretario perpetuo de la Academia de Ciencias,comenzó a publicar la obra de Lavoisier,cuyo primer volumen apareció en 1862.
Edouard Grimaux,miembro de la Academia,profesor de la Escuela Politécnica y del Instituto de Agronomía,uno de los primeros biógrafos de Lavoisier,sucedió a Dumas al frente de la edición.
En 1955 el primer Comité Lavoisier confió a René Fric,un ingeniero químico,la tarea de publicar íntegramente la correspondencia de Lavoisier.
Se le franqueó el acceso al legado Chazelles ; mas,para mayor comodidad,se llevó a su casa de Clermont-Ferrand cartas pertenecientes a la Academia.
Esta circunstancia parece haber producido alguna confusión entre los dos fondos.
La familia Chabrol,heredera de los Chazelles,entregó,en 1991,la correspondencia privada del sabio que se habían ido reservando de generación en generación.
Con la donación,creyóse,se reunía todo el fondo Lavoisier,salvo ciertas cartas que Mme de Chazelles había vendido a la Universidad de Cornell.
La verdad es que faltaban también las sacadas por Fric de la Academia.
Pero,¿dónde estaban? ¿Perdidas para siempre? La edición de las cartas correspondientes al período de 1784 a 1788 presentaba lagunas.
A fines de 1992,un particular comunica que está en posesión... de cartas que parecen haber pertenecido a Lavoisier: una casa en demolición,una pared que se resiste,un armario empotrado abierto,un fajo de documentos que aparece... ; el hombre se aproxima,parecen cartas viejas... Tras largas investigaciones,se identifica al autor.
Se avisa al Comité Lavoisier.
Las cartas son restituidas.
La casa en demolición era la de R. Fric.
y la correspondencia hallada,la que había sacado de la Academia de Ciencias y andaba perdida.
A ella han vuelto 214 minutas (o borradores),87 cartas dirigidas al químico y 45 documentos diversos.
La tenacidad del historiador de la ciencia no deja cancha al azar.
Se ha encontrado el curso de Cauchy sobre ecuaciones diferenciales,del que se desconocía que hubiera llegado a la imprenta.
Pero Christian Gilain,historiador de la matemática,estaba convencido de que dormía en alguna parte.
Cauchy,el primero en establecer una teoría rigurosa de las ecuaciones diferenciales,fue profesor de la Escuela Politécnica.
Constaba que había enseñado su método a los estudiantes de segundo curso,método que había pervivido a través de un Entre la correspondencia de Lavoisier recientemente encontrada destaca una carta de Talleyrand: " Tengo todavía un servicio que demandaros,Señor,y que espero obtener de vuestra acostumbrada amabilidad.
Sería el de procurarme un ' gros ' o dos de platino fundido [I gros = 3,82 gramos] por aire deflogistizado (oxígeno) en su más perfecta pureza.
Lo necesito para establecer su verdadera elasticidad.
El portador me traerá vuestra respuesta o volverá a buscarla en el momento que os ruego le indiquéis.
Recibid anticipadamente mi agradecimiento y la seguridad de mi devoción más sincera e inviolable.
Talleyrand-Périgord.
" Figura también la respuesta de Lavoisier: " Señor: en estos momentos sólo dispongo de trozos muy pequeños de platino fundido con aire deflogistizado,pero lo tengo muy puro tratado por otros procedimientos.
Si éste satisface igualmente la petición,pondría dos ' gros ' a la disposición de la persona que tengáis a bien enviarme.
Os ruego que esto no sea antes del jueves por la mañana,pues he de someterlo a una última preparación.
Tengo el honor de ponerme a vuestra disposición con mi más respetuosa devoción.
M. Lavoisier.
" (Archivos de la Academia de Ciencias) escueto resumen que Cauchy escribió en 1836 y a través de las lecciones del Padre Moigne,quien en 1844 hizo una exposición inspirada en los cursos de Cauchy.
Otros matemáticos completaron y perfeccionaron la teoría de Cauchy en las postrimerías del siglo pasado.
Pero,¿cuál era el contenido original de la teoría de las ecuaciones diferenciales? ¿En qué dirección iba mejorando con los años el método? Para averiguarlo se precisaba documentación propia,en particular el texto de las lecciones impartidas en la Escuela Politécnica.
El curso de primero había sido publicado,pero el de segundo año no figuraba en las Obras completas.
Pero ese volumen,¿nonato?,recogía la única exposición sistemática personal de su teoría de las ecuaciones diferenciales.
En sus propias palabras: " El primer,y tal vez hasta aquí único,método para resolver un sistema cualquiera de ecuaciones diferenciales es,me parece,el que he publicado en las Lecciones de segundo año para la Escuela Real Politécnica.
" De lo que se desprendía que el curso había sido impreso.
A mayor abundamiento,el consejo rector de la Escuela Politécnica había solicitado,a los profesores de análisis,que,para mayor aprovechamiento del alumnado,redactasen al menos aquella parte de sus cursos que no figurase en las obras clásicas.
La pesquisa histórica condujo hasta las bibliotecas del Instituto de Francia y de la Escuela de Ingenieros de Caminos.
En la librería de ésta se encontró un ejemplar donde figuran las nueve primeras lecciones.
Había pertenecido al director del centro.
El que se halló en el Instituto de Francia es más completo.
Contiene,en forma de pruebas de imprenta,cuatro lecciones suplementarias ; se trata del ejemplar que Cauchy habría ofrecido a la Academia.
¿Por qué Cauchy nunca publicó íntegramente su curso de segundo año? Según Ch.
Gilain,la dirección de la Escuela Politécnica recomendaba que los profesores de análisis simplificaran sus métodos.
A Cauchy se le sugirió que cinco lecciones dedicadas a consideraciones generales sobre la integración " podían ser convenientes en la Facultad de Ciencias,pero no en la Escuela Politécnica,donde el tiempo apremia a los alumnos ".
Ante esa situación,Cauchy terminó por ceder,lo que afectó a la redacción de las lecciones.
¿Cómo pudo escapárseles a los editores de las Obras completas este documento impreso,conservado en una biblioteca de extenso fondo cauchiano? Se fiaron del plan de las obras establecido por el primer biógrafo y por la Academia.
Sólo la investigación de un aspecto teórico preciso podía hacer salir a los estudiosos de la trampa de un estudio falsamente exhaustivo de los archivos del matemático.
Manuscritos extraviados,correspondencias volatilizadas,obras olvidadas... Para asegurarse de que la correspondencia de los sabios contemporáneos no sufrirá estos azares,los archiveros intentan reunir los manuscritos de los científicos de renombre.
En Inglaterra,por ejemplo,un comité recoge,todos los años,de sus herederos,la correspondencia de una decena de los sabios más eminentes que hayan fallecido ese año.
Estos manuscritos son inmediatamente almacenados a la espera de que un historiador de la ciencia los examine algún día.
Aproximadamente el 3 por ciento de las rocas sedimentarias presentes en la corteza terrestre están constituidas por evaporitas,es decir,por asociaciones de minerales formados a partir de salmueras sometidas a evaporación.
Se conocen unos 200 minerales evaporíticos ; entre ellos,cloruros,sulfatos,boratos y carbonatos.
Los más abundantes son la halita (NaCI),el yeso (CaS04.
2 H20) y la anhidrita (CaS04).
Los minerales evaporíticos precipitan cuando las concentraciones de solutos en la salmuera rebasan sus respectivos productos de solubilidad.
Las salmueras son soluciones iónicas naturales altamente concentradas y de origen muy dispar.
Principalmente se forman por evaporación de agua marina (con una concentración salina de 35 gramos por litro y composición más o menos constante) o por evaporación de aguas continentales (con concentraciones y composiciones variables que dependen de la naturaleza del terreno en el área de drenaje).
También pueden generarse por redisolución de evaporitas preexistentes o por procesos de interacción entre el agua y las formaciones rocosas.
La evaporación constituye,sin embargo,el proceso más eficaz de formación y concentración de salmueras.
Ello nos indica que la aridez climática será un factor necesario en la génesis de las evaporitas.
Pero,por sí sola,no basta la aridez.
Se precisa también el llamado factor de restricción,o grado de aislamiento de la salmuera original,que evita su nueva dilución.
Sin restricción nunca llegaría a formarse una masa de salmuera adecuada para que,a partir de ella,cristalicen minerales evaporíticos.
Consideremos,por ejemplo,la superficie del mar frente a la costa pacífica americana,en la zona tropical árida.
Se registra allí una fuerte evaporación.
Sin embargo,nunca llegarán a precipitar minerales evaporíticos por causa de la misma,ya que el océano es un sistema abierto,sometido a un proceso de constante homogeneización debido a las corrientes marinas.
Para que puedan formarse evaporitas se requiere un dispositivo que aísle la salmuera y permita su concentración progresiva.
Dicho dispositivo tiene que posibilitar la entrada continua de agua marina que compense el agua perdida por evaporación.
La laguna costera Ojo de Liebre,situada en la Baja California mexicana,y la Bocana de Virrila en Perú son dos ejemplos que caracterizan el fenómeno.
Los anteriores ejemplos de formación de evaporitas marinas actuales explican los mecanismos del proceso,pero no las dimensiones de las grandes cuencas salinas desarolladas en el pasado geológico,como pudieran ser las de la cuenca del Zechstein norte-europeo,la cuenca devónica canadiense de Elk Point,la cuenca del Keuper europeo y la cuenca del Messiniense mediterráneo,de las que no hay equivalentes funcionales.
El momento geológico actual,en período postglacial,no es adecuado para el desarrollo de grandes cuencas evaporíticas marinas.
Por dos razones principales.
En primer lugar,el nivel de los océanos,relativamente alto y con tendencia general ascendente a causa de la progresiva disminución de la masa de hielos continentales polares.
Los períodos idóneos para la generación de cuencas evaporíticas marinas de grandes dimensiones han sido,de preferencia,regresivos.
En efecto,cuando baja el nivel oceánico,crece la probabilidad de que los mares (o brazos de mar) marginales queden separados (o restringidos) del océano global.
La segunda razón tiene que ver con la herencia topográfica de la reciente orogenia alpina,que implica la existencia de relieves bastante abruptos,con pocos mares epicontinentales.
Estos mares,cuando quedan restringidos,facilitan la formación de grandes cuencas evaporíticas (" gigantes salinos ").
EL Mediterráneo actual es un mar algo restringido,conectado con el Atlántico sólo a través del estrecho de Gibraltar.
El balance hídrico de la cuenca (entradas de agua de procedencia atlántica y entradas de agua de río,menos salidas de agua hacia el Atlántico y pérdidas de agua por evaporación) determina que su salinidad actual en superficie sea de 38 gramos por litro,esto es,aproximadamente un 3 por mil más elevada que la salinidad oceánica.
Las causas de ello vienen definidas por la interacción de los factores de aridez y restricción,antes mencionados.
Dada su latitud,la cuenca mediterránea presenta la aridez adecuada para generar evaporitas,pero no está suficientemente restringida.
Sí lo estuvo durante el período Messiniense (hace 5,5 millones de años),cuando se formaron salmueras cuya concentración posibilitó la precipitación incluso de sales potásico-magnésicas,altamente solubles.
Las evaporitas messinienses tienen una disposición concéntrica: las sales más insolubles y primeras en precipitar,como el yeso,lo hicieron preferentemente en la periferia de la cuenca,mientras que las más solubles (halita y sales potásicas) se situaron en sus partes centrales (principalmente bajo el actual fondo del mar Mediterráneo).
La formación de evaporitas también se da en medios continentales,siempre que existan las condiciones necesarias de aridez y restricción.
Ocurre así en los sistemas lacustres endorreicos de zonas áridas y semiáridas.
Cuando la entrada de aguas continentales (ríos,fuentes termales,cursos subterráneos) se compensa con las salidas por infiltración y evaporación,el balance hídrico de la cuenca lacustre se aproxima a cero y el lago salino podrá tener una masa de salmuera estable.
Si la fuerte evaporación determina un balance negativo,el lago evolucionará hacia modelos de tipo playa o tipo salar.
En estos últimos,la salmuera se encuentra,de manera casi permanente,a cierta profundidad bajo el fondo seco del lago,rellenando la porosidad del sedimento.
Las cuencas endorreicas intramontañosas constituyen los lugares idóneos para el desarrollo de lagos salinos,que suelen aparecer en depresiones de origen tectónico sin drenaje exterior.
La restricción a que aludíamos antes viene condicionada,en estos casos,por razones morfológicas y estructurales.
La aridez depende de la latitud (factor siempre importante) y del efecto de sombra pluviométrica generado por los altos relieves circundantes,que actúan como una trampa fría para las masas de aire húmedo,pues condensan el vapor de agua que queda atrapado en forma de nieve o lluvia en el exterior de la cuenca.
En virtud de ello,la depresión intramontañosa muestra una aridez anómalamente alta.
A diferencia de lo que sucede con las evaporitas marinas,los ejemplos continentales actuales abundan en número y diversidad y permiten desentrañar el origen de las evaporitas continentales fósiles.
Ejemplos de cuencas continentales actuales son las depresiones intramontañosas de la Cordillera Californiana (Estados Unidos) y del Altiplano andino (Bolivia,Argentina y Chile),las sebjas y chotts nord-saharianos (de Marruecos a Egipto) y los lagos de la meseta del Tibet.
Cuencas continentales fósiles son las que dieron lugar a los Yesos de Zaragoza (de edad miocena) y a los Yesos de Barbastro-PuentelarreN (de edad eoceno-oligocena),a los megacuerpos salinos neógenos de Arizaro y Pastos Grandes (Puna argentina) y a la Cordillera de la Sal (margen occidental del Salar de Atacama,norte de Chile).
Ya desde antiguo se ha intentado construir modelos que expliquen el comportamiento de las cuencas salinas,marinas y continentales,y den cuenta de las secuencias minerales observadas en las series evaporíticas fósiles.
Se empezó por idear experimentos de evaporación a escala de laboratorio,en condiciones controladas de temperatura y tasa de evaporación,utilizando agua de diversas procedencias.
Conviene recordar a este propósito los trabajos del químico italiano Usiglio,a mediados del siglo pasado,y los de los alemanes J. H. Van t'Hoff,D'Ans y Janecke y del ruso Valyashko,en el presente siglo.
Gracias a tales estudios se llegó a conocer el comportamiento básico de los sistemas evaporíticos ; aportaron,además,datos fisicoquímicos que sólo podían ser determinados experimentalmente.
A lo largo de la última década,varios grupos de investigación norteamericanos han simulado el proceso de evaporación-precipitaN mediante ordenador,aprovechando el desarrollo de programas de cálculo del comportamiento termodinámico de soluciones altamente concentradas (modelo de interacción iónica de Pitzer).
Charles Harvie y John Weare,de la Universidad de California en San Diego,trabajaron en las condiciones de equilibrio mineral salmuera.
Hans Eugster,de la Universidad Johns Hopkins,y Ronald Spencer,de la de Calgary,avanzaron en el terreno de la modelización de secuencias de precipitación a partir de agua marina o de mezclas de ésta con otras de diversa procedencia (río y fuentes termales).
En 1992,nuestro grupo desarrolló un programa que simula la evaporación y las entradas y salidas de agua y iones en solución en la cuenca.
Nos fundamos en el modelo conceptual de una cuenca evaporítica hidrológicamente abierta propuesto por Ward Sanford y Warren Wood,del Servicio Geológico de los Estados Unidos en Reston.
Nuestro programa,aplicable a las cuencas marinas y a las continentales,proporciona,en todo momento,datos relativos a la evolución composicional de la salmuera,así como sobre los minerales que precipitan y la cantidad en que lo hacen.
Por su propia naturaleza,los procesos evaporíticos promueven la concentración de sales muy solubles,la mayoría de las cuales reviste interés económico.
Los sistemas evaporíticos actuales permiten el beneficio de los minerales que precipitan con periodicidad estacional.
Citemos,además de la halita,carbonatos sódicos (natrón,trona y termonatrita),en los lagos alcalinos ; sulfatos sódicos (mirabilita y thenardita) y sulfatos magnésicos (hexahidrita y epsomita) en los lagos sulfatados,y boratos (bórax y ulexita) en lagos con fuerte influencia de manantiales termales.
En las salinas artificiales se explotan sales en régimen de cosecha.
Se parte de agua marina o de manantial (según se trate de salinas marítimas o continentales),produciéndose la evaporación en balsas someras.
En las salinas marítimas suele beneficiarse cloruro sódico,o sal común.
Consisten en un circuito de gran longitud compartimentado en balsas,donde se introduce agua marina por un extremo.
A lo largo del circuito se genera un gradiente de salinidad en el que van precipitando,de manera secuencial,carbonato cálcico,yeso y halita.
Aunque la configuración de los circuitos de la salina varía,la disposición clásica dibuja una espiral centrípeta.
Las salmueras más concentradas quedan así protegidas de la dilución en la zona central del dispositivo,en donde se sitúan los cristalizadores.
En algunos sistemas evaporíticos actuales se explotan directamente sus salmueras cuando éstas contienen elementos de especial interés,por ejemplo,potasio,boro y litio.
En las salmueras del Salar de Atacama,saturadas en cloruro sódico,se acumula litio.
La rentabilidad de su explotación es manifiesta,considerado el enorme cubicaje de salmuera existente y la dificultad de extracción de ese metal a partir de menas alternativas (micas de litio).
La presencia de boro y de litio en las salmueras de muchos salares andinos se atribuye a la lixiviación de los materiales volcánicos adyacentes al salar,causada por la circulación de agua termal en células convectivas,cuyas surgencias (en forma de manantiales termales o géyseres) vierten en los lagos.
Investigadores argentinos de la Universidad Nacional de Salta trabajan,desde hace algún tiempo,en proyectos de evaluación,mediante sondeo,de la potencialidad económica de las salmueras existentes en algunos salares de la Puna argentina.
El interés extractivo de las evaporitas fósiles radica en su contenido mineral.
La halita,el yeso y la anhidrita se emplean en la industria (química,textil y alimentaria),en metalurgia y en la construcción.
La importancia de sus yacimientos depende,pues,de su localización y de su pureza.
Las evaporitas de origen marino pueden ser mena de sales potásicas (silvita y carnalita) ; las continentales,de carbonatos sódicos,sulfatos magnésicos (epsomita),sulfatos sódicos (thenardita y glauberita) y boratos (bórax y colemanita).
Ejemplos de yacimientos potásicos son los de Elk Point (Canadá),los de la cuenca pérmica germano-polaca y los de la cuenca eocena surpirenaica española.
Entre los yacimientos de sulfato sódico se conocen,desde antiguo,los miocenos de la cuenca de Madrid,explotados ahora en Villarrubia de Santiago.
Yacimientos de boratos de grandes dimensiones son los turcos de Emet y Kirka,los estadounidenses de Boron y Ryan,así como el de Tincalayu,en la Puna argentina.
Los métodos de extracción habituales en rocas evaporíticas fósiles son la explotación en cantera,si nos referimos a los sulfatos de calcio y yacimientos de boratos,y el laboreo subterráneo o la disolución,en el caso de los cloruros.
De acuerdo con el espesor del recubrimiento,los sulfatos de sodio fósiles se benefician por laboreo subterráneo o por disolución en superficie.
Por su estabilidad geomecánica,las formaciones evaporíticas de halita que se hallen entre 500 y 1000 metros de profundidad pueden,además,convertirse en óptimos almacenes subterráneos de reservas estratégicas de hidrocarburos o en repositorios de residuos radiactivos.
Los materiales salinos se comportan plásticamente,siendo autosellantes y,por tanto,muy impermeables a los fluidos.
Los almacenamientos subterráneos de hidrocarburos (líquidos o gaseosos) resultan menos caros y dañan menos el entorno que los superficiales ; además son menos peligrosos debido al aislamiento absoluto de posibles comburentes.
La técnica de construcción de las cavidades para el almacenamiento subterráneo (de decenas a centenares de metros de diámetro) se basa en la disolución controlada desde la superficie,a través de sondeos.
La sal debe tener un contenido en insolubles (arcillas y sulfatos) inferior al 20 por ciento,para evitar la excesiva acumulación de residuos que entorpece el proceso de disolución.
Los hidrocarburos gaseosos (gas natural),almacenados a sobrepresión (por encima de los 100 bars),fluyen espontáneamente de la cavidad.
Los combustibles líquidos (crudos,o productos ya refinados,como gasóleo,o propano y butano licuados) han de extraerse bombeando salmuera saturada con sal,hacia el interior de la cavidad.
Cuando se bombea combustible hacia la misma,éste desplaza la salmuera volumen a volumen a través de un sistema de vasos comunicantes.
El mismo principio explica el interés que encierran las formaciones salinas fósiles para enterrar residuos radiactivos.
Al carácter impermeable y autosellante de la sal común hemos de sumar su estabilidad y conductividad térmica,altas,y la relativa facilidad de laboreo del macizo rocoso salino.
Ello permite la construcción de dispositivos que garanticen el aislamiento de los residuos durante un período de tiempo lo suficientemente largo como para que los radionúclidos que contienen rebajen su radiactividad hasta niveles tolerables.
Los residuos radiactivos pueden presentar una actividad gamma baja,media o alta.
En los casos de baja y media actividad bastará con garantizar el aislamiento del sistema por un período de 300 años,debido a que la vida media de sus radionúclidos no es superior a 30 años.
El combustible nuclear " quemado " en los reactores de las centrales posee una actividad gamma alta ; se trata de residuos que contienen los productos de fisión,transuránidos producidos por captura neutrónica del uranio,así como sus productos de desintegración radiactiva,con períodos de semidesintegración de hasta 24.000 años.
En este caso,debe garantizarse la integridad del almacenamiento durante un período de unos 100.000 años,tiempo suficiente para que haya perdido buena parte de su radiactividad.
La rotura de estanqueidad del almacenamiento implicaría la entrada de agua en el mismo causando,sucesivamente,la lixiviación de los residuos,el transporte de los radionúclidos a través de la geosfera y,si existe conexión hidráulica con los acuíferos superficiales,la introducción de los radionúclidos en la biosfera a través de ríos y lagos.
De ahí la importancia de garantizar tiempos de estanqueidad que aseguren la merma de radiactividad y,por ende,del riesgo para la naturaleza viva.
Las formaciones salinas cumplen,en principio,las condiciones necesarias,aunque deben considerarse otros factores complementarios.
Si nos ceñimos a los aspectos técnicos,convendrá hacer hincapié en su espesor,situación geológica (zonas de estabilidad tectónica),homogeneidad y bajo contenido en impurezas,fluidos intersticiales y su respuesta frente al incremento térmico y a la radiación.
Nuestro grupo trabaja ahora en la caracterización del macizo salino,con especial atención a la determinación y cuantificación de fluidos intersticiales,y al análisis del comportamiento de la sal cuando es expuesta a radiaciones gamma.
La sal común fósil contiene siempre pequeñas cantidades de salmuera (normalmente inferiores al I por ciento en peso),que se halla en los bordes de los granos de la roca salina o atrapada en inclusiones,dentro de los cristales de halita.
El fluido que rellena las inclusiones intracristalinas es,en la mayoría de los casos,vestigio de la salmuera primaria de la cuenca,a partir de la cual precipitó la halita.
Estas salmueras originales pueden perdurar indefinidamente,siempre que los procesos que ha sufrido la sal subsiguientes a su sedimentación no sean importantes.
Además de la primaria,hay otro tipo de salmuera situada en los bordes de grano de la halita,cuya composición es la de los fluidos que circularon por el macizo salino después de la sedimentación.
Componen esos fluidos las salmueras primarias,movilizadas durante la compactación del sedimento salino,las salmueras originadas por transformaciones minerales debidas a incremento de temperatura y las salmueras de procedencia externa.
Es importante conocer la cuantía y tipos de agua existentes en las formaciones salinas a la hora de abordar el emplazamiento de los residuos radiactivos.
Los residuos almacenados pueden,en virtud de la propia desintegración radiactiva,alcanzar temperaturas altas dentro de los bidones: entre 200 y 400 grados Celsius.
Debido al gradiente de temperatura,las salmueras atrapadas dentro de los granos de sal migrarán hacia los bordes de grano.
Desde allí,y por difusión térmica,proseguirán emigrando hacia el foco de calor,es decir,hacia el emplazamiento de los residuos,lo que puede acarrear problemas de corrosión.
Además de las inclusiones fluidas,los minerales hidratados que suelen acompañar a la halita son también fuentes potenciales de agua en las formaciones salinas elegidas para almacenamiento de residuos.
Para conocer la composición exacta de las salmueras atrapadas en la sal hemos utilizado un microscopio electrónico de barrido dotado de un analizador por dispersión de energías,al que se ha adaptado un dispositivo criogénico.
El montaje permite microanalizar,con precisión aceptable,inclusiones líquidas con tamaños de hasta 20 micrometros,previa congelación a temperatura de nitrógeno líquido.
Las formaciones salinas contienen,también,cantidades variables de diónador,podremos contar con una herramienta muy potente en la reconstrucción del funcionamiento de cuencas evaporíticas antiguas.
Un ejemplo de ello es nuestro trabajo sobre la evolución de la cuenca salina messiniense de Lorca,en el sureste español.
Dicha sal se depositó en una cuenca marina marginal conectada con un Mediterráneo ya hipersalino a causa del bloqueo parcial de la conexión atlántica a través del estrecho de Gibraltar.
Partiendo de una secuencia vertical de muestras tomadas de un sondeo a testigo continuo que atravesaba toda la formación salina (de unos 200 metros de espesor),se realizaron análisis sistemáticos de inclusiones de salmuera primaria.
Se recogieron datos de concentraciones en la salmuera de los iones Na +,K +,Mg2 +,Ca2 +,Cl - y so42 -.
A partir de los mismos,elaboramos los perfiles evolutivos verticales de concentración de las salmueras primarias en la cuenca salina.
Dichos perfiles muestran una importante diferencia de comportamiento en el tercio superior de la formación.
Los resultados experimentales debían ajustarse a los modelos teóricos de evolución de las salmueras sometidas a evaporación.
Para ello,utilizando el programa de simulación de evaporación y mezcla de aguas antes mencionado,acotamos los valores de sus variables para ajustarlos a dicha evolución.
La modelización permitió establecer la existencia de dos etapas en la génesis del cuerpo salino,proporcionando para cada una de ellas las entradas y salidas de fluidos hacia y desde la cuenca,así como su composición.
La parte inferior del cuerpo halítico de Lorca se formó en una cuenca con alimentación básicamente marina.
La cuenca se cerró al mar y las salmueras,a partir de las que precipitaron las sales del tramo superior de la serie de Lorca,fueron alimentadas por aguas de origen continental con reciclaje parcial de las sales marinas formadas con anterioridad.
Los resultados obtenidos son coherentes con los datos mineralógicos (sulfatos minoritarios acompañantes de la halita) y con la disminución progresiva del contenido en bromo del registro sedimentario.
a nueva metodología compara los resultados de simulaciones matemáticas con datos analíticos de salmueras fósiles atrapadas en la sal,y se revela como llena de posibilidades en el terreno de la cuantificación aplicada a los procesos sedimentarios y,por tanto,en la reconstrucción de la historia de la Tierra.
Arqueología e informática.
Pompeya como ejemplo.
Con la generalización del uso de los ordenadores personajes,comienzan los primeros tanteos experimentales de la aplicación de la informática a la arqueología.
Se pretende sacar el máximo partido,que la nueva técnica ofrece,de los innumerables datos aportados por las excavaciones y buscar entre los mismos posibles correlaciones,contradicciones o superposiciones,se trate de documentación figurativa (fichas de unidades estratigráficas,fotografías y croquis) o de objetos materiales (cerámicas,útiles y otros).
El recurso a la combinatoria o a la estadística permite obviar la parcialidad de las clasificaciones elaboradas en el pasado según criterios no homogéneos.
La aplicación de la nueva herramienta ayuda a resolver,entre otros,dos grandes problemas: por un lado,la estructura multidimensional de los fenómenos históricos,que pueden así analizarse desde la perspectiva de las diversas disciplinas,y,por otro,facilita la comprensión de los hallazgos de la arqueología.
El tratamiento integrado de imágenes,textos,gráficas,animación,sonido y simulación se convierte en un medio especialmente atractivo y eficaz para museos,exposiciones y parques arqueológicos.
Gracias a la introducción de los procesos informáticos,la arqueología ha avanzado bastante en el conocimiento de la topografía y el paisaje urbano de los núcleos antiguos.
Pero también se han visto beneficiados otros aspectos más escurridizos,como el descubrimiento de la sutil red de relaciones que,encerradas en los datos,nos explican el contexto social,cultural,político y religioso del momento.
Un buen ejemplo de cuanto venimos diciendo lo constituye el Proyecto Neápolis,aplicado desde el año 1986 a la ciudad de Pompeya y sus alrededores.
Bajo el patrocinio de IBM Italia y la FIAT Engineering,los resultados obtenidos hasta el momento forman parte de una gran exposición que,con el título de Redescubrimiento de Pompeya,se ha exhibido en Nueva York y Amsterdam.
En la primera fase del proyecto trabajaron más de cien especialistas entre arquitectos,arqueólogos,historiadores del arte y técnicos en informática,adscritos en buena parte a las universidades de Nápoles y Roma.
Creada una detallada base de datos geográfica,los mapas permitieron al equipo investigador confeccionar una estructura tridimensional en la que inscribir la información pertinente sobre la zona,lo que ha supuesto barajar múltiples entradas relativas a la ciudad: 12.000 entradas sobre objetos individuales,8700 imágenes digitales en color de los frescos y objetos y más de 22.000 fichas de excavación que incluían imágenes de 7000 páginas de los diarios de excavación.
Pero además,se ha reconstruido la historia del Vesubio a través del estudio de los materiales depositados en sus erupciones a fin de valorar sus riesgos potenciales ; se sabe,así,que la próxima erupción será de intensidad media,la mayor desde la de 1631,aunque sólo hay un 10 por ciento de posibilidades de que el viento lleve de nuevo las cenizas sobre Pompeya.
Ante la imposibilidad actual de un estudio global informatizado de todo el patrimonio mueble e inmueble de un yacimiento que lleva quinientos años de estudios y excavaciones,se ha optado por aplicar las técnicas informáticas a los materiales que corrían mayor peligro,es decir,las pinturas y los mosaicos,cuya reconstrucción electrónica es la fase siguiente a su tratamiento por ordenador.
Al espectador de la exposición se le ofrecen las claves para la lectura de la pintura pompeyana,desde la naturaleza de los pigmentos empleados hasta la explicación formal del significado de las representaciones,algunas de éstas tan complejas y difíciles como las del ritual dionisíaco de la Villa de los Misterios.
Con el concurso de la fotoplanimetría se ha reconstruido el área del Vesubio y de la antigua Pompeya,para situar adecuadamente las pinturas y los mosaicos de cada uno de los barrios,insulae (manzanas de casas) y paredes o vanos de las casas.
Esa técnica permite,además,abordar con mayor precisión la red viaria y la morfología del relieve en la antigüedad,es decir,cómo era Pompeya en el momento de la erupción.
(El Proyecto ha servido también para planificar el futuro de la zona en lo concerniente al abastecimiento de agua y el trazado carreteras.
) La vida política,social,cultural,profesional y económica de Pompeya,y en general de las comunidades vesubianas (Herculano,Stabia,Oplontis),en el siglo primero antes de Cristo,se recoge en gráficas y cuadros estadísticos.
Se han ubicado los edificios de culto de la ciudad para saber su intensidad y frecuencia así como los edificios públicos,como la basílica,sede de la actividad judicial,a fin de establecer la exacta relación entre arquitectura pública civil y arquitectura pública sacra.
Lo mismo se ha realizado con los edificios comerciales,diferenciándolos por sus actividades,como las tabernas,tiendas de venta de comida y bebida,y con los prostíbulos y edificios industriales,fueran de preparación del garum (conserva de pescado) o dedicados a la producción textil.
Todo ello,de obvio interés por sí mismo,presenta la importancia añadida de la comparación con otras ciudades y otros momentos históricos.
Sin embargo,pese a los casi treinta años que arqueología e informática llevan trabajando juntas,no podemos ofrecer todavía una visión risueña de ese maridaje.
Muchas veces,la documentación a la que se enfrenta el arqueólogo es tan fragmentaria y,a veces,de interpretación tan controvertida que parece que el uso de la informática no tendría sentido ; tal sería el caso,por ejemplo,de la clasificación de muchas monedas antiguas,dudosa por la identificación del tipo que lleva acuñado o por la fecha de su emisión y que depende de la libre capacidad interpretativa del catalogador.
Por ello,a los elementos usuales adoptados internacionalmente para la identificación de una moneda (tipo,leyenda,peso y posición de los cuños,ceca y autoridad emisora,procedencia y tipo de contexto) o sea,a los datos descriptivos,o,en términos informáticos,alfanuméricos,deberían ir orgánicamente acompañados de un archivo de imágenes,lo que explica que el uso del ordenador para catalogación en numismática haya sido muy reducido.
Aunque la reciente utilización de discos ópticos de 800 kilobytes como soporte para la memorización de las imágenes podría cambiar muy pronto el panorama.
Pero hay otro campo dentro de la arqueología en el que el ordenador puede desempeñar un gran papel.
Se trata del ámbito de la realidad virtual.
Este medio se utiliza en arqueología para prefigurar soluciones de cubiertas de edificios,de pavimentaciones y suelos,de restauraciones pictóricas y,en general,en todos aquellos casos en los que una intervención directa sobre el material arqueológico parece poco clara,ahorrando de esta forma medios,trabajo y riesgo.
La simulación es particularmente indicada en el campo de la restauración pictórica,en la que pigmentos y soporte son tan frágiles que no resisten una excesiva experimentación de las distintas técnicas de restauración.
Sólo cuando se han realizado todas las comprobaciones en el computador puede pasarse a la intervención directa sobre el monumento.
No menos interesantes son las posibilidades que la realidad virtual ofrece en el campo de la arquitectura.
Dentro del Proyecto Neápolis,podemos experimentar con un edificio,las termas de Stabia: rotación y despliegue de la estructura,incorporación de las partes destruidas,pero,sobre todo,entrar en el edificio,verlo desde dentro,captar sus volúmenes.
Los investigadores pueden también ensayar distintas hipótesis sobre las técnicas de construcción comparando los modelos informáticos con los restos de estructuras que la excavación ha proporcionado.
En este campo de la realidad virtual,el futuro inmediato,según los indicios,puede ser sorprendente.
En el reciente festival Immagina 93,celebrado en Montecarlo el pasado mes de febrero,la multinacional IBM presentó su restauración con imágenes informatizadas de la desaparecida abadía de Cluny.
Las aplicaciones prácticas de la realidad virtual y el ciberespacio en arqueología permitirán crear parques arqueológicos en los que podamos sentirnos verdaderamente transportados a cualquier época del pasado.
Tolerancia inmunitaria: Células dendríticas del timo.
Los mecanismos de defensa de los que dispone el organismo para enfrentarse a infecciones víricas,bacterianas o parasitarias,o para combatir la proliferación de células tumorales,residen en el potencial de un tipo de células sanguíneas,los linfocitos,para reconocer estructuras moleculares extrañas,es decir no propias (antígenos extraños o no-propios),y distinguirlas de las moléculas propias (antígenos propios).
Este fenómeno se basa en la existencia de un extenso repertorio de receptores antigénicos localizados en la superficie celular,que confieren a cada linfocito la capacidad de reconocimiento específico de un solo tipo de antígeno.
La enorme diversidad de especificidades antigénicas de las poblaciones celulares linfocitarias se genera como resultado de un mecanismo complejo de reordenación de los genes que determinan las cadenas polipeptídicas que componen los receptores antigénicos de los linfocitos B (inmunoglobulinas) o de los linfocitos T (" receptores de las células " o RCT).
Este proceso de reordenación ocurre durante la diferenciación de los linfocitos B en la médula ósea,y de los linfocitos T,cuyo origen es el timo.
Debido a que la reordenación de los genes de las inmunoglobulinas y de los RCT tiene lugar al azar,las cadenas polipeptídicas resultantes conforman no sólo receptores antigénicos capaces de unirse eficazmente a un antígeno extraño,y combatir por tanto una infección vírica o bacteriana o un proceso canceroso,sino también receptores que pueden reconocer con alta afinidad estructuras moleculares propias del organismo.
Como consecuencia,se generan linfocitos autorreactivos con capacidad potencial para desencadenar una respuesta inmunitaria contra antígenos propios,que en determinadas circunstancias podría generar una enfermedad autoinmune ; ocurre así en la esclerosis múltiple y la artritis reumatoide,condiciones en las que los linfocitos del propio paciente reconocen como extraños la proteína mielínica básica (PMB) y el colágeno tipo II,respectivamente.
Sin embargo,el organismo posee mecanismos responsables de la inactivación o eliminación de los linfocitos autorreactivos,es decir,capaces de inducir un estado de tolerancia hacia los antígenos propios.
nicamente en el caso de que las células autorreactivas escapen a esos sistemas de control se desarrollará una enfermedad autoinmune.
En los últimos años se ha llevado a cabo un gran esfuerzo para comprender el proceso de generación de tolerancia,y esta labor de investigación ha dado lugar a importantes resultados,particularmente en lo que concierne a los mecanismos de control de los linfocitos T (células T) autorreactivos.
Estos se basan,sobre todo,en la eliminación de los clones de células T durante su diferenciación,por un proceso de selección que requiere la confrontación entre los RCT de las células T y las moléculas codificadas por el complejo principal de histocompatibilidad (MHC) presentes en la superficie de células estromáticas especializadas del timo.
Distintas aproximaciones experimentales - - como el estudio de la reconstitución tímica en ratones irradiados o inmunodeficientes,o el desarrollo de ratones transgénicos - - han demostrado que las principales células estromáticas responsables de la inducción de tolerancia intratímica de las células T son las células dendríticas del timo.
Las células dendríticas derivadas de la médula ósea,que se localizan en el timo y en las áreas T de los órganos linfoides periféricos - - como el bazo o los ganglios linfáticos - -,donde reciben el nombre de células interdigitantes,y en diversos tejidos epiteliales,donde son denominadas células de Langerhans.
Las células dendríticas periféricas están especializadas en la presentación antigénica y la activación de las células T maduras ; sin embargo,su función en el timo tiene que ver con la inactivación de los clones T autorreactivos.
El proceso de selección que realizan las células dendríticas en el timo implica la presentación de péptidos derivados de antígenos propios o no propios,en el contexto de moléculas MHC,a las células T en el curso de su diferenciación intratímica.
Las células T cuyos RCT reconocen péptidos propios o no propios con desmesurada afinidad,reciben de las células dendríticas una señal de inducción de muerte celular por apoptosis (muerte celular programada) que determina,por tanto,la selección negativa de los clones T autorreactivos.
La investigación reciente se ha concentrado en el estudio de la señalización celular durante la selección negativa,pero escasean los datos sobre la diversidad y la naturaleza de los antígenos hacia los cuales se induce la tolerancia intratímica.
En efecto,aunque muchos admiten que,durante la diferenciación intratímica,se elimina una gran variedad de clones T cuyos RCT reconocen moléculas o antígenos propios como los presentes en la superficie de distintos tipos celulares,o como los componentes de la matriz extracelular o las proteínas séricas,tan sólo se ha descrito la deleción intratímica de un número reducido de clones T: en particular,clones T anti-moléculas MHC,anti-superantígenos bacterianos o de tipo Ml,anti-antígeno H-Y,anti-VCML (virus de la coriomeningitis linfocítica) y anti-hemoglobina.
Si bien el análisis del repertorio de las células T autorreactivas eliminadas en el timo es sumamente difícil,pues requeriría el desarrollo de ratones transgénicos con RCT específicos para cada antígeno posible considerado,el estudio del origen de las células dendríticas del timo reviste particular interés a la hora de conocer el potencial de deleción clonal de estas células.
En efecto,tradicionalmente se ha considerado que las células dendríticas tímicas tienen un origen extratímico y emigran de los órganos linfoides periféricos al timo convertidas en células maduras y funcionales.
De acuerdo con esta hipótesis,y teniendo en cuenta la función primordial de las células dendríticas periféricas como células captadoras,procesadoras y presentadoras de antígenos,éstas podrían transportar en el curso de su migración al timo,y consiguientemente presentar a las células T en curso de diferenciación,además de antígenos propios intra - y extratímicos,antígenos exógenos extraños (derivados de virus,bacterias o parásitos).
En virtud de ello,los clones T específicos correspondientes podrían ser eliminados,induciéndose no sólo un estado de tolerancia hacia los antígenos propios,sino también hacia los antígenos exógenos,de lo cual resultaría una incapacidad transitoria de defensa contra éstos y,por tanto,el desarrollo de un proceso patológico.
Sin embargo,los estudios llevados a cabo por Ken Shortman y el autor en el Instituto Walter and Eliza Hall de Melbourne,sobre el origen de las células dendríticas del timo de ratón,utilizando sistemas de reconstitución tímica tras irradiación,han demostrado que estas células se originan en una población intratímica de precursores con capacidad de formación tanto de células T,como de células dendríticas.
Estos datos sugieren,por un lado,que existe una estrecha relación entre ambos linajes celulares,de acuerdo con estudios anteriores que habían demostrado que las células dendríticas expresan moléculas de membrana consideradas marcadores de células T. Por otro,implican que el potencial de deleción de clones T de las células dendríticas tímicas está limitado a su capacidad de presentación de antígenos,principalmente propios,presentes en el seno del microambiente tímico.
Puesto que el acceso al parénquima tímico de antígenos extratímicos está restringido por la barrera hemato-tímica,debe haber otros mecanismos periféricos de tolerancia que garanticen la inactivación de las células T autorreactivas cuyos RCT reconocen antígenos propios extratímicos,y que podrían ocasionar lesiones autoinmunitarias.
El reciente descubrimiento de diferentes subpoblaciones de células dendríticas en el timo - - que pueden ser aisladas por su expresión diferencial de moléculas de membrana específicas - - respaldan la esperanza de que en un futuro más o menos cercano lleguemos a desentrañar los mecanismos implicados en la transmisión de las señales responsables de la selección negativa de las células T autorreactivas,lo que abrirá el camino para métodos terapéuticos más eficaces en el tratamiento de las enfermedades autoinmunitarias.
Entre los diversos monumentos que inmortalizan la personalidad de Hertz,muerto hace ahora cien años,sobresale la torre de televisión dedicada " A Heinrich Hertz,hijo de la ciudad de Hamburgo ".
Allí nació el 22 de febrero de 1857,primogénito de una familia acomodada.
Su padre,Gustav Ferdinand Hertz,fue senador y presidente de la audiencia ; descendía de banqueros y joyeros judíos,procedentes de Hildesheim,que se habían convertido a la fe luterana.
Su madre,Anna Elisabeth Pfefferkorn,era hija de un médico militar de Frankfurt.
A los tres años,cuenta la madre en sus Recuerdos," Heins " le sorprendió cierto día con su prodigiosa memoria,recitándole el centenar de fábulas que ella le había ido leyendo.
Desde los seis hasta la preparación para la confirmación luterana a los 15,frecuentó una escuela privada,dirigida por Wichard Lange.
Los dos años siguientes recibió clases particulares en casa,preparándose para entrar en el Instituto humanístico y superar un año más tarde el examen de bachillerato.
Demostraba dotes sobresalientes para todo (menos para la música y el canto).
Las clases particulares en casa incluían,después de cenar,las de un maestro tornero.
Con el torno profesional que sus padres le regalaron construyó sus primeros aparatos de física,entre ellos un espectroscopio.
En el Instituto será el primero en griego.
Incluso en árabe hizo sus progresos.
Pero sus planes eran claros: " Si logro,escribe,superar el examen de bachillerato,pienso ir a Frankfurt del Main,y trabajar allí un año con un maestro de obras prusiano,como es obligatorio para poder hacer más tarde el examen de estado en la especialidad de ingeniería ; sólo en el caso de que se viera que no soy apto para ese oficio,o que mi afición por las ciencias creciera aún más,me dedicaría a las ciencias puras.
Que Dios me ayude a abrazar aquello para lo que sea más capaz.
" Superado su examen de bachillerato,se traslada por un año a Frankfurt.
Allí trabajó como delineante,primero en la Inspección General de Arquitectura y más tarde a las órdenes de ingenieros de caminos,que le encargaron,por ejemplo,elaborar los planos del gran puente superior sobre el Main.
El trabajo le deja muchas horas libres,que aprovecha para proseguir sus lecturas de los clásicos,sin dejar el árabe.
Estudia matemáticas,asiste a clases de química y se pone en contacto con la Sociedad de Física.
Lee libros de física,especialmente el Tratado de Física Experimental d e Adolf Wullner.
" De nuevo he cobrado gran placer por las ciencias,con la lectura del Wullner.
Pero no me persuadiré que deba renunciar a lo que me había propuesto como meta más hermosa " (ser ingeniero).
El semestre siguiente,se trasladó al Politécnico de Dresde.
Divide las clases de allí en interesantes y aburridas,entre las primeras menciona la historia de la filosofía y las matemáticas,especialmente las que le da Leo Konigsberger sobre cálculo diferencial e integral y mecánica analítica.
El curso siguiente,1876 - 77,hace el servicio militar en Berlín.
Desde allí escribe a sus padres: " He cumplido veinte años,y por así decir,tengo a mis espaldas un tercio de mi vida.
Y me siento tan débil e insignificante y tan incapaz de hacer algo... Cada día me muestra lo inútil que soy en este mundo.
Sé algo de griego y algo de matemáticas y algo de esto y aquello o aquel apuro? ' que yo pudiera dar entonces un paso al frente,es algo que aún no me ha pasado nunca.
Así que tengo puesta en el futuro una esperanza mayor que la satisfacción del pasado.
" En octubre de 1877 se traslada a Munich,dispuesto a estudiar ingeniería.
A fines de mes,tiene ya decididas las asignaturas que seguirá en la Escuela Técnica Superior.
No parece demasiado convencido,sin embargo: " He estado también dando vueltas a lo que he dicho con frecuencia en el pasado: que preferiría ser un científico eminente a ser un ingeniero eminente,pero preferiría ser un ingeniero mediocre a ser un científico mediocre.
" Sin excluir la otra altemativa,le confiesa a su padre: " Si me dices que debo estudiar ciencias,lo recibiré como un gran regalo tuyo.
" Ante la respuesta paterna afirmativa,se matriculó inmediatamente en la Universidad.
Allí encontró a Philipp von Jolly,quien le trazó un plan a su medida: estudiar matemáticas y mecánica en sus fuentes históricas,como la Mécanique Analytique de Lagrange,la Mécanique Céleste de Laplace y la historia de las matemáticas de Jean Étienne de Montucla.
Seguirá algunas clases: laboratorio de física,laboratorio de química y matemáticas superiores (funciones elípticas y de variable compleja).
Estudia de nuevo el Tratado de Física Experimental de Wullner.
Lee también las Acta Eruditorum,la vieja publicación mensual científico-literaria fundada en Leipzig en 1682,y al ver los descubrimientos físicomatemáticos realizados a finales del siglo XVII,comenta: " ¡Lástima no haber vivido cuando se podía descubrir algo realmente nuevo! " Acabados esos dos semestres,Jolly le aconsejó continuar en otra universidad: Leipzig,Berlín o Bonn.
Optó por Berlín,construyó una brújula tangente enormemente sensible.
Llega a Berlín en octubre de 1878 y se matricula de su tercer semestre de física.
Asiste a una clase de Gustav R. Kirchhoff,famoso ya por su ley de radiación.
Sólo sigue otra clase teórica de dinámica analítica.
Se entusiasma muy pronto con el laboratorio de física,el mejor de su época,montado en 1870 por Hermann von Helmholtz.
Lleno de admiración describe a sus padres su propio laboratorio.
dotado de herramientas,soplado de vidrio,gas,agua corriente y electricidad procedente de baterías instaladas en otra habitación,para que no moleste el vapor de los ácidos ; como detalle,los galvanómetros de suspensión se apoyaban en repisas de hierro empotradas en la pared y,para evitar las vibraciones que sobre ellos pudieran ocasionar los coches de caballos de la Dorotheenstrasse,se habían levantado pilastras de piedra maciza desde los fundamentos del edificio.
Pero echa de menos su brújula.
Ese año de 1870,Helmholtz publicó su estudio sobre la ' ¡selva sin senderos " de las diversas teorías electrodinámicas,desarrolladas a partir de las investigaciones de Ampere.
Tales teorías explicaban la fuerza ejercida entre corrientes y la inducción de nuevas corrientes mediante un potencial electrodinámico instantáneo,del mismo modo que las fuerzas de Coulomb entre cargas y la inducción de nuevas cargas se habían explicado mediante un potencial electrostático.
El estudio de Helmholtz resolvía el enigma de que pudieran concordar con la experiencia teorías basadas en expresiones matemáticas muy diferentes del potencial electrodinámico,demostrando que todas esas expresiones matemáticas dan el mismo resultado al ser integradas para obtener el potencial de circuitos eléctricos cerrados los únicos utilizados hasta entonces en los experimentos.
Helmholtz presentaba también la reciente teoría de Maxwell,de la que sólo se habían publicado entonces tres artículos.
Pero,por más que la teoría de Maxwell,basada en las experiencias e intuiciones de Faraday,constituya una típica teoría de campos,con acciones transmitidas por contiguidad y en el tiempo,Helmholtz la reinterpretaba como una teoría electrodinámica más,asignándole una nueva expresión del potencial que daba cuenta de la formación de ondas de polarización en el seno de los dieléctricos del tipo de las ondas de luz obtenidas por Maxwell.
Esta reinterpretación de Helmholz formada,la concepción británica de Faraday y Maxwell,y sólo así comenzó a conocerla Hertz.
Por otra parte,el estudio de Helmholtz renovaba las antiguas disputas con Wilhelm Weber,quien en su escuela de Gotinga había desarrollado desde 1847 la más bella de las teorías electrodinámicas.
Con estas nuevas disputas se relaciona el premio ofrecido por la universidad al alumno que mostrase que la electricidad " se mueve con masa inerte en los cuerpos (conductores) ".
(Hoy diríamos que el cociente mle entre la masa y de la inducción de corrientes,puede estudiarse mediante medidas de precisión de la intensidad de las corrientes autoinducidas en un circuito en el momento de abrirlo o cerrarlo.
Helmholtz le propuso a Hertz atacar el problema,le proporcionó bibliografía y le ofreció el laboratorio.
En noviembre de 1878 Hertz está sumergido en el problema y el 4 de mayo del año siguiente pudo entregar,bien lacrado,su trabajo " Experimentos sobre la determinación de un límite superior de la energía cinética de la corriente eléctrica ".
Micobacterias: Porinas de la pared celular.
La imagen típica que se tiene de una bacteria grampositiva es la de una célula procariota envuelta por una espesa capa de peptidoglucano que puede alcanzar hasta 80 nanómetros de espesor,en tanto que la bacteria gramnegativa presenta una capa de peptidoglucano mucho más fina,rodeada por una membrana externa.
Pero hay bacterias grampositivas que tienen lípidos en la pared celular ; pertenecen principalmente al grupo de las micobacterias.
Entre las micobacterias se encuentran patógenos muy importantes para el hombre.
Recordemos Mycobacterium ruberculosis y M. Ieprae,agentes,respectivamente,de la tuberculosis y la lepra,además del complejo M. avium-intracelular ; éste,junto con M. tuberculosis,constituyen las infecciones sistémicas bacterianas más frecuentes en enfermos con sida.
Si exceptuamos el caso de M. Ieprae,el número de infecciones producidas por micobacterias está aumentando en la mayoría de los países,industrializados o en vías de desarrollo.
La terapia para tratar estas infecciones es lenta y no son muchos los fármacos antimicrobianos disponibles.
Las micobacterias oponen resistencia natural a la inmensa mayoría de antibióticos.
Por si fuera poco,crece el número de cepas resistentes a los antimicrobianos en uso.
A la baja permeabilidad de su pared celular se ha atribuido la resistencia micobacteriana a los antibióticos.
No obstante ser grampositivas,las micobacterias tienen una gran cantidad de lípido en la pared celular ; y se supone que la parte lipídica sería la responsable de la baja permeabilidad de la pared.
Aunque no se conoce bien la organización de la capa lipídica,sí se sabe que difiere de la membrana citoplasmática de las células,así como de la membrana externa que encontramos en las bacterias gramnegativas.
El principal componente lipídico de la pared celular está formado por los ácidos micólicos que se hallan unidos por enlaces covalentes al peptidoglucano a través de la unión de arabinanos y galactanos.
Además de éstos,hay otros lípidos que están libremente asociados a la parte lipídica de la pared de las micobacterias.
La barrera hidrofóbica impuesta por los lípidos ejerce el mismo efecto que la membrana externa de las bacterias gramnegativas y actúa como un potente obstáculo contra la permeabilidad.
La baja permeabilidad de la pared celular tiene un efecto determinante en la acción de los antibióticos en las micobacterias.
El flujo de antibiótico desde el medio exterior hacia sus dianas naturales en el interior celular es muy bajo y,en consecuencia,no basta la concentración de antibiótico en la célula para inhibir el crecimiento de ésta o provocar su muerte.
Para poder desarrollar nuevos fármacos antimicrobianos que actúen sobre las micobacterias,importa conocer los sistemas de transporte de la pared celular.
La investigación reciente del autor,Vincent Jarlier,de la Universidad de París Vl,y Roland Benz,de la Universidad de Wurzburgo,han demostrado que existen canales hidroMicos en la pared celular de las micobacterias.
Estos canales,constituidos por proteínas,permitirían el paso de moléculas de pequeño tamaño por difusión pasiva a través de la porción lipídica de la pared celular.
Estas proteínas que atraviesan la pared celular micobacteriana comparten algunas propiedades con las porinas que se encuentran en la membrana externa de las bacterias gramnegativas,mitocondrias y cloroplastos y que son responsables del transporte pasivo a través de la membrana externa presente en estas células y orgánulos celulares.
Pese a ser grampositivas las micobacterias,la existencia de porinas y de una gran cantidad de lípido sugiere que la parte lipídica de la pared celular se halla altamente organizada,probablemente a la manera de las membranas celulares.
La porina presente en la pared celular micobacteriana dispone de un solo canal lleno de agua,que permite el paso de moléculas de un lado a otro de la pared.
Aunque el poro mide unos dos nanómetros de diámetro,estas proteínas no abundan en la pared celular,razón por la cual,y no obstante ese tamaño notable del poro,la permeabilidad de la pared celular es baja y las micobacterias oponen tanta resistencia a los antibióticos.
Los estudios desarrollados posteriormente por Trías y Benz,descritos en The Journal of Biological Chemistry,en marzo del año pasado (los resultados precedentes aparecieron en Science),han demostrado que este canal tiene cargas negativas,lo que significa que los productos dotados de carga positiva pueden atravesar con mayor rapidez la pared celular.
La apertura del canal está regulada por el voltaje: bastan pequeñas diferencias de potencial entre ambos lados del canal para cerrarlo y contribuir así a la baja permeabilidad de la pared celular.
La pared celular de las micobacterias es la menos permeable de los procariotas,se trate de bacterias grampositivas o gramnegativas.
No obstante,y como ya apuntaban Cranefield y Hoffman en 1968,el ECG convencional proporciona poca información directa sobre la actividad cardíaca,obviando algunos episodios cruciales de la misma.
Dichos eventos (del orden del microvolt) se hallan enmascarados en la línea basal.
El microscopio óptico,que permite sólo observar los aspectos generales,es al microscopio electrónico,que descubre pormenores específicos.
Podemos,pues,definir la electrocardiografía de alta resolución (EAR) como la técnica gracias a la cual podemos detectar y analizar señales electrocardiográficasN de baja amplitud,imposibles de detectar sobre la superficie del cuerpo por los procedimientos habituales.
His-Purkinje,que proporciona información sobre la localización de problemas en la conducción atrioventricular.
Hacia mitad de la década,ElSherif,Scherlag y Lazzara apuntan la hipótesis según la cual potenciales de pequeña amplitud observados a partir del final del complejo QRS (denominados usualmente pospotenciales o potenciales tardíos) pueden identificar a los pacientes con alto riesgo de taquiarritmias ventriculares (que constituye el marcapasos natural del corazón,fijando su ritmo de activación),prepotenciales de la onda P y la señal del nodo auriculoventricular.
Compete a la EAR obtener una señal de muy baja amplitud (necesitada quizá de ser amplificada centenares de miles de veces) con la mejor razón de señal a ruido (S / N) posible.
Las fuentes de ruido son diversas,pero pueden agruparse en cuatro grupos primarios: la señal de 50 hertz de la red de alimentación el ruido de interfase entre electrodo y piel ; el propio amplificador,y los potenciales electromiográficos,debidos a la actividad muscular (respiración y otros).
Las fuentes de ruido superan en amplitud a la propia señal útil,y sus contenidos frecuenciales se hallan solapados con los de ésta,haciendo inviable la posibilidad de eliminación por filtrado.
Se han desarrollado diversas técnicas de mejora de la S / N. El promediado de señal constituye una de las habituales.
Usada inicialmente para distinguir del ruido las trocardiografía en 1963 por Hon para extraer el ECG del feto del de la madre.
En 1973,recurrieron a ella los grupos de Flowers,Stopczyk y Berbari en registros del haz de His.
El promediado de señal es una técnica de procesado,digital casi siempre,en la que se mejora la relación S / N de formas de onda que se repiten periódicamente por medio de una sumación sucesiva.
Al promediar,las componentes aleatorias de la onda (el ruido),se reducen en un factor n (siendo n el número de pulsos promediados),mientras que las componentes determinísticas (la señal deseada) permanecen inalteradas.
El método requiere una referencia,que puede ser un estímulo sensorial en el caso de potenciales evocados cerebrales o la onda R en el caso del ECG,y es óptimo si el ruido no guarda correlación alguna,la señal útil es invariante con el tiempo y el intervalo entre el punto de referencia y el de interés permanece constante.
En los últimos años,la técnica de promediado de señal se ha estandarizado,es de marcada importancia como marcador de arritmias severas y muerte súbita en pacientes que han sufrido un infarto.
El estudio del ECG-EAR promediado se realiza en el dominio del tiempo y en el de la frecuencia,mediante un registro simultáneo de las tres componentes X-Y-Z del vector cardíaco.
Se extraen por procesado digital de la señal una serie de parámetros complementarios que permiten un diagnóstico de presencia o ausencia de potenciales tardíos.
El proceso está ampliamente difundido,y diversos fabricantes han desarrollado dispositivos para su obtención.
La principal limitación de la técnica de promediado de señal,consustancial con ella misma,es la imposibilidad de detectar cambios dinámicos latido a latido,como ocurre algunas veces con los pospotenciales comentados.
Esto ha dado origen a otra rama de la EAR,paralela a la técnica de promediación,y que suele denominarse EAR pulso a pulso.
Como su nombre indica,el estudio del registro de alta resolución obtenido se realiza con cada pulso individual.
La contaminación atmosférica,uno de los obligados tributos del progreso industrial y técnico,arrastra consecuencias negativas para los individuos,los ecosistemas y el entorno.
Con el efecto de invernadero,del que no es inseparable,la lluvia ácida constituye una forma habitual de esas alteraciones.
El adjetivo " ácida " que acompaña al sustantivo " lluvia " alude a su concentración de iones de hidrógeno,a su pH. A tenor del mismo,dividimos las soluciones en básicas,neutras y ácidas.
El valor de referencia es el pH del agua,solución neutra prototípica,cifrado en 7. Por encima de este valor hablaremos de una solución básica y ácida,por debajo ; de acuerdo con ello,una solución con un pH 2 será más ácida que otra con un pH 5.
Por precipitación ácida hemos de entender una deposición sólida (nieve o granizo),lluvia o una agrupación de gotitas de niebla con pH menor que S,6. Este límite se establece al admitir que la concentración media y natural del dióxido de carbono es de 340 partes por millón (ppm),que corresponde a un pH de 5,6 a la temperatura de 15 °C para la lluvia no contaminada.
Las precipitaciones ácidas pueden producirse lejos de los focos de contaminación,asociadas a fenómenos de transporte transfronterizo.
Este puede ser a escala planetaria,lo que,para latitudes medias,quiere decir que abarca distancias de 10.000 km y una duración temporal superior a 100 horas ; o a escala sinóptica,cuya dimensión espacial es del orden de 1000 km y su duración está comprendida entre 10 y 100 horas ; hay también una mesoescala propia de fenómenos con duración entre 10 y 1 horas y dimensiones en torno a 100 km. Junto a estas precipitaciones se dan otras que descargan en las proximidades de las fuentes emisoras,cuya dimensión espacial está por debajo de los 100 km y no duran más de una hora.
Se las denomina convectivas y se originan a partir del fuerte calentamiento solar de zonas del suelo,que genera corrientes verticales ascendentes.
Tales lluvias locales pueden ser de carácter fuertemente ácido (hasta por debajo de 3,6).
Las precipitaciones ácidas de cualquier escala dejan sentir su influencia en los ecosistemas acuáticos,ya que las especies están adaptadas a unos límites muy estrictos del pH del medio,y terrestres,por la alteración que provocan en el estrato superficial.
(La mayoría de las cosechas,por ejemplo,requieren un pH superior a 4,5.
) Aunque de menor importancia,se han registrado también precipitaciones de carácter altamente básico,con no menores efectos negativos.
Entre los compuestos que confieren el carácter ácido a las precipitaciones destacan el ácido sulfúrico (H2SO4) y el ácido nítrico (HNO3),que se forman en la atmósfera a partir de precursores químicos emitidos desde tierra por causas naturales o artificiales: el dióxido de azufre (SO2) y óxidos de nitrógeno (NOX),respectivamente.
La energía necesaria para las reacciones de oxidación involucradas la aporta la radiación solar,en particular la de onda corta,comprendida en la banda de 240 a 300 nanómetros.
Además,según sea la temperatura a la que se realicen las reacciones,así serán la cinética y el orden de las mismas y las constantes de disociación de los compuestos ; es decir,la temperatura influirá en el tiempo que tardan las reacciones en producirse.
EL SO2 es emitido a la atmósfera tanto por fuentes naturales como antropogénicas,siendo la aportación de las primeras superior al 50 %.
El 95 % del SO2 presente en la atmósfera se genera dentro de ella a partir de la oxidación del sulfuro de hidrógeno procedente de la descomposición de la materia orgánica.
Las fuentes artificiales de SO2 son,fundamentalmente,la combustión de carburantes fósiles (carbón y fueloil) y la fundición de minerales.
Los óxidos de nitrógeno han recibido menor atención.
Sin embargo,en Europa y en los Estados Unidos su contribución a las deposiciones ácidas es mayor que la atribuible al dióxido de azufre,ya que,por ejemplo,en las centrales térmicas y en los ambientes urbanos la conversión de los óxidos de nitrógeno es más rápida que la de los óxidos de azufre.
El monóxido de nitrógeno (NO),precursor principal del ácido nítrico,procede directamente,en un 80 %,de fuentes naturales,mientras que casi todo el NO2 atmosférico es de origen antropogénico.
La actividad bacteriana que degrada los compuestos nitrogenados representa la principal fuente natural de NO2,mientras que la combustión es la principal fuente antropogénica de óxidos de nitrógeno.
Conviene recordar que los focos de emisión humanos están concentrados,1. ESTUdIOS RECIENTEMENTE realizados para evaluar los efectos de la contaminación atmosférica sobre Europa ponen de manifiesto que en España el porcentaje de árboles dañados se ha duplicado de 1989 a 1991,fecha en que estaba afectado alrededor de un 7 % del total.
Por lo general,en áreas urbanas e industriales,lo que explica el predominio de las emisiones de origen antropogénico en la tasa de contaminación atmosférica a escala regional.
Hay otros compuestos que contribuyen también a la acidez de las precipitaciones.
Así,los ácidos orgánicos que tienen su origen en vertidos a la atmósfera de compuestos alifáticos y aromáticos (hidrocarburos de cadena abierta y cadena cerrada respectivamente).
Esta contaminación adquiere particular intensidad en las zonas más industrializadas de Estados Unidos y Europa.
En algunas zonas de Gran Bretaña,la precipitación con elevada concentración de ácido clorhídrico (HCI) tiene la misma importancia que la relacionada con el ácido nítrico.
Antes de cerrar este apartado de factores contaminantes hemos de aludir a la presencia en la atmósfera de cationes.
Estas partículas de carga positiva - - iones calcio (Ca2 +),sodio (Na +),potasio (K +),magnesio (Mg2 +) o amonio (NH4 +) - - proceden de la superficie terrestre y,sobre todo,del mar.
Se trata de sustancias,presentes en el suelo,que son muy solubles,por lo que sufren arrastre en disolución hasta los ríos y de allí al mar.
Cuando se producen fuertes evaporaciones en la superficie de los océanos,estos cationes alcanzan la atmósfera.
Debido a su basicidad,son capaces de neutralizar los efectos de las sustancias ácidas e,incluso,de generar precipitaciones de carácter básico de consecuencias igualmente negativas para los ecosistemas.
Una vez instalados en la atmósfera,los precursores padecen la oxidación a la que se debe finalmente la acidificación de la lluvia.
No sucede de una única manera.
Empecemos por la transformación del dióxido de azufre en H2SO4.
Por paradójico que resulte,la transformación directa de aquél en éste contribuye poco a la concentración total media de ácido sulfúrico en la atmósfera.
El ozono (O3),contaminante secundario formado en la atmósfera por fotooxidación,es agente de la oxidación directa del dióxido de azufre en trióxido de azufre (SO3),como lo es el oxígeno atómico (O -).
El oxígeno,muy inestable,puede reaccionar con cualquier sustancia de la atmósfera,aunque las concentraciones sean muy bajas,incluso del orden de 100.000 átomos por centímetro cúbico.
Importan más las vías indirectas de oxidación del SO2.
En ellas intervienen radicales hidróxidos (OH,OHO - y CH2OHO -),todos los cuales actúan en fase gaseosa.
También puede ser adsorbido en la superficie de algunos sólidos,donde ocurre entonces su oxidación,ya sea por el ozono o por otras moléculas.
El SO2,oxidado directa o indirectamente,pasa a S03,que,en presencia del vapor de agua de la atmósfera,se transforma en ácido sulfúrico.
Se forma ácido nítrico desde los óxidos de nitrógeno,sobre todo a partir del monóxido (NO).
Este compuesto se transforma en la atmósfera en dióxido de nitrógeno (NO2),bien mediante una cadena de reacciones iniciadas por radicales libres hidróxidos (OH -),en las que participan fundamentalmente moléculas de tipo orgánico,bien mediante una oxidación directa por el ozono (se ha determinado que la constante de reacción,cuando intervienen radicales hidróxidos,es del orden de 100 veces superior a la de las reacciones en que el ozono sirve de oxidante).
Una vez constituido,el dióxido de nitrógeno se transforma a su vez,en presencia de radicales OH -,en ácido nítrico.
La vida media del dióxido de nitrógeno en la atmósfera en presencia de radicales hidróxidos es de sólo un día,frente a los 7,7 días del dióxido de azufre,por lo que cabe concluir que las precipitaciones ácidas cuyo carácter se debe al ácido nítrico (HNO3) están asociadas a fenómenos de escala sinóptica,mientras que las que deban su acidez al ácido sulfúrico pueden estar relacionadas con fenómenos de transporte a escalas superiores.
Si se exceptúan las precipitaciones de carácter convectivo,las deposiciones ácidas se producen a grandes distancias del foco emisor.
Las trayectorias seguidas por las masas de aire hasta el lugar donde se desencadena la precipitación influirán decisivamente en su naturaleza.
Así,por ejemplo,el carácter ácido de una masa de aire que haya tenido su origen en una zona contaminada y que llegue a la península Ibérica a través del océano,se verá atenuado,o incluso neutralizado,por los cationes que capte durante su desplazamiento marítimo ; o,al contrario,una masa de aire de origen marítimo que tenga carácter básico puede llegar a produCir PreCiPitaCIOneS ácidas Si pasa por una región contaminada.
No cabe esperar,pues,que el pH de precipitaciones recogidas en distintos momentos en una misma estación meteorológica - - alejada de los focos de emisión - - sea el mismo,aun cuando la emisión de precursores no haya experimentado mientras tanto cambio alguno.
Si las corrientes de aire que llegan hasta la estación en momentos distintos siguen trayectorias que pasen sobre focos ácidos y básicos diferentes,el pH de las precipitaciones que lleven asociadas diferirá mucho,drásticamente incluso.
Estos efectos transfronterizos no serán perceptibles en estaciones que estén demasiado cerca de un foco emisor potente,cuya acidez enmascarará la emisión traída de lejos por las corrientes.
En el año 1979 se firmó el Convenio de Ginebra sobre Contaminación Atmosférica Transfronteriza y se estableció el Protocolo EMEP (acrónimo de " European Monitoring and Evaluation Programme ",o Programa Europeo de Evaluación y Seguimiento) que fue ratificado por España en 1983.
Según este convenio,los países firmantes se comprometen a la instalación de estaciones que midan,por una parte,parámetros meteorológicos típicos (temperatura,punto de rocío,velocidad del viento,presión y otros),y,en cuanto a contaminantes,concentraciones de dióxido de carbono (CO2),óxidos de nitrógeno (NOx),dióxido de azufre,ozono troposférico,metano (CH4),partículas en suspensión (núcleos de Aitken,es decir,partículas de diámetro inferior a 0,1 micrometros de diámetro,polvo y otras),cantidad de precipitación y turbiedad atmosférica.
El agua recogida se examina en el laboratorio,donde se determina su pH y las concentraciones de aniones y cationes.
Las estaciones EMEP han de cumplir ciertos requisitos ; entre ellos,que se instalen en un medio rural (ajeno a la influencia de las ciudades) y estén lejos de zonas edificadas.
2. REAS CONTAMINANTES y contaminadas de nuestro país.
Como la distancia máxima entre estaciones de control de la contaminación recomendada por el Programa EMEP es de 300 km,son puntos posibles para su ubicación Noya (La Coruña),Figueras (Gerona),Roquetas (Tarragona),Logroño,Ciba (Valladolid),San Pablo de los Montes (Toledo) y La Cartuja (Granada).
Excepto en Ciba,en todos estos lugares operan ya estaciones.
El foco contaminante depende de la intensidad de la fuente ; en el caso de grandes centrales térmicas,será de al menos 40 km. En el caso de que la distancia sea menor,la estación estará dotada de registros de viento para eliminar los datos procedentes de la dirección en que se encuentren las fuentes contaminantes ; para su instalación se evitarán asimismo los valles porque en ellos,en las noches de invierno,se producen a veces estratificaciones atmosféricas muy estables,que impiden la ventilación de la zona.
Las ubicaciones ideales serán,pues,las laderas o estribaciones de las montañas,ya que tampoco deberán estar sometidas a vientos muy fuertes.
En definitiva,se le buscará un alojamiento tal que la 3. HISTOGRAMAS de pH para estaciones de contaminación de procedencia distante esté lo menos perturbada posible por las fuentes contaminantes del entorno.
Teniendo en cuenta dónde están los focos con mayor concentración de contaminantes de España y que la distancia máxima entre estaciones marcada por el Programa EMEP ha de ser de 300 km,se obtiene que los puntos posibles para la ubicación de las estaciones de observación son los siguientes: Noya (La Coruña),Figueras (Gerona),Roquetas (Tarragona),Logroño,Ciba (Valladolid),San Pablo de los Montes (Toledo) y La Cartuja (Granada).
La primera estación en entrar en funcionamiento fue la de San Pablo de los Montes en 1984.
Actualmente todas están operando,excepto la de Ciba.
Recientemente ha entrado en funcionamiento otra estación EMEP en Menorca.
Nosotros nos hemos centrado en las primeras estaciones operativas: San Pablo de los Montes,La Cartuja,Logroño y Roquetas.
Del análisis de los datos disponibles suministrados por dichas estaciones en el período comprendido entre los años 1986 y 1990,se desprende que el porcentaje de observaciones en los que el pH de la precipitación recogida está por debajo del umbral de 5,6 establecido para que una precipitación sea considerada ácida oscila entre un 37,4 % en San Pablo de los Montes y un 2,3 % en Roquetas.
En cuanto a la precipitación con pH superior a 7,2,es decir,básica,su incidencia varía entre un 22,7 % en La Cartuja y un 2,4 % en San Pablo de los Montes.
Los valores extremos del pH oscilan entre 2,9 y 9,3,ambos registrados en Logroño.
Pese a que los días en que el pH de la precipitación puede considerarse neutro varían entre el 60,2 % de San Pablo de los Montes y el 82,6 % de Logroño,el efecto de las precipitaciones ácida y básica sobre el ambiente no deja de ser importante,como han puesto de manifiesto estudios recientes sobre el estado de nuestros bosques.
Tal y como cabe esperar de que haya diferentes focos contaminantes y distintas trayectorias de masas de aire,no es a un mismo anión al que se debe la mayor contribución al pH en cada estación ; así,en San Pablo de los Montes el de mayor contribución es el anión nitrato (NO3),en La Cartuja el sulfato (S04) y en las de Logroño y Roquetas el cloruro (C1 -).
En cuanto a los cationes,la basicidad de la precipitación viene determinada por el ion calcio (Ca2 +) en San Pablo de los Montes,La Cartuja y Logroño,y por el ion magnesio (Mg2 +) en Roquetas.
Para detectar la dependencia existente entre los distintos iones y el pH se utilizará el coeficiente de correlación,que nos indica que aquélla está siempre por encima del nivel del 90 %,excepción hecha del anión sulfato en la estación de Logroño.
Estas medidas de pH extremo son análogas a las efectuadas en otros lugares del planeta que padecen los efectos de la lluvia ácida.
Como en ellos,también aquí se observa que ésta se produce con mayor frecuencia que la básica,salvo en la estación de Roquetas.
Si se analizan los valores medios de la acidez,se ve que caen entre un pH medio de 4,2 para Roquetas y de 5,6 para La Cartuja,resultados que coinciden con los de otros lugares,dado el mismo criterio de medida.
El problema es aún más preocupante habida cuenta de que las estaciones EMEP han de estar alejadas de focos emisores,lo que lleva a pensar que en un núcleo contaminado,como una gran ciudad o los alrededores de una central térmica,estos valores de pH pueden resultar aún más extremos.
Como se ve,estaciones hasta cierto punto próximas exhiben patrones de acidez distintos.
Semejante discrepancia tiene que ver con la naturaleza de las masas de aire y con las trayectorias que siguen en su desarrollo.
Para determinar la procedencia de las masas de aire según su origen,se las clasifica en cuatro grupos,coincidentes con los cuatro sectores determinados en la circunferencia por las direcciones fundamentales de la rosa de los vientos.
Así,el sector o cuadrante 1 es el de los flujos que irrumpen entre el norte y el este ; el sector 2,entre el este y el sur ; el sector 3,entre el sur y el oeste,y el sector 4,entre el oeste y el norte.
Un método sencillo para dibujar la trayectoria de una masa de aire es el de las " retrotrayectorias de período 4 horas " ; consiste en tomar como origen el lugar de observación y,teniendo en cuenta el viento existente,determinar dónde estaba esa masa una fracción de tiempo atrás.
Este proceso se itera hasta averiguar la posición que ocupaba 48 horas antes.
Su fundamento físico es cinemático y dinámico a la vez ; estriba tanto en el movimiento de la masa de aire (cinemática) como en las fuerzas que intervienen en el origen de dicho movimiento (dinámica).
Las trayectorias se trazan en las superficies isobáricas de 950,850 y 750 hectopascales,correspondientes,aproximadamente,a alturas de 500,1500 y 2500 metros sobre el nivel del mar (el hectopascal,o HPa,es la unidad de presión equivalente a 100 newton por metro cuadrado).
Veamos qué trayectorias proporcionaron valores extremos de acidez para cada una de las estaciones.
En San Pablo de los Montes,el mínimo valor de pH registrado en el período bajo estudio es de 3,4.
Al analizar las concentraciones de los aniones medidos el día en que se obtuvo ese pH mínimo,se observa una concentración muy alta del ion sulfato ; aparecen también altas concentraciones del catión amonio,que neutralizan en parte la acidez de la precipitación ; es decir,si no hubiese sido por la contribución de este catión,la acidez hubiese resultado mayor.
Las trayectorias para los niveles 950,850 y 700 HPa nacen en el sur de Inglaterra y noroeste de Francia,y antes de llegar a la estación de San Pablo pasan por Bilbao.
Su origen y el paso por esta zona contaminada fueron responsables de la acidez detectada.
El máximo de pH medido en esta estación durante el período estudiado fue de 8,1.
La trayectoria en 950 HPa tiene su origen en el primer sector ; en 850 HPa y 750 HPa nacen en el centro de Argelia y atraviesan el Mediterráneo por la zona no contaminada del Estrecho.
Estos aspectos son los responsables del carácter básico de la precipitación.
En la estación de La Cartuja la máxima acidez corresponde a un pH de 5,6.
La ubicación de esta estación marca el carácter apenas ácido de las precipitaciones recogidas.
En este caso el máximo se debió a una masa de aire que procedía,en los tres niveles,del océano Atlántico y penetró por la zona contaminada del NO peninsular.
Este último hecho ha sido el responsable de la neutralización del carácter básico que correspondería a la precipitación por su origen e incluso la ha dotado de un carácter ligeramente ácido.
El máximo de pH se registró con un valor de 8,5 El análisis de las trayectorias,en los tres niveles estudiados,pone de manifiesto que se trataba de una masa de aire que procedía del océano Atlántico,al SO de la península,penetraba por el estrecho de Gibraltar y bordeaba las zonas contaminadas.
Su carácter básico se puede atribuir a una alta concentración del catión sodio y a la inexistencia de especies ácidas.
Ese mismo día se registró precipitación también en Roquetas ; su pH fue 6,5,valor que denuncia el paso de la trayectoria por la vertical de Cartagena,zona fuertemente contaminada que neutraliza el carácter básico original de la precipitación.
En la estación de Logroño el valor mínimo de pH fue de 2,9.
En todos los niveles el origen de esta trayectoria se encuentra en el norte de Alemania ; nace,pues,en la zona contaminada de Europa,para más tarde atravesar otra zona contaminada en el sur de Gran Bretaña.
Son corrientes del NE.
El máximo de pH,de valor 9,3,corresponde a fuertes vientos del NO peninsular.
El análisis de las trayectorias demuestra que provienen del océano Atlántico y llegan al punto de observación sin atravesar ninguna zona contaminada.
Esto justifica la ausencia de aniones capaces de compensar el carácter marcadamente básico de una masa de aire que atraviesa el océano Atlántico.
En la estación de Roquetas los valores de máxima acidez fueron de 5,0.
El análisis de las trayectorias es el siguiente: en la topografía de 950 HPa la trayectoria nacía en la zona contaminada de Cartagena ; en 850 HPa el origen era el mismo y la masa de aire transcurría por la zona contaminada de Levante hasta llegar a Roquetas ; en 700 HPa la trayectoria era del cuarto cuadrante.
El carácter ácido de la precipitación venía marcado por la alta concentración de iones cloruro y sulfato.
El máximo valor de pH detectado en la estación fue de 8,4.
El análisis de las trayectorias pone de manifiesto que se trataba de una masa de aire de origen mediterráneo que llegaba hasta Roquetas sin atravesar focos de contaminación.
Un análisis de las trayectorias seguidas por las masas de aire como el descrito anteriormente se ha llevado a cabo para cada uno de los días en los que se ha registrado precipitación durante el período bajo estudio,es decir,desde 1986 hasta 1990,en cada una de las estaciones.
La conclusión no varía: el carácter ácido o básico de una precipitación no sólo viene determinado por el origen de la masa de aire,sino que influye de modo decisivo la trayectoria que esta masa haya recorrido hasta el lugar donde se produce la precipitación.
Pero las trayectorias seguidas por las partículas están determinadas por la ubicación de las estructuras sinópticas,es decir,por las disposiciones recurrentes de ciclones y anticiclones que definen los diferentes flujos que llegan a las estaciones de medida.
Parece claro,por tanto,que habrá una fuerte dependencia de la acidez de la precipitación con respecto a las distintas situaciones sinópticas reinantes.
Para diferenciarlas unas de otras,éstas se han clasificado en tipos numerados del 1 al 8. La que está asociada a los mayores valores de acidez es la que corresponde a flujos del primer cuadrante (la número 5).
Este tipo de situación dominaba en la península cuando se registraron los mínimos de pH en las estaciones de Logroño y San Pablo de los Montes,con valores de 2,9 y 3,4 respectivamente.
Se da más frecuentemente en primavera,cuando alcanza también su máxima persistencia,en torno a los seis días.
Estas masas de aire proceden de los focos contaminantes del centro de Europa.
Le sigue,por la acidez de sus precipitaciones,la situación número 7,con un anticiclón de gran espesor instalado sobre la península ; ocurre frecuentemente en otoño y persiste,en otoño e invierno,hasta 16 días.
Esta situación tiene los mismos efectos,por lo que a la contaminación se refiere,que la octava,caracterizada por un anticiclón de carácter frío y de poco espesor al noroeste peninsular.
Acontece,sobre todo,en verano.
Su persistencia máxima es de seis días.
Cuando alguna de estas situaciones se presenta en la época estival,se desarrolla una estructura de baja térmica,cuyo comportamiento dinámico es similar al de una borrasca - - caracterizado por la existencia de corrientes ascendentes - -,si bien su origen es el calentamiento solar intenso.
Si estas condiciones van acompañadas de un grado de humedad suficiente,las corrientes ascendentes pueden provocar condensaciones rápidas que desarrollan una nubosidad de tipo cumulonimbo.
Estas estructuras desencadenan la mayoría de las precipitaciones intensas en los meses de verano sobre la península,y se denominan convectivas debido a las fuertes corrientes verticales ascendentes que las originan.
Evidentemente,el tipo de precipitación asociada es de carácter local.
Con esta situación dominando sobre la península se han detectado valores de pH 3,6 en la estación de San Pablo de los Montes.
Los valores mayores de basicidad aparecen asociados a la situación sinóptica número 4,definida por flujos procedentes del noroeste de la península Ibérica.
Este aspecto quedó claramente reflejado en la precipitación más básica registrada durante el período bajo estudio,con un pH de 9,2 en la estación EMEP de Logroño.
La estrecha relación entre la acidez de las precipitaciones y las trayectorias transfronterizas de las masas de aire asociadas se manifiesta también de forma directa como correlación entre la lluvia y los sectores de penetración de estas masas.
Las trayectorias del sector 1,que son las dominantes en los meses de mayo y enero del período bajo estudio,corresponden a masas de aire que tienen su origen en el centro de Europa.
El transporte transfronterizo será,pues,de carácter ácido.
Fenómeno que puede verse acentuado si su penetración en la península se realiza a través de la zona industrial de Bilbao.
Las trayectorias del sector 2,que en el período estudiado aparecen con más frecuencia en los meses de febrero y septiembre,aportan un transporte transfronterizo de carácter básico.
No obstante,si penetran en España por la zona contaminada de Cartagena,ese carácter puede verse neutralizado e incluso llegar a tornarse ácido.
Cuando las trayectorias provienen del sector 3 - - predominantes en los meses de septiembre y noviembre en el período estudiado - -,aportan precipitaciones básicas.
Sin embargo,se han detectado dos factores que pueden enmascarar e incluso predominan sobre esta basicidad.
Uno es el origen remoto de la trayectoria ; el otro,el pasillo de penetración en la península.
Si la fuente pertenece a una zona altamente contaminada de las costas norteamericanas,su posible neutralización,debido al paso por el océano,quizá no sea completa.
Si la penetración en la península se realiza a través del foco contaminado de Cádiz y Huelva,la precipitación puede tener carácter ácido.
Cuando las trayectorias provienen del cuarto sector,el tipo de precipitación correspondiente debería ser básico.
No obstante,el origen remoto de la trayectoria y su penetración por la zona contaminada del noroeste de la península afectan al carácter de la precipitación.
Todos estos efectos de larga distancia quedan ocultos en las proximidades de los focos de contaminación.
Gran parte de las precipitaciones que se producen en nuestro país tienen carácter convectivo.
Son,pues,precipitaciones de carácter local,que arrastran la contaminación de los focos cercanos y dan lugar a lluvias de marcado carácter ácido.
Se ha comprobado que este efecto prevalece sobre el posible transporte transfronterizo.
Teniendo en cuenta las observaciones de precipitación ácida analizadas por nuestro equipo,así como los datos de estaciones EMEP recogidas en Europa desde 1986 hasta 1990,se ha podido completar un mapa de isolíneas de pH en el que por primera vez se incluye la península Ibérica.
Sobresalen dos núcleos de acidez con valores de pH de 4,6,siendo el situado en la zona centro de carácter marcadamente local.
Se ha demostrado que el origen de estos núcleos está relacionado con precipitaciones de carácter convectivo en el entorno de los focos de contaminación,fundamentalmente aglomeraciones urbanas y centrales térmicas.
Si se compara el pH de las precipitaciones en nuestra península con el medido en el resto de Europa,se pone de manifiesto que,aun sin llegar a los extremos globales de acidez que allí se dan,sí se trata de un problema preocupante.
Prácticamente toda la península se encuentra afectada por la precipitación ácida ; las zonas donde esta contaminación en la precipitación es menor son el noroeste y el sureste.
Sólo los dos focos de acidez 4,6 son comparables con los de los países del norte de Europa,y en ellos los efectos observados sobre el ecosistema deberían ser similares a los sufridos en esos países.
Estos valores son valores medios ; los extremos inferiores son mucho más bajos.
Si se tiene en cuenta,además,que las estaciones EMEP han de hallarse lejos de los focos contaminantes,cabe pensar que los valores de pH medidos dentro de estos focos serán menores,y las consecuencias sobre el ambiente aún más graves.
Estudios recientes encaminados a determinar los efectos de la contaminación atmosférica sobre los bosques europeos,dentro del Programa Internacional de Cooperación de la Convención de Ginebra de 1985,han puesto de manifiesto que en España el porcentaje de árboles dañados se ha duplicado de 1989 a 1991,fecha en que era de alrededor del 7 %.
La situación de nuestros bosques no parecerá tan preocupante si se la compara con la de los bosques centroeuropeos en general (20 % de árboles dañados en 1991),pero ésta debería tomarse como ejemplo de lo que puede suceder si no se pone límite a las emisiones de gases contaminantes a la atmósfera.
Recordemos que según el inventario CORINAIR de 1985 SOIO éramos superados en las emisiones a la atmósfera de óxidos de nitrógeno,dentro de la CE,por Francia,Italia,Reino Unido y Alemania,y en los vertidos de dióxido de azufre tan sólo quedábamos detrás del Reino Unido y Alemania.
EL premio Nobel de medicina de 1993 lo han recibido Richard J. Roberts y Philip A. Sharp por haber descubierto,en 1977,el carácter discontinuo de la información genética.
Según James Darnell,cuyos trabajos cimentaron en buena medida el de Roberts y Sharp,este hallazgo quizá sea el avance más impresionante de la biología desde el descubrimiento de la doble hélice del ADN.
Roberts nació en Inglaterra,estudió química en la universidad de Sheffield y bioquímica en Harvard.
En 1977 trabajaba en Cold Spring Harbor,donde ha permanecido hasta que,el año pasado,aceptó la dirección científica de New England Biolabs.
Por su parte,Philip A. Sharp estudió química en la universidad de Urbana,especializándose más tarde en bioquímica en el Instituto de Tecnología de California.
Pasó luego a Cold Spring Harbor,pero en 1977 trabajaba ya en el Centro Oncológico del Instituto de Tecnología de Massachusetts.
Hoy tiene su propia empresa,Biogen.
El ARN mensajero lleva,del núcleo celular al citoplasma,la información contenida en un gen para que sea leída en los ribosomas por la maquinaria celular de síntesis proteica.
Parece natural que el ARN mensajero se ajuste al mensaje que porta,el fragmento de ADN original.
Pero a principios de los años setenta se sabía que los otros tipos de ARN - - ribosómico y de transferencia - - sufren un proceso de maduración: la molécula final se origina a partir de una molécula precursora más larga.
Quedaba pendiente si el ARN mensajero de las células eucariotas - - es decir,dotadas de núcleo diferenciado - - experimentaba un proceso similar entre su formación durante la transcripción del ADN y su lectura para la construcción de una proteína.
En procariotas,donde transcripción y traducción son casi simultáneas,no cabía pensar que fuese así.
En 1976 aparecía ya claro que los cortes que transformasen el precursor en la molécula madura que se traduce en los ribosomas no eliminarían los extremos del precursor,sino que arrancarían segmentos de su interior.
En el proceso de maduración no se perderían ni la caperuza ni la cola (llamada " de poli-A ") moleculares,añadidas,durante la transcripción,al comienzo y final de la secuencia de nucleótidos que forma la molécula de ARN mensajero.
Pero se suponía que ese proceso era imposible.
Una cosa era la escisión de piezas auxiliares introducidas durante la transcripción por los cabos de la secuencia transcriptora y otra un mecanismo que rompiera la continuidad de la información genética.
En menos de un año cambió la visión del proceso.
A mediados de 1977,en el congreso anual del laboratorio Cold Spring Harbor,los grupos de Roberts y Sharp dieron cuenta de los resultados que habían obtenido al hibridar una cadena de ADN de adenovirus - - los causantes de los resfriados - - con su correspondiente ARN mensajero,para formar un mixto de ADN y ARN.
En sus microfotografías destacaban tres bucles monocatenarios de ADN que sobresalían en las zonas donde se habían unido el ADN y su ARN complementario.
Significaba ello que las transcripciones de los tres segmentos génicos en cuestión habían sido cortadas del mensajero durante la maduración y que los cuatro trozos restantes se habían engarzado posteriormente ; sólo a ellos,pues,se debía la codificación de la proteína asociada a ese gen.
El también premio Nobel Thomas Cech,presente en el congreso,cuenta que " el publico quedó atónito ; fue uno de esos momentos en que el mundo se pone cabeza abajo ".
Walter Gilbert,de Harvard,llamó intrones a los segmentos o secuencias colocados entre fragmentos codificadores,y exones a estos últimos.
No tardó en comprobarse que casi todos los genes de las células eucariotas tienen intrones (las bacterias carecen de ellos).
Más largos que los exones codificadores por lo común,su tamaño y número varían de un gen a otro ; a modo de ejemplo,los genes que determinan el colágeno,una proteína del tejido conjuntivo de cadena muy larga,contienen unos cuarenta intrones.
Amén de transformar radicalmente nuestra interpretación del código genético,el descubrimiento de los intrones ha permitido descorrer el velo sobre fenómenos ni siquiera imaginados.
El proceso de corte y empalme (splicing) puede efectuarse de varias maneras ; cabe,por consiguiente,que un gen codifique más de una proteína,como ocurre con el gen de la cadena pesada de los anticuerpos: de una larga molécula del ARN precursor correspondiente se obtienen dos ARN maduros diferentes.
Cech demostró que el ARN de Tetrahymena termophila,un protozoo,podía cortarse y volver a recoserse sin necesidad de enzimas mediadoras El ARN podía incluso catalizar la síntesis de otros ARN,hacer de enzima gracias a un intrón (véase " Función enzimática del ARN ",por Thomas R. Cech ; INVESTIGACI N Y CIENCIA,enero de 1987).
Los errores cometidos en el proceso de corte y empalme producen talasemias,alteraciones hereditarias de la síntesis de hemoglobina.
Los intrones podrían cumplir,tal vez,misiones reguladoras,determinando cuándo deba expresarse un gen en una proteína.
Walter Gilbert ha ido más lejos con su modelo evolutivo: la reordenación de los exones crearía nuevos genes,no las mutaciones.
Nobel de química Reacción en cadena de la polimerasa Kary B. Mullis y Michael Smith,galardonados con el Nobel de química de 1993,sólo comparten dos cosas.
Primera,haber descubierto algo relativo a la multiplicación del material genético ; y segunda,la gran importancia científica y económica de sus técnicas.
Mullis nació en 1944 en Lenoir (Carolina del Norte).
Estudió química en el Instituto de Tecnología de Georgia y en la Universidad de California en Berkeley.
No enseña ni investiga en institución académica alguna ; en estos momentos es asesor bioquímico y trabaja en casa.
Cuenta que la idea por la que ha obtenido el premio se le reveló súbitamente durante un viaje en coche (véase " Reacción en cadena de la polimerasa " por Kary B. Mullis,INVESTIGACI N Y CIENCIA,junio de 1990).
La carrera y la imagen pública de Smith son más habituales.
Nació en Blackpool en 1932,estudió bioquímica en Manchester hasta 1956 y se marchó al Canadá donde sigue ; allí enseña en la Universidad de la Columbia Británica.
Mullis trabajaba en 1983 en la empresa Cetus,de California.
Andaba tras mecanismos que permitieran descubrir qué letra del alfabeto genético se encontraba en un punto determinado del ADN.
Creyó,equivocadamente,que uno de los pasos del proceso no era viable.
Buscando algún truco que lo evitara dio con algo mejor,un método sencillo de multiplicación,o amplificación,de cualquier fragmento de ADN: la reacción en cadena de la polimerasa.
Gracias a la misma,podemos crear muestras suficientes para abordar una secuencia determinada de ADN,sin que requiera demasiado esfuerzo el obtenerlas.
La molécula de ADN consta de dos hebras - - dos sucesiones de sólo cuatro componentes básicos,los nucleótidos - - complementarias,como el positivo y el negativo de una fotografía.
Cualquiera de las dos cadenas sirve de molde para la síntesis de su complementaria.
En la célula,esa labor la desempeña una enzima la ADN polimerasa,que recorre la cadena y coloca junto a cada nucleótido de la cadena original su complementario en la de nueva formación,que así va creciendo a partir de una breve secuencia (u oligonucleótido) ya unida (hibridada) a la cadena (el ' cebador ").
Para replicar un determinado fragmento de ADN con la reacción en cadena de la polimerasa se empieza por calentar y separar las dos hebras ; una vez enfriadas,se hibridan dos segmentos: un segmento a la izquierda del fragmento a duplicar de una de las hebras y un segmento a su derecha en la otra hebra con sus respectivos complementarios,oligonucleótidos que hay que aportar al proceso y que hacen de cebadores.
La polimerasa va extendiendo éstos,duplicando los fragmentos de las hebras originales que se quieren copiar.
Una vez lograda la copia,se itera el proceso.
De esta forma se tiene una especie de reacción en cadena: en cada ciclo se duplica el número de copias,y bastan veinte ciclos para obtener más de un millón de copias de la secuencia de partida.
La técnica ha rendido muchísimos servicios a la ciencia de los últimos años.
Se aplica,además,a la determinación del sexo de embriones,criminología y reconstrucción de genes prehistóricos.
Smith descubrió,iniciados los ochenta,cómo cambiar un nucleótido por otro en una determinada secuencia génica,con la sustitución consiguiente en el aminoácido cifrado de la proteína codificada.
Hasta que no ideó esta técnica de mutagénesis ortoespecífica,las modificaciones corrían a cargo del azar,radiaciones inducidas y productos químicos.
En el procedimiento de Smith se separa con calor la doble hebra de un plásmido,fragmento de ADN circular que encontramos en los genomas bacterianos.
Se sintetiza una breve secuencia complementaria de un segmento del ADN separado,salvo en el nucleótido que se quiere alterar.
Se hibridan la cadena circular de ADN y este oligonucleótido.
Esta copia se cierra mediante una enzima ligasa,y se tiene así un plásmido - - bucle de ADN - - en una de cuyas cadenas está el nucleótido correcto y en la otra el mutado.
Este producto se inserta en el genoma de una bacteria,que sintetizará tanto la forma normal como la alterada de la proteína.
La ventaja reside en que sólo hay que conocer unos cuantos nucleótidos a izquierda y derecha del deseado,y no todo el gen.
La mutagénesis ortoespecífica se utiliza para investigar la estructura y función de los genes,pero,en principio,vale también para alterar controladamente las proteínas y dotarlas de nuevas propiedades.
Si bien el diseño de proteínas ya ha reportado algunos éxitos,como aumentar la estabilidad de ciertas enzimas,en general está limitado por nuestro desconocimiento de la vinculación que existe entre la estructura de una proteína y su función.
Más esperanzadores son los experimentos dirigidos a producir anticuerpos catalíticos que aceleren de forma selectiva reacciones químicas.
Nobel de física Púlsares y relatividad general El premio Nobel de física de 1993 ha sido concedido a Russell Alan Hulse y Joseph Hooton Taylor por el descubrimiento y análisis exhaustivo y preciso de un objeto celeste muy singular,el PSR 1913 + 16.
El descubrimiento se realizó en el Observatorio Radioastronómico de Arecibo (Puerto Rico) en 1974,durante el desarrollo de la tesis doctoral de Hulse,que dirigía Taylor.
La denominación PSR1913 + 16 significa que se trata de un púlsar situado en un punto de la esfera celeste cuyas coordenadas astronómicas son 19 horas y 13 minutos de ascención recta y 16 grados de declinación norte (positiva).
Este lugar pertenece a la constelación del guila.
La palabra púlsar es una contracción de la expresión " pulsating star ",es decir,estrella pulsante o estrella que emite pulsos.
Los púlsares fueron descubiertos en 1967.
Se los considera estrellas muy compactas (su densidad es similar a la del núcleo atómico),con una masa que ronda 1,4 veces la del Sol ; su tamaño,en consecuencia,es muy reducido: no excede de unas pocas decenas de kilómetros.
Tienen dos propiedades importantes que les son características.
En primer lugar,giran rápidamente,pudiendo alcanzar una velocidad de rotación de varias centenas de revoluciones por segundo,aunque en el caso del PS R 1913 + 16 de Hulse y Taylor la velocidad es sólo de unas 17 vueltas por segundo,que,sin embargo,es enorme comparada con la vuelta que da la Tierra cada 24 horas o a la que da el Sol cada 25 días aproximadamente.
Como es fácil de comprender,estas impresionantes velocidades de rotación sólo son posibles en estrellas que tienen la densidad citada,y por tanto una intensidad del campo gravitatorio en la superficie extremadamente elevada,pues de otro modo se desintegrarían por la simple acción de la fuerza centrífuga.
La segunda propiedad importante de los púlsares es que,lo mismo que la Tierra,poseen un campo magnético,cuya intensidad sobrepasa,sin embargo,lo imaginable ; la brújula se orientaría hacia el norte magnético del púlsar a una velocidad asombrosa.
Las dos características descritas provocan que los púlsares emitan,en la dirección de su eje magnético,una intensa radiación electromagnética con un alto grado de focalización,de manera que podemos imaginar que la radiación sale por dos conos muy estrechos orientados según el norte y el sur magnéticos de la estrella.
Ahora bien,como en la Tierra,el eje magnético no coincide con el de giro,con lo cual los conos estrechos de radiación describen a su vez un cono en torno al eje de rotación,y,si da la casualidad de que en esta precesión uno de los conos estrechos barre la Tierra,detectaremos una señal electromagnética de tipo impulsivo a cada vuelta de la estrella alrededor de su eje ; de ahí le viene el nombre de púlsares a este tipo de estrellas.
Como es natural,la duración y el aspecto del impulso electromagnético recibido en la Tierra dependen,respectivamente,de la abertura y de la forma de los haces de radiación.
(Aspecto quiere decir aquí el perfil de la gráfica de la intensidad en función del tiempo).
En el caso del púlsar de Hulse y Taylor,la duración del impulso es de 8 milisegundos y su aspecto el de una pareja de montículos muy picudos separados por un valle.
Ahora bien,como la estrella da 17 vueltas por segundo,el impulso se repite aproximadamente cada 59 milisegundos.
¿Qué es lo que tiene de particular el PSR1913 + 16 para que sus descubridores hayan merecido el premio Nobel? Al fin y al cabo,ya se conocían bastantes púlsares cuando Hulse y Taylor hallaron el suyo,y desde entonces ha venido aumentando la tasa de detección.
La primera propiedad fundamental del PSR1913 + 16 es la extraordinaria estabilidad de la duración,aspecto y período de los impulsos electromagnéticos que emite,lo que lo convierte en un reloj de precisión capaz de competir con los mejores relojes atómicos.
Esta propiedad no es baladí: el púlsar está seguramente sometido a enormes tensiones centrífugas como consecuencia de la alta velocidad de rotación,y por tanto tiende a sufrir serias perturbaciones en sus características internas ; sin embargo,el período entre pulsos sólo aumenta del orden de una millonésima de billonésima de segundo en cada segundo,lo cual da una idea además de la extrema precisión con que se han realizado las mediciones.
Además,estos impulsos electromagnéticos son la única información que se recibe de la estrella ; por tanto,todos los datos aquí comentados se han construido a partir exclusivamente de ellos.
La segunda característica diferenciadora de PSR1913 + 16 es que no se trata de una estrella solitaria,como los púlsares que ya se conocían,sino que forma parte de un sistema binario,es decir,de una pareja de estrellas que describen órbitas elípticas en torno al centro de masas del sistema formado por ambas,circunstancia,por otro lado,bastante frecuente en estrellas de tipo corriente.
La presencia de la estrella compañera sólo es detectable gracias al movimiento del púlsar protagonista,ya que no se recibe ninguna señal electromagnética emitida por ella.
Se trata probablemente de otro objeto de características similares al púlsar,pero cuyos haces estrechos de radiación no barren la Tierra.
Su masa es muy similar a la de PSR1913 + 16,por lo que las dos órbitas resultan ser también muy parecidas,condición que,unida a la elevada excentricidad de las mismas,lleva a grandes variaciones en la velocidad de ambos objetos en el transcurso de un período orbital,que dura unas siete horas y tres cuartos.
La inclinación del plano orbital del sistema con respecto a la línea de visión desde la Tierra es de unos 44 grados.
Interesa destacar que,de todos los púlsares conocidos,sólo unos pocos forman parte de un sistema binario ; el de Hulse y Taylor,el primero que se viese con tal característica,sigue siendo raro en este sentido.
Estas propiedades del PSR 1913 + 16 se complementan de manera tan sutil que,gracias a ellas,Hulse y Taylor pudieron descubrir el rasgo más importante y específico de este objeto celeste: la emisión de ondas gravitacionales por el sistema binario donde está integrado.
Este tipo de ondas había sido predicho por la teoría de la gravitación de Einstein ; sin embargo,su detección directa en la Tierra ha sido imposible hasta el momento,pues según todos los cálculos la radiación que proviene del exterior es debilísima.
Las observaciones de Hulse y Taylor constituyen la primera prueba,aunque indirecta,de la existencia de tales ondas en la naturaleza,lo que a mi juicio constituye una magnífica razón para la concesión del premio Nobel a estos investigadores.
La teoría de la gravitación de Einstein,también llamada de la relatividad general,es una generalización de la teoría de la gravitación de Newton,cuyo enunciado esencial afirma que los cuerpos celestes se atraen en razón directa al producto de sus masas y en razón inversa al cuadrado de su distancia.
La relatividad general,sin embargo,no se puede resumir de manera tan sencilla y exige el conocimiento de técnicas matemáticas.
No obstante y a los efectos que aquí nos interesan,entenderemos los aspectos más destacados de la manera en que esta teoría generaliza la de Newton,si la comparamos cualitativamente con la teoría electromagnética de Maxwell.
En ésta,los agentes que generan el campo electromagnético son las cargas eléctricas ; se dan situaciones distintas si las cargas están en reposo,se mueven con velocidad constante o poseen un movimiento acelerado.
En el primer caso,generan un campo eléctrico de tipo culombiano idéntico al gravitatorio de Newton,con que se cambien cargas por masas.
En el segundo caso,cuando las cargas se mueven con velocidad constante,además del campo eléctrico aparece un campo magnético,cuyo efecto más notable es añadir una fuerza suplementaria sobre aquellas otras cargas que también se encuentren en movimiento.
Esta situación no tiene parangón en la teoría de Newton,pues la masas en movimiento no generan una fuerza añadida a la gravitatoria habitual.
Sin embargo,en el marco de la relatividad general las masas que se mueven generan,además del campo newtoniano habitual,un campo " extranewtoniano " que da lugar a una fuerza suplementaria sobre otras masas móviles análoga a la fuerza magnética ; se puede comprobar que es de la misma naturaleza que la fuerza de Coriolis,la que actúa sobre un objeto en movimiento situado en una plataforma giratoria ; por esta razón,a dicho campo extra se le suele llamar campo de Coriolis.
Finalmente,cuando las cargas eléctricas se aceleran,parte de los campos eléctricos y magnéticos que generan se propaga a la velocidad de la luz y da lugar a las ondas electromagnéticas habituales,desde los rayos gamma hasta las ondas de radio pasando por la propia luz ; pues bien,según la relatividad general,cuando se acelera fuertemente un sistema de masas se produce una propagación de parte de los campos de Newton y de Coriolis que origina,y se genera un nuevo tipo de ondas,las gravitatorias.
Estas ondas,al igual que las electromagnéticas,transportan energía y viajan a la velocidad de la luz,pero su intensidad suele ser muy inferior,ya que en situaciones comparables la interacción gravitatoria es muchísimo más débil que la electromagnética.
Como ya se ha indicado,los componentes del sistema binario PSR1913 + 16 sufren fuertes aceleraciones en su movimiento ; por lo cual,de acuerdo con lo expuesto en el párrafo anterior,deben emitir energía en forma de radiación gravitatoria.
Ahora bien,si esto es cierto,la pérdida de energía correspondiente se traducirá en una aproximación de las órbitas de los componentes del sistema y,consecuentemente,en una disminución de sus períodos orbitales.
Hulse y Taylor midieron esa disminución,y obtuvieron resultados acordes con las predicciones de la relatividad general.
Quedó así de manifiesto la emisión de ondas gravitatorias.
El mecanismo mediante el cual se realizó la medición es muy interesante.
Para describirlo,simplificaremos la situación imaginando que la masa de la estrella compañera del púlsar es muy superior a la de éste,tanto,que permanece quieta y el púlsar describe una órbita a su alrededor.
Supondremos,además,que esta órbita es circular y que en el plano que la contiene se encuentra también la línea de visión desde la Tierra.
Según este esquema,cuando el púlsar se aleja de la Tierra se produce,por efecto Doppler,una disminución de la frecuencia de recepción de los impulsos electromagnéticos del pulsar ; por el contrario,cuando el púlsar se acerca a la Tierra,el mismo efecto Doppler provocará un aumento en la frecuencia de recepción de los impulsos.
Estas variaciones en el período entre los impulsos electromagnéticos recibidos en la Tierra tendrán la misma periodicidad que la órbita ; por consiguiente,una medición de las mismas indica no solamente el movimiento del púlsar,sino también su período orbital.
Cuando se representan la intensidad de los impulsos y el período de los mismos en función del tiempo se observa que esta segunda gráfica es de tipo sinusoidal,con un período que coincide con el período orbital del púlsar.
Notemos que el proceso descrito no tiene sentido en el supuesto de que el plano de la órbita sea perpendicular a la línea de visión desde la Tierra,por lo que la inclinación que tiene realmente el sistema binario de Hulse y Taylor debe considerarse otra de sus propiedades notables.
Una vez que se conoce el período orbital del púlsar es fácil verificar si se mantiene constante o no en el transcurso del tiempo.
Pues bien,desde el descubrimiento de este objeto en 1974 hasta el presente se ha detectado una disminución en dicho período de unos 14 segundos,cantidad que concuerda de manera impresionante con la predicha por la teoría de Einstein.
Las observaciones de Hulse y Taylor constituyen un ejemplo de investigación experimental de alta precisión y con una proyección teórica de gran alcance,pues coloca la última guinda que le faltaba a la teoría de la relatividad general de Einstein.
Además,a mi juicio,reafirman las esperanzas depositadas en el éxito futuro del proyecto de construcción de detectores interferométricos de ondas gravitatorias,que sin duda abrirán el paso a la astronomía gravitatoria,es decir,la observación del cosmos mediante este tipo de ondas,en sustitución de las electromagnéticas.
Este avance,si se produce,permitirá el acceso a una información inédita del universo ; las ondas gravitacionales no se detienen ante casi ningún obstáculo interpuesto entre la fuente y el detector,mientras que las ondas electromagnéticas son absorbidas en buena medida por todos los objetos celestes.
El hecho de que las características físicas,químicas y biológicas de los humedales fluctuen tan intensamente es consecuencia de la elevada relación superficie / volumen de sus aguas.
Se trata de ecosistemas acuáticos someros ; por tanto,pequeñas variaciones en algunas de las variables que determinan el balance de agua (lluvia,evapotranspiración,aportes y pérdidas por escorrentía superficial y flujos subterráneos) provocan grandes variaciones de la superficie cubierta por agua que,si ocurren inducidas por los cambios climáticos,muestran su gran heterogeneidad temporal y espacial.
Bajo el clima mediterráneo,las fluctuaciones del nivel del agua en los humedales son muy extremas y más irregulares que con otros regímenes climáticos.
En las estaciones más secas del año,muchos humedales interiores y costeros se secan total o parcialmente.
Por la misma razón,la variabilidad climática,se pueden inundar rápida o lentamente,según como tengan lugar las lluvias.
Cualquier plan de gestión de estos ecosistemas debería tender a mantener las fluctuaciones naturales del nivel del agua.
Existen numerosos ejemplos de humedales en los que se ha estabilizado el nivel del agua (laguna de Sariñena) y en los que se ha alterado su hidroperíodo (lagunas costeras del delta del Ebro).
Las consecuencias de estas perturbaciones,ocasionadas por vertidos de aguas residuales y desviaciones de flujos de agua,han sido claras y siempre del mismo tipo: pérdidas inmensas de recursos naturales entre ellos de las pesquerías y de la calidad del agua.
Los humedales deben entenderse como parte integrante del paisaje.
Sus variaciones temporales y espaciales son indicadoras de la dinámica de un sistema más amplio que abarca la cuenca hidrográfica superficial y subterránea.
Esta fue,en síntesis,la contribución de los últimos años de trabajo de Fernando González Bernáldez (véase " Las aguas subterráneas en el paisaje " por F. González Bernáldez y cols.
; INVESTIGACI N Y CIENCIA,abril de 1987).
Desafortunadamente,esta percepción no se tuvo en cuenta durante los años 70 y 80 y,hoy en día,zonas húmedas protegidas legalmente,como las Tablas de Daimiel y Doñana,presentan su ciclo hidrológico gravemente alterado por las extracciones de agua subterránea para riego de cultivos de rentabilidad perecedera,o nula,si se considera el alto coste ambiental que supone la puesta en regadío de áreas conectadas hidrológicamente con zonas húmedas de gran valor.
En este sentido,el paso del tiempo no supone ningún progreso,porque en otras zonas,como los Monegros,se evidencia la misma falta de integración de los conocimientos científicos en la planificación del desarrollo.
Otro ejemplo ilustra la falta de aplicación de los conocimientos científicos en la planificación de la gestión hidrológica.
Frente a las avenidas de agua de gran caudal y baja frecuencia por los cauces fluviales,se manejan dos argumentos para justificar grandes obras públicas que incrementarían aún más las regulaciones artificiales de los caudales: el agua que llega al mar se pierde sin utilidad y las inundaciones de las llanuras aluviales provocan graves pérdidas.
Ambas razones son falsas.
Está demostrado que los aportes de agua continental al mar contribuyen a la riqueza pesquera de las zonas próximas a las desembocaduras.
La disminución de estos aportes también ha ocasionado el desequilibrio a favor de la erosión de gran parte de la líneas de costa del mar Mediterráneo,y para intentar remediarla se está derrochando muchísimo dinero.
Las graves pérdidas por inundación de zonas próximas a los cauces de los ríos no lo serían tanto,ni los esfuerzos de contención de las riadas tan grandes ni frecuentes,si se hubieran preservado las zonas húmedas de las llanuras aluviales que cumplirían la función,entre otras,de amortiguación de las riadas (fundamentalmente rebajando la onda de la avenida y alargando su período).
No es extraño que los tramos donde ocurren los mayores daños sean los anteriores y posteriores a la existencia de diques que estrechan el cauce fluvial,ya que representan un freno para el paso de la avenida y el agua sale de ese estrecho pasillo con mucha mayor energía.
Afortunadamente,la mayoría de estos procesos dinámicos son bien conocidos en sus componentes más sencillos.
Durante todo el siglo xx se han usado las mismas bases conceptuales para la planificación de la política hidráulica (y sus técnicas derivadas,canalización,embalsado y trasvase de agua) que se heredaron de la Edad Media,pero que aplicadas hoy constituyen un enorme gasto por la acumulación desequilibrada en el espacio de las actividades humanas y poca rentabilidad a largo plazo por la irregularidad en el tiempo de las estructuras económicas.
Existen métodos de gestión hidráulica alternativos a los tradicionales que tienen en cuenta el principio de aceptar pérdidas,o su equivalente de evitar grandes gastos,a corto plazo para asegurar buenos rendimientos,o su equivalente de mayor seguridad en el rendimiento sostenido,a largo plazo.
Estos métodos requieren una gran integración de los trabajos de ingeniería en la conservación de procesos y áreas naturales.
La preservación de las zonas húmedas manifestando todas sus características ofrece el signo más genuino del progreso humano,un progreso basado en la aplicación de los conocimientos científicos para obtener un bienestar más duradero.
Signos positivos se observan en los esfuerzos por aplicar las recomendaciones del Convenio de Ramsar.
Pero la falta de atención a propuestas de alcance europeo,como la Declaración de Grado (febrero de 1991) o el desarrollo de políticas sectoriales desconectadas y sin integrar en un objetivo común de reequilibrio del uso del territorio,a través de la promoción de una economía de rendimientos más duraderos y preservadora de los recursos naturales,son signos negativos que amenazan la persistencia de numerosos humedales mediterráneos más allá del siglo xx.
Ciencias del cuaternario: La antracología.
En el ámbito de las ciencias relativas a la era cuaternaria,la antracología,que estudia la madera carbonizada procedente de los yacimientos arqueológicos,ocupa un lugar destacado.
Estos carbones vegetales son restos de fuegos de funcionalidad diversa.
La madera que el hombre trae al asentamiento puede agruparse en dos categorías principales: madera de manufactura,para la construcción y talla de objetos artesanales o domésticos,y madera combustible,para hacer fuego.
Estas dos categorías ofrecen una doble clase de información.
Por un lado,la madera combustible interesa a la paleoecología o arqueobotánica ; por otro,las maderas de manufactura importan en paleoetnología o arqueoetnobotánica.
La madera combustible constituye un buen punto de apoyo para la reconstrucción de paleopaisajes,y en esa línea de trabajo ha descollado la llamada escuela de Montpellier.
A través de la identificación de las especies leñosas (árboles y arbustos) encontradas en los yacimientos,y teniendo en cuenta sus condiciones de vida y su relación con otras especies,se recrea el paisaje vegetal con las formaciones o biotopos en torno a los asentamientos humanos.
De este modo,se traza la evolución y variaciones que el paisaje vegetal ha ido sufriendo a lo largo del tiempo,como consecuencia de las fluctuaciones climáticas y de la presión que el hombre ha ido ejerciendo en el medio.
El fuego intencionado,la recogida de leña y los trayectos recorridos introducen un filtro cultural que condiciona el espectro antracológico resultante.
A su vez,ponen de manifiesto la estrecha relación entre el hombre y su entorno.
La metodología de estudio se divide en dos partes.
La primera tiene lugar en el yacimiento,donde se recogen los carbones en campañas sucesivas.
La segunda,de laboratorio consiste en la determinación anatómica de las muestras y en el subsiguiente tratamiento de datos.
Se procura que la recogida de carbones y restos orgánicos sea exhaustiva.
El grado de conservación de carbones en el sedimento varía de una zona a otra.
Los yacimientos estudiados hasta ahora en la región mediterránea han ofrecido siempre abundantes restos gracias a una recuperación sistemática a través del cribado de sedimento.
Pero otras regiones,así la cantábrica,sometida a un clima atlántico,no ofrecen posibilidades tan óptimas para la conservación de los carbones.
Por su tamaño mínimo,no resulta fácil darse cuenta de la presencia de carbones,que,además,al encontrarse impregnados en arcilla,son muy quebradizos.
Para que la recuperación sea eficaz,se emplea la técnica de flotación: se introduce el sedimento en una cubeta llena de agua donde se disuelve un producto,el calgón,para disolver los carbonatos ; debido a su baja densidad,los carbones flotan y van desgajándose de un sedimento arcilloso sin destruirse ni fragmentarse ; se recuperan con ayuda de un colador y van depositándose en el recipiente correspondiente,etiquetado.
El secado de las muestras se efectúa al aire libre o en un secador de aire caliente ; se prefiere lo segundo si nos encontramos en zonas donde la humedad ambiente es constante.
Cuando coinciden un sedimento apelmazado y carbones muy pequeños,no basta con el proceso de flotación.
La recuperación se lleva a cabo durante la siguiente etapa: la criba del sedimento con agua.
Se dispone el sedimento en cribas de S milímetros para la fracción gruesa y en cribas de 2 milímetros para la fracción fina,y va lavándose cuidadosamente con agua corriente.
Las cribas se dejan secar y se procede a una labor de selección de carbones y demás restos orgánicos.
Cada yacimiento presenta sus propias características.
La funcionalidad,contexto cultural y económico del mismo condicionarán la intensidad con que el sitio fue ocupado y las veces que fue frecuentado.
Todo esto va a influir en el espectro antracológico resultante: variedad de taxones,abundancia o escasez de carbones,etcétera.
Mientras se realiza la operación de muestreo y recogida,conviene atender a la distribución espacial de los carbones en el yacimiento,para conocer la procedencia de los mismos.
Suelen aparecer concentrados en estructuras de combustión (culinarias o para calentarse) o bien diseminados por el sedimento del nivel de ocupación.
Según L. Chabal,estos últimos reflejan el muestreo más completo de la vegetación porque son el producto de las recogidas diarias de leña efectuadas,así como el resultado de los numerosos vaciados de hogares realizados en un lapso de tiempo no determinado.
Los concentrados en hogares representan los últimos restos de combustión,suelen ser pobres en especies y suministran menos información que los carbones dispersos.
Por consiguiente,la distinción entre concentrados y dispersos es tarea obligada durante la fase de recogida.
Nosotros nos hemos atenido a este supuesto,seguido por numerosos trabajos antracológicos recientes,tan sólo en la fase de muestreo.
En los yacimientos cantábricos,los carbones aparecen diseminados por todo el nivel de ocupación,sin configurar ninguna estructura de combustión clara.
De ahí que nos hayamos visto obligados a prescindir del mencionado supuesto durante la interpretación de los resultados.
Entendemos que interviene siempre la selección por parte del hombre,ya se trate de carbones dispersos o de carbones concentrados dentro de un mismo yacimiento.
La diversidad específica que se refleje en cada espectro antracológico dependerá de diferentes factores: económicos (recursos disponibles y buscados,función de cada yacimiento y estacionalidad) y topográficos (diversidad de biotopos).
A pesar de esta selección,la interpretación paleoecológica es posible de acuerdo con la naturaleza de cada especie hallada.
El análisis de laboratorio se propone determinar de qué madera carbonizada se trata.
Para ello,se procede mediante fractura del carbón hecha con la mano orientada hacia los tres planos anatómicos: transversal,longitudinal tangencial y longitudinal radial.
Cada una de estas tres secciones se observa en un microscopio de reflexión.
Esta técnica sencilla y rápida permite estudiar un gran número de muestras sin ningún tipo de manipulación química que las contamine.
Los carbones,una vez determinados,pueden ser recuperados para efectuar dataciones radiométricas.
Los carbones pueden también observarse a través del microscopio electrónico de barrido para una mayor precisión de ciertos detalles anatómicos,gracias a la profundidad de campo que este instrumento posee.
La anatomía de las diferentes especies presenta una amplia variabilidad ; se determinada aquélla ayudándose de claves de identificación propuestas por atlas morfológicos,completados con colecciones de referencia de especies actuales carbonizadas y ficheros fotográficos de las mismas.
Se llega fácilmente al nivel taxonómico de género y de especie ; sólo los géneros de leguminosas ofrecen todavía alguna ambigüedad,y lo mismo puede decirse de las especies de los géneros Juniperus (enebros),Salix (sauces) y ciertas ericáceas atlánticas,debido también a la mala conservación de las muestras.
La ambigüedad se extiende a los robles caducifolios y Betula (abedules) ; si bien en estos casos hay que culpar a posibles hibridaciones.
Una vez efectuado el análisis cualitativo,se procede al tratamiento cuantitativo de los datos.
La unidad de base considerada es un fragmento de carbón arbitrario.
Otros autores prefieren,por unidad de base,la masa media o peso de los carbones.
Obviamente,la identificación botánica requiere el estudio de cada fragmento de carbón,no cada gramo de carbón.
Adviértase,además,que cada taxón presenta una sobrefragmentación o subfragmentación,que influye en la frecuencia relativa del número de fragmentos.
Ambos parámetros,el fragmento y la masa media,conducen a resultados paleoecológicos equivalentes.
Y así hemos utilizado el número de fragmentos,vía más cómoda y rápida a la hora de efectuar análisis antracológicos de un muestrario abundante.
La interpretación antracológica se basa en la variación de las frecuencias relativas de cada taxón.
Pero,¿cuál es el número mínimo de carbones a estudiar para obtener una imagen fiable de la vegetación? Para responder a esa pregunta se crean curvas taxonómicas que ponen el número de taxones identificados en función del número de fragmentos analizados.
En dichas curvas podemos observar que el número de taxones aumenta con el número de fragmentos analizados.
A partir de cierto umbral,la curva tiende a estabilizarse.
De acuerdo con la investigación realizada en Montpellier,esa cifra oscila entre 100 y 600 fragmentos.
Las curvas taxonómicas que nosotros hemos construido ofrecían una amplia variabilidad,ya que la riqueza en taxones no siempre estaba en relación con el número de fragmentos estudiados.
Los ejemplos de la cueva del Castillo y la de Santa Catalina son ilustrativos.
Las razones de una mayor o menor estabilidad de las curvas taxonómicas hay que buscarlas en el tipo de formación vegetal que se pretende reconstituir y,sobre todo,en el hombre y el tipo de actividades económicas ejercidas en cada período cultural.
En otras palabras,se trata de incluir el contexto arqueológico y al hombre en el centro de la antracología: admitir la selección humana de todos los carbones,dispersos o concentrados.
Los resultados de la flora tienen que interpretarse en términos de vegetación.
Para ello es necesario un conocimiento de la vegetación potencial de la zona circundante a cada yacimiento,así como de la repartición de las diferentes asociaciones vegetales de la región entera.
El punto de partida sería,pues,la vegetación actual,primeramente a través del estudio ecológico de cada taxón (autoecología) y,de ahí,pasar a la sinecología o estudio de las agrupaciones vegetales.
Si cada taxón posee un nicho característico,será factible la interpretación estructurada de los carbones dispersos y concentrados.
En la región cantábrica,la distinción entre concentrado y disperso nos ofrecía siempre la misma diversidad florística.
Las investigaciones concernientes a otras zonas peninsulares (Andalucía oriental) y Canarias parecen llegar a las mismas conclusiones.
Se ha de cotejar los resultados paleoecológicos y paleoclimáticos con una las secuencias de referencia regionales de la zona,ya que éstas ofrecen una dinámica continua y general de las fluctuaciones climáticas a lo largo del Pleistoceno superior y Holoceno.
El hombre se halla,pues,en el centro de la antracología.
El es quien realiza las recogidas diarias de leña en un medio que le ofrece recursos económicos de diversa índole.
Las características físicas de la región y el tipo de economía son dos factores importantes.
El resultado de todo ello nos conduce a la reconstitución de las relaciones hombre-medio bajo una óptica paleoeconómica,otro tipo de información que,unido a la reconstrucción y evolución del paleopaisaje,enriquecerá las interpretaciones antracológicas de la madera en cuanto combustible.
¿Qué podemos decir ahora del uso de la madera para la fabricación de utensilios o construcción? Hemos aludido al interés de la escuela de Montpellier por la madera combustible," le bois du feu ",punto de partida de la antracología.
Pero tampoco se ha olvidado de la etnobotánica,por más que la mayoría de las veces se trata de aspectos particulares,como puede ser el consumo de frutos.
El hombre empleó también los carbones para pintar.
Un trabajo reciente efectuado en cuevas cántabropirenaicas y que reúne antracología,dataciones y estudios de arte rupestre,ha ofrecido resultados interesantes.
En resumen,un espectro antracológico puede ser objeto de diferentes niveles de interpretación: paleoecológico,paleoeconómico,paleoclimático,paleoetnológico o arqueobotánico.
Todo depende del aspecto que queramos poner de relieve,aunque para nosotros todos ellos forman un conjunto indisociable y de él depende la mayor o menor riqueza interpretativa.
(Paloma Uzquiano,de la Universidad de Montpellier.
) Los compuestos carbonicos se hallan extensamente distribuidos en nuestro entorno.
Además de su utilización en la industria química,son componentes de alimentos,metabolitos intermediarios y productos de peroxidación de los lípidos,y pueden alcanzar concentraciones bastante altas en el organismo.
Los compuestos o dicarbonílicos se encuentran en cantidades apreciables en café,mantequilla,cerveza y otros alimentos.
En experimentos realizados con la cepa SalmoneIla typhimurium TA100 en el test de Ames,se ha establecido que estos compuestos poseen actividad mutagénica.
El uso de bacterias como material de ensayo es práctica habitual en toxicología genética.
Por constituir los ácidos nucleicos el vehículo universal de la información genética,cualquier daño producido sobre ADN bacteriano debe ser extrapolable,aunque con ciertas limitaciones,a los demás organismos ; ello explica la estrecha relación entre mutagénesis inducida en Salmonella (la capacidad para producir mutaciones en estos microorganismos) y carcinogénesis en mamíferos.
El test de Ames utiliza estirpes mutantes de S. typhimurium incapaces de crecer en medios carentes de histidina.
Nosotros nos hemos servido de la estirpe TA100,en cuyo ADN la secuencia original de un cordón (CTC,que determina el aminoácido leucina) se sustituye por otro triplete (CCC,que codifica la prolina) en el mutante.
Cuando se forme la cadena complementaria del ADN del mutante aparecerá,por tanto,la secuencia GGG en un sitio específico.
Las cepas de S. typhimurium se muestran sensibles a la acción de mutágenos: carecen,por deleción,del gen que determina el sistema reparador de escisiones y presentan pérdida parcial de los lipopolisacáridos de la pared bacteriana,lo que incrementa su permeabilidad ante moléculas que normalmente no la atraviesan.
Además,la presencia de un plásmido,pKM101 o factor R,merma la capacidad del sistema reparador del ADN.
En el ensayo,se colocan unos mil millones de bacterias mutantes en un medio de cultivo sin histidina,donde no pueden medrar.
Se añade el compuesto a-dicarbonílico,que provoca el tránsito de la secuencia GGG a la GAG,sustitución del nucleótido central que supone la reversión a la forma original de la bacteria.
Las bacterias así mutadas pueden desarrollarse en un medio sin histidina y formar colonias.
El número de estas colonias refleja la potencia del compuesto a-dicarbonílico o la actividad mutagénica.
El mecanismo molecular de la actividad mutagénica de los a-dicarbonilos se ha relacionado con la capacidad que éstos poseen para formar aductos con restos de guanina de los ácidos nucleicos.
(Llámase aducto al compuesto donde se ha producido la adición del derivado a-dicarbonílico a la molécula de guanina.
) Para el autor,las actividades mutagénicas dependen de la estructura de los compuestos a-dicarbonílicos y de su reactividad química,esto es,de su capacidad para formar aductos.
Para demostrarlo,escogimos una serie de posibles mutágenos ; se estudiaron las actividades mutagénicas y sus reacciones con guamna y guanosina.
En primer lugar,se comprobó que el acetilbenzoilo es tóxico a dosis muy bajas,del orden de micromol por caja ; por encima de estas dosis,el número de revertientes es menor que la propia cifra de revertientes espontáneos.
A dosis inferiores no se detectó actividad mutagénica: el número de revertientes no difiere del de espontáneos.
(Las medidas de actividad mutagénica se dan en revertientes por micromol.
) Otro compuesto,el glioxilato sódico,resultó ser inactivo.
En este caso,se pudo llegar a dosis más elevadas sin que el producto mostrara toxicidad.
Ahora bien,una tendencia creciente nos indica que es activo frente a la estirpe TA100,aunque con una potencia mutagénica muy baja.
Algo semejante ocurre con la canforquinona.
Los aldehídos son claramente mutagénicos,pues el número de revertientes netos es proporcional a la dosis,registrándose hasta 2500 revertientes por caja.
Superan de lejos a las de las cetonas,lo que se explica por la mayor reactividad del grupo carbonilo en el primer caso.
El mecanismo de acción de los mutágenos se relaciona con la interacción con los restos de guanina de los ácidos nucleicos y,por tanto,con la capacidad para formar aductos con dichos restos.
Las reacciones de formación de aductos se han estudiado fundamentalmente por espectroscopía en el ultravioleta y en el visible,pero existe un inconveniente para el uso de esta técnica,aparte de su baja sensibilidad: sólo puede utilizarse con glioxal y metilglioxal,que no dan espectro en la zona de longitudes de onda empleadas ; no puede usarse el resto de los compuestos estudiados,ya que absorben fuertemente a dichas longitudes de onda.
Por tanto,se hace necesario plantear otro enfoque experimental.
Ahora bien,las técnicas electroquímicas presentan el problema de que hay que conocer muy bien los procesos de oxidación o reducción de los reactivos.
Afortunadamente,los compuestos a-dicarbonílicos han recibido especial atención y su comportamiento electroquímico se conoce con suficiente detalle.
Los compuestos dicarbonílicos suelen estar hidratados en disolución acuosa.
Para poder determinar las constantes de formación de los aductos,hemos de calcular las constantes de equilibrio de hidratación,KH.
En todos los casos hemos utilizado la voltametría de barrido lineal de potencial a velocidades de barrido suficientemente altas ; hemos recurrido,pues,a técnicas electroquímicas.
Se han calculado las constantes de hidratación de los diferentes derivados carbonílicos en función de la temperatura,para obtener en cada caso la variación de entalpía del proceso de hidratación.
Los valores de KH nos permiten calcular las actividades mutagénicas corregidas por la hidratación,ya que es el compuesto dicarbonílico libre el que reacciona con los restos de guanina para producir esta actividad.
En los aldehídos,la actividad mutagénica del fenilglioxal es menor que la del metilglioxal y la de éste,inferior a la del glioxal ; en las cetonas,la actividad mutagénica de la canforquinona es menor que la de la 3,4 - hexanodiona,inferior a la del diacetilo.
De lo que se infiere que disminuyen al aumentar el tamaño del sustituyente ; y de ese modo parece que es el efecto estérico el que determina la actividad dentro de cada grupo,por encima de los factores electrónicos.
Teniendo en cuenta la influencia de los efectos estéricos se podría calcular la actividad del acetilbenzoilo,que resulta ser semejante a la de la 3,4 - hexanodiona.
La distinta actividad de cada grupo de compuestos debe guardar relación con las diferencias moleculares del agrupamiento a-dicarbonílico,que se ve alterado por los sustituyentes adyacentes a los átomos de carbono,lo que se traduce en variaciones de las energías de los orbitales HOMO y LUMO,responsables de la reactividad.
Los potenciales de reducción se pueden relacionar con la energía del LUMO de las moléculas.
Así pues,podemos tomar dichos potenciales como parámetros relacionados con la estructura molecular.
Se observa entonces una correlación positiva entre la actividad mutagénica corregida y el potencial de onda media,por lo que se concluye que la actividad mutagénica está relacionada con la energía del LUMO del agrupamiento a-dicarbonílico y,por tanto,con la estructura de los compuestos estudiados.
Las constantes de formación de los aductos entre los compuestos a-dicarbonílicos y guanina y guanosina se han obtenido también por técnicas electroquímicas,a partir de la influencia de esta reacción química sobre los procesos de reducción.
Los valores de KF en el caso de la guanosina son alrededor de 10 veces mayores que los conseguidos con guanina.
También en nuestro país el telégrafo óptico vertebró una primitiva red de comunicaciones.
I Dan fe de ello las torretas en ruina que todavía coronan cerros y colinas por numerosos lugares de,la península.
En 1850 una larga cadena de torres podía hacer llegar mensajes desde Cádiz hasta Irún o hasta Barcelona por medios puramente ópticos.
Y si bien es cierto que tal red,al nivel de las más avanzadas de Europa,nacía con cincuenta años de retraso,ello no puede achacarse a falta de esfuerzo y de ingenio sino al caos y desgobierno en que vivió este país durante gran parte del siglo pasado.
El verdadero iniciador del telégrafo óptico en España fue el científico canario Agustín de Betancourt,que disfrutó en Francia de una beca del rey Carlos III y mantuvo estrechas relaciones con las Academias de aquel país.
En 1794 conoció y examinó con gran interés el invento de Chappe,así como el telégrafo que simultáneamente desarrollara el inglés Murray.
El resultado fue la concepción de un sistema original de telegrafía óptica que sometió primeramente - - de 1796 a 1798 - - al examen de la Academia de Ciencias francesa en abierta competición con el sistema de Chappe.
Pese a los elogiosos informes de la Academia,la autoridad de Chappe vetó la introducción en Francia del sistema.
Betancourt regresó entonces a su patria,y aquí sí fue premiada su inventiva: Carlos IV dispuso en 1799 la construcción de una línea de telegrafía óptica entre Madrid y Cádiz.
Tal línea comprendería unas 60 estaciones intermedias y,de haberse terminado a su tiempo,hubiese sido la segunda de Europa.
El propio Betancourt había de dirigir la obra y fabricar los aparatos necesarios.
Sin embargo,parece que la línea no llegó a funcionar sino entre Madrid y Aranjuez,y más bien en plan experimental.
La escasísima información disponible no permite especular sobre las causas del fracaso.
Por supuesto serían de índole técnica,pues el sistema Betancourt superaba en no pocos aspectos a sus contemporáneos.
En efecto,la transmisión de señales podía ser alfabética o codificada,y se basaba en la posición angular de una flecha montada en la parte superior de un mástil (a modo de una T).
Dicha posición se controlaba mediante un torno,que al mismo tiempo hacía girar un catalejo (en la estación cabecera de línea) o dos catalejos (uno " a " hacia atrás y otro " b " hacia adelante,en las estaciones intermedias) de tal forma que mantuviera el paralelismo entré la citada flecha y un hilo diametral de la lente.
De este modo,con un solo movimiento de manivela se aseguraba no solamente la recepción de la señal enviada por la estación precedente (identificada por la posición de la rueda del torno al colocar el hilo de la lente del catalejo " a " paralelo a la flecha de aquella estación),sino también la retransmisión automática de esa señal hacia la estación siguiente.
En efecto,al mismo tiempo se posicionaba la flecha de la propia estación y el hilo del ocular del catalejo " b ",lo que permitía comprobar la correcta recepción de la señal por la estación siguiente.
Otro ensayo de telegrafía semafórica,limitado al ámbito militar,fue el diseñado por el coronel Hurtado y que funcionó entre 1805 y 1820 en la bahía de Cádiz.
Posteriormente (1830) el oficial de marina Juan José Lerena demostró un telégrafo óptico de su invención y estableció una nueva línea entre Madrid y Aranjuez,a la que seguirían otras entre Madrid y San Ildefonso (La Granja),Madrid y El Pardo,y finalmente Madrid-Valladolid,que sólo llegó hasta la Sierra de Guadarrama por corte de la asignación.
La convulsa historia de nuestro país entre 1808 y 1843 puso serias trabas a todo serio intento de progreso aplicable a la sociedad civil.
Tuvo que llegar la mayoría de edad de Isabel II y la relativa calma del período siguiente para que finalmente se estableciera un sistema telegráfico nacional.
Así,en 1844,un Real Decreto creaba v regulaba el servicio de Telégrafos.
Por aquellas fechas ya funcionaban telégrafos eléctricos en algunos países,y se daba por seguro que llegarían a prevalecer sobre los viejos sistemas ópticos.
En España,no obstante,se decidió que la nueva red fuera óptica,muy probablemente por la mayor seguridad que ofrecía el sistema de torretas - - en realidad,pequeñas fortalezas - - ante los ataques de bandoleros y guerrilleros,y también porque la complicada orografía española dificultaba el tendido de líneas con postes y conductores.
Por otro lado,la organización del servicio a través de la red óptica iba a servir como transición y entrenamiento para el telégrafo eléctrico,que se iniciaría en 1855.
En sus principios,la telegrafía tuvo la finalidad primordial de mantener el orden público en todo el territorio español.
Fue por lo tanto un eficaz instrumento del gobierno,y sólo mucho más tarde,ya en época del telégrafo eléctrico,se abriría al uso público.
Su explotación se encomendó a la Dirección General de Caminos,Canales y Puertos,y posteriormente a la nueva Dirección General de Telégrafos,dentro del Ministerio de la Gobernación.
El coronel José María Mathé,diseñador de un sistema semafórico enteramente original,fue encargado de la construcción y gestión de la red óptica.
La primera línea,Madrid-Irún (por Valladolid y Burgos),se puso en servicio en octubre de 1846.
Comprendía 52 torres,y tenía su cabecera en el cuartel de Guardias de Corps (llamado luego del Conde Duque),en Madrid.
A ésta siguieron otras dos líneas: la de Madrid a Barcelona por Valencia,y la de Madrid-Cádiz,con 59 torres,inaugurada en 1853.
Con estas líneas entroncaban varios ramales (Cuenca,Badajoz,Zaragoza,etcétera).
Las torres del sistema de telegrafía óptica de Mathé ostentaban como dispositivo señalizador un bastidor con tres bandas oscuras paralelas,regularmente espaciadas.
Por unas guías verticales situadas en el centro del bastidor se hacía ascender o descender una pieza móvil (el indicador),cuya posición determinaba la cifra de código transmitida (del cero al nueve) y,en su caso,las señales de repetición y de error.
De modo análogo,a un costado del bastidor había una bola que se desplazaba verticalmente para señalizar incidencias del servicio,tales como averías,niebla,ausencia o cualquier evento que impidiera el debido funcionamiento.
Cada torrero repetía la señal recibida de la estación anterior y se aseguraba de su correcta recepción por la estación siguiente.
Los mensajes telegráficos iban cifrados y las claves variaban periódicamente,dado el carácter reservado y exclusivamente oficial de las comunicaciones cursadas.
Por ello los torreros solamente podían entender y verificar el preámbulo y el final de los " telegramas ": la categoría del mensaje,la estación de origen y la de destino,la fecha y la hora,el número de registro y la extensión del texto en grupos de cifras de código.
Poco a poco,cundió entre las capas más ilustradas de la nación la idea de que el sistema óptico había perdido vigencia,y que había que reemplazarlo por el telégrafo eléctrico,no afectado por fenómenos meteorológicos tan frecuentes como la niebla o la intensa lluvia.
Todo ello preparó el camino para la implantación oficial de la telegrafía eléctrica.
Esta tuvo lugar en el año 1855,bajo la dirección del mismo José María Mathé,ya por entonces Brigadier y Director General del nuevo Cuerpo de Telégrafos.
En 1857 se desmontó la última torre óptica,en la línea de Andalucía.
Muy pronto de ellas sólo quedó el mudo recuerdo de unas piedras sobre las colinas,pero los torreros lograron ser reconvertidos a telegrafistas,y el telégrafo se popularizó abriendo cauces para actividades privadas que harían progresar la sociedad española.
La vida de los seres depende en buena parte de la acción de las proteínas,sus biomoléculas más versátiles.
La expresión del mensaje genético contenido en los ácidos nucleicos se realiza,sobre todo,a través de las proteínas ; a saber: mediante enzimas que facilitan la replicación y transcripción del ADN ; enzimas que promueven la obtención de energía y la síntesis de otras biomoléculas ; proteínas transportadoras ; proteínas estructurales y reguladoras ; proteínas reparadoras y proteínas destructoras,entre otras.
Las proteínas,y entre ellas las enzimas,se forman a partir de veinte aminoácidos distintos.
La combinación de éstos en largas cadenas,con centenares de unidades cada una,posibilita la formación de millares de proteínas diferentes,adaptadas a los diversos procesos biológicos.
Los organismos se valen del control de la actividad de esta multitud de proteínas para cumplir sus propias funciones.
Pero,¿cómo poder controlar específica y particularmente tal cantidad de proteínas distintas? La respuesta constituye el cuerpo del artículo.
Los mecanismos de control de la actividad proteica han ido perfeccionándose a lo largo de la evolución.
En ese camino,han adoptado diversas formas: control genético de la cantidad de proteína sintetizada,control directo de su actividad mediante substancias como substratos o inhibidores que se unen a sus centros activos (que son las regiones de la molécula donde se lleva a cabo su acción biológica),o control indirecto de su actividad por unión de substancias fuera de su centro activo (regulación alostérica).
Dicho control también puede efectuarse por modificación química de aminoácidos implicados en el plegamiento espacial de la proteína o en su funcionamiento.
Estas formas de control suelen ser reversibles,lo que significa que el proceso se inicia y cesa,para volver a iterarse,cuantas veces sea preciso a lo largo de la vida de la proteína.
El control es irreversible cuando se produce una degradación parcial de la proteína,acometida por unas enzimas denominadas proteasas.
A este proceso de degradación parcial lo llamamos proteolisis limitada.
Los mecanismos de control reversible se caracterizan por su universalidad: la actividad biológica de prácticamente todas las proteínas resulta afectada por uno o varios de ellos al mismo tiempo.
En cambio,los mecanismos de control irreversible suelen presentar una distribución más restringida,estando dedicados a proteínas que poseen una acción biológica muy potente o de especial riesgo para su entorno (enzimas degradadoras,neuropéptidos,hormonas y otras),para las que es necesario un control de su actividad hasta alcanzar el lugar de destino y hasta el momento apropiado.
Estas proteínas se sintetizan,por lo común,en forma de precursores inactivos con una extensión peptídica que bloquea su funcionalidad,extensión que es eliminada o modificada por proteolisis limitada cuando son activadas.
Otras veces,la degradación limitada de proteínas tiene relación con los procesos encargados de dirigir las proteínas recién sintetizadas hacia su definitiva ubicación intracelular o extracelular.
Este es el caso de los segmentos " pre ",secuencias aminoterminales adicionales con las que se sintetizan las proteínas que deben ser secretadas fuera de la célula o que tienen su destino en membranas o en orgánulos celulares.
Los segmentos en cuestión conducen la proteína hasta su asiento final,cumplido lo cual son eliminados por proteolisis limitada.
Además de la degradación limitada,debemos considerar la degradación total que sufren todas las proteínas al final de su vida.
Gracias a este mecanismo de control irreversible se ajusta la concentración de proteínas en tejidos y fluidos biológicos mediante el equilibrio entre degradación y síntesis.
Ello permite,además,su renovación constante,eliminando las afectadas por procesos de envejecimiento.
Las moléculas responsables de los mecanismos de control de la actividad proteica por degradación irreversible son las proteasas,enzimas de gran rendimiento catalítico,es decir,capaces de catalizar a gran velocidad reacciones hidrolíticas en las cadenas polipeptídicas de las proteínas substrato.
La acción proteolítica de estas enzimas se halla controlada,a su vez,por inhibidores,cuyo efecto es el de impedir que su acción degradativa se lleve a cabo o se extienda.
Y,al ser las proteasas moléculas peligrosas para su entorno,se suelen sintetizar en forma de precursores inactivos,las proproteasas o zimógenos.
El estudio de las proteasas y de sus precursores está facilitando una mejor comprensión de los procesos de regulación por proteolisis limitada,así como de los mecanismos de inhibición y activación de precursores mactivos en general.
Se conoce c bastante bien la bioquímica de las n proteasas,su estructura tridimensional y mecanismo de actuación,comPletándose entre las primeras enzimas que se cristalizaron.
A tenor del aminoácido o metal que posean en su centro activo,podemos clasificar las proteasas en cuatro familias: serinaproteasas,asparticoproteasas,cisteinaproteasas y metaloproteasas.
Pertenecen a la primera familia varias enzimas digestivas,como la tripsina,la quimotripsina y la elastasa.
Las tres se caracterizan por constar de dos barriles beta entre los que se encuentra el centro activo,donde hay un aminoácido serina indispensable para su acción catalítica.
Los barriles beta deben su nombre a que la cadena polipeptídica adopta una estructura extendida y ondulada (estructura beta) que se repliega en segmentos antiparalelos que adoptan la forma de un barril.
Las serinaproteasas intervienen en la coagulación sanguínea,defensa inmunitaria y fecundación,entre otras funciones.
La segunda familia está representada por la pepsina gástrica,proteína que consta de dos regiones globulares o dominios similares,formados esencialmente por cadenas beta,y asociados en forma de " croissant ",entre los cuales discurre una amplio surco que contiene el centro activo.
El aminoácido esencial es,aquí,un aspártico.
En este mismo grupo de proteasas se inscribe la quimosina,responsable de la coagulación gástrica de la leche en mamíferos neonatos,y la renina,enzima que circula por el torrente sanguíneo y activa la angiotensina,hormona que controla la presión arterial y el balance de electrolitos.
La tercera familia,la de las cisteinaproteasas,encuentra en la papaína uno de sus miembros más representativos ; se extrae de la papaya.
La estructura de la papaína contiene dos dominios abrazados,lo que le sirve de protección contra la autólisis,con un aminoácido cisteína no oxidado en su centro activo.
Diversas catepsinas,mayoritarias en los lisosomas (orgánulos celulares en donde se degradan proteínas),son cisteinaproteasas.
Se ha observado la participación de cisteinaproteasas en la replicación y acción de parásitos y virus,así como en procesos patológicos,tales como distrofia muscular,osteoporosis e invasividad tumoral.
Por último,los componentes de la heterogénea familia de las metaloproteasas poseen un metal,zinc por lo común,en su centro activo.
Sus cadenas polipeptídicas presentan plegamientos tridimensionales muy diversos.
Pertenecen a esta familia las carboxipeptidasas digestivas y tisulares y algunas aminopeptidasas,así como las metaloendopeptidasas.
Las últimas citadas,entre las cuales destacan las colagenasas,intervienen en la degradación del tejido conjuntivo y revisten interés en patologías reumatoides y en invasión tumoral.
¿Comparten las cuatro familias algún rasgo en el mecanismo que siguen para controlar la actividad de las proteínas? Sí,y consiste en que las proteínas susceptibles de regulación por proteolisis limitada suelen sintetizarse en la forma de proproteína.
es decir,como precursor inactivo.
La acción de la proteasa sobre la proproteína provoca la ruptura de uno o varios enlaces peptídicos,generando la forma activa de la proteína.
En unos pocos casos,sin embargo,esa proteolisis limitada no actúa como generadora de la actividad de la proteína substrato,sino que produce un cambio en su funcionalidad.
La proteolisis limitada suele restringirse a determinadas regiones de la proteína localizadas frecuentemente en el extremo aminoterminal de su secuencia de aminoácidos o entre dominios globulares.
Esas regiones presentan un plegamiento espacial laxo para facilitar la acción proteolítica y,a veces,una secuencia de aminoácidos que ha de ser reconocida por la proteasa activante.
Cuando los péptidos escindidos de la proproteína pertenecen a una zona interna,la proteína activa queda,de ordinario,separada en dos o más trozos o cadenas polipeptídicas,que no se sueltan del todo,sino que permanecen unidas a través de puentes disulfuro.
Este sería el caso de la insulina,hormona cuya deficiencia provoca la diabetes: se sintetiza en forma de una sola cadena,la proinsulina,y se convierte en dos,A y B,al ser activada por proteolisis limitada y perder un trozo central,cadena C. Ocurre así también en numerosas proproteínas implicadas en la coagulación sanguínea.
Las proproteínas pierden uno o varios péptidos en la proteolisis.
La naturaleza tiende a ahorrar la pérdida de grandes regiones proteicas que son de síntesis costosa.
En alguna ocasión,sin embargo,los péptidos o fragmentos de activación eliminados pueden superar,en tamaño,a la propia proteína activa.
Nos referimos,por ejemplo,a la protrombina y al plasminógeno,proteínas sanguíneas que pierden,respectivamente,274 y 560 aminoácidos de sus 582 y 791 iniciales.
Las proteínas de la coagulación sanguínea nos sirven de modelo para comentar la cascada de activación.
En virtud de este fenómeno,característico de los sistemas regulados por proteolisis limitada,varios precursores inactivos de proteasas se activan consecutivamente de forma específica,multiplicando así la acción inicial de unas pocas moléculas.
A modo de ejemplo,el fibrinógeno,para cumplir su misión formadora de coágulos que taponen las heridas,debe transformarse proteolíticamente en fibrina ; a esa transformación le precede una cascada de activación muy compleja en la que intervienen,por lo menos,una docena de factores,muchos de los cuales son proproteasas que se activan en un orden secuencial.
La existencia de esa cascada de coagulación permite que la activación de unas pocas moléculas de factores iniciales en el momento de producirse la herida (procalicreína,factor XII,factor VII,etcétera) engendre miles de moléculas de fibrina.
La proteolisis limitada opera cambios en la estructura tridimensional y en la funcionalidad de las proteínas.
Sírvanos de muestra los zimógenos,o precursores inactivos,de proteasas digestivas.
Por estudios comparativos de difracción de rayos X de cristales de serinaproteasas (tripsina y quimotripsina) y de sus zimógenos (tripsinógeno y quimotripsinógeno) se sabe que la proteolisis limitada da lugar a la rigidez y definición del centro activo,es decir,crea el ambiente espacial adecuado para los aminoácidos implicados en la catálisis y sitúa los aminoácidos de la enzima que fijarán específicamente el substrato de la reacción.
La investigación llevada a cabo en el laboratorio de H. Neurath y K. Walsh,de la Universidad de Washington,y en el Instituto Max Planck de Munich por R. Huber y W. Bode,ha resultado de gran importancia en el conocimiento de estos sistemas.
Se conocen peor los cambios estructurales y de función de los zimógenos de proteasas reguladoras,como las implicadas en la coagulación de la sangre o en la transformación proteolítica de la acetilcolinoesterasa,enzima que hidroliza la acetilcolina después de que ésta haya realizado su acción neurotransmisora.
La acetilcolinoesterasa sufre una proteolisis limitada que la escinde en dos fragmentos con actividad tripsina y carboxipeptidasa,respectivamente.
No se conoce la función de las proteasas generadas,aunque se ha sugerido su participación en procesos de proliferación y crecimiento neuronal.
Son numerosos los sistemas proteicos regulados por digestión proteolítica limitada.
Gracias a investigaciones sobre el ADN de genes que codifican proteínas sabemos que es frecuente que se sinteticen con regiones extra,y que esas regiones extra van desapareciendo durante los procesos de transporte y maduración de las proteínas.
Así ocurre en el colágeno del tejido conjuntivo ; esta proteína sufre,durante su maduración,la degradación de sus extremos globulares una vez se han ensamblado en hélice las regiones centrales de tres de su moléculas,estructura que explica sus propiedades mecánicas.
Enumerar todos los sistemas en los que interviene la proteolisis limitada requeriría demasiado espacio.
No obstante,no deberíamos dejar de mencionar que la proteolisis limitada interviene también en el desarrollo celular,la fecundación,la transmisión de señales hormonales y nerviosas o en los mecanismos de defensa inmunitaria,por citar los más notorios.
Dijimos antes que el segmento " pre " era una secuencia aminoterminal que ciertas proteínas poseen,responsable en muchos casos de su destino celular.
Este segmento,de 16 a 26 aminoácidos de longitud en células eucariotas,es la primera parte de la proteína que se sintetiza en el ribosoma ; esta secuencia señalizadora suele ser escindida por acción de proteasas específicas,una vez que la proteína ha atravesado la membrana del retículo endoplasmático o del orgánulo celular al que va destinada.
Desde que en 1970 D. Sabatini y G. Blobel,entonces en la Universidad de Yale,propusieron la hipótesis de la secuencia señal según la cual la señal responsable de la unión de los ribosomas al retículo endoplasmático (y por tanto del inicio del tránsito de las proteínas hacia su destino extracitoplasmático) es una secuencia situada en el extremo aminoterminal de la cadena polipeptídica naciente,se han caracterizado multitud de segmentos " pre " en proteínas,lo mismo en eucariotas que en procariotas.
Muchas proproteínas son también preproproteínas ; en particular,todos los zimógenos digestivos y proproteasas de fluidos biológicos,así como prohormonas (proinsulina) y protoxinas (promelitina).
Las proteasas cumplen también una función destacada en la replicación vírica.
En retrovirus (el del sida incluido),rinovirus (el del catarro común entre ellos),enterovirus (por ejemplo,los de la polio y de la hepatitis A),y en diversos virus de plantas,se ha demostrado que las proteínas de la cápside se sintetizan en forma de poliproteínas precursoras y que,antes de su ensamblaje,deben escindirse en sus unidades mediante degradación limitada a cargo de proteasas.
En los virus la proteasa está ya incluida en la larga cadena de una poliproteína precursora.
La acción autocatalítica de la proteasa favorece su liberación al medio y la escisión de las demás unidades proteicas.
La especificidad enzimática y la secuencia de aminoácidos varían de una proteasa vírica a otra,aunque siempre podemos reducirlas a una de las cuatro clases mencionadas.
Las proteasas de los retrovirus,por ejemplo,son homólogas de asparticoproteasas ; las de rinovirus y ciertos virus de plantas se incluyen entre las serinaproteasas,no obstante poseer algunas de ellas un aminoácido cisteína reactivo en su centro activo ; en los picornavirus,adenovirus y togavirus se han encontrado cisteinaproteasas.
Se sospecha que algunas proteasas de virus necesitan la concurrencia de las proteasas de las células huésped y de otros factores para procesar las poliproteínas,sin que ello contradiga la opinión de que la mayoría de las proteasas de que se valen los virus estén codificadas en su propio ADN.
Por eso,el diseño de inhibidores específicos contra las proteasas víricas puede convertirse en eficaz estrategia antivírica con bajo riesgo para el huésped.
Así se espera que ocurra con el síndrome de inmunodeficiencia adquirida (SIDA).
Después de clonar y secuenciar el ácido nucleico constitutivo de una de sus formas (la HIVI,la más infectiva),se consiguió sintetizar invitro la proteasa responsable del procesamiento de sus poliproteínas precursoras.
A continuación,se cristalizó la proteasa y se determinó su estructura tridimensional mediante difracción de rayos X. Esta estructura es muy semejante a la de la proteasa del virus del sarcoma de Rouy a la pepsina de mamíferos.
Sin embargo,existe una gran diferencia entre el número de aminoácidos de la pepsina (327 residuos) y el de las proteasas de virus,que no llegan a la mitad de esta cifra ; dicho de otro modo,el virus del sida se ahorra,para codificar su proteasa,la mitad de la cantidad de ácido nucleico que los animales emplean para sintetizar la pepsina.
Para compensar esta diferencia,se hace necesario que dos unidades de proteasa se asocien y conformen el centro activo.
Actualmente,y gracias a estos conocimientos estructurales,ya se han podido diseñar inhibidores específicos de la proteasa del virus del sida.
De acuerdo con ciertas pruebas clínicas preliminares,podrían frenar el desarrollo del virus sin comportar efectos laterales.
Se cree que pueden constituir un fármaco más seguro que las vacunas contra proteínas de superficie del virus,ya que éstas mutan muy rápidamente a lo largo de diversas generaciones del microorganismo.
Pero las proproteínas más estudiadas y que han servido de modelos para las demás han sido los zimógenos de proteasas digestivas.
Abordaremos las procarboxipeptidasas,precursoras de las carboxipeptidasas,proteasas cuya misión consiste en degradar secuencialmente las proteínas substrato a partir de su extremo carboxiloterminal.
Estos zimógenos se presentan en el páncreas de los vertebrados en formas de elevada masa molecular (unos 400 aminoácidos),que generan enzimas activas de especificidades distintas: las carboxipeptidasas A y B. Las enzimas activas A digieren proteínas alimentarias,escindiéndoles los aminoácidos apolares ; las enzimas B escinden aminoácidos cargados positivamente (lisinas y argininas).
Las procarboxipeptidasas A se suelen encontrar en mezclas de formas monoméricas y de complejos cuaternarios con otras proproteasas,mientras que las procarboxipeptidasas B se hallan siempre constituidas en monómeros.
La resolución de la estructura tridimensional de las procarboxipeptidasas pancreáticas y la identificación de sus mecanismos de inhibición y activación se han llevado a cabo en nuestro laboratorio de la Universidad Autónoma de Barcelona en colaboración con el laboratorio de R. Huber,del Instituto Max Planck de Munich,y el de K. Wuthrich,del Instituto Politécnico de Zurich.
Para ello,cristalizamos las proenzimas A y B de páncreas de cerdo y estudiamos su estructura por difracción de rayos X. Se observa que el gran segmento de activación - - formado por los 95 residuos aminoterminales en cada zimógeno - - adopta una conformación globular y cubre el centro activo de la región enzimática,impidiendo la correcta fijación de substratos.
La técnica de resonancia magnética nuclear nos ha permitido conocer el plegamiento espacial del segmento de activación aislado en disolución y comparar esta estructura con la que se muestra integrada en el zimógeno.
En los dos zimógenos se ha observado que cuatro regiones distintas del segmento de activación se pliegan en cinta beta y forman una hoja beta antiparalela,es decir,se sitúan una al lado de la otra con sentidos de avance contrarios.
Sobre una de las caras de esta hoja beta se colocan dos hélices alfa que unen las cuatro cintas beta.
La otra cara de la hoja beta se une al centro activo de la región enzimática.
Durante la digestión,y al ser secretadas al intestino junto con otras hidrolasas pancreáticas,las procarboxipeptidasas A y B sufren la degradación de su segmento de activación y se tornan funcionales.
En esa proteolisis limitada,en la que la acción hidrolítica principal corresponde a la tripsina,ambas proenzimas pierden un cuarto de su molécula.
La tripsina produce un corte rápido en el nexo de unión entre la región de la enzima activa y su segmento de activación.
Después de este primer paso,la tripsina prosigue lentamente su acción hidrolítica sobre aminoácidos adyacentes,recortando el segmento de activación desde su extremo carboxiloterminal.
En esta acción de recorte participa la propia carboxipeptidasa generada.
A medida que va mermando la longitud del segmento de activación,se van debilitando el número y la intensidad de las fuerzas de tipo no covalente que lo unen a la enzima activa y provocan su inhibición,hasta que se produce la disociación total.
El proceso de disociación,y la activación consecuente,ocurren con mayor celeridad en las procarboxipeptidasas B que en las A. Quedan diversas preguntas en el aire.
¿Qué función biológica desempeñan los complejos cuaternarios de estas proenzimas con otras proproteasas? ¿Pueden desempeñar los fragmentos de activación,una vez liberados una función secundaria,tal como de tipo hormonal o transportadora? La utilización de un segmento de 95 aminoácidos (tamaño equivalente al de muchas proteínas pequeñas) para mantener inactivas tales proenzimas y acometer su degradación posterior,parece un dispendio excesivo,en especial si recordamos que,en ciertas carboxipeptidasas tisulares (las del sistema nervioso,por ejemplo),podría bastar un segmento de activación de 5 aminoácidos para el mismo fin.
Existen,en efecto,carboxipeptidasas no digestivas,que cumplen importantes funciones proteolíticas de regulación en el procesamiento de hormonas Y neuropéptidos (carboxipeptidasa H / E),en el control de la actividad de péptidos y hormonas peptídicas en circulación por el torrente sanguíneo (carboxipeptidasa N),en el control tisular de la actividad de hormonas peptídicas (carboxipeptidasa M) y en fenómenos de proteolisis en reacciones anafilácticas (carboxipeptidasa MC).
En todos los casos citados se han encontrado secuencias de aminoácidos que descienden,evolutivamente hablando,de las carboxipeptidasas pancreáticas.
No obstante,sólo en las formas H / E y MC se ha observado la presencia de formas precursoras y de segmentos de activación.
Por si fuera poca la variedad de proteasas de los sistemas biológicos,en los últimos años se está descubriendo que los seres vivos han desarrollado todo un arsenal de moléculas inhibidoras,muchas de ellas de estructura proteica.
A veces,esos inhibidores protegen a los tejidos o fluidos biológicos de la activación prematura de proproteasas,de la pérdida de proteasas de orgánulos que los contienen (como los lisosomas),o del ataque de las proteasas de organismos invasores o predadores ; en otros casos,sin embargo,los inhibidores modulan la actividad proteolítica.
Pertenecerían a este último grupo los inhibidores de proteasas que se hallan en la sangre,fluido que puede considerarse una auténtica sopa de inhibidores (a ellos corresponde un 10 por ciento,en peso,de las proteínas de la sangre).
El más espectacular de estos inhibidores sanguíneos es la alfa macroglobulina,proteína de unos 700 aminoácidos,dividida en cuatro subunidades que forman en el espacio una gruta en cuyo interior se esconde el sitio inhibidor.
Este inhibidor,que se encuentra en cantidades de aproximadamente 2,6 gramos por litro de sangre,une proteasas de diversa especificidad,que le producen un corte en el sitio bloqueante ; en virtud de ello se establece un enlace covalente entre ambas moléculas y el cierre de la gruta,englobando a la proteasa e inactivándola.
Otros inhibidores sanguíneos,como el inhibidor alfal de proteinasas,la alfalantitripsina,la alfalantiquimotripsinN,la antitrombina III,y el inhibidor I del activador del plasminógeno,son menores (constan de unos 450 aminoácidos) y muestran entre sí homologías estructurales que indican una semejanza conformacional y un origen evolutivo común.
De hecho,todos estos últimos se inscriben en la familia de las " serpinas ",cuya función inmediata consiste en limitar en el tiempo y en el espacio la acción proteolítica de los factores involucrados en la coagulación de la sangre y en la fibrinolisis.
Los inhibidores proteolíticos acostumbran ser,sin embargo,bastante menores ; poseen su región de inhibición localizada en un pequeño bucle.
Este es el caso de la familia de los ovomucoides (en los huevos de aves y anfibios),las familias de inhibidores pancreáticos de vertebrados,y la de ciertos inhibidores vegetales.
Hablamos de familias,ya que existen muchas variantes de cada inhibidor.
También varía su tamaño,que,a medida que decrece,ofrece mayor proporción de puentes disulfuro estabilizadores.
Todos los inhibidores hasta ahora citados inhiben serinaproteasas,y su bucle de inhibición sufre una rotura proteolítica al unirse a la proteasa.
También hay inhibidores de cisteinaproteasas y metaloproteasas.
Ejemplos de los primeros son las cistatinas,proteínas de unos 100 aminoácidos y sin puentes disulfuro,que se encuentran en lisosomas de distintos tejidos (tiroides,glándula adrenal,células pancreáticas y neuronas corticales),así como en fluidos biológicos (sangre,orina),y las estefinas (en granulocitos polimorfo nucleares) y kininógenos (en sangre).
Entre los inhibidores de metaloproteasas destacan los de carboxipeptidasas que se t han extraído de solanáceas (patata,tomate).
Son proteínas de topología 11 parecida a ciertos inhibidores de serinaproteasas también aislados de vegetales (semillas de calabaza),con un núcleo globular muy pequeño y extraordinariamente estable y una cola que actúa de región inhibidora.
La posibilidad de modular funciones reguladas por proteasas ha c estimulado la investigación aplicada n sobre los inhibidores.
Así ha sucedido con los inhibidores de proteasas vegetales,cuya manipulación genética e inserción en plantas no portadoras ha permitido atajar plagas,al producir transtornos digestivos en organismos fitófagos que las atacan.
De manera parecida,el suministro de inhibidores o el estímulo de su producción podría facilitar la resolución de diversas patologías de animales superiores en las que intervienen proteasas.
Se está ensayando con inhibidores de la trombina como anticoagulantes,de la renina como hipotensivos y de la proteasa neutra de leucocitos en el enfisema pulmonar y en otras patologías del tejido conjuntivo.
Cabe también recordar el uso potencial de inhibidores de proteasas para detener el desarrollo del virus del sida o el progreso de la metástasis en ciertos cánceres,como el de mama,en el que está involucrada la proteasa estromelisina,así como en la prevención de la degeneración artrítica debida a la colagenasa y a otras metaloproteasas.
El estudio de la estructura y función de las proteasas y de los sistemas proteicos regulados por proteolisis limitada ha sido y continúa siendo uno de los campos de investigación principal en bioquímica.
En la actualidad,estas proteínas constituyen uno de los modelos más utilizados para la aplicación de metodologías de manipulación y transformación genéticas y de ingeniería de proteínas,gracias a su variedad de formas,al profundo conocimiento que se tiene de ellas y a la potencialidad de sus aplicaciones biotecnológicas y biomédicas.
Fusión nuclear Altemativa energética EL nueve de diciembre del año pasado marcó un hito en el desarrollo de la energía de fusión controlada.
A las 23 horas,8 minutos,en el tokamak TFRT,del laboratorio de física del plasma de la Universidad de Princeton,se produjeron de manera controlada entre tres y cinco millones de watts de potencia de fusión durante 0,2 segundos.
El experimento se realizó en plasmas de deuterio con un 50 por ciento de mezcla de tritio.
El control de las reacciones de fusión de núcleos de átomos ligeros dejaba así de ser un sueño lejano para asentarse sobre bases científicas y técnicas,que aseguraban la viabilidad de la fusión nuclear como alternativa energética de aquí a treinta años.
La importancia de los experimentos realizados en el 1111 R,acrónimo de reactor tokamak de prueba de fusión,se puede resumir en cuatro puntos.
En primer lugar,confirman los resultados obtenidos en los experimentos anteriores con plasmas de deuterio y tritio en el tokamak del JET,instalado en Culham,operando con deuterio y un 11 por ciento de tritio.
Queda así establecida la capacidad científica y técnica de controlar actualmente reacciones de fusión durante tiempos entre 0,2 y 1 segundos,con la producción de hasta seis millones de watts de potencia,equivalentes a una energía de fusión superior a los tres millones de joules.
En segundo lugar,al utilizar plasmas con concentraciones de tritio del 50 por ciento,se están estudiando por vez primera las reacciones de fusión con la mezcla adecuada de combustible para un reactor de fusión de la primera generación.
En tercer lugar,la potencia de fusión generada y la cantidad de tritio empleada permiten obtener en el TFTR un número suficientemente elevado de partículas alfa,o núcleos de helio.
La energía de las partículas alfa,la quinta parte de la energía total liberada en la reacción,sirve para mantener caliente el plasma a las temperaturas de fusión,en lo que se conoce por plasma autoencendido o en ignición.
Sépase que la investigación de la generación de partículas alfa y del calentamiento del plasma mediante la energía de las mismas constituirá un punto crucial en el diseño del próximo dispositivo de fusión,que habrá de operar en condiciones de reactor casi estacionarias.
Por último,la realización en el TFTR de experimentos con elevadas concentraciones de tritio a alta potencia permite abordar,a lo largo de este año,los problemas tecnológicos relacionados con el manejo del tritio en condiciones de un reactor de fusión: interacción del isótopo con los materiales,inyección,extracción y recuperación del tritio,licenciamiento y seguridad del reactor,etcétera.
No se ve que,en un futuro más o menos mediato,pueda desarrollarse una fuente de producción de energía a gran escala que satisfaga la demanda energética prevista para mediados del siglo XXI: unos 100 billones de watts anuales o 520.000 millones de barriles de petróleo anuales ; esto es,unos 7,6 billones de litros de petróleo anuales o 100.000 millones de toneladas equivalentes de carbón.
Las perspectivas de desarrollo de fuentes de energía en cuantía suficiente se basan en tres posibles alternativas: avances técnicos que abaraten los costes de aprovechamiento de la energía solar,mejoras en la solución de los problemas de seguridad y tratamiento de residuos radiactivos de las centrales de fisión nuclear y control y desarrollo de la fusión nuclear.
Los problemas ambientales y de escasez de las energías basadas en los combustibles fósiles (carbón y petróleo),así como la limitación intrínseca de densidad de potencia (watts por centímetro cúbico) de las energías renovables (geotérmica y eólica) determinan que el porcentaje de contribución de esas energías a la satisfacción de la demanda a largo plazo,aunque seguirá siendo apreciable,no permita que se las considere la solución alternativa,sino meramente complementaria.
La creación y control de reacciones de fusión en las condiciones requeridas en un reactor guiaron las etapas de diseño (1973 - 1978),construcción (1978 - 1982) y operación (desde 1982) de los grandes tokamaks,el JET de la Comunidad Europea,el TFRT norteamericano,el JT - 60 japonés y el T - 15 de la antigua Unión Soviética.
Como consecuencia de las investigaciones realizadas en esos dispositivos a lo largo de los dos últimos años se han dado tres pasos de gigante en el camino del desarrollo de la fusión nuclear.
El primero consistió en el control de las reacciones de fusión durante un segundo,con producción de energía de fusión de 2,2 megajoules en plasmas de deuterio mezclado con un 11 por ciento de tritio.
Sucedió en noviembre de 1991,en el JET.
Medio año largo después,en julio de 1992,la Comunidad Europea aprobó,ante el éxito del JET,la fase de diseño de la ingeniería del ITER,un reactor de fusión experimental.
Tras este segundo paso,el tercero ha sido la producción de seis millones de megawatts con que comenzábamos.
En la primera semana de noviembre de 1991,el tokamat del JET había demostrado que se podía producir,de forma controlada,energía de fusión en el rango comercial de los millones de joules durante un segundo.
Semejante logro abrió las puertas para que los responsables de la política científica internacional firmasen un acuerdo cuatripartito (Comunidad Europea,Japón,Estados Unidos y Confederación de Estados Independientes) sobre el diseño de la ingeniería y anteproyecto del tokamak internacional ITER.
Este tokamak,con un coste estimado de 6000 millones de dólares en caso de construirse antes de 1998,estaría capacitado para producir entre 1000 y 3000 megawatts de energía eléctrica (el triple de la potencia de una central nuclear actual),en forma semicontinua de períodos de 20 a 60 minutos.
El ITER,que comenzaría a funcionar entre 2005 y 2020,serviría para demostrar la viabilidad técnica de la fusión como fuente de energía y constituiría el paso previo para la erección de un reactor de fusión de demostración (DEMO),prototipo ya de un reactor comercial.
Tanto los resultados del JET de 1991,como el comienzo de las actividades del ITER-EDA (acrónimo éste de " actividades de diseño en ingeniería ") de 1992,permitieron trabajar,en diciembre de 1993,con plasmas de deuterio mezclado al 50 por ciento con tritio a inyección de alta potencia (30 megawatts).
Los frutos más notables cosechados han sido los siguientes: producción rutinaria de tres a seis millones de megawatts de fusión controlada ; obtención de altas temperaturas iónicas (350 millones de grados C) durante más de 0,2 segundos,sin signos apreciables de contaminación del plasma ni deterioro de las propiedades termonucleares de los mismos ; producción de energía total de fusión por encima de los 3,6 megajoules ; descubrimiento de inestabilidades en el plasma provocadas por la presencia de partículas alfa ; obtención de una mejora de hasta un 30 por ciento en el confinamiento de la energía en comparación con plasmas de deuterio mezclado con deuterio,y,por último,predicción de la posibilidad de obtener potencia de fusión de hasta 10 megawatts a finales del año en curso.
Resultados que,tomados en su conjunto,confirman la convicción de que la energía de fusión nuclear constituye una alternativa energética,no sólo posible sino también viable.

