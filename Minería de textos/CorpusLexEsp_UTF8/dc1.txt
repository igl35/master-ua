Esto es posible gracias a la diminuta red neuronal que da vida a su cerebro.
Debido a ella,tienen,además,la facultad de aprender y reaccionar ante nuevas situaciones.
Si dos neuronas se activan a la vez ante un estímulo determinado,sus conexiones se refuerzan.
De repetirse,es probable que sea más fácil que la segunda célula nerviosa se dispare cuando lo hace la primera.
Esto es lo que se llama aprendizaje hebbiano.
Los mbitis se reproducen de forma sexual.
Cuando dos de ellos se aparean sus genes,al igual que los nuestros,se mezclan.
De dicha combinación nace una criatura con características propias.
Tras sucesivas generaciones,los individuos de una especie pueden extinguirse o evolucionar a una estirpe mejor adaptada al medio.
Los mbitis también mueren.
Unas veces porque agotan su plazo de vida ; otras,debido a que presentan taras genéticas letales,a la escasez de alimento o a que son devorados.
El nombre de este ecosistema es Mbítiworld,y está localizado en los microchips y en la unidad de discos del Departamento de Electrónica y Tecnología de Computadores de la Universidad de Granada.
Su creador ha sido el equipo de jóvenes investigadores y estudiantes que dirige la el físico Juan Julián Merelo.
Este mundo solo existe en el universo de la pantalla de un ordenador,y sus habitantes son el resultado de complejas lucubraciones matemáticas.
Las criaturas tienen un ADN digital,y sus genes están hechos a base de ristras de bits o unidades de información.
La idea - dice Merelo - es dar vida a un orbe con animales cazadores y presas para estudiar cómo comparten el hábitat,qué proporción tiene que haber de cada tipo y otros aspectos ecológicos... Más adelante queremos hacer del Mbitiworld un mundo lo más parecido al real.
Para ello estamos introduciendo nuevos parámetros,como las distintas estaciones del año y diferentes climas.
" Pero mantener un sistema ecológico artificial no es una tarea nada sencilla.
" La mayoría de los ecosistemas con depredadores suelen ser bastante inestables,en el sentido de que tarde o temprano se extinguen.
Esto es debido a que acaban con las presas y,por tanto,se mueren de hambre.
Ello induce a pensar que,en la evolución de la vida sobre la Tierra,los depredadores tuvieron que aparecer muy tarde ",comenta Federico Morán,del Departamento de Bioquímica y Biología Molecular de la Facultad de Químicas de la Universidad Complutense de Madrid,y que ha participado en la construcción de esta naturaleza simulada.
Los mbitis,al igual que otras criaturas diseñadas de esta forma,no tienen ni carne ni cutícula,tampoco huesos y,sin embargo,muchos los consideran seres vivos artificiales.
Son los hijos de una joven ciencia bautizada con el nombre de vida artificial o,como dicen los anglosajones,Artificial Life o A-life.
Su meta es intentar reproducir los aspectos esenciales de los seres vivos en un medio artificial,esto es,construido por el hombre.
" Un objetivo prioritario de esta ciencia es crear y estudiar organismos sintéticos que remeden a los naturales... Intentamos estudiar la vida no como realmente la conocemos,sino tal y como podría ser en un ambiente diferente del terrestre,en otros planetas,con otros materiales e,incluso,en los universos artificiales definidos en un entorno computación,ha manifestado Christopher Langton,uno de sus principales impulsores que desarrolla su labor científica en Los Alamos National Laboratory y el Instituto de Santa Fe,en Nuevo México (EE UU).
Expertos en computación,físicos,microbiólogos,genetistas,matemáticos,biólogos y otros especialistas trabajan mano a mano en la creación de todo tipo de células,animales,plantas y,a menudo,comunidades enteras de las más variopintas formas vivientes.
Son los dioses de una nueva Creación.
Desde siempre,el hombre había soñado con la posibilidad de generar vida artificial,ya fuera ensamblando bielas y engranajes,o manipulando órganos y tejidos en los sótanos de hospitales y laboratorios,como hacía el doctor Frankenstein,para conseguir un autómata capaz de emular algunas de las actividades propias de los seres vivos.
No es raro encontrarse en Los congresos y reuniones de vida artificial a científicos curioseando y disfrutando de los videojuegos que presentan algunas casas comerciales,como la compañía norteamericana Maxis.
Esta ha lanzado al mercado algunos programas,como el SimLife y el SimAnt,diseñados con las técnicas más revolucionarias de esta nueva ciencia.
A diferencia de otros juegos de ordenador,cuyo único objetivo es aniquilar a los invasores o sortear obstáculos.
el usuario puede generar en éstos las criaturas que le vengan a la imaginación,y llevar a cabo con ellas fascinantes experimentos.
Para ello,selecciona ambientes,ecosistemas y comportamientos simulados.
Las plantas y animales interaccionan entre sí,como ocurre en el mundo real.
" Es un juego pacifista que resulta de refinadísimos algoritmos genéticos,dice Vladimir 1. Pokhilko,de la empresa Animatek,en Moscú,cuando se refiere al HFish,un mundo de peces irisados concebido por él.
Además,la hembra debe poder poner otra nidada si fuese necesario.
A pesar de su precocidad,los pájaros de costa sufren terribles pérdidas de huevos y polluelos.
Una población de gallinetas moteadas estudiada en Minnesota en 1975 produjo 157 huevos,pero ningún polluelo: un solo visón se comió 87 huevos.
Igualmente,un estudio de jacanas en la Guyana observó que de 52 nidos habían fracasado 44 en una misma temporada de puesta.
Ante estas dificultades,las aves costeras han llegado a un acuerdo.
El macho se hace cargo de la responsabilidad poco onerosa de sacar adelante por sí solo a los precoces polluelos y deja,por tanto,libre a su compañera para abandonar el nido y engordar de nuevo.
La ventaja a corto plazo para el macho es que ella puede producirle otra nidada rápidamente,en el caso de que un depredador destruya la primera.
A largo plazo,si la hembra no termina agotada en una temporada de reproducción,es más probable que sobreviva para la siguiente,y el macho podrá entonces aparearse de nuevo con ella.
Al igual que en las parejas humanas,parece ser que a las aves que han logrado una relación armoniosa gracias a la experiencia les va mejor en la crianza que a los recién casados.
Pero hay un inconveniente que repercute en la generosidad del macho.
Sus intereses y los de su compañera no son necesariamente los mismos: tanto él como ella intentan maximizar la transmisión de sus propios genes.
Una vez que él ha asumido individualmente la responsabilidad paterna,ella puede utilizar su tiempo en lo que le parezca.
Tal vez escoja permanecer disponible para su compañero,en el caso de que se destruya la primera puesta y él pida otra que la sustituya.
Pero también podría elegir buscar otro macho que esté inmediatamente dispuesto para la segunda nidada.
A la larga,ella puede incluso mejorar,siempre que se modere y no muera de agotamiento.
Naturalmente,otras hembras tendrán la misma idea.
El resultado será que todas ellas se encontrarán compitiendo por una oferta de machos cada vez más reducida.
A medida que avanza la temporada de reproducción,la mayoría de los machos quedan atados a su primera nidada y no pueden aceptar más responsabilidades paternales.
Aunque el número de adultos machos y hembras puede ser igual,la relación de hembras disponibles sexualmente con respecto a los machos puede llegar a ser de hasta 7 a 1 entre las gallinetas moteadas y otros tipos de ave como los falaropos de Wilson.
Estas crueles estadísticas son las que radicalizan aún más el cambio de papeles en las aves de poliandria clásica.
Aunque las hembras ya tenían que ser ligeramente de mayor tamaño que los machos para poner huevos grandes,evolucionan para ser aún mayores y así poder ganar las peleas con otras hembras - y también,de paso,para dominar a los machos -.
La hembra es,por lo tanto,la que hace la corte,al contrario de lo que sucede habitualmente.
La mayoría de las,aproximadamente,200 especies de aves costeras no practica la poliandria,ya que la mayor parte vive en latitudes altas,en donde la temporada de reproducción es muy corta y no deja tiempo para criar una segunda nidada.
Sólo entre la minoría de especies que habita en climas tropicales o templados,como las jacanas,es más fácil de encontrar.
La denominada poliandria cooperativa,algo diferente de la clásica,está perfectamente representada por un tití peruano / que,junto con otras veinte especies más de titíes,se encuentra entre los monos más pequeños del mundo,con un peso de apenas medio kilo.
Estos simios peruanos viven en grupos formados por los padres,su descendencia de años anteriores y las crías pequeñas más actuales.
La combinación de adultos más frecuente es un trío constituido por una hembra y dos machos que la comparten casi por igual.
Puesto que la hembra está receptiva constantemente y no muestra señales visibles de cansancio,sus compañeros se acoplan con ella en cualquier momento,incluso cuando está preñada o en período de lactancia.
La vida sexual del tití en comparación con las normas de la mayoría de los demás mamíferos,excepto los humanos,es sorprendente.
Porque copular cuando la hembra no es fértil resulta un enorme derroche de tiempo y energía para el animal.
Hay dos razones fundamentales por las que los titíes son lo suficientemente perversos como para elegir la poliandria.
Una de ellas se deriva de su tamaño.
Ser el mono más pequeño les hace vulnerables a todo un ejército de depredadores,y por eso los titíes salvajes parecen estar asustados y vigilantes constantemente.
Cuando un grupo está comiendo,uno de ellos permanece de centinela.
Para un tití,intentar la paternidad en solitario sería poco menos que un suicidio.
Incluso la atención de dos padres no basta,debido que uno de ellos estaría forzado a comer y a llevar el cachorro al mismo tiempo,lo que no es una tarea fácil dada su pequeña envergadura.
La necesidad de la poliandria es aún más imperiosa por otra razón.
Para compensar las pérdidas ante sus depredadores,los titíes generalmente traen al mundo gemelos (al ser primates de doble mama no pueden permitirse camadas más grandes).
En el momento del nacimiento,cada gemelo pesa ya una décima parte del peso de su madre,tanto como si una mujer diera a luz bebés de 6 kilos.
Al llegar el destete,los gemelos pesan entre ambos casi la mitad que su madre.
dado que los titíes viven en los árboles,las hembras se ven obligadas a acarrear los gemelos constantemente mientras son pequeños.
De hecho,llevar una sola cría comporta ya una carga significativa ; tanto,que el mono que la traslada rara vez puede alimentarla o buscar comida.
Además la madre no puede hacer mucho en cuanto al transporte,pues tiene que comer el doble de lo normal para producir suficiente leche.
Sin la protección de la nieve,tendrían que pasar semanas antes del brote de las plantas,y los animales salvajes e insectos carecerían de su primera alimentación,vital tras el largo sueño.
Una buena nevada conserva asimismo la tierra cultivable de los temibles efectos del hielo,y la mantiene en sus justas proporciones de humedad y temperatura.
De ahí viene el sabio refrán popular de " año de nieves,año de bienes.
Sin embargo,el exceso de precipitaciones puede acarrear consecuencias catastróficas.
Algunas zonas de Estados Unidos registran nevadas récord,como las acaecidas en Monte Rainer (Washington) en 1971 y 1972,años en los que se formaron hasta 31 metros de nieve.
Uno de los efectos más temidos del exceso de precipitaciones o de la acumulación sobre puntos indeseados son las avalanchas.
Las principales ocurren en los Andes,el Himalaya y las montañas de Alaska,pero el mayor peligro para la vida humana y las propiedades se encuentra,sin duda,en los Alpes,donde toda la región está densamente poblada,sin excluir,por supuesto,los vulnerables valles.
Los aludes tienen lugar en las laderas con inclinación superior a los 22 grados,y pueden iniciarse por un aumento de temperatura,una fuerte ráfaga de viento,un ruido intenso o el simple paso descuidado de un esquiador.
Una avalancha puede enterrar un pueblo entero en unos segundos,arrancar árboles y destruir edificios a su paso.
Entre las pocas medidas efectivas que pueden adoptarse para evitarlas,se encuentran los modernos sistemas de viso,que han alcanzado gran éxito.
En Algunas zonas se colocan cargas explosivas para provocar aludes intencionados allí donde la nieve se comienza a desestabilizar.
Otras medidas que se emplean,aunque menos eficaces,son la repoblación forestal y la edificación de estructuras desviantes.
En cualquier caso,la nieve es un bien codiciado,y,como quiera que las precipitaciones se están reduciendo paulatinamente,la ciencia nos ha dotado de la suficiente tecnología como para producir nieve artificial.
Lo que se entiende como tal no tiene nada de artificio.
No se trata de un producto de laboratorio,ni de una fórmula secreta ni de magia.
La naturaleza nos ha dotado de los componentes imprescindibles para la fabricación del elemento blanco: el aire y el agua.
Así que la tecnología no tiene más que utilizarlos debidamente.
La nieve artificial se produce dentro de un cañón que mezcla aire a presión y agua en las justas proporciones.
La mezcla se precipita a través de una fila de toberas que sirven de atomizador.
El truco consiste en pulverizar el aire y el agua en gotas pequeñísimas,enfriadas como consecuencia de la pérdida de presión dentro de la tobera.
Los diminutos cristales helados se unen a otros que salen de otras toberas y son expulsados fuera del cañón ya con la forma definitiva de auténticos copos de nieve.
El cañón puede expeler los copos a más de cincuenta metros de distancia.
Así,por el camino,los cristales se unirán unos con otros y aumentará el espesor de la nieve.
Cuanto más frío y más seco sea el aire,más rápidamente se formarán los copos.
La sequedad del aire ambiental es muy importante,ya que en ese caso la diferencia de presión entre el cristal de hielo y el vapor de agua alcanza su máxima efectividad,pudiéndose producir la sublimación originaria de los copos.
Otra técnica diferente es el cañón de nieve de aire comprimido.
Como éste no contiene propulsor,hay que introducir una mayor cantidad de aire en su sistema de toberas.
El aire a presión no se aplica a la producción de nieve sino a su equipo de distribución.
La desventaja de esta técnica es que se necesita un edificio donde colocar los compresores y los depósitos de aire comprimido ; después hay que instalar unas conducciones que introduzcan el aire comprimido en el cañón.
En el caso del cañón a propulsión,el proceso es mucho más sencillo,ya que se puede montar sobre un vehículo todo terreno y transportarlo a cualquier lugar.
Lo único que se necesita es una toma de agua cercana.
La pila comenzó a construirse el 16 de noviembre en una pista doble de squash que contaba con un balcón en su parte superior.
Alguien trazó con tiza un marco sobre el suelo y se colocó la primera fila de ladrillos.
Era una fila muerta,sin uranio.
Luego,una segunda fila,y a partir de ahí cada fila alterna tenía pseudoesferas de uranio.
Los físicos eran asistidos por universitarios de física contratados para colocar los ladrillos.
Trabajaban en turnos de 12 horas.
El turno de día era conducido por Walter Zinn ; el de la noche,por Herb Anderson.
Algunos ladrillos tenían ranuras para insertar y retirar las varillas de control.
Dichas ranuras debían ser cuidadosamente alineadas fila por fila.
Había diez varillas de control hechas de láminas de cadmio es conocido el enorme apetito del cadmio por los neutrones y clavadas a largas tiras de madera.
Insertadas en la pila,las varillas absorbían la cantidad suficiente de neutrones para impedir que las fisiones del uranio se multiplicaran en una reacción en cadena.
A medida que se extrajeran las varillas,más neutrones estarían disponibles para conducir las fisiones.
La medicina nuclear,es decir,el uso de radiaciones ionizantes,como los rayos X o el radio,como medio de diagnóstico y tratamiento médico de ciertas enfermedades es conocida desde antiguo,mucho antes de que comenzara la llamada era nuclear,tras el estallido de la primera bomba atómica.
Mas tarde,también los rayos gamma emitidos por determinados radionucleidos han sido empleados en el tratamiento de tumores malignos.
Aparte de ello,los isótopos radiactivos se utilizan hoy en los laboratorios de investigación de medicina y biología,tanto como medios de diagnóstico en relación con técnicas de trazadores como en gammagrafías y en la investigación del metabolismo de ciertas sustancias.
Todas estas técnicas se basan en la facilidad con que se puede seguir la evolución y la trayectoria de cantidades prácticamente insignificantes de un radionucleido,a través de un medio como,por ejemplo,el cuerpo humano.
La investigación nuclear ha dado lugar en los últimos tiempos al desarrollo de un verdadero arsenal de aparatos de radiodiagnóstico y tratamiento terapéutico,como la tomografía axial computarizada,la bomba de cobalto,las gammagrafías o la más reciente resonancia magnética nuclear,que han transformado por completo la forma de afrontar la identificación y el tratamiento de numerosas enfermedades.
En menor medida,las radiaciones ionizantes se han empleado también como medio de esterilización para hacer frente a determinadas plagas de insectos,y en la conservación de alimentos,dada su gran capacidad para actuar sobre el material genético y con ello,la posibilidad de inhibir la proliferación de bacterias y gérmenes.
EL efecto de las radiaciones ionizantes producidas por la fisión nuclear o cualquiera otra fuente radiactiva depende en primer lugar del tipo de radiación: alfa,beta o gamma.
El primero es provocado por los átomos de helio y es fácilmente detenido por una hoja de papel ; el segundo proviene de la emisión de electrones rápidos y puede pararse interponiendo una hoja de aluminio,el tercero está compuesto por fotones que tienen un alto poder de penetración,aunque no pueden traspasar el plomo.
Sobre el cuerpo humano las radiaciones actúan en relación con su naturaleza,la dosis recibida y la parte del organismo que haya sido expuesta.
Cuando son absorbidas por las células se producen ionizaciones y excitaciones que llegan a romper los enlaces químicos de las moléculas que constituyen la materia viva dando lugar a recombinaciones que pueden generar auténticos venenos.
Muchas de estas reacciones químicas y cambios moleculares desenvocan en alteraciones metabólicas en la célula,cambios genéticos,inhibiciones en determinadas funciones e incluso muerte celular.
Se suelen dividir estos efectos en dos grupos: somáticos y genéticos.
Los primeros se manifiestan en forma de lesiones apreciables en el propio individuo irradiado,mientras que los segundos,al afectar a su material genético,se manifiestan únicamente como alteraciones hereditarias a través de su descendencia.
Ciertos efectos somáticos como eritemas,anemias,necrosis y esterilidad,entre otros,se hacen notar inmediatamente,en una clara relación causa efecto que depende de la dosis recibida.
Otros tardan en aparecer incluso años,y la relación causa efecto no es tan clara.
Se trata de los cánceres de piel,huesos o pulmón,cataratas,anemias aplásticas y leucemias,cuya probabilidad aumenta con la dosis absorbida por los tejidos.
Los efectos genéticos,por último,son consecuencia de mutaciones provocadas por la radiación sobre las gónadas concretamente sobre el material hereditario de sus células.
EL desarrollo de la energía nuclear comenzó siendo militar y,lógicamente,por ese camino continuó.
Aunque las investigaciones que se llevaran a cabo tuvieran un fin pacifico,durante mucho tiempo los organismos y laboratorios nucleares se mantuvieron bajo el manto y la tutela del secreto militar.
Este acceso directo a la investigación nuclear ha proporcionado a los ejércitos la posibilidad de disfrutar de cada vez más y mejores armas nucleares.
La bomba atómica comenzó siendo un monstruo descomunal y enormemente pesado.
La primera bomba de hidrógeno de tipo Mk17,por ejemplo,era tan grande como una locomotora y pesaba más de 20 toneladas.
En la actualidad,un ingenio de similar poder destructivo podría ser transportado en un simple maletín de viaje.
La fiebre armamentista alimentada por la llamada guerra fría desorbitó los arsenales nucleares de las grandes potencias - Estados Unidos y la desaparecida Unión Soviética -,que llegaron a tener capacidad para destruir varias veces la propia Tierra.
Felizmente la sensatez se impuso y la tendencia es hoy la contraria: menos arsenales menos cabezas nucleares y más presupuestos para la solidaridad mundial.
EL efecto de las radiaciones ionizantes producidas por la fisión nuclear o cualquiera otra fuente radiactiva depende en primer lugar,del tipo de radiación: alfa,beta o gamma.
El primero es provocado por los átomos de helio y es fácilmente detenido por una hoja de papel ; el segundo proviene de la emisión de electrones rápidos y puede pararse interponiendo una hoja de aluminio ; el tercero está compuesto por fotones que tienen un alto poder de penetración,aunque no pueden traspasar el plomo.
Sobre el cuerpo humano las radiaciones actúan en relación con su naturaleza,la dosis recibida y la parte del organismo que haya sido expuesta.
Cuando son absorbidas por las células,se producen ionizaciones y excitaciones que llegan a romper los enlaces químicos de las moléculas que constituyen la materia viva dando lugar a recombinaciones que pueden generar auténticos venenos.
Muchas de estas reacciones químicas y cambios moleculares desembocan en alteraciones metabólicas en la célula,cambios genéticos,inhibiciones en determinadas funciones e incluso muerte celular.
Se suelen dividir estos efectos en dos grupos: somáticos y genéticos.
Los primeros se manifiestan en forma de lesiones apreciables en el propio individuo irradiado ; mientras que los segundos,al afectar a su material genético,se manifiestan únicamente como alteraciones hereditarias a través de su descendencia.
Ciertos efectos somáticos como eritemas,anemias,necrosis y esterilidad,entre otros,se hacen notar inmediatamente,en una clara relación causa efecto que depende de la dosis recibida.
Otros tardan en aparecer incluso años,y la relación causa efecto no es tan clara.
Se trata de los cánceres de piel,huesos o pulmón,cataratas,anemias aplásticas y leucemias,cuya probabilidad aumenta con la dosis absorbida por los tejidos.
Los efectos genéticos,por último,son consecuencia de mutaciones provocadas por la radiación sobre las gónadas ; concretamente sobre el material hereditario de sus células.
Los resultados sorprenden,en no pocas ocasiones,a propios y extraños.
Hoy,las modernas ciencias de la dermatología y la cosmética caminan en ese sentido y centran todo su empeño en aislar los principios inmediatos que hacen posible la regeneración de las células epiteliales o de los cabellos.
Uno de los más modernos tratamientos anticalvicie se compone precisamente de un liofilizado de líquido ammiótico,el que protege a los fetos,unido a un preparado azufrado de origen orgánico llamado S Carboximetilcisteina.
Pero lo cierto es que mamíferos,aves,reptiles,anfibios,peces e invertebrados presentan netas diferencias en la constitución de sus respectivas pieles,cosa que se advierte a simple vista.
Una cebra,un águila real,una serpiente de cascabel,una rana bermeja,un leucisco cabezudo,un grillo o una simple araña de jardín tienen muy poco que ver entre sí en lo que se refiere a su atuendo de protección frente a las inclemencias atmosféricas.
El mecanismo evolutivo encierra la clave de esta diversidad.
Con la regulación de la temperatura interna como cometido básico para garantizar su supervivencia,la piel que presentan hoy día los animales,ya sea gruesa,áspera,fina,delicada,vistosa ; compuesta de pelo,lana,plumas,cerdas,escamas,caparazones,etcétera,no es tan completa ni evolucionada como la del ser humano.
De entre todas las especies,la que nos es,más cercana es la de los grandes simios,de recia pelambre repartida por todo su cuerpo en función de una larga serie de atribuciones que desempeña a la perfección.
El brazo,por ejemplo,presenta el pelo curiosamente vuelto hacia arriba,detalle que han explicado los científicos constatando que sólo así evita el primate que el agua de la lluvia llegue hasta sus manos y le dificulte sus movimientos usuales.
en opinión de los biólogos,seguir la pista evolutiva a la capa protectora de los animales.
Para muchos de ellos,una de las primeras formas de piel tuvo que ser algo así como una sencillísima membrana,resultante del endurecimiento de la sustancia celular,el citoplasma,expuesta a la acción del aire y el oxígeno.
Más tarde esa membrana se transformaría en coraza quitinosa o en caparazón calcáreo,semejantes a los que presentan,respectivamente,diferentes tipos de insectos y moluscos.
En cualquier caso,según continúan asegurando los científicos,la secuencia evolutiva de la piel de los animales pasa claramente de los peces a los reptiles y llega posteriormente a las aves.
porque,pese a sus variadas formas,todos estos animales tienen su piel conformada por escamas.
En el caso de los dos primeros grupos,peces y reptiles,tal aseveración no suscita la más mínima controversia.
Pero con las aves,más de uno de nuestros lectores pensará que los sesudos hombres de ciencia se equivocan.
Ellos entonces nos invitarán a observar sus patas,donde,en efecto,existen escamas claramente conformadas.
Es más,incluso las propias plumas no son otra cosa que escamas modificadas,con los bordes deshilachados y desplegados,concluirían los más avezados.
Comparar la piel de los mamíferos y anfibios se presta a una mayor polémica.
De hecho,se han lanzado numerosas conjeturas sin demostración científica alguna,ya que la falta de recubrimiento en los anfibios (lo cual,por otra parte,les hace muy resbaladizos para sus depredadores,faceta de gran utilidad para su supervivencia),y las pieles cubiertas de pelo,lana o cerdas de los mamíferos,muy especializadas y perfeccionadas y sin claros precursores en la carrera evolutiva,hacen la comparación más problemática.
Sea como fuere,lo que está claro es que la piel supone para los animales el principal medio para aislarse del exterior.
Esta cubierta of rece al individuo protección de muy diversa índole.
Mediante gruesas capas de pelo o lana,plumones compactos o,en su defecto,mantas adiposas,por ejemplo,no sólo le aíslan a la perfección del frío y la humedad,sino que además,le crean una reserva protéica para prevenir la escasez de alimento.
EL tigre no busca ser arrogante,sino invisible El mimetismo o camuflaje es otra importante misión que cumple la piel en numerosas especies de animales que,por alguna razón,precisan pasar desapercibidos.
Esto es así tanto para los depredadores,caso del tigre o el lince,cuyos vistosos dibujos,lejos de buscar la belleza o la arrogancia,les sirven para confundirse entre la fronda mientras acechan a sus víctimas,como para las presas,más necesitadas de este recurso que los anteriores.
Como ejemplo sirva la de la mismísima cebra,de conocido y curioso diseño a base de rayas blancas y negras.
Su atractiva imagen,pese a resultar casi insultante de cerca,se convierte en prácticamente invisible de lejos.
Al igual que la cebra,otros muchos animales consiguen mimetizarse gracias a sus cripticas libreas.
El no va más lo alcanzan aquellos animales que cambian de color al momento o incluso mudan sus pieles o plumas para adecuarse a los tonos cromáticos del entorno,tal como hacen la liebre variable y la perdiz nival,pardas en verano y totalmente blancas en invierno.
También la comunicación puede tener su vehículo en la piel de los animales.
Sin caer en la megalomanía,es de rigor constatar que el hombre posee la piel más perfeccionada.
Ningún animal,ni siquiera los evolutivamente más cercanos,atesora en su piel semejante cúmulo de funciones.
Considerada como el mayor y más extenso órgano del cuerpo humano,hace posible la adaptación inmediata al frío o al calor mediante sus mecanismos de regulación de la temperatura (mayor o menor circulación sanguínea,pigmentación,sudoración,etcétera).
La piel humana puede,además,autogenerarse permanentemente.
Es dúctil y flexible y con diferente espesor según el emplazamiento corporal (de cinco cienmilímetros en los párpados a los casi cinco milímetros en las plantas de los pies).
En ciertas regiones del firmamento,contra un fondo de tinieblas hormigueante de estrellas,los telescopios nos muestran apariencias fantasmagóricas,semejantes a nubes movidas por el viento.
Estas nubosidades cósmicas,llamadas nebulosas revelan maravillosas difuminaciones de matices en las imágenes tomadas gracias a la más moderna instrumentación astronómica.
Pero su importancia va mucho más allá de la que podríamos atribuirles como fascinantes objetos de observación.
Son la materia prima de la creación,el manantial de donde fluyen los mundos que pueblan el universo.
Infinidad de nebulosas de colores y formas sorprendentes,relucen de uno a otro confín de la bóveda estrellada.
Una de las más brillantes y gran diosas es la famosa nebulosa de Orión,emplazada en la constelación del mismo nombre.
De ella se ha dicho que es la Capilla Sixtina del firmamento debido a su indiscutible belleza.
Pero,además de constituir desde siempre un blanco privilegiado de las miradas de los astrónomos,últimamente ha incrementado su interés: esta diadema celeste es una especie de criadero estelar,una región del espacio donde,en este mismo momento,se están formando nuevos y refulgentes soles.
Para los astrónomos esto constituye una prueba de sus teorías sobre el nacimiento de las estrellas.
La nebulosa de Orión se encuentra situada a 1.700 años luz del Sistema Solar - un año luz supone una distancia de casi diez billones de kilómetros -.
Es una nube de gas y polvo,brillantemente iluminada y que aparenta más densidad de la que en realidad tiene: como media,la densidad de los gases en cada centímetro cúbico de su volumen es un trillón de veces menor que en la atmósfera terrestre.
A pesar de ello,hay en Orión suficiente masa como para formar aproximadamente 110 soles como el nuestro.
El 90 por ciento de su masa es hidrógeno,que se ha disociado en átomos ionizados por la radiación ultravioleta procedente de las luminosas estrellas jóvenes inmersas en la nebulosa.
Los astrónomos saben que el brillo emitido por esta nebulosa es una manifestación de la fase final del proceso de gestación estelar.
Además de hidrógeno incandescente,existen en su interior átomos de helio e indicios de otros átomos más pesados.
Finalmente,alrededor de un 1 por ciento de la masa de Orión está formado por una neblina de granos de polvo de tamaño microscópico.
¿Ciencia ficción televisiva? No tanto.
¡Es la telemedicina! Una técnica de hoy que está más cerca del mañana.
Gracias a ella,los cardiólogos pueden,por ejemplo,vigilar en directo desde París el ritmo cardíaco de los pilotos durante un duro rally automovilístico.
Pequeño detalle: la competición se desarrolla en la jungla sudamericana de la Guayana francesa.
Una cajita de cien gramos basta para memorizar en 32 segundos un electrocardiograma simplificado del conductor y enviarlo.
El vínculo con sus supervisores médicos es un teléfono portátil con antena parabólica que lanza señales a un satélite de comunicaciones para que las retransmita por enlaces hasta la capital gala.
Cada vez se multiplicarán más los satélites de comunicaciones.
Giran en el plano del Ecuador,a 36.000 kilómetros de la Tierra,y se mueven con ella en órbita geoestacionaria.
La mayoría de los teléfonos vía satélite se conectan a la red internacional Inmarsat,lanzada por la ONU en 1979 con la finalidad primaria de que los médicos en tierra firme auxiliaran a los marinos enfermos en alta mar.
Así nació la telemedicina,un hito tan importante como el de la palabra radiada supliendo al código Morse,para dar paso ahora a los bits informáticos.
Hoy,esta red es una cooperativa internacional formada por 65 países,con 18.500 terminales telefónicos que se intercomunican a través de cuatro satélites.
Sus aplicaciones son prodigiosas.
Gracias a ellos,los médicos que trabajan entre las víctimas de las hambrunas en Somalia pueden conectar sus ordenadores portátiles para indagar en lo más avanzado de la investigación médica de los países industrializados.
Por el momento,su elevado coste - 3.500.
000 pesetas - los hace prohibitivos para un usuario normal.
Pero entretanto,cualquier enfermo cardíaco,inquieto por la velocidad de sus latidos que podrían presagiar una amenaza de infarto,puede abonarse a la Sociedad de Teleasistencia para Cardíacos,creada por el doctor Patrick Teboul.
Marcará su número y el electrocardiograma del paciente saltará veloz desde la cajita hasta la pantalla del monitor del cardiólogo de guardia en París.
Su ordenador podrá imprimirlos para analizarlos atentamente y tomar enseguida una decisión.
Como la de recetarle telefónicamente los medicamentos y su posología.
Los diabéticos se benefician de la teleasistencia con un aparato que les mide la glucosa,y determina así la dosis necesaria de insulina a inyectar para prevenir problemas en la visión o úlceras causadas por una deficiente circulación sanguínea.
El ritmo fetal y las contracciones uterinas de una embarazada hipertensa pueden vigilarse también por medio de un maletín conectado a distancia con su ginecólogo.
Así mismo,un microordenador de cabecera registra todos los cuidados diarios que recibe un enfermo asistido por telemedicina en su domicilio ; y el balance se transmite a los grandes ordenadores de los responsables médicos.
Resulta una solución más simple que una hospitalización normal.
Y así,miles de millones de señales médicas circulan a diario continuamente por las redes internacionales de telecomunicación como simples conversaciones telefónicas,pasando de mano en mano por los distintos especialistas de un hospital.
Ya en marzo de 1990,los especialistas del Centro Médico de Houston,Texas,pudieron examinar a diez kilómetros de distancia a una joven de doce años.
Transmitieron su imagen de televisión en alta definición por fibra óptica.
Resultó una solución a la crisis de la medicina rural.
Una compañía de Grenoble atiende con vigilancia médica constante a los pacientes cardíacos obesos que se encuentran convalecientes mientras ejecutan ejercicios de rehabilitación.
MARIMAR IIMÉNEZ Cambridge quiere convertirse en el Silicom Valley europeo,en un nuevo estandarte de la investigación.
Allí,Olivetti trabaja estrechamente con el Departamento de Ingeniería y el Laboratorio de Informática de la Universidad de esta ciudad británica,con los que comparte varios proyectos de investigación basados en comunicaciones avanzadas multimedia.
" Buscamos que la información siga a la persona,y no al revés como ocurre hoy ",dijo Andy Hopper,responsable de investigación de la compañía italiana.
En el campo multimedia trabajan en dos proyectos,Pandora y Medusa,dos sistemas de comunicación para transmitir texto vídeo y audio a través de Redes en Modo de Transferencia Asíncrono (ATM).
Entre sus aplicaciones destacan el videocorreo y la videoconferencia.
El primero permite grabar mensajes (vídeo y audio) y enviarlos a terceras personas,que los ven a través de una ventana en la pantalla de su estación de trabajo.
" El videocorreo es más rápido que el fax y el teléfono,y además es más efectivo y personal ",dijo Lucio Pinto,vicepresidente para las estrategias tecnológicas de Olivetti.
Pandora ya funciona.
" Actualmente,unos 40 sistemas están operando en el Laboratorio de Investigación de Olivetti y la citada universidad,unidos por una red de fibra óptica privada que transporta datos a una velocidad de 500 millones de bits por segundo.
Cada dispositivo Pandora está unido a una estación de trabajo Unix,cámara CCD pantalla de alta resolución,teléfono,micrófono y altavoz.
Esta red ofrece también imágenes de televisión,radio y otros servicios multimedia.
Por ejemplo,el sistema puede grabar automáticamente las noticias de la televisión de forma que cada usuario acceda a ellas cuando lo desee.
Medusa es un proyecto y busca dar un paso más,añadiendo nuevos elementos de video y audio.
El número de videocámaras por usuario se incrementará de uno (en Pandora) a ocho ó 16.
" Así,en una conversación de videoteléfono,el usuario podrá seleccionar qué imagen quiere ver.
Y en reuniones,la audiencia participará mirando alrededor de la habitación ",explica Pinto.
M. J. Un proyecto de comunicación avanzada de Olivetti se refiere a los sistemas de localización basados en.
tarjetas electrónicas,conocidas como insignias activas.
Esta empresa ha creado unos pequeños dispositivos de bolsillo que pueden utilizarse para poner en marcha y configurar ordenadores automáticamente y enviar llamadas telefónicas o mensajes a un receptor cuando éste no está en su mesa de trabajo.
Es un interface manos libres con el ordenador,funciona con baterías y transmite señales infrarrojas.
Las insignias están conectadas a sensores fijos distribuidos por la oficina,y éstos,a su vez,a ordenadores.
" Entre sus aplicaciones está el poder actualizar la información sobre la localizacion de las personas para mejorar la eficacia de una oficina ",señaló Lucio Pinto,de Olivetti.
Es posible conducir las llamadas telefónicas o los mensajes de videocorreo al teléfono o estación de trabajo más próximos.
" Así no se pasarán llamadas en caso de que una persona esté reunida ",afirmó.
La insignia tiene dos botones para transmitir mensajes.
" Se asemeja a un ratón informático ",explicó Pinto,quien asegura que con ella " es posible caminar y utilizar a la vez un sistema informático ".
- Y continúa: " yo puedo seleccionar el teléfono de un colega y pedir que se marque automáticamente su número.
El sistema calcula mi ubicación y la suya,y establece la comunicación.
Un CD ROM es un disco compacto de sólo lectura (a diferencia de los disquetes,que los usuarios pueden también grabar) de formato idéntico a los discos compactos de música.
Pero en los CD ROM se almacena no sólo música sino cualquier tipo de información digitalizada,que el usuario puede recuperar mediante un lector especial (drive) conectado al ordenador,con una mínima potencia.
También hay que incorporar unos programas capaces de gestionar la información del CD ROM.
Si además el usuario quiere sacarle realmente partido a todas las capacidades de este sistema multimedia,necesita dispositivos de reproducción de sonido,tarjeta y monitor de video adecuados,así como procesador y memoria de altas prestaciones capaces de presentar las imágenes en movimiento con suficiente realismo,aunque todavía queda mucho por hacer en este última técnica.
De momento,las mayores aplicaciones del CD ROM se han desarrollado en el campo de las grandes bases de datos utilizadas en actividades profesionales como la abogacía,la medicina o la documentación.
" La gran ventaja del CD ROM es que permite almacenar de una manera económica grandes cantidades de información,con posibilidades multimedia y con un software (soporte lógico) de recuperación que puede localizar una palabra concreta,esté donde esté,interrogar a la base acerca de cualquier cuestión e interrelacionar varios parámetros para localizar una información ",afirma Alfonso López Yepes,profesor del departamento de documentación de la Facultad de Ciencias de la Información de Madrid.
Precisamente,acaba de ser presentado en España un nuevo software de recuperación de datos para archivos CD ROM,el Windows Personal Librarian (WPL),creado por la empresa estadounidense Personal Library Software,que mejora ostensiblemente las técnicas de recuperación de información.
La innovación del WPL es que no se limita a mostrar al usuario un conjunto de documentos susceptibles de satisfacer su demanda,sino que,además,calcula a gran velocidad la pertinencia de cada uno de esos documentos - - su cualidad para satisfacer una necesidad de información concreta - - y los presenta ordenados según ese grado de pertinencia,de modo que el usuario sabe cuántos documentos ha seleccionado el programa y cuáles de ellos son los que más información sobre su consulta poseen,pudiendo eliminar el resto.
WPL permite,además,calcular,a partir de los documentos existentes en la base de datos qué otros términos pueden estar relacionados con la consulta del usuario,presentándolos en una ventana como ideas para nuevos conceptos de búsqueda.
Películas y mapas En la actualidad existen unas veinte bases de datos disponibles en castellano,cantidad que resulta ridícula si la comparamos con las cerca de dos mil que hay publicadas en inglés.
Buceando por los extensos catálogos de los importadores de CD ROM del mercado norteamericano,se encuentran cosas tan dispares como una guía de cine con criticas de expertos a casi todas las películas actuales ; un atlas mundial con mapas de alta resolución que se renueva cada seis meses ; la historia de la guerra del Golfo a través de artículos,fotos y grabaciones ; una visita al Smithsonian Museum ; el diario Pravda a texto completo y traducido al inglés desde 1987,además de centenares de enciclopedias,diccionarios,patentes,directorios,cursos de arte y un larguísimo etcétera.
De las bases de datos existentes en,español,la de mayor aceptación es el Aranzadi,edición trimestral de la prestigiosa editorial de información jurídica,que facilita el trabajo a abogados y juristas.
En 1989 se editó un CD ROM con los Ùndices de legislación 1930 - 1989 y la Legislación del Estado,y hace escasamente dos meses apareció la Jurisprudencia del Tribunal Supremo,ambas a texto completo.
Otra base disponible en este formato es la de la agencia española del ISBN,con los más de 600.000 títulos de la producción editorial española.
El Consejo Superior de Investigaciones Científicas (CSIC) tiene disponible en disco óptico la mayor parte de la información científica o técnica publicada en España,con más de 350.000 datos.
Así,la base ICYT contiene todas las referencias a los artículos de investigación en ciencia y tecnología extraídos de publicaciones periódicas.
El IME contiene referencias de artículos de 321 revistas médicas especializadas.
Las bases ISOC incluyen datos bibliográficos sobre literatura científica en el ámbito de la historia,la filosofía,estudios sociológicos,economía,investigación,arte,arquitectura,transportes,medio ambiente,cartografía,etcétera.
También está teniendo buena aceptación la base de datos Iberlex del Boletín Oficial del Estado,en un solo disco CD ROM que contiene toda la legislación española desde 1968 - - a texto completo desde 1985 - - y la de la CE desde 1986.
Una base de datos medioambiental creada por la Asociación de Fabricantes de Bienes de Equipo ; la revista Anales Españoles de Pediatría (desde 1989 hasta 1992) ; un archivo de prensa con 6 ;).
000 referencias y resúmenes de noticias aparecidas desde 1989 en 191 diarios y revistas nacionales y extranjeras.
MALCOLM W. BROWNE Los promotores de Parque Jurásico,la última película de Steven Spielberg,esperan que multitudes de fanáticos de los dinosaurios abarroten los cines.
Basada en parte en recientes adelantos de la biología,los efectos visuales y de animación utilizados pueden dejar boquiabierto al público.
También muchos científicos parecen ansiosos por ver esta película,que presenta algunos de los últimos descubrimientos en la conservación del material genético (ADN) de animales extinguidos.
Pero los biólogos se muestran críticos con la premisa especulativa de que los dinosaurios resucitarán algún día.
Y algunos investigadores están molestos por lo que,según ellos,es un sesgo anticientífico de la trama.
En su enfoque de la biotecnología,dicen,Porque Jurásico resucitó el mito del doctor Frankenstein de científicos amorales que desencadenan fuerzas incontrolables.
Los trucos científicos sobre los que se basa la historia fueron ideados por investigadores,que conocen los obstáculos que habría que superar para resucitar fósiles con 80 millones de años.
Algunos trabajaron en la película como asesores de los constructores de maquetas,expertos en robótica y animación por ordenador.
Parque Jurásico está basada en el libro de Michael Crichton,publicado en 1990,cuya ingeniosa premisa es que los mosquitos y las moscas de la era mesozoica (230 a 65 millones de años) pudieron haber picado a los dinosaurios antes de posarse en la resina fresca de un árbol,quedarse pegados y fosilizarse.
La resina se secó,se convirtió en ámbar y algunos de estos insectos se conservaron casi intactos hasta hoy.
Si se da por hecho que aun puede haber sangre de dinosaurios en esos insectos fosilizados,es fácil para la ficción dar un salto y dar vida a clones de dinosaurio.
Fue Chaties R. Pellegrino del Centro Rockville (EE UU) el primero en publicar la idea sobre la conexión entre bichos en ámbar y la resurrección de los dinosaurios.
Pellegrino es escritor y catedrático de Paleobiología.
En 1985 afirmaba en la revista Omni: " Tres décadas más de avances tecnológicos y podremos extraer y leer el ADN del estómago de las moscas,en el que,con suerte,encontraremos sangre y piel de dinosaurios ".
Ya que estos insectos volaban entre los dinosaurios y extraían su alimento de ellos seguía diciendo,es posible que los científicos algún día descubran los códigos genéticos de criaturas que sólo conocemos por sus huesos y huellas.
Código genético " Si han desaparecido algunas partes del código genético ",escribía," se podría descifrar lo que falta e insertar los párrafos.
Quizá podríamos sacarlos de animales vivos,para conseguir un conjunto completo de proteínas necesarias para la supervivencia de los dinosaurios originarios.
Después,todo lo que hace falta para fabricar un dinosaurio se podría publicar en forma de cromosomas.
Podríamos introducir éstos en un núcleo de célula,encontrar una yema de huevo e incubar nuestra idea,y señalan que nadie ha podido hacer un clon,ni de un animal vivo con un código genético intacto,y mucho menos de un animal desaparecido,cuyos genes,en su mayoría,han sido destruidos o dañados.
Russell Higuchi,de Sistemas Moleculares Roche (California),hizo circular hace poco entre sus colegas un texto en el que condenaba Parque Jurásico: " contiene errores de bulto sobre la capacidad de la tecnología del ADN ".
" Si puede devolverse la vida a los dinosaurios,¿quién sabe de lo que es capaz la malvada tecnología genética? ".
Parque Jurásico habría sido una fantasía de principio a fin,de no ser por varios avances científicos fabulosos.
En 1962,George O. Poinar,patólogo de insectos de la Universidad de California en Berkeley,descubrió en la costa de Dinamarca un trozo de ámbar.
Luego estudió junto a su esposa,Roberta Hess,especialista en microscopía en Berkeley,un mosquito de las setas incrustado en un trozo de ámbar que tenía una antigüedad de 40 millones de años.
Con un microscopio electrónico distinguieron las células musculares del mosquito.
Vieron el núcleo,con la cromatina (contiene los genes de las células) y la mitocondria.
En 1984,tres bioquímicos de Berkeley,entre ellos Allan C. Wilson,trabajaron en la piel conservada de un animal que se había extinguido 140 años antes y extrajeron suficiente ADN de la carne como para determinar algunas de las secuencias de los pares de base,los eslabones moleculares de la doble hélice.
Wilson y sus colegas demostraron que es posible hacer descubrimientos útiles a partir del ADN de animales muertos.
Por desgracia,estas muestras rara vez contienen el suficiente ADN recuperable,es escaso o está en muy malas condiciones.
Pero en 1985 se inventó la técnica de la reacción en cadena de la polimerasa (PCR),capaz de hacer suficientes copias exactas de una molécula de ADN para su análisis.
Los biólogos empezaron a explorar los códigos genéticos de todo tipo de fósiles y material momificado que contenían rastros casi inapreciables de ADN: viejos restos fósiles,antiguo maíz de los Andes,aves de Nueva Zelanda incapaces de volar,termitas,un mamut de hace 40.000 años,los cerebros momificados de indios de Florida de hace 7.500 años... Aunque todo esto es una ciencia fascinante es incapaz de devolver la vida a los dinosaurios.
Entonces entra en escena la ficción.
Crichton,autor de populares novelas y guiones de ficción científica,es licenciado en Medicina.
El libro y la película Parque Jurásico están basados en Jack Horner,un paleontólogo estadounidense especialista en dinosaurios que además ha trabajado como asesor en la película.
Crichton reconoce el tono anticientífico de la mayoría de sus novelas,incluida Parque Jurásico.
" Me sorprende que no se haya dado cuenta más gente ",dijo en una entrevista.
" Soy un entusiasta de la ciencia,pero hay una tendencia hacia el cientifismo,hacia la aceptación acrítica de ideas científicas,y una tendencia a desechar las ideas que la ciencia no puede abordar ".
También Spielberg considera peligrosos algunos aspectos de la ciencia,incluida la biotecnología.
" Todo avance de la ciencia implica una reacción equivalente y contraria y normalmente una pérdida para el medio ambiente ",dijo en una entrevista.
" La ciencia es intrusiva.
No prohibiría completamente la biología molecular porque es útil en la investigación de remedios contra el sida,el cáncer y otras enfermedades pero también es peligrosa,y de eso trata Parque Jurásico ".
MARIMAR JIMÉNEZ Expertos de la compañía Fujitsu,con ayuda de la Universidad Hokkaido,en Sapporo (Japón),intentan construir un ordenador que responda a ordenes mentales.
O lo que es lo mismo: una máquina que lea el pensamiento.
Quizá para muchas personas esto resulte ficción científica ; sin embargo,los técnicos afirman que bastará con que una persona piense que quiere que el cursor de su pantalla suba o baje para que éste se desplace de acuerdo a ese pensamiento sin que medien teclado,ratón o cualquier otro interfaz entre usuario y máquina.
Todo gracias a un superconductor muy sensible.
El primer paso ha sido probar la existencia del diálogo silencioso,para convertirlo en un nuevo canal de entrada (input) de datos.
" Cada vez que una persona articula un pensamiento,se emite un tipo de modelo diferente de ondas cerebrales ",explican fuentes de la compañía.
Los esfuerzos se han centrado después en ver si es posible emitir órdenes positivas y negativas,grabando ondas cerebrales por electrodos y amplificándolas.
" La información se manda desde el cerebro utilizando electrodos sujetos a la cabeza,a través de un amplificador y un conversor analógico - - digital que traslada los datos al ordenador para su proceso ",continúan los técnicos.
En el experimento 12 electrodos fueron conectados a la cabeza de una persona.
Ésta se sentó frente a dos LEDs (diodos emisores de luz),uno rojo y otro verde,con la cabeza sujeta para evitar movimientos que generasen ondas cerebrales que interfirieran en la prueba.
Hacia la acción Los LEDs fueron conectados y apagados al azar y las personas tenían que pensar la letra a ante la luz roja y no pensar nada cuando parpadeara la verde.
" Los resultados indican que hay una reacción en la parte posterior de la cabeza 0,3 segundos después de que el sujeto ve un LED rojo.
Y,en 0,42 segundos,la reacción llega a la parte delantera del cerebro,donde está d centro de acción.
Esto muestra el progreso del mero pensamiento a la acción ",añaden.
El equipo de Fujitsu está construyendo actualmente un ordenador personal para interpretar los modelos cerebrales con la ayuda de un vehículo ultrasensible llamado Squid (Dispositivo Superconductor de Interfaz Cuántico).
La ventaja de usar Squids es que no es necesario conectar electrodos al sujeto.
En su lugar,se utiliza un casco especial que parece un complejo secador de pelo y una habitación aislada del campo magnético,que encarece mucho el proyecto.
" Nuestro objetivo es crear un ordenador intuitivo que pueda entender tus pensamientos,incluso cuando tú paseas alrededor de una habitación ",ha dicho Michael Beirne,portavoz de Fujitsu en Tokio,a The Sunday Times.
El problema es que los actuales superconductores trabajan sólo a temperaturas cercanas al cero absoluto (- - 273 grados centígrados).
Pero Fujitsu espera desarrollar squids que funcionen a una temperatura relativamente más elevada (70 bajo cero) y eventualmente a temperatura ambiente.
El virus no puede sobrevivir de forma autónoma.
Para desarrollarse y reproducirse es necesario que se aloje en una célula huésped vegetal,animal o humana ; esto es en una célula siempre viva.
Pero no en cualquier célula.
Cada tipo de virus elige una clase de célula diferente,a la que se adapta como una llave lo haría a su cerradura bien engrasada.
una vez introducido en el organismo,el virus es arrastrado por el torrente sanguíneo o cualquier otro liquido humoral que circule por el cuerpo de la persona.
Así por ejemplo,suponiendo que se trate del virus que provoca la hepatitis B,pasará olímpicamente de las células que encuentre en su camino,hasta llegar al hígado.
Ahora el terreno le resulta conocido ; su servicio de información sabe perfectamente como funciona el de las células hepáticas y,además,conoce el santo y seña.
No le resta sino entrar.
A partir de aquí,la suerte de la célula está echada.
El virus,compuesto por sustancias que también forman parte de la célula,se adhiere a ella de tal forma que ya es imposible recuperarlo ; incluso se vuelve insensible a la acción del anticuerpo especifico.
Es la denominada fase de absorción y en ella parecen intervenir,en primer lugar,fuerzas de naturaleza electrostática.
A continuación,se forman enlaces químicos estables,facilitados por la complementariedad de las estructuras superficiales del virus y la célula huésped.
Acto seguido,se produce la penetración,que se realiza de forma diferente para cada tipo de virus.
Una vez alcanzado el núcleo de la célula,donde se encuentra el material genético,se inicia el llamado período de latencia o eclipse,también de duración variable según las características del invasor (mientras que el virus del catarro emplea menos de 24 horas en este también llamado período de incubación,el del SIDA Síndrome de Inmunodeficiencia Adquirida puede prolongarse durante años).
En esta fase,el virus se dedica a poner a su servicio la vulnerada célula.
Su ácido nucléico - ya se trate de ácido desoxirribonucléico (DNA) o ribonucléico (RNA) - asume la dirección de los procesos metabólicos de su huésped,obligándole a producir las sustancias que aquél necesita para reproducirse.
La célula no tiene más remedio que abandonar sus funciones específicas,aquéllas para las que fue creada,pues su información genética ha sido alterada por el virus y desviada hacia la sintetización de nuevas proteínas.
Se trata de las llamadas proteínas precoces,casi todas de naturaleza enzimática,entre las cuales se encuentran los enzimas específicos que permitirán la formación de un nuevo ácido nucléico del mismo tipo que el del virus.
Estas son las ácido nucleicopolimerasas.
Pero el trabajo aún está incompleto.
La célula huésped debe ser reprogramada para producir otro tipo de proteínas,conocidas como proteínas tardías,destinadas a constituir la cápsula o envoltura de los nuevos viriones.
Nos encontramos,pues,con una célula huésped en la que se han formado,por separado,dos grandes depósitos independientes: uno de ácido nucléico vírico infectivo y otro de subunidades protéicas de revestimiento del virus.
Ninguna de estas sustancias podrían ejercer por sí mismas una acción vírica en la naturaleza.
Para cumplir su fatídica misión es preciso que estos dos componentes fundamentales se reúnan y formen viriones completos y maduros.
Esto tendrá lugar en la fase de maduración.
Una vez formados,los nuevos viriones deben salir de su cubil.
En algunos casos,la célula es literalmente destruida para liberarlos ; en otros,la liberación es fruto de un proceso,más o menos largo,con mayor o menor deterioro de la célula según e! virus de que se trate,pero sin rotura definitiva de la misma.
A todo esto,el sistema inmunológico del organismo ha sido ya avisado de la presencia del invasor y de su cruel actividad devastadora.
Verdaderas legiones de anticuerpos específicos,adaptados a las características de cada tipo de virus,se han preparado para la gran batalla.
Esta se desencadena y es ahora cuando aparece la mayor parte de los síntomas que produce la enfermedad.
Generalmente,son los anticuerpos fabricados por el sistema inmunológico los vencedores,ya que su capacidad de reproducción es normalmente mayor que la de los virus,pero no siempre es así.
Depende,en primer lugar,del estado de forma del organismo infectado - ya se trate de una planta,animal o del propio ser humano - y,por otra parte,del tipo de virus.
Algunos,como el causante del SIDA,los denominados oncovirus,inductores de algunos tipos de cáncer,entre otros,parecen de momento inatacables,una vez que se ha consumado la infección.
En caso de que el sistema inmunológico haya vencido a la enfermedad,ésta no sólo desaparece de forma momentánea,sino que el organismo queda prácticamente inmunizado contra ella.
Según el tipo de virus que le haya invadido,los anticuerpos formados con posterioridad pueden sobrevivir durante varios años,incluso de por vida.
Es por ello que algunas enfermedades,como el sarampión,la rubéola o las paperas,entre otras,se padecen una sola vez en la vida y otras como la viruela se hayan erradicado definitivamente.
Cabe preguntarse entonces,¿por qué algunas enfermedades de origen vírico,como los catarros o la gripe,pueden sufrirse en repetidas ocasiones? La razón es que existe toda una gama de virus que provoca este tipo de enfermedades.
Algunos de ellos son capaces,incluso,de sufrir mutaciones,evitando de esta forma el ataque directo de los anticuerpos segregados para combatir una gripe o un catarro anterior.
Lo que está claro es que el lector no sufrirá nunca ni la misma gripe ni el mismo catarro.
Los virus son las formas de existencia más primitivas que conocemos.
Sin embargo,logran poner a su servicio,incluso logran a matar,al organismo vivo más desarrollado que existe: el ser humano.
Es este un hecho que hace temblar,para algunos,el edificio que la humanidad ha construido sobre los conceptos primitivo y superior.
Pero hay algo más: ¿cuáles son los mecanismos que rigen la existencia y la forma de actuar de los virus?,¿por qué un determinado virus,como el del SIDA,no aparece,o no se manifiesta en toda su plenitud sino hasta 1983,año en que es identificado?,¿por qué no antes,o después? Es evidente que los virus necesitan células,ya sean animales o vegetales,para sobrevivir.
Por tanto,difícilmente pudieron existir antes que éstas.
Hay que advertir,además,que no puede ser casual que los virus estén compuestos por los mismos elementos esenciales que constituyen el material genético y la base fundamental de la vida,los ácidos nucléicos.
Esto lleva a pensar que la formación de la vida,hace cuatro mil quinientos millones de años,llevaba implícita la aparición posterior de los diferentes tipos.
El guerrero cuyos pasos hemos seguido puede ser lavaplatos en una hamburguesería,universitario,empleado de banca o ejecutivo.
En todo caso,es una de las 20.000 personas que en el último año han adquirido su carnet para jugar en el Photon.
Este carnet cuesta diez dólares,y es un requisito necesario para poder jugar.
A cambio del dinero,cada jugador recibe una tarjeta con un año de validez,su fotografía y un código computerizado.
El precio,aparte,de cada partida es de tres o cuatro dólares,según el sitio.
Los días más animados,la gente puede hacer cola esperando su turno durante media hora en algunos casos.
Cuando llega el momento,veinte jugadores se dividen en dos equipos.
A cada jugador se le suministra una pistola " láser ",un cinturón de baterías,un peto electrónico y un casco que emite luz intermitente.
Una vez vestidos,pasan a un decorado de 900 metros cuadrados que representa un terreno planetario,lleno de pasadizos y escondites.
Existe también una torre que sirve de base a cada equipo y un juego de luces y sonido,como el de la mejor discoteca.
Durante la partida cada jugador intentará dos cosas: sobrevivir y matar el mayor número posible de enemigos.
Cada contrincante muerto supone treinta puntos,que van siendo apuntados en un marcador electrónico.
Claro que,por otra parte,cada vez que un jugador sea acertado perderá a su vez treinta puntos.
Cuando el tiempo termina,se suman las puntuaciones de ambos equipos.
¿Y cómo funciona este juego? Aunque cueste creerlo,su funcionamiento es tan sencillo que todas estas operaciones están controladas únicamente por dos ordenadores personales y cinco programas.
El carnet de socio es en realidad un pase con un circuito legible para la computadora,que el jugador conecta,junto con su arma láser,al ordenador central antes de empezar la partida.
El casco dispone de un juego de auriculares que orientan al jugador sobre sus progresos: un rechinar suena cada vez que se efectúa un disparo,que finaliza con un tono intermitente y agudo si se ha alcanzado al adversario,u otro más grave si,por error,ha disparado sobre uno de sus compañeros de equipo.
Un tono similar avisa al jugador cuando ha sido alcanzado.
Aunque sus organizadores no of recen información muy clara sobre cómo opera todo este sistema de captación de impactos,todo parece indicar que el cañón de la pistola es en realidad un sensor que actúa como receptor,de un modo similar a los lápices ópticos que leen en las pantallas de los ordenadores.
La pantalla de ordenador,es decir,el emisor del campo que la pistola puede registrar,se encuentra en este caso en el peto electrónico y el casco de cada jugador.
Si el cañón de la pistola se dirige hacia el emisor y se aprieta el gatillo,el impacto queda registrado.
Un exacto sistema cronométrico permite determinar qué jugador ha hecho cada disparo,y quién lo ha recibido.
Los efectos de luz,sonido y humo se encargan de poner el toque final.
Este espectacular juego lleva poco más de un año - exactamente desde septiembre de 1985 - funcionando en los Estados Unidos,y su éxito ha sido tal que se ha vendido a países como Canadá o Japón,y en estos momentos se mantienen negociaciones con posibles compradores en Francia,Inglaterra y la República Federal de Alemania.
No se tienen noticias de que los aficionados españoles vayan a poder contar con él en un futuro próximo,aunque nunca se sabe.
Pero hay que tener en cuenta que el permiso para instalar el Photon vale medio millón de dólares (unos 75 millones de pesetas),precio que no incluye el coste de las instalaciones y el equipo ; una suma que puede dejar frío a cualquier comprador más eficazmente que la más mortífera y moderna pistola láser.
Una nueva antena plana para la recepción de la televisión vía satélite ha sido recientemente presentada y patentada por la firma Televés.
En la fase de experimentación,ofrece,en opinión del padre del Invento - el ingeniero en telecomunicaciones Juan Antonio Gómez -,alguna ventaja sobre la popular parábola.
Permite recibir simultáneamente dos o tres satélites sin necesidad de ser reorientada ; su fabricación es sencilla y barata,por el material plástico que utiliza ; las piezas son de reducido tamaño y fáciles de transportar y colocar.
Además,en el momento de ser comercializada,su precio será interior al de las parabólicas Su desarrollo teórico se ha basado en el conocimiento de las zonas de Fresnel,por la amplitud existente entre la propagación de las ondas de la luz y las electromagnéticas,en cuanto a su difracción o desviación en todas las direcciones por igual,cuando llegan a un punto cualquiera.
Sabiendo también que en cualquier plano existente entre un emisor y un receptor e crean estas zonas con distinta fase en sus círculos pares e impares,la función de la antena,y más concretamente de sus anillos de plástico,es producir un giro de 180 en la onda electromagnética que pasa a través de ellos.
Con este giro produce un cambio de fase y un aumento en su contribución,con lo que a obtiene una ganancia de 19 decibelios El primero de ellos,la parábola,está construida de aluminio recubierto de materiales reflectantes y tiene por misión reflejar las ondas electromagnéticas y concentrarlas en el foco de la misma.
Su alimentador dispone de un guía ondas que actúa como filtro,permitiendo sólo la entrada de las señales deseadas.
Al emitir los satélites con muy baja potencia,el diseño de este dispositivo junto con el tamaño del plato es de gran importancia para el rendimiento del sistema.
La señal captada por la antena es,por tanto,muy débil pero de una frecuencia muy elevada,de unos once GHz (gigahercios),imposibles de ser admitidos por el televisor.
Las frecuencias que se utilizan en las emisiones convencionales están entre los cincuenta y ochocientos MHz (megahercios).
Ante estas características,la unidad exterior,adosada al alimentador,debe procesar la señal para su posterior utilización.
El primer paso que realiza es ampliar la señal por medio de un amplificador (LNA) y más tarde convertirla,con un conversor (LNC),a una banda inferior comprendida entre los 900 y 1.700 MHz. Lo habitual es que se instalen convertidores de baja señal de ruido que admitan las dos polaridades con las que se transmiten las ondas de televisión por satélite.
Las emisiones terrestres utilizan una polaridad lineal,pero los satélites,para que no se produzca superposición entre los diferentes canales,además de enviarlos con una separación entre ellos de 19,18 MHz,emplea cada uno polaridades circulares distintas,bien horizontal o vertical.
La diferencia entre la circular y la lineal es que en la primera el campo eléctrico que recibe la antena avanza girando sobre su propio eje.
Un cable coaxial traslada la nueva señal a la unidad interior,dispositivo electrónico de gran complejidad que realiza la función de sintonía y demodulación de un canal concreto dentro del conjunto de canales que salen del conversor.
Al igual que un sintonizador convencional,extrae la información de video y sonido que queremos obtener,una vez transformada la frecuencia a otra intermedia de 130 MHz apta ya para ser recibida por el televisor.
De esta unidad interior existen dos tipos básicos,uno de sintonía variable y otro de sintonía fija.
El primer tipo está especialmente indicado para sistemas de recepción individual.
Por medio de un solo aparato se reciben varios canales y se puede elegir uno de ellos mediante un mando de sintonía.
Las de sintonía fija realizan la misma función,pero para cada canal se instala una unidad interior con una entrada de frecuencia prefijada,invariable y sin posibilidad de ser cambiada.
Utilizada en instalaciones colectivas,en comunidades de vecinos u hoteles,éstos deben contratar la cantidad concreta de canales que quieren recibir.
Estas unidades interiores deben realizar una función más.
Las señales sufren otra modificación ; tienen que ser desmoduladas para volver a ser moduladas.
En la televisión por satélite la señal de vídeo es modulada en frecuencia,mientras que las terrestres son moduladas en amplitud ; lo que se produce es un cambio de modulación de frecuencia a amplitud.
Por ahora,salvo el canal Sky Channel que va codificado,las ondas de televisión circulan libremente por el espacio,por lo que no se necesita un decodificador que permita su recepción.
Los fabricantes de antenas no temen que en el futuro el resto de los canales sean codificados,pues opinan que la compra de un aparato decodificador no encarecerá demasiado el sistema.
Los precios de la instalación de una antena parabólica dependen del ancho de la misma y de los aparatos receptores - moduladores que se empleen.
Por la mala posición de España con respecto a los haces del satélite,se necesitan antenas de diámetros mayores.
En el norte de España se suelen colocar de 1,80 metros,y su precio va de las 500.000 a las 700.000 pesetas ; en el centro son de 2,4 metros,y en el sur de 3 metros,y su coste oscila entre las 500.000 y el millón de pesetas ; y si son para comunidades están entre el millón y los dos millones de pesetas.
Es evidente que,ante estos precios,sean personas con altos ingresos las que deciden colocar una.
La demanda se amplia con los hoteles,comunidades de vecinos,discotecas,academias de idiomas y empresas informativas.
El número de antenas colocadas en nuestro país no supera las mil.
En el futuro,pese a la puesta en órbita de nuevos satélites de comunicaciones en 1992,se piensa que dominarán los llamados de radiodifusión directa (DBS),exclusivamente para televisión,de alta potencia y con cobertura nacional o regional,lo que conllevaría una reducción considerable de las antenas.
Otorgadas las frecuencias y las posiciones en el espacio a cada país en 1977,éstos sufren un gran retraso,sólo Francia y Alemania siguen adelante con sus proyectos.
Pero el TDF1 francés y TVSAT alemán aún no han sido lanzados al espacio.
Los motivos pueden estar en el desarrollo tecnológico que ha permitido equipos receptores de alta sensibilidad con muy buena calidad de imagen.
También,en la gran potencia de los DBS,son los que va a ser muy difícil guardar las fronteras.
Europa se va a convertir en una maraña de señales que puede llegar a saturar la oferta televisiva.
Los japoneses consiguieron a la primera algo que los norteamericanos durante muchos años sólo fueron capaces de soñar: el 13 de agosto de 1986 lanzaban al espacio dos satélites con un nuevo cohete autodesarrollado de la clase H - 1. Y,por lo que sabemos,aquello fue nada más el principio de una carrera que ya intranquiliza a las otras tres potencias espaciales: Estados Unidos,Europa y la Unión Soviética.
¿Se convertirá Japón - que ya ha conquistado el mercado mundial con sus cámaras,automóviles y equipos de alta fidelidad - en un serio contrincante también en el espacio? De momento,parece que no.
Está claro que los nipones no llegaran a ser competidores poderosos en el terreno de la aeronáutica tan de prisa como lo fueron en el campo electrónico y de los bienes de consumo.
La tecnología espacial resulta demasiado costosa y necesita un largo período de desarrollo que puede abarcar varias décadas.
Pero no hay que olvidar el hecho de que Japón viene invirtiendo,desde hace quince años,considerables sumas de dinero en la investigación aeroespacial.
Y desde 1975 el país gasta en ello más que Francia,una nación espacialmente avanzada.
Sólo en 1986,por ejemplo,duplicaron ya el presupuesto de los británicos.
La historia del programa espacial japonés se remonta en realidad,a los primeros años sesenta,cuando Hideo Itokawa funda el Instituto de Ciencias Aeronáuticas y Espaciales (ISAS).
Desde entonces hasta hoy,Japón ha puesto en órbita más de treinta satélites,tiene previsto lanzar sesenta más en los próximos diez años y prevé la visita a la Luna para antes de 1990.
A pesar de estos ambiciosos planes y esfuerzos económicos,los japoneses han aprendido por experiencia que el desarrollo de los grandes cohetes lanzadores - y únicamente cuando éstos se poseen puede un país considerarse una potencia espacial - no se lleva a cabo de golpe: tan sólo el quinto intento,realizado en 1970,de lanzar un satélite de 24 kilos con un cohete de propulsor sólido (tipo Lambda - 4 S) - cuya tecnología resulta hoy modesta si la comparamos con los modernos lanzadores espaciales -,se vio coronado por el éxito.
Recordemos que más o menos por la misma época se venía abajo el proyecto de construir un cohete europeo,después de algunos lanzamientos fallidos.
Sencillamente,que utiliza hidrógeno y oxígeno como combustible.
Los expertos hablan de propulsión criogénica ; de entre todos los sistemas de cohetes éste es el que of rece,con gran diferencia,el mayor aprovechamiento de energía.
Tiene una pega,sin embargo: su tecnología resulta extremadamente compleja.
El único lanzador criogénico de Europa,por ejemplo,consigue 6,4 toneladas de impulso y precisamente se encuentra en la tercera fase del Ariane: sí,ésa fue la responsable de los dos últimos lanzamientos fallidos.
También los motores del transbordador espacial norteamericano son de propulsión criogénica.
Para este año de 1987,en el que se espera la puesta en funcionamiento completa del H - 1,el cohete podrá transportar ya una carga de 500 kilos hasta la órbita geoestacionaria (36.000 kilómetros de altura) ; o bien,una carga de 2.200 kilos hasta una órbita de 1.000 kilómetros.
A este fin,los nipones ya han iniciado conversaciones con Estados Unidos.
Y,por cierto,las negociaciones no van a ser fáciles,ya que Japón continúa utilizando el motor Delta,y eso significa que no pueden lanzar más que satélites pertenecientes a una de las dos naciones.
Por todo ello,el H - 1 no representa todavía más que un paso a medias en la carrera espacial.
Sin embargo,los japoneses ya tienen planificada su arma secreta: trabajan desde hace dos años - con la enorme inversión de 800 millones de dólares - en la construcción del cohete H - 2,de mucho mayor tamaño,que podría barrer en los mercados del espacio.
Con un peso de 255 toneladas en el momento del lanzamiento,y formado exclusivamente por dos etapas,el H - 2 será mucho más ligero que el moderno cohete europeo Ariane - 4. Asimismo estará en condiciones de transportar una carga de dos toneladas hasta una órbita geoestacionaria.
cómo han logrado los japoneses ese milagro? El H - 2 será el primer cohete lanzador del mundo que constará de dos etapas con propulsión de hidrógeno y oxígeno.
El gigantesco propulsor de la primera fase (LE - 5) - desarrollado por la empresa Mitsubishi - generará un impulso de cien toneladas.
Ya ha superado con éxito las primeras pruebas.
Pero la verdadera sensación lo constituye el hecho de que el nuevo motor no es simplemente una versión aumentada del H - 1,sino que funcionará según el principio de alta presión de corriente principal,al igual que ocurre con los motores del transbordador espacial norteamericano.
Además,según la opinión de expertos,su construcción resulta mucho más sencilla.
La diferencia más importante entre un motor normal y otro de alta presión estriba en el hecho de que en los primeros el combustible es conducido,mediante bombas de alta potencia,hasta la cámara de combustión,y es impulsado por un generador autónomo cuyos gases de escape salen al exterior sin ser aprovechados ; en el caso del motor de alta presión,en cambio,existe una precámara - situada delante de la cámara de combustible - en la que se queman el hidrógeno y el oxígeno.
Los gases de escape accionan una turbina que genera la energía para las bombas.
Luego,los gases precalentados entran en la cámara principal,y allí es donde se consigue la auténtica combustión que genera el impulso.
En líneas generales,este motor de alta presión funciona como el motor a reacción de los aviones ultrasónicos con postcombustión.
¿Qué podía haber inducido al poeta para verse arrastrado a aquel descabellado lance? Se dice que un problema de celos provocados por la actitud excesivamente galante hacia su mujer del barón Georges d'Anthes,un realista francés asiduo de los salones de sociedad.
Sus amigos íntimos aseguraron,sin embargo,que en el trasfondo se escondía una intriga política astutamente urdida por los círculos cortesanos,incapaces de comprender el genio de un poeta abierto a las sospechosas ideas de su tiempo.
Aunque el zar Nicolás I intentó resarcir a su viuda con una renta anual de 11.000 rublos y la edición de sus obras completas,entre la rancia nobleza reinaba la complacencia: habían callado para siempre a un poeta " non grato ".
El duelo ha constituido siempre un sistema ruin,pero eficaz para neutralizar a un rival personal o político.
Si,como en el caso de Alejandro Pushkin,se encontraba en juego el honor de la persona,la respuesta era inmediata: se arrojaba el guante y sólo había que concertar fecha y hora,escoger el terreno y las armas,y nombrar los padrinos.
A veces los lances se cerraban con un saldo fatal,pero el ofendido se veía forzado a elegir entre una muerte digna o una posición de escarnio en la sociedad.
Porque,más que patrimonio y hacienda,más que la propia vida,el honor y la honra culminaban la escala de valores del hombre hasta hace poco menos de un siglo.
Gregorio Marañón explica en su ensayo sobre Don Juan que " la reacción psicológica específica del varón es el culto del honor,de la honra llevado hasta el máximo sacrificio ; si es necesario hasta la venganza y el crimen,que el honor se justifica siempre ".
Los celos,las diferencias políticas o indiscreciones eran razones suficientes ; pero también un desplante,una simple descortesía o una mirada que se sospechara ambigua empujaban a personajes como Alejandro Dumas,el duque de Wellington,William Pitt o Espronceda a batirse para lavar una cuenta,personal.
El duelo como reparación de ofensas no fue una práctica habitual en el mundo antiguo.
De todas formas,entre griegos y romanos existió una forma peculiar de combate,no para dirimir agravios personales sino para decidir la victoria entre dos pueblos en discordia y evitar el choque de los dos ejércitos enemigos.
Tal carácter revistieron los combates entre David y Goliat,Héctor y Aquiles... Cuentan Tácito y César en sus libros que las tribus germanas solían resolver sus batallas en combates singulares a espada.
Más tarde la invasión de los bárbaros introdujo el denominado " duelo judiciario " o juicio de Dios durante la Edad Media,época en que los nobles y hombres libres lo utilizaron como procedimiento para zanjar sus diferencias.
A partir del siglo Ix se desarrolló en el seno de la Iglesia un movimiento de hostilidad contra el duelo judicial.
El Concilio de Letrán lo prohibió en 1215 y,al robustecerse el poder público por los códigos civiles,los monarcas adoptaron medidas contra él.
Después del lance tenían la obligación de redactar un protocolo escrito,esencial sobre todo en el caso en que uno de los duelistas cayera herido mortalmente,ya que sobre el otro recaería la responsabilidad penal.
Se establecieron también tres tipos de duelo: los decretorios o a muerte,los propugnatorios o a primera sangre,en los que se combatía para lavar el honor pero sin ánimo de matar y los satisfactorios,en los que se estaba dispuesto a desistir del enfrentamiento en cualquier momento si el ofensor prestaba la debida satisfacción.
Salvados los aspectos formales,la regla máxima del duelo consistía en demostrar que en el lance se batían dos caballeros de honor,no dos maleantes.
Su comportamiento debía ser escrupulosamente correcto: aunque la angustia y el miedo hiciera presa en ellos,aunque el corazón se les saltara del pecho,su actitud debía mantener una impasible serenidad.
En el caso fatal de ser alcanzado,Traveller aconseja en su libro " El arte de los duelos ",de 1836,mantener la sangre fría hasta el final,para morir decorosa y dignamente.
De todas formas los duelos no fueron tan cruentos como hoy podamos imaginar.
No fue hasta la adopción de la pistola como arma reglamentaria,durante la Revolución Francesa cuando el duelo se convirtió en un juego de azar mortal.
Las armas de fuego aventajaban a las blancas en que derribaban las diferencias físicas entre ambos duelistas.
Pero dice Larra que " con su concurso nada le queda que hacer al valor si no morir,porque la destreza es infame si hay superioridad e inútil si hay igualdad ".
El reto consistía entonces en que ambos tuvieran las mismas oportunidades de abandonar el terreno siendo el vencedor.
Y para que el azar jugara en estos enfrentamientos un papel todavía más relevante,no se admitían pistolas de cañón rayado,de mucha mayor puntería.
Las estrías practicadas en ellos descomponen la inercia de la bala en un movimiento giratorio que estabiliza su trayectoria,logrando una mayor precisión en el blanco.
Por eso los duelistas,al no disponer más que de armas de cañón liso,de gran alcance pero mucho menos certeras,se entregaban al factor suerte.
Hoy día todas las armas de fuego reglamentarias,salvo las escopetas de caza,tienen sus cañones rayados.
Durante todo el siglo pasado el mejor seguro de vida para todo caballero de honor era pues practicar,practicar y practicar.
Recuerden,si no,los múltiples duelos que tienen lugar en la película Barry Lindon.
Sin embargo el duelista no sólo debía estar preparado en todo momento ; estrategas de oficio conocían perversos ardides para desajustar de alguna manera la igualdad que garantizaban las reglas del juego.
Cuando la ofensa había sido grave,el ofendido tenía derecho a utilizar su propia arma,que antes podía manipularse rayando el cañón hasta la mitad sin que se percibiese a simple vista.
Artimañas poco caballerosas pero muy eficaces a la hora de salvar el pellejo.
Cuando se hace pasar una corriente eléctrica del ánodo al cátodo comienza el baile de iones y electrones.
Todos se combinan entre sí,sodio y flúor de la criolita,aluminio y oxígeno de la alúmina.
La intensidad de la corriente alcanza los 130.000 amperios,aunque la tensión no llega a los cinco voltios.
Finalmente la carga positiva del ión de aluminio pasa al cátodo,recoge allí un par de electrones y se precipita como aluminio.
El metal permanece líquido en todo momento,ya que la temperatura en la celda (955 ° C),mantenida por efecto de la intensidad de la corriente,es muy superior al punto de fusión del aluminio (650 ° C).
El oxígeno de la alúmina emigra hacia el ánodo (los bloques de carbono que penden sobre la cuba) y forma con él dióxido de carbono,que se desprende en estado gaseoso.
Los ánodos de carbono se van oxidando así lentamente,debiendo reemplazarse por lo general cada 25 días.
En cambio,los bloques de carbono en el fondo de las cubas se desgastan bastante menos.
En sentido estricto deberían tener una duración ilimitada,pero también acaban degradándose.
Una vez cada cinco o seis años,cuando las celdas reciben un nuevo revestimiento refractario,se aprovecha para renovar el cátodo de carbono.
Con cierta frecuencia se registra,aquí y allá,un fenómeno denominado efecto ánodo.
Este se produce cuando no hay suficiente alúmina en la cuba.
El flúor de la criolita se combina con el carbono del ánodo formando fluoruro de carbono en forma gaseosa.
Tal compuesto permanece como una campana colgando del ánodo,constituyendo una barrera eléctrica a la función electrolítica.
La solución al problema consiste en perforar esta corteza de alúmina y criolita y añadir alúmina fresca.
La campana de gas desaparece al ser traspasada con una vara de sauce de unos cuatro metros de longitud.
Por chocante que parezca,es cierto.
Para fabricar uno de los materiales más modernos que se conoce es necesaria la intervención de una humilde rama de sauce.
Una vez precipitado,el aluminio fundido se extrae de la cuba por aspiración y se transporta hasta las naves de moldeo.
En los hornos de fundición comienza la elaboración del aluminio propiamente dicha,sometiendo el metal fundido a un tratamiento esencial: la aleación.
El aluminio puro es un material muy blando que tiene que ser tratado adecuadamente para proporcionarle dureza y resistencia.
El proceso de la aleación se puede comparar con la manera de preparar una sopa en la cocina.
El horno de fundición que contiene el aluminio liquido sería el puchero con el caldo.
Con mucho cuidado se van añadiendo las especias en cantidades muy pequeñas,que varían entre el 0,1 y el 8 por ciento.
Estas especias para sazonar el aluminio son,según los casos,minerales como manganeso,bismuto,magnesio y otros parecidos,pero también se puede añadir sílice,hierro o cobre.
Según la combinación de los ingredientes y variando sus proporciones se pueden obtener hasta 150 clases distintas de aluminio.
Cuando la aleación pedida por el cliente está en su punto,el aluminio se extrae del horno de fundición ; una salida espectacular,en la que el metal incandescente irrumpe como una cascada de lava hacia el fondo de os moldes.
Poco a poco,y ayudado por un chorro de agua fría,el aluminio va solidificándose.
La fundidora continua deja salir planchas de distinto grosor y anchura,mientras que de otros moldeadores surgen largas barras cilíndricas o tetraédrias.
Las planchas de aluminio se suministran al cliente en bruto o ya laminadas ; láminas que servirán para,por ejemplo,recubrir el fuselaje de un avión o para envolver alimentos.
Las barras,por el contrario,se suelen moldear por extrusión: al igual que la nata es comprimida por la manga del pastelero para darle un perfil determinado,así se prensa el aluminio caliente por un orificio previamente conformado.
De esta manera se fabrican perfiles de todas clases,por ejemplo para el bastidor de un automóvil o el mástil de un barco,en el que se incluye la ranura para la vela mayor.
El aluminio inyectado se emplea para fundir culatas y bloques de motores o para fabricar llantas de coches y motocicletas,así como otras muchas piezas.
La llamativa versatilidad del aluminio se debe a una serie de propiedades que caracterizan a este metal y que le hacen destacar por encima de otros.
En primer lugar,es un material muy ligero - la cuarta parte de la densidad del acero - y a la vez muy resistente si se le alea convenientemente.
La industria aeronáutica aprovecha masivamente esta cualidad: actualmente el 80 por ciento del peso estructural de un avión corresponde al aluminio utilizado en su construcción.
También automóviles,camiones y material rodante ferroviario se benefician del ahorro de peso y energía que significa el empleo de este metal.
EL aluminio no se oxida.
O mejor dicho,se oxida en seguida.
Expuesto al aire,inmediatamente se forma una fina película de óxido sobre su superficie que lo protege de una oxidación posterior más profunda.
El espesor de esta película protectora puede aumentarse adicionalmente mediante la técnica del anodizado.
Tal cualidad ha sido bien aprovechada durante muchos años en arquitectura y construcción de estructuras expuestas a las inclemencias del tiempo,como puentes,postes de alumbrado,revestimientos para barcos,etcétera.
También es dúctil,es decir,susceptible de ser estirado en forma de alambre.
Y además conduce muy bien el calor y la electricidad.
Estas dos cualidades combinadas han convertido al aluminio en el material más utilizado en el transporte de energía eléctrica a través de líneas de alta tensión.
Otra gran ventaja del aluminio es su no toxicidad,propiedad aprovechada por la industria alimentaria y farmacéutica para envasar todo tipo de productos,desde pasta de dientes,hasta galletas.
Por último,y pasando por alto otras cualidades muy interesantes,como el antimagnetismo,reflectancia,resistencia a las bajas temperaturas,etcétera,es importante destacar la capacidad de reciclaje del aluminio.
Esta práctica,ampliamente extendida,no sólo ahorra material,sino también grandes cantidades de energía: el reciclaje consume el cinco por ciento de la energía necesaria para obtener por electrólisis la misma cantidad de aluminio a partir de la bauxita.
A pesar de todas estas ventajas,el aluminio se las tiene que ver desde hace poco con tres importantes competidores con un amplísimo campo de aplicación: el plástico,la cerámica y las fibras de carbono.
La irrupción de estos nuevos materiales en casi todos los acabados industriales ha provocado un receso en la demanda de aluminio.
Los fabricantes del metal más versátil que se conoce han reaccionado invirtiendo grandes cantidades de dinero en investigación,con objeto de desarrollar nuevas aleaciones que mejoren todavía más las ya existentes.
Solamente así podrán mantener una hegemonía forjada durante todo un siglo.
A comienzos del siglo XXI los diseñadores genéticos habían logrado construir artificialmente animales y seres humanos con una fidelidad asombrosa.
La última generación de replicantes,llamados Nexus 6,superaba en fuerza y habilidad a sus creadores y tenía una inteligencia equivalente.
Fueron utilizados como esclavos en la colonización de otros planetas.
La única manera de distinguirlos de un ser humano era sometiéndolos a una prueba para analizar su respuesta emocional.
Respecto a los animales,sólo un minucioso análisis de una pluma,pelo o escama podía desvelar si existía o no la marca del fabricante.
Este mundo de habitantes sintéticos fue plasmado al celuloide por Ridley Scott en su película de ciencia ficción Blade Runner.
¿Podría darse en un lejano futuro una situación parecida? Trasladémonos al siglo XX,al momento actual,en que esta aventura está empezando en la realidad.
Un científico observa al microscopio un óvulo fecundado de ratón,futuro embrión de un ser vivo.
Con su mano izquierda acciona una palanca,moviendo milimétricamente un diminuto aspirador de vidrio.
Acercándose poco a poco al óvulo,consigue finalmente inmovilizarlo.
En el campo de visión del microscopio aparece una aguja finísima.
Con precisión absoluta el científico acerca la aguja hasta llegar a la cubierta del óvulo.
Luego la introduce,llegando al núcleo,la zona donde se encuentra la información genética del padre y la madre.
La cubierta del óvulo se ha comportado como si fuera goma elástica y apenas ha ofrecido resistencia.
Luego inyecta cientos de copias de un gen extraño.
Cuando crezca el embrión,se convertirá en un animal nuevo.
Llevará en sus entrañas una información que jamás tuvieron sus antepasados.
La hibridación de animales tiene tras de si una larga historia.
Hace miles de años alguien en Asia Menor tuvo la feliz idea de cruzar una yegua con un asno.
De esta unión salió la mula,híbrido con el vigor y el tamaño de un caballo pero con la resistencia y paciencia de un asno.
El hombre fue experimentando y seleccionando animales y plantas a su antojo.
I Las variedades de trigo cultivadas actualmente fueron elegidas de entre un conjunto de especies,a lo largo de cientos de siglos.
Pero estos animales produjeron la proteína del factor IX de coagulación de la sangre.
Vistas así las cosas,no resulta descabellado imaginar que en un futuro tendremos granjas donde vacas,ovejas y cabras se comportarán como laboratorios farmacéuticos que producen drogas proteínas,enzimas... Si se quiere disponer de grandes cantidades de una sustancia en especial,bastará ordeñar periódicamente al animal que la produzca,sin necesidad de sacrificarlo.
Aunque la ingeniería genética utiliza desde hace tiempo las levaduras y las bacterias para producir proteínas industrialmente,si la proteína es compleja,la bacteria no sabe fabricarla bien.
Imaginemos esa proteína como un largo cordón que necesita adquirir una forma determinada para poder cumplir su función biológica.
Para conseguirlo,es preciso atar el cordón en determinados puntos mediante enlaces químicos.
Las bacterias se muestran incapaces de colocar estos enlaces,y una gran proporción de las proteínas que fabrican resultan inservibles.
En los animales transgénicos,la producción futura de tales proteínas no presentará ningún problema.
La posibilidad de introducir genes en un animal permite corregir defectos genéticos.
La cirugía actual es incapaz de operar en los genes,quitar los defectuosos y colocar unos sanos.
Pero pueden introducirse estos genes con la esperanza de que suplanten a los defectuosos.
Así se ha logrado corregir un tipo de enanismo en ratón,suministrándole el gen de la hormona del crecimiento.
En otro caso fue posible arreglar una deficiencia genética de su sistema inmunitario.
Los macrófagos,unas células del sistema inmunitario,se encargan de reconocer y atrapar a cualquier invasor.
En este caso se mostraban incapaces de cumplir su función,debido a unos genes defectuosos.
Con los nuevos de recambio,la deficiencia fue subsanada.
Y no hay que olvidar que los animales transgénicos pueden enseñarnos mucho sobre nosotros mismos.
Podremos dotarlos de genes que dirijan la fabricación de anticuerpos contra diferentes tipos de virus y bacterias.
Les habremos inmunizado contra multitud de enfermedades de por vida.
¿No podría hacerse lo mismo con los futuros embriones humanos? De ser así,los niños del futuro nacerían vacunados contra futuras infecciones.
Hoy los médicos saben que las grasas de origen animal crean problemas en la salud de nuestras arterias.
Se forman tapones de grasa que obstruyen los vasos sanguíneos,y aumenta el riesgo de un infarto.
Introduciendo genes adecuados en los embriones de los futuros animales domésticos,podemos cambiar la composición de las grasas.
Cerdos,corderos,terneros transgénicos... cuya carne sería menos perjudicial para nosotros e incluso nos protegería de ataques cardíacos.
Los corrales estarían repletos de gallinas transgénicas.
Sus huevos contendrían altas dosis de proteínas humanas.
Implantando el gen de la insulina en los embriones,obtendríamos razas de gallinas que pondrían huevos con la insulina que necesitan las personas diabéticas.
Los animales transgénicos comienzan ya a ser una realidad.
Existen laboratorios en el mundo a los que se les puede pedir un animal con un gen en particular.
Ministerios de Agricultura de los países más industrializados - - Canadá,Estados Unidos,Gran Bretaña,Francia - - están empezando a desarrollar programas.
Hay una intensa búsqueda para tratar de encontrar animales hechos a medida.
Se está estudiando cómo aumentar la cantidad de lana introduciendo genes que lleven la información para fabricarla.
La composición de la leche puede sufrir cambios con manipulación genética.
Una gran parte de la humanidad no tolera la leche debido a la lactosa.
Para eliminarla existen genes que,introducidos,fabricarían enzimas que la destruirían.
Dentro de algún tiempo,los granjeros podrán disponer de animales cuyo mantenimiento no precise de las costosas proteínas para su alimentación.
Las fibras vegetales carecen de un aminoácido llamado lisina.
Un grupo de científicos está barajando la posibilidad de incorporar un gen en los animales domésticos que lleve las instrucciones para sintetizar este aminoácido.
Los animales no tendrían la necesidad de obtenerlo de la dieta.
Su patrimonio genético se lo daría.
Parece comprensible que,además de los factores fundamentales (política,economía,entorno empresarial y otros),se volvieran los ojos a los técnicos,ya que éstos se han convertido en un elemento de primer orden para entender el funcionamiento diario del mercado de valores.
El mercantilismo al que se acusó de profundizar la crisis parece sencillo en los resultados ; pero parte de unos programas y sistemas expertos muy sofisticados que necesitan gran cantidad de datos.
Los grandes fondos de inversión habían desarrollado en los últimos años una operativa basada en la posibilidad real de que los programas decidieran qué títulos comprar,cuántos y cuándo,y a la inversa.
Y el 19 de octubre,en Wall Street,se dio la inversa: los niveles de precios a la baja dispararon los mecanismos de ventas,que lo único que hacían era provocar más ventas.
Borja Ussia,miembro del broker español Beta Capital estaba en Nueva York ese lunes negro y a su vuelta dijo que " el principal responsable de la bajada han sido las computadoras y los program trading " (programas para enlazar operaciones bursátiles entre varios mercados).
Estos programas tienen unos niveles de emergencia,que una vez alcanzados disparan los mecanismos de ventas,sucesivamente,hasta producir esa bajada que nadie comprendía ni podía parar.
Los program trading son un software adecuado al comercio de valores y activos financieros y nacieron como consecuencia del volumen alcanzado por algunos fondos,que manejan cifras entre los 3.000 y los 10.000 millones de dólares,lo que hace imposible que un equipo de managers,por amplio que sea,pueda organizar las inversiones y desinversiones de una cartera con esa producción.
Además,la existencia de fondos de libre entrada y salida hace que los pequeños inversores,en situaciones límite,caigan presa del pánico y llamen a los gestores para vender sus participaciones,lo que implica que el fondo debe vender de inmediato por igual cantidad.
Así sólo es posible que un ordenador controle la liquidez del fondo y dispare las órdenes oportunas de forma sistemática.
Los ordenadores no se detienen a pensar.
Algo tan obvio significó que,llegados los niveles de emergencia,la sucesión de ventas pareciera no encontrar un limite a la baja cuando las acciones de muchas empresas se abarataron tanto que se hizo aconsejable su compra.
Pero eso los ordenadores no podían entenderlo.
Los mercados de valores han cambiado radicalmente gracias a unas redes informáticas que convierten a todas las bolsas en un único mercado global.
Han sido muchos años de avances en telemática e información computerizada para permitir que el flujo de dinero no conozca fronteras,y que de pronto la baja de Wall Street se extienda con dramático furor alrededor del globo.
hace ya tiempo que la bolsa,gracias a la electrónica,es un único gran mercado dominado por la informática que no conoce colchones amortiguadores temporales.
Cuando un operador de la bolsa de Nueva York lograra sobreponerse a los lamentos y jadeos que le produjeron la actividad y la sorpresa de la increíble sesión que acababa de vivir,antes de poder superarlo por completo comenzarían a llegar los datos por los que al momento comprendería que sus intereses en los i mercados orientales estaban corriendo t una suerte parecida.
Más tarde,con el c ocaso en las bolsas asiáticas,le tocaría r sufrir viendo cómo sus acciones de IBM,General Motors o cualquier otra,se depreciaban sin mesura en la City londinene,Franfurt,París o Milán.
¿Cuál había sido la principal diferencia entre la crisis de 1929 y la de octubre de 987? En palabras de un financiero de Chicago,en esta ocasión eran las computadoras quienes se arrojaban por las ventanas.
Pero para llegar a este grado e sofisticación informática habían sido precisos muchos cambios que han proporcionado una nueva filosofía de la inversión.
Aunque en los primeros años 70 e ejecutaron unos primitivos programas ara actuar en los mercados financieros,los ordenadores de entonces no permiten grandes cosas.
No seria hasta 1982,Con la llegada de las operaciones sobre posiciones e índices futuros,cuando los program trading comenzaran a hacerse indispensables en la estrategia de los grandes inversores.
La bolsa que salga de la reforma proyectada por el Ministerio de Economía no sólo será diferente porque la ley así lo quiere.
Los cambios en las formulas operatorias irán unidos a los cambios en el equipo informático,y las máquinas cambiarán también la fisonomía del mercado y de la forma de acceder a él.
En el futuro la bolsa española también podrá ser una " bolsa a domicilio ": contará con servicios teleinformáticos para recibir información en tiempo real y para emitir órdenes de compra y venta a un broker desde un ordenador personal.
La Bolsa de Madrid ya ha adquirido buena parte del material informático que hará posible el mercado continuo.
En principio,el mercado madrileño está desarrollando un soporte basado en la conexión de los terminales de ordenador instalados en los despachos de agentes y mediadores,con las entidades financieras y la propia bolsa.
La conexión se realizará con líneas punto a punto a través de la red digital Iberpac de Telefónica.
El sistema de Toronto y París,por el que parecen haberse decidido los encargados de diseñar la reforma técnica de la bolsa,cuenta con un ordenador que realiza por si solo las operaciones.
El agente introduce en él un modelo de operación con los elementos y las condiciones decididas,y la máquina,atendiendo a esas características,busca el comprador o el vendedor adecuado.
Londres y Nueva York cuentan con un sistema algo me nos sofisticado,en el que el ordenador central sólo sirve como soporte de órdenes y ofrece los datos al mediador,que,a la vista de ellos,cursa la orden.
Todo este corolario de conexiones y posibilidades de información interactiva hará que la fisonomía de los parquets españoles cambie,e incluso desaparezca.
Las reuniones de los operadores y agentes en un corro,donde a grito limpio se forman los precios y se pactan las operaciones encargadas por los clientes,pasaran a formar parte de la historia de los mercados bursátiles.
La bolsa del futuro ampliara esta capacidad de recibir información y canalizar órdenes a todos los mercados de valores de la Comunidad Europea gracias al proyecto IDIS.
A partir de ahora mismo,la bolsa española comienza el camino para conseguir una centralización en el establecimiento de sistemas unificados,que permitirá un precio único para los valores en todo el país,a pesar de poder seguir operándose en cualquier plaza.
En suma,las bolsas españolas están encaminando sus pasos hacia un modelo que proporcione los medios técnicos que cambiarán la tradicional forma de operar en bolsa para hacerse más rápidos,fiables,conocidos,fríos y adecuados a los tiempos que corren.
Pero la rentabilidad de una inversión seguirá dependiendo del acierto en las decisiones de quien coloca su dinero.
Las máquinas ayudan,pero,evidentemente,a tanto no llegan.
La situación antes descrita podría ser una de las últimas consecuencias de un desarrollo tecnológico que actualmente sólo tiene lugar en los laboratorios,pero que en un futuro más o menos lejano podría revolucionar la vida de los individuos.
La idea de construir componentes electrónicos sobre una base orgánica no es nueva.
Ya en 1974 los científicos del centro de investigaciones de IBM,en Yorktown Heights,propusieron utilizar moléculas biológicas como elementos electrónicos de conmutación.
Desde el punto de vista fisiológico,la vida es ante todo un fenómeno electroquímico.
El cuerpo humano,por ejemplo,conduce la electricidad porque ciertas moléculas alteran su estructura química,produciendo la conocida sensación de calambre.
Estas sustancias portadoras de las cargas eléctricas son las hemiquinonas,que transfiriendo un átomo de hidrógeno de una molécula a la siguiente cambian sus propiedades eléctricas,igual que un transistor electrónico.
Por lo tanto,dichas moléculas son susceptibles de almacenar informaciones del mismo modo que un ordenador: como una secuencia de solamente dos estados diferentes.
Esta forma de codificar la información se denomina binaria.
Una de las razones por las que los científicos intentan sustituir el silicio por materia orgánica es que la tecnología microelectrónica convencional está a punto de llegar a los limites físicos que impedirán continuar integrando cada vez más los circuitos.
Pronto se alcanzará la infranqueable barrera del silicio: cuando los transistores alcancen dimensiones moleculares,ya no podrán ser más pequeños.
Si esto es así,¿por qué no utilizar directamente moléculas? Al intentar construir los biochips de modo que su funcionamiento fuera similar al de los circuitos integrados convencionales,los pioneros de la biolectrónica aplicaron a las sustancias orgánicas los mismos conceptos y métodos de fabricación que en el caso del silicio.
En los chips convencionales existe un material semiconductor,el silicio,que impurificado controladamente presenta unas zonas con exceso de portadores de cargas positivas y otras con exceso de cargas negativas libres.
Una estructura k similar construyeron los biólogos informáticos de IBM Arieh Aviram y Philip Seidden.
Tomaron moléculas orgánicas con exceso de electrones (Tetrahidrofulvalén) y otras a las que les faltan cargas negativas (TetrahidrohidrodimetaM).
Separando ambas capas por un aislante y colocando el conjunto entre dos placas metálicas,obtuvieron el primer componente electrónico biológico,que cuando es de silicio se llama diodo (sólo deja pasar la corriente eléctrica en un sentido).
También en Japón se han producido importantes descubrimientos en la investigación bioelectrónica.
Un grupo de estudiantes de la universidad de Tokio,bajo a dirección del profesor Toshihiro Akaite,consiguieron demostrar experimentalmente que una proteína natural extraída Del corazón de un caballo,el Citocromo,se comporta como un semiconductor n determinadas circunstancias.
En efecto,al hacer pasar una corriente eléctrica través de la citada sustancia,se pueden observar procesos químicos de oxidación y reducción,visibles por un cambio de color.
El profesor Akaike afirma que,aunque las investigaciones de los pioneros de la bioelectrónica habían sido una gran ayuda,fueron él y su equipo quienes consiguieron desarrollar por primera vez un dispositivo de conmutación y memoria a base de proteínas naturales.
. Sin duda se trata de un importante paso en la carrera hacia el biochip,cuyo alcance podría compararse con el de la invención del transistor de silicio,que sustituyó a las válvulas de vacío en los ordenadores.
Sin embargo,al igual que transcurrió mucho tiempo desde el transistor hasta el primer circuito integrado,también habrá que esperar algunos años hasta conseguir el primer biochip propiamente dicho.
Un chip,tanto de silicio como orgánico,está compuesto por numerosos diodos,transistores y cables conductores.
Para comprimir muchos elementos biológicos en un chip,se intentaron aprovechar también las técnicas convencionales de integración de circuitos,consistentes en formar capas de distintos tipos de material y eliminar determinadas zonas por fotoexposición,formando así las complejas - - y minúsculas - - estructuras del chip (ver MUY 82).
El modelo de chip que James McAlear,biólogo de la universidad de Harvard (Estados Unidos),patentó en 1978 seguía el mismo principio.
Partiendo del conocimiento de que las proteínas son capaces de organizarse para formar seres VIVOS,pensó que también serían un material idóneo para formar la matriz de un circuito electrónico.
En primer lugar formó una finísima capa (monomolecular) de proteínas sobre una placa de vidrio y la cubrió con una lamina,igualmente delgada,de plexiglás.
Aplicando un rayo láser alteró as propiedades químicas de la lámina protectora,de modo que aparecían unos canales que podían ser disueltos con alcohol.
Bajo estos canales quedaba nuevamente al aire la capa de proteínas.
Finalmente,al introducir el chip en una solución de plata,la materia orgánica se encargó de organizar la plata en una red de finísimos hilos conductores.
Este engendro aún no se podía considerar un auténtico biochip.
Con la única diferencia de que los engranajes y palancas serán macromoléculas o combinaciones químicas que cambiarán de estado muy rápidamente.
Por supuesto,aún serán mucho más lentos que los electrones.
Pero para compensar,los biochips trabajarán en paralelo y serán capaces,por ejemplo,de reconocer formas o texturas casi instantáneamente,lo cual en los ordenadores actuales resulta un proceso lento,arduo y costoso.
Este es precisamente el principal campo de aplicación de los chips orgánicos y en el que se están consiguiendo ya importantes resultados: los biosensores.
Potencialmente es posible construir auténticos órganos sensoriales artificiales,como detectores de determinadas sustancias,bacterias u olores,o incluso ojos artificiales.
Los sensores biológicos que existen actualmente en funcionamiento (un híbrido de componentes electrónicos de silicio y sustancias biológicas de efecto electroquímico) utilizan preferentemente enzimas,sustancias biológicamente activas que,como un catalizador,inician y controlan determinados procesos,sin sufrir modificaciones en su estructura ni consumirse.
Su propiedad más notoria es la capacidad de detectar determinadas sustancias,reconociendo la forma o la estructura de sus moléculas.
Por otra parte,las enzimas comparten esta propiedad con los anticuerpos,las células de los seres vivos superiores que reconocen a los intrusos por su forma o por la textura de su superficie y los inactivan acoplándose perfectamente a sus contornos.
En el caso de los biosensores,un componente eléctrico convencional,recubierto por una película de enzimas,detecta,amplifica y digitaliza las reacciones electroquímicas desencadenadas por las enzimas al entrar en contacto con las moléculas que se pretenden reconocer.
Cuando una enzima detecta y captura una molécula,se puede traducir estrictamente como la variación de un bit de información.
Cuando la enzima suelta la molécula,la información vuelve a su estado anterior.
Por supuesto,los procesos no son tan simples como parece.
Los seres vivientes tienen una auténtica voluntad propia y sus componentes básicos también.
Así llegamos a la cuestión que se tuvieron que plantear los investigadores: ¿cómo se programa una enzima,o todo un biochip? La respuesta es poco esperanzadora: no se puede.
Pero tal vez ni siquiera sea necesario.
Las enzimas y las demás sustancias orgánicas activas ya saben de antemano lo que quieren.
Sólo hace falta que el usuario del biochip,el hombre,quiera lo mismo.
Desgraciadamente no suele ser éste el caso.
Por lo tanto,o bien hay que educar las sustancias utilizadas (lo cual es poco menos que imposible),o bien hay que fabricar los elementos de conmutación biológicos a la medida: entramos de lleno en el campo de la ingeniería genética.
Desde que se conocen los ácidos nucléicos y su función como portadores de información en la biología molecular,los genetistas no han dejado de investigar la manera de manipular las cadenas de ADN (ácido desoxirribonucléico),para influir en los caracteres hereditarios de los seres vivos y moldearlos a voluntad (esta práctica se conoce como eugenesia).
Se sabe que los ácidos nucléicos influyen en la formación de las proteínas,haciendo que adopten una u otra estructura,de acuerdo con la información que contienen.
También se sabe que las proteínas son sofisticadísimas máquinas con ciertas similitudes con los circuitos integrados actuales.
Con estos conocimientos en la mano,hoy ya es posible diseñar estructuras protéicas artificiales - con ayuda de ordenadores - y criarlas genéticamente.
Siguiendo esta línea de trabajo,algún día será posible construir un ordenador capaz de reproducirse y copiarse a su imagen y semejanza,según el mismo principio en el que se basa toda la vida: la transmisión de la herencia genética codificada en la doble espiral de ADN.
La paradoja es que podríamos llegar a diseñar un ordenador del que conocemos su estructura física,pero no entendemos su funcionamiento,igual que ocurre con nuestro propio cerebro.
Si algún día se llega a construir uno de estos ordenadores - - más o menos independiente - - con materia orgánica,al tratarse de un sistema vivo podría implantarse directamente en el cuerpo humano.
Y en caso de que se pudiera conseguir dicho injerto,se abrirían inmensas posibilidades.
Los ciegos volverían a ver,los paralíticos volverían a caminar,los intelectualmente torpes se convertirían en malabaristas del pensamiento.
Además del entrenamiento y emplazamiento de los animales en los hogares de los inválidos,Helping Hands ha editado una cinta de vídeo que muestra todo el proceso de entrenamiento del mono,además de un folleto de 108 páginas,y en Canadá,Argentina e Israel se han comenzado proyectos similares.
Hasta hace poco tiempo,era un problema encontrar monos adecuados.
La mayoría de los que inicialmente se utilizaron en el programa eran animales rechazados por los zoológicos o procedentes de otros experimentos de laboratorio.
En cualquier caso,todos eran ya adultos.
Para los entrenadores,no conocer el pasado del mono ni la manera en que había sido tratado anteriormente les hacía difícil determinar cómo estos podrían adaptarse al trabajo de ayuda a incapacitados.
Pero ahora,cada año aumenta el número de ejemplares jóvenes,y muchos nacen en la Fundación Primatológica Mannheimer,de Florida,donde Helping Hands ha establecido una colonia de crianza.
Además del continuo funcionamiento y de las extensas sesiones de entrenamiento en el centro Einstein,los monos se instalan en " hogares adoptivos ",seis u ocho semanas después de nacer,para que se acostumbren a vivir con seres humanos.
Durante su estancia,cada mono es meticulosamente observado para determinar si es válido para las pruebas de asistencia a gente incapacitada.
Después de tres años,cuando el mono alcanza la madurez,se le quitan los dientes (para evitar que muerda) y,siempre y cuando haya pasado todas las pruebas,se le lleva a vivir al hogar de un parapléjico.
Aunque parte del condicionamiento del animal incluye en ocasiones descargas eléctricas de bajo voltaje (Willard compara la fuerza de esta corriente con la de la electricidad estática),muchos de los que adoptan al mono las sustituyen por suaves regañinas.
Una vez instalados en el hogar de un minusválido,los monos pasan buena parte de su tiempo en espaciosas " jaulas " en las que se les ha enseñado a comer,dormir y hacer sus necesidades,y a las que se dirigen cuando se les ordena.
Los monos capuchinos utilizados por Helping Hands son de una inteligencia poco común pueden dominar una tarea en unos treinta minutos,después de haber aprendido el procedimiento básico.
Tras ese aprendizaje,su exactitud al realizar esa tarea se acerca al cien por cien.
Para sus dueños minusválidos,como Sue Strong,los monos son un regalo del cielo en más de un aspecto.
Obviamente,representan un enorme beneficio,práctico: el minusválido puede indicar al mono qué tarea debe realizar en cualquier momento,de día o de noche.
De todos modos,además de los aspectos prácticos de la relación entre hombre y mono,la mayoría de sus dueños hallan también en los animales una satisfacción personal.
Esto es debido,quizá,a la conocida habilidad del género cebus para crear fuertes lazos con los seres humanos.
También se debe al simple hecho de la presencia misma del mono,que no sólo proporciona una constante compañía a personas que reciben visitas en muy raras ocasiones,sino que también facilita la conversación entre la persona incapacitada y otros seres humanos.
Por ejemplo: en un experimento,un parapléjico fue emplazado,con su silla de ruedas,en medio de un centro comercial repleto de gente.
En el espacio de una hora,sin el mono,sólo dos personas se detuvieron a hablar con él.
Pero en la hora siguiente - con el mono sobre su hombro - fueron 71 extraños los que se pararon para charlar.
Sue Strong explica este fenómeno: " Cuando salgo en mi silla de ruedas,todo lo que la gente ve es la silla,pero cuando salgo con mi mono,el mono es la única cosa que ven.
Nadie repara en la silla en absoluto.
" Está claro que los beneficios sociales que estos monos capuchinos dan a sus dueños son tan cruciales como los beneficios físicos.
Esto se evidencia en la manera que tiene de explicar su relación con Henrietta Sue Strong,que ha estado paralítica desde que fue victima de un accidente de automóvil cuando tenía 22 años: " Cada uno ha tenido que aprender cómo piensa el otro.
Tengo que tener cuidado para no esperar demasiadas cosas de ella.
Olvido que no puede leer mi mente,pero,¡fíjese en esto!,Henrietta tiene una cara que sólo una madre podría amar.
" El sucesor del CSL - 2 - - aunque éste todavía se encuentra en servicio - - se llama,naturalmente,Larga Marcha - 3. Su primera misión,de carácter experimental,tuvo lugar el 29 de enero de 1984,con el lanzamiento del satélite de comunicaciones,también experimental,China - 14,de 900 kilos de masa.
Sin embargo,su vida plenamente operativa comenzó tres meses más tarde,al situar en órbita geoestacionaria el China - 15.
El evento constituyó un doble hito en el programa espacial chino: era la primera vez que se lanzaba un ingenio a tanta altura y además se trataba del primer satélite de telecomunicaciones y televisión operativo.
Para este tipo de misiones - - la satelización geoestacionaria a 37.786 kilómetros de altura - - los chinos han habilitado un segundo cosmódromo,de nombre Xi chang,localizado mucho más al sur,concretamente en una región montañosa cerca de Tailandia a 28,5 ° de latitud norte.
La antigua base de Jiayuguang,considerada la principal,queda reservada a los lanzamientos a órbitas bajas.
Los cohetes,construidos en la fábrica de maquinaria pesada Xin-Xin en Shanghai,se transportan por vía férrea hasta.
La base que corresponda y se montan insitu.
Al China - 14 siguieron otros,y con tanto éxito que las autoridades de Peking decidieron ofrecer los servicios de sus dos cohetes CSL - 2 y CSL - 3 para colocar en órbita satélites de empresas privadas o públicas de otros países.
A tal fin han creado un organismo expresamente dedicado a comercializar los productos espaciales chinos,bautizado con el sugerente nombre de Industrias Gran Muralla.
Una de sus primeras gestiones,además de contactar con posibles clientes,ha sido la edición de manuales del usuario de los cohetes disponibles.
Así,el correspondiente al Larga Marcha - 3 especifica que tiene 44,7 metros de alto por un diámetro en la base de 3,35 metros,con una masa al despegue de 202 toneladas (casi la misma que la del Ariane 1).
Sus tres etapas - - las dos primeras propulsadas por el mismo combustible del Larga Marcha - 2 y la última,de alta energía,por hidrógeno y oxígeno líquidos - - le otorgan la potencia suficiente para situar cuatro toneladas en una órbita baja o 1.400 kilos en una órbita geoestacionaria a 35.786 kilómetros de altura.
La fiabilidad de la puesta en órbita es del 99,7 por ciento,según se asegura en el manual,con un margen de error de tan sólo siete kilómetros en el perigeo y 0,07 ° en la inclinación,gracias a un sistema especial de doble ignición en la tercera etapa.
También se describe con pelos y señales la cronología del vuelo.
Los motores de la primera fase entran en ignición tres segundos antes del despegue.
Después de diez segundos y a doscientos metros de altura,el cohete comienza a ladearse para alcanzar el punto óptimo de inclinación.
A los 129 segundos se desprende la primera etapa y empieza a funcionar la segunda,hasta agotar el combustible a los 258 segundos del despegue.
La primera fase de la tercera etapa se enciende a los 713 segundos,y la segunda a los 910,extinguiéndose definitivamente 1.201 segundos después de haber despegado.
En atención a las necesidades del cliente se ofrecen dos conos superiores para alojar el o los satélites (puede situar más de uno en órbita).
El modelo A tiene 5,84 metros de alto por 2,6 de diámetro en la base,mientras que el modelo B,más espacioso,tiene 7,27 metros de alto por tres.
de diámetro.
La apertura de los conos queda garantizada por un juego de catorce cerrojos explosivos y seis de.
resorte.
Complementan el manual datos exhaustivos sobre vibraciones,temperaturas,aceleraciones y hasta del clima que reina en la base de lanzamiento.
Pero la oferta china para competir en el suculento mercado del lanzamiento de satélites artificiales no se queda ahi.
Además de fiabilidad,también aseguran una campaña de lanzamiento extraordinariamente corta,de tan sólo 38 días,así como posibilidad de efectuar hasta cuatro lanzamientos por año.
Y lo mejor de todo,el precio: llevar al espacio un satélite a través de Industrias Gran Muralla cuesta únicamente 20 millones de dólares,una verdadera ganga en comparación con las tarifas de las compañías europeas,japonesas y norteamericanas.
Tantos éxitos han entusiasmado a los chinos hasta tal punto que ya están pensando en vuelos tripulados,transbordadores reutilizables y hasta en una estación orbital permanente.
Con vistas a estos proyectos han comenzado a desarrollar el Larga Marcha - 4,que contará con diferentes versiones y que presumiblemente tendrá capacidad para colocar hasta nueve toneladas en órbita baja.
Su puesta en servicio esta prevista para 1992,justo cuando los japoneses tendrán listo su lanzador H - 2,de similares características.
En realidad deberíamos formularla: ¿pueden pensar los animales? Porque éste es el quid de la cuestión.
Hasta mediados de siglo no existía ni el beneficio de la duda.
. Los biólogos eran tajantes: el animal,siendo un ser altamente complejo,carece de la capacidad de pensar,cualidad exclusiva y esencialmente humana.
Coincidían en considerar al animal una especie de robot animado al que determinadas acciones sólo podían provocar reacciones programadas.
El ejemplo más clásico es la conducta del insecto que,como un autómata,vuela y vuela alrededor de un foco de luz,sin poder substraerse de la hipnótica atracción de su luminosidad.
Con el tiempo,sin embargo,los especialistas en comportamiento.
animal han podido observar en ellos conductas opuestas.
Hábitos y gestos que rebaten la teoría de que sólo el hombre es un ser pensante.
Se podrían citar muchos ejemplos que confirmarían que los animales sí tienen clara conciencia de sus actos y que sus reacciones son premeditadas.
Los macacos del Japón son una clara muestra.
Estos monos ¡vaya que si piensan! Cuando recolectan granos de cereal en las playas y tienen las manos llenas,los arrojan al agua.
¿Por qué? Muy sencillo.
Por que se han dado cuenta de que mientras la arena se hunde,los granos flotan en el agua.
De esta forma pueden recogerlos luego y comérselos limpios.
Las últimas noticias que nos han llegado de estos pulcros macacos del Pacífico son que han depurado su técnica de lavado hasta tal punto,que ya no lanzan los granos al agua,sino que los enjuagan sin soltarlos.
Se ha comprobado también que la mayor parte de los animales utilizan toda suerte de lenguajes gestuales para comunicarse entre sí,como lo hacen los sordomudos de nuestra especie.
Valiéndose de ellos,los delfines pueden convertirse en interlocutores del hombre,las abejas los emplean en el " baile de la miel ": mediante movimientos rítmicos que semejan una danza,indican a sus congéneres dónde se halla el dulce néctar.
Pero pensar supone asimismo planear las actividades ordenadamente,calculando al mismo tiempo sus efectos prácticos.
En este sentido el comportamiento de la nutria marina es especialmente peculiar.
Cuando los moluscos de los que se alimenta están tan adheridos a las rocas que no puede desprenderlos con uñas ni dientes,la nutria no se lo piensa dos veces.
Busca en el fondo del agua alguna piedra que supone adecuada y la transporta hasta el lugar donde se encuentra el molusco elegido.
Acto seguido utiliza la piedra como herramienta para arrancarlo.
Fue el biólogo K. W. Kenyon el que demostró que tal artimaña no está inscrita en los genes de la especie,porque sólo los ejemplares más débiles - - las crías o los más viejos - - son los que,sin excepción,se sirven de este inteligente método.
Al resto de las nutrias les bastan sus fuertes garras y afilados colmillos para obtener el botín preciado.
El reino animal.
no es,pues,tan irracional.
La mayoría de sus integrantes piensan y razonan mucho más de lo que creíamos hace unas pocas décadas.
Sin apartarnos del ámbito de la zoología,pero remontándonos muchos miles de años atrás,la ciencia sigue preguntándose: ¿por qué desapareció este animal prehistórico que en un tiempo llegó a dominar el planeta? Muchos son los que coinciden con el geólogo californiano Walter Alvarez en que su exterminio pudo deberse a un colosal cataclismo.
Según esta teoría un gigantesco asteróide debió caer sobre la Tierra,pulverizándose con el impacto y levantando una descomunal nube de polvo que oscureció el cielo.
Las temperaturas en el planeta descendieron y un gran número de especies vivientes,entre ellas el dinosaurio,se extinguió.
Los paleontólogos han podido demostrar que hace 65 millones de años una devastadora hecatombe sacudió la Tierra,exterminando la mayoría de las especies marinas.
Sin embargo,esta hipótesis no nos sirve a la hora de explicar por qué sobrevivieron los mamíferos prehistóricos,una especie mucho más débil en apariencia que los supersaurios.
El científico norteamericano William Clemens sostiene la tesis de que los mamíferos... AL enigma de cómo surgió el hombre,el profesor David Pilbeam,de la Universidad de Harvard,responde: " Suponíamos que los humanos éramos una especie de primos lejanos del mono,pero cada vez está más claro que,en vez de primos,somos sus hermanos.
" Sin embargo,sus palabras no han disipado esa controversia en torno a nuestro mutuo grado de parentesco que existe entre paleontólogos y genetistas.
Ambos defienden teorías contrapuestas.
Mientras los paleontólogos,en base a los restos fósiles hallados,sostienen que la especie humana se separó genealógicamente del simio hace veinte millones de años,los genetistas retrasan mucho este desvío del tronco común.
Para ellos,la independencia genética de la humanidad es tan reciente,que ni siquiera llega a los cinco millones de años atrás.
El sensacional descubrimiento en China del cráneo de un Ramapithecus - - al que durante un tiempo se le atribuyera ser el eslabón perdido entre el hombre y el primate - - parece apoyar esta.
última teoría.
Hasta la fecha sólo se habían hallado del Ramapithecus pequeñas astillas óseas a las que se les dató por error una antigüedad de 15 millones de años.
Como su dentición se parece mucho a la de los posteriores en seguida se erigió al - Ramapithecus como candidato a " primer homínido ",lo cual situaría el inicio de la estirpe humana y su separación de los antropomorfos por esas fechas.
Pero ahora el análisis del cráneo chino descarta esta hipótesis.
Los resultados son contundentes: el Ramapithecus tiene sólo cinco millones de años y además no fue ascendiente directo del hombre... sino del orangután.
La confirmación ha dado al traste con las especulaciones de los paleontólogos,confirmando las tesis que defendían los genetistas.
Fueron precisamente ellos los que descubrieron que,desde el punto de vista genético,la estirpe humana posee un parentesco más estrecho con el chimpancé que,por ejemplo,el caballo con el asno.
Los datos reunidos por una larga tradición experimental han permitido a la biología moderna demostrar que cuanto mayor sea la diferencia genética entre dos especies,tanto más dilatado será el tiempo que habrán de recorrer ambas independientemente la una de la otra en su camino evolutivo.
Así pues,dado que la identidad genética del ser humano con respecto al primate alcanza el sorprendente porcentaje del 99 por ciento,las dos especies no pueden haber divergido - - esto es,separado del tronco común - - con anterioridad a la fecha de los cinco millones de años atrás.
De todos modos la cifra resulta asombrosa.
Para que la Ciencia la acepte sin reservas,la genética habrá de probar algún día cómo se pueden desarrollar dos especies tan diferentes partiendo de una diversidad genética tan imperceptible.
lave de la cuarta incógnita que nos ocupa es el misterio de la vida.
Porque de un misterio se trata.
¿De qué secretos mecanismos se vale la Creación para generar formas con vida? ¿Qué es en realidad la vida? ¿Ha surgido por generación espontánea? En el campo de la biología los interrogantes serían interminables.
Podríamos preguntarnos,por ejemplo,¿por qué no hay en la naturaleza huevos cuadrados? Si en el desarrollo de los organismos todo dependiera sólo de la casualidad,la evolución genética,lógicamente,habría podido crear sin duda huevos cúbicos.
Y es que el programa genético es la base de toda forma orgánica.
En todos los seres vivos el proceso es el mismo.
Las células acaban muriendo,pero son renovadas por otras nuevas que acuden a sustituirlas.
Lo curioso del caso es ¿cómo sabe la célula de la nariz,por ejemplo,que su espacio vital es la punta de la nariz y no el hígado o el cerebro? ¿Quién se encarga de programar la información en los genes? Quizá la teoría de los campos morfogenéticos desarrollada por Rupert Sheldrake podría aclararnos un poco esta cuestión.
Para el biólogo británico,junto a causas de origen genético,toda forma y comportamiento se basa en invisibles planes de construcción,esto es,en campos morfogenéticos que dirigirían y configurarían el universo en su conjunto,tanto lo animado como lo inerte.
A pesar de que estos campos carecen de materia o energía,actúan y pueden transformarse a través del espacio y del tiempo.
Así,cada vez que surgen átomos,moléculas,células y organismos - - unidades morfológicas - - se crea,según Sheldrake,un campo morfogenético que imprimirá su sello particular en todas las sucesivas unidades del mismo tipo.
En el caso de que un componente de un determinado género o especie adopte un comportamiento nuevo,su campo se modificará también,adaptándose a la nueva circunstancia.
Pero en cambio,si éste conserva su nuevo comportamiento durante el tiempo suficiente,la resonancia morfológica - - interacción entre todos los miembros de una especie - - influirá en todo el género.
Como ejemplo de esta causalidad configurativa Shelcirake suele citar la misteriosa formación de los cristales,un enigma que la Ciencia aún no ha logrado resolver.
Tan pronto como se forma el cristal en un sitio,puede reproducirse con mayor rapidez y facilidad inmediatamente después en cualquier lugar del mundo.
Sheldrake comenta que la explicación convencional a este fenómeno suele ser que las moléculas de los cristales se adhieren a las batas de los químicos y que son éstos los que las propagan al viajar de laboratorio en laboratorio.
Honestamente opino que mi teoría de los campos es mucho más plausible ".
La tesis de Rupert Sheldrake en la que se expone que todas las experiencias de los seres vivos de todos los tiempos son útiles y se acumulan y conservan en esos campos podría incluso explicar otros fenómenos que hasta hoy los científicos no han sabido abordar.
Así por ejemplo,en un experimento paralelo realizado en Nueva York y Munich en el que ratas sin adiestrar debían hallar la salida al intrincado laberinto del alcantarillado de las poblaciones,mientras las americanas no lo lograron,las ratas de Munich - - sin haber tenido contacto previo con las de Nueva York - - coronaron con éxito la difícil tarea.
Otro ejemplo de esta silenciosa comunicación por resonancia entre diferentes individuos de una misma especie se puso de manifiesto en otro espectacular experimento.
El objeto del test eran parejas de imágenes: una de ellas nítida y la otra una versión distorsionada del mismo motivo.
Una vez que los telespectadores británicos hubieron visto ambas imágenes,se procedió a comprobar si,gracias a esta proyección previa,personas de otros países que no habían tenido contacto alguno con los británicos eran capaces de reconocer mejor las imágenes difusas.
Efectivamente el test resultó un éxito.
Parecía como si de alguna manera los telespectadores británicos les hubieran transmitido su conocimiento por vía telepática.
No hay duda de que la teoría de la causalidad formante explica la repetición de las formas y comportamientos,aunque aclara poco de su creación e inicios,porque,como el mismo Sheldrake admite," hoy por hoy es todavía prácticamente imposible explicar a la luz de la Ciencia como ha sido creada la vida ".
Anclados aún en el vasto imperio de n la biología,nos topamos con nuestra quinta incógnita.
Porque en este ámbito no sólo nos preguntamos qué es la vida.
También qué es esa actividad intelectual a la que denominamos pensamiento,cómo funciona el cerebro.
J. Z. Bunge,profesor de anatomia londinense,trata de facilitarnos la comprensión de la enorme complejidad del cerebro humano con esta gráfica comparación: imaginemos una gigantesca sala de una oficina de magníficas proporciones,en la que trabajan 50.000 millones de empleados - - diez veces la población que habita actualmente en el planeta - -.
En su mesa de trabajo cada uno de ellos posee una central telefónica que en fracciones de segundo le comunica con el mundo exterior y con los otros departamentos de la megaoficina.
De la misma manera,los 50.000 millones de células nerviosas o neuronas que trabajan en el cerebro están entrelazadas en una inconmensurable red encargada de transmitir intermitentemente corrientes de órdenes e informaciones.
Como es lógico,nuestro cerebro no sólo se asemeja a una simple instalación E telefónica.
Constituye también una especie de computadora supereficaz que,a modo de programador,nos comunica tan ; o con las líneas emisoras y receptoras de nuestro propio cuerpo como con las del entorno exterior.
Y no sólo eso.
El cerebro humano es capaz de interpretar estas informaciones,extrayendo de ellas nuestras ideas,pensamientos,percepciones y recuerdos.
No obstante,no acepta sumisamente todo cuanto nuestra computadora cerebral suministra.
Al recordar o concentrar nuestra atención sobre algo concreto,filtramos,elegimos y posteriormente incluso modificamos nuestras acciones y comportamientos.
El cerebro nos proporciona asimismo la imagen de nuestro propio yo.
La persona que sufre de amnesia total quizá ignore quién sea,pero siempre reconoce que efectivamente es.
" El yo surge de la inconmovible consciencia de ser ",explica el científico John C. Eccles.
Comprendido esto,aún nos resta preguntar: ¿Dónde se halla localizada esa consciencia? " Ni en el cerebro,ni en ninguna otra zona de nuestro cuerpo ",asegura el neurólogo estadounidense K-H. Pribram.
Lo que denominamos consciencia no tiene nada que ver con una realidad tangible.
Habría que concebirla más como campos emisores de ondas invisibles que serían captadas por nuestros cerebros como si éstos fueran antenas.
De todas formas,el nortearnericano ignora de dónde pueden proceder estas ondas concienciales.
A pesar de las innumerables investigaciones que los neurólogos han efectuado sobre nuestro órgano inteligente,aún permanecen muchas lagunas oscuras.
Ultramodernos aparatos han conseguido averiguar,eso si,que la región de la corteza cerebral - - esa masa gris que contiene nada más y nada menos que 8.000 millones de neuronas y está formada por multitud de pequeños pliegues que abarcan el ochenta por ciento de la cavidad craneal - - es el asiento donde se originan pensamientos e ideas.
Sin embargo,cómo se genera su actividad sigue siendo La física aún sigue empeñada en hallar la partícula elemental,aquella que sea realmente indivisible.
Para ello,tendría que cuestionarse previamente qué es la materia.
La respuesta vino de la mano del genial Einstein,cuando formuló su teoría de que materia y energía no eran sino dos caras diferentes de la misma moneda.
Según su tesis,la masa podría transformarse en energía y la energía en masa siempre que se encontrase el sistema adecuado.
Muestra de ello seria la energía nuclear,en la que ínfimas cantidades de materia son capaces de liberar energía a gran escala,como sucede en el caso de la bomba atómica.
El valor de la materia que se libera resulta de su famosa ecuación E=mc2,esto es,energía igual a masa,multiplicado por el cuadrado de la velocidad de la luz.
Los aceleradores de partículas han demostrado que la fórmula es correcta y que es posible también el proceso inverso.
Al colisionar allí los protones entre sí,su materia parece aumentar porque la energía cinética ha aumentado su masa.
La energía que se libera por el choque de dos partículas genera nueva materia que,como Einstein sostenía,no es más que energía solidificada.
También de materia y energía,pero ambas supercomprimidas,versa nuestra última pregunta.
Para responder a ella está la cosmogonía,área astronómica que investiga la historia del universo retrocediendo hasta su hora cero,hasta quince millones de años atrás en que una colosal explosión dio origen a nuestro cosmos.
Algunas teorías sugieren que el BigBang fue resultado de la implosión de un universo preexistente.
Probablemente entonces este universo no sufriera la constante expansión que observamos en el.
actual.
Paradójicamente,se encontró la solución en la potasa... que se extraía de la misma madera.
Así que las primeras vidrierías tuvieron que convertirse en algo parecido a circos ambulantes: se instalaban en las proximidades de un bosque y,cuando habían acabado con todos los árboles,buscaban otro emplazamiento en la zona maderera más próxima.
Además,tenía que ser trasladado desde sus escasos lugares de producción hasta los hogares donde iba a ser utilizado.
Y eso sin olvidar que aún no estamos hablando de grandes láminas,sino de cristales de pequeño formato.
Aquellos que tenían un menor poder adquisitivo debían contentarse con instalar sucedáneos de cristal ; éstos consistían en láminas de piel muy fina o delgadísimas planchas de mármol casi traslúcido.
La luz pasaba a través de esos sustitutos,pero no la imagen.
Por muy delgados que fueran,estos materiales seguían siendo opacos.
Es decir,que lo que realmente se apreciaba en el vidrio no era su propiedad de dejar pasar la luz ; esto también lo conseguían otros materiales,y aunque resultaban más baratos,eran menos solicitados.
I Lo importante era que el vidrio de ventanas permitía el contacto visual con lo que hubiera más allá de él.
Y este deseo de ver la mayor cantidad posible de cosas parece haber sido uno de los anhelos más ancestrales y primarios de la humanidad.
Ya durante el florecimiento de la cultura goda (siglos V y Vi a. de C.),en la que desconocían la existencia de la ventana como tal,existía,sin embargo,la palabra augadauro (" puerta para los ojos "),acepción que también encontramos en la lengua anglosajona con el vocablo eagthyrl,que significaba " ojo de aire " y daría paso,en el inglés actual,a la palabra window.
Fenster y fenetre significan ventana,respectivamente,en alemán y francés,y las dos vienen de la latina fenestra,que a su vez procede del concepto antiguo griego phaino,que significa ¡Haz visible! ".
De todos modos,con o sin vidrio,la ventana ha tenido siempre un especial efecto simbólico.
Numerosas culturas la han utilizado para representar el contacto entre el interior y el exterior,entre nuestro mundo y el más allá.
El Libro de los Muertos egipcio habla de una abertura en forma de ventana custodiada por Horus,el dios con cabeza de halcón: a través de aquella abertura,la oscuridad de la noche daba paso todos los días al sol de la mañana,para inundar el mundo de luz.
Un muro cretense de más de 3.500 años de antigüedad,situado junto al palacio de Knossos,presenta la imagen de una ventana tras la cual se vislumbra una misteriosa belleza.
Hasta la fecha no se ha logrado interpretar esa imagen con total certeza.
Según la historiadora norteamericana del arte Carla Gottlieb,posiblemente la mujer observaba algún tipo de ceremonia religiosa.
Algo parecido ocurre con las pinturas descubiertas en las cerámicas romanas: en innumerables copias aparecen figuras de mujer asomadas al marco de una ventana,sin que nadie haya podido averiguar su significado.
Si en todas estas civilizaciones la ventana ha tenido un papel de importancia,en el cristianismo no ha sido una excepción.
Los textos bíblicos dicen que,durante un encuentro con sus seguidores,Cristo trazó en el aire la señal de la cruz.
Esta cruz fue interpretada por los primeros cristianos como una especie de escalera de luz púrpura por la un pecador podía subir al cielo.
Y esta señal de la cruz aparece por todas partes en las ventanas de las iglesias.
Ya que hablamos de iglesias,sin duda fue la época del arte gótico religioso la culminación de los vidrieros y constructores de grandes ventanales.
Aquella abertura practicada en los muros para que entrara la luz,que hasta entonces había variado de formas en múltiples ocasiones,adquirió una nueva dimensión.
El sistema arquitectónico de aquellas grandes catedrales,a base de bóvedas de crucería contrarrestadas en el exterior por los arbotantes,permitió abrir huecos cada vez mayores en los espacios intermedios.
Los muros llegaron a volverse casi transparentes por la evolución de la vidriería pero,aun así,no llegaban a abrirse hacia afuera ; no dejaban pasar el mundo exterior.
Se convirtieron en una artística cortina de vidrio que miraba hacia el recogido mundo de dentro.
El brillo de aquellas vidrieras coloreadas,dirigido siempre al interior de las catedrales,iluminó siglos de rezos,meditación y acontecimientos piadosos.
La religión y las ventanas han tenido además otra relación,ésta muy ligada con el cuerpo humano ; si,como dice el dicho,la cara es el espejo del alma,los ojos han sido siempre considerados como las ventanas de la misma.
Los filósofos griegos,los escritores clásicos,los textos religiosos y aun nuestro mundo actual han recogido y utilizado esta interpretación.
Un ejemplo lo tenemos en el retrato que Alberto Durero hizo en el año 1500 del teólogo alemán Philipp Melanchton: en él aparece un hombre desaliñado,cuyos ojos reflejan toda la pasión de un visionario religioso.
Y para reforzar aún más esa impresión de ventana hacia el alma,llevan un crucero de ventana.
El pintor belga Cornelius Gale dio una visión más sombría,con su cuadro La muerte negra vuela a través de la ventana equivocada,pintado en 1601.
En la pintura puede verse un rostro fantasmagórico,y en vez de los huecos de sus ojos aparecen negras contraventanas cerradas.
es que los artistas han comprendido que las cosas que se ven por una ventana no tienen por qué ser buenas,necesariamente.
El pintor danés Jan Steen lo demostró con su obra El avaro y la muerte,de 1654,donde un viejo usurero espera a la Parca ante una ventana.
Un reloj de arena que sostiene en su mano da la idea de que su tiempo,inexorablemente,se está terminando.
Este simbolismo religioso del ventanal fue perdiendo conciencia general con la llegada del protestantismo,la época de la llustración y la nueva era tecnológica.
Pero permaneció el sentimiento de que la ventana posee carácter,ejerce una cierta fuerza enunciativa sobre la persona que habita tras ella.
Una vez secularizada y liberada de su contenido religioso,la nueva ventana se abre paso y el siglo XIX se entrega a verdaderas orgías de ventanas,arquitectónicamente hablando.
Un ejemplo: más de 80.000 cristalitos (equivalentes a un tercio de la producción anual de la industria vidriera inglesa) para cubrir una superficie de 84.000 metros cuadrados,utilizó el arquitecto escocés John Paxton al construir el Palacio de Cristal de Londres,inaugurado para la Gran Exposición de 1851.
Y,no mucho después,todas las grandes metrópolis del mundo contaron con algún palacio de cristal parecido,verdaderos gigantes transparentes sostenidos por esbeltos enrejados de metal.
Probablemente el invernadero del rey Luis 11 de Baviera haya sido uno de los más bellos: en medio de plantas tropicales,cobertizos de cañas,cascadas de agua y un vivero de-peces en el que se reflejaba una luna artificial,aquel rey debió habitar,en su mundo de cuento,en una perpetua ensoñación de tiempos mejores.
El nuevo furor arquitectónico quedó perfectamente expresado en unas palabras del poeta Paul Scheerbart: " Sin palacios de cristal,la vida es una carga.
" Con ellas se refería a la sensación de ligereza que producían los espacios elevados y llenos de luz.
Pero,si esta sensación era tan subyugante,¿por qué los hombres no vivimos hoy día en casas de cristal? Como ya se ha dicho antes,probablemente no lo soportaríamos.
Aquella brillante luz que el conocimiento científico trajo a la humanidad no sólo reforzó las ideas de progreso,sino que creó nuevos temores elementales: al infinito,a lo desconocido,a ir más allá.
El hombre volvió a refugiarse en el tenebroso interior de sus viviendas,repletas de muebles y tupidos cortinajes que escondían sus ventanas.
Cortinas de encaje y visillos a media altura desconectaban al hombre del amenazante mundo exterior y convertían su vivienda en un castillo donde refugiarse.
¿De qué? No se sabe con seguridad,pero aquella disposición era el equilibrio entre los deseos de recogimiento y los de libertad casi ilimitada.
La evolución arquitectónica de la ventana siguió adelante.
¡Viva,viva,viva y mil veces viva nuestro imperio de la no violencia! ¡Viva lo transparente! ¡Lo claro! " Así gritaba el arquitecto Bruno Taut,y sus palabras fueron traducidas en la opulenta arquitectura de cristal,creada por los grandes maestros de la época,leyendas vivientes como Mies van der Rohe y Le Corbusier ; pabellones y grandes salas de exposiciones,pero también construcciones más profanas en las que un material inequívoco obtuvo el triunfo: el cristal.
Y aquellos frentes de los edificios donde sólo " había ventanas y existencia de luz,que caracteriza al hombre libre ",en palabras de Taut.
Muchas edificaciones eran verdaderos sueños llevados al mundo real.
cifras de captura de muchas especies se han estabilizado y otras,incluso,han comenzado a disminuir.
Al parecer,estamos sobrepasando lo que los técnicos denominan " rendimiento máximo sostenible ".
En lo referente al mar,nos comportamos como nuestros antepasados cazadores del paleolítico.
Era preciso un nuevo enfoque tan radical como el que supuso en el período neolítico la aparición de las técnicas agrarias: sembrar para cosechar.
Y este cambio tiene un nombre preciso: se llama acuicultura.
Sin embargo,la acuicultura no es una actividad reciente.
El primer texto de esta materia aparece en China allá por el año 475 antes de Cristo dando noticia del cultivo de la carpa.
No es extraño,pues,el liderazgo de Oriente en acuicultura: más de un 84 por ciento de la producción mundial,con Japón y China a la cabeza.
Por cierto que la situación no es la misma para todos los países.
Mientras que en muchos lugares del tercer mundo el pescado es la única fuente barata de proteínas disponible,en las naciones sobrealimentadas del mundo occidental representa más bien un alimento altamente digestible,bajo en calorías y cuyo consumo disminuye el riesgo de enfermedades cardiovasculares.
En el caso concreto de España,los más de 40 kilos por habitante y año que ingerimos vuelven insuficientes el millón trescientas mil toneladas anuales de productos marinos capturados por nuestra flota.
Ello nos obliga a unas importaciones que sobrepasan los 70.000 millones de pesetas,y es precisamente en este terreno donde la acuicultura tiene mucho que decir.
Para responder a estos interrogantes un equipo de MUY se ha desplazado hasta el noroeste de la península.
Nuestro primer objetivo es el Instituto Oceanográfico de La Coruña,dependiente del MAPA (Ministerio de Agricultura,Pesca y Alimentación).
Guillermo Román es un biólogo entusiasta que dedica sus esfuerzos al cultivo de moluscos bivalvos y especialmente al de tres especies de pectínidos de alto valor comercial: la vieira,la zamburiña y la volandeira.
" La situación actual en nuestro país sobre la pesca de pectínidos no requiere muchos comentarios - - afirma Román - -,o se actúa rápida y drásticamente,o muy pronto estos moluscos habrán de ser borrados de nuestra lista de recursos pesqueros.
El problema es la escasez de semilla,es decir,de individuos jóvenes.
" Los sistemas para obtener esa semilla para su posterior engorde son dos: o bien directamente del mar,o de una instalación denominada " thatchery " donde se cría el molusco bajo condiciones controladas.
Pero ambos procedimientos tienen sus inconvenientes.
En el primero de ellos se aprovecha la tendencia natural de los moluscos bivalvos a " fijarse " sobre una superficie una vez que las larvas han sufrido la metamorfosis con la que adquieren su forma de adultos.
Se sumergen en el mar colectores para que los animales se adhieran espontáneamente a ellos.
Pero los resultados que se obtienen no son muy alentadores,debido sobre todo a la escasez de individuos.
Y el cultivo en " thatchery " trae aparejado multitud de problemas,porque lo cierto es que aún no se sabe demasiado sobre el ciclo biológico de estas especies.
Cuando estos problemas estén resueltos,la obtención de elevadas cantidades y alta calidad de semilla permitirá no sólo que los cultivos industriales crezcan más rápidamente,sino que también se podrá llevar a cabo una repoblación de los bancos naturales.
por supuesto,en el Oceanográfico no son éstas las únicas líneas de investigación.
Alejandro Pérez y José Miguel Fuentes estudian otras especies: almeja,ostra y mejillón.
Hacia el año 35 se capturaban unos 50 millones de ostras en las rías gallegas,pero hoy se considera una especie al borde de la extinción en todo el litoral español.
Se intentó cultivar artificialmente,pero con la semilla importada de Francia - - 100 millones de unidades al año - - llegaron también las enfermedades.
El 80 % de las ostras no consiguieron sobrevivir.
En la actualidad,prácticamente toda la ostra que se consume en España es de importación.
Pero toda esta historia podría variar radicalmente en el futuro si los esfuerzos de estos investigadores se vieran coronados por el éxito.
En la actualidad,se estudian varias poblaciones de individuos,al parecer residuos de un único stock original,evaluándose factores tales como la resistencia a las enfermedades,el índice de crecimiento o las tasas de mortalidad.
Con los resultados que se obtengan,dentro de 2 - 3 anos,los científicos comenzarán a realizar experimentos de mejora genética de especies vigorosas,resistentes y perfectamente adaptadas.
Experimentos similares se están llevando a cabo con el mejillón,pero esta vez,intentando prevenir un desastre antes de que suceda.
Guillermo Rornán nos lleva hasta O Grove.
cela para sus experimentos con pectínidos: " Colaboración entre la empresa pública y la privada ",comenta Juan con una amplia sonrisa.
Mientras el viento y la lluvia arrecian,el radar de superficie del barco explora el mar frente a nosotros.
De pronto,en la pantalla aparecen infinidad de puntos.
Lucía hace un gesto circular con el brazo: " Son bateas ".
Increíble,pero cierto.
Casi 3.000 bateas donde se cultivan la mayor parte de las 200.000 toneladas anuales que convierten a nuestro país en el primer productor del mundo.
Vigo comienza a mostrar señales de sobrecarga.
La enorme riqueza en la fitoplacton de estas aguas no basta para alimentar a tantos animales.
Además,ahí abajo,en el fondo,las cien toneladas diarias de desechos que producen los mejillones están comenzando a causar alteraciones.
Anochece cuando regresamos a La Coruña.
Llueve tan intensamente que decidimos hacer un alto en el camino y,de paso,charlar con la gente del Centro Experimental de Vilaxoán,dependiente de la Xunta de Galicia.
Carmen Pérez Acosta trabaja en la misma línea de Román ; estudiando también la vieira como una de las especies alternativas o complementarias al cultivo del mejillón.
Francisco Cortés analiza las interacciones entre todos los elementos que confluyen en la explotación de las rías.
Por su parte,Ricardo Aznar se ha centrado en el cultivo de peces,sobre todo en rodaballos.
" Unos bichos muy sedentarios de los que en uno o dos años se obtienen ya ejemplares de dos kilos.
También aquí,como en el caso de los moluscos,el mayor problema es la escasez de individuos jóvenes.
Por esta causa,la inmensa mayoría de los que son adquiridos por las empresas españolas para su posterior engorde han de ser importados de Escocia o de Noruega.
Hay aquí un cuello de botella que factorías como Tinamenor,ubicada en Santander,intentan cubrir con una enorme producción de alevines de dorada,rodaballo,lubina,además de almejas y ostras.
Pero la investigación para conseguir supervivencias más altas de estas y otras especies prosigue en muchos lugares.
Nuestro viaje relámpago por el noroeste nos lleva hasta Vigo.
Allí los biólogos tanto del Centro Oceanográfico como del Instituto de Investigaciones Marinas (dependiente del CSIC) trabajan activamente en varias líneas de investigación.
Uxío Labarta y Miguel Planas,en proyectos de nutrición larvaria,ostra,almeja y mejillón,con nuevas.
técnicas de microencapsulado de fitoplacton.
Y un panorama que terminará por hacérsenos familiar: instalaciones de cultivo de rodaballo,en las cuales se lleva a cabo el proceso completo,desde la puesta de huevos hasta la obtención de ejemplares de talla comercial.
Caminando entre enormes tanques con una capacidad de varios miles de litros,donde nadan estos peces planos,el biólogo José Iglesias nos explica los puntos oscuros que existen todavía sobre su cultivo.
Como hemos tenido ocasión de comprobar,muchas de esas investigaciones son seguidas de cerca por la iniciativa privada.
Cumarsa es una empresa radicada en O Grove que participa en el proyecto Eureka,de la CEE,junto a una empresa noruega.
Se espera que su investigación conjunta permita obtener puestas viables de ostra a lo largo de todo el año.
Cultipec es otra empresa que se dedica al rodaballo.
" Buscamos apoyo en los centros de investigación - - nos cuenta José Luis Rodriguez,biólogo - - y en breve presentaremos al CDTI (Centro para el Desarrollo Tecnológico e Industrial) un proyecto para estudio de la fisiología larvaria... " Por supuesto,no es el noroeste espanol la única zona del país que ha apostado fuerte por la acuicultura.
Por ejemplo,en la provincia de Cádiz la empresa Esperanza Siglo XIX,pionera de esta actividad en España,que este año espera conseguir un millón y medio de alevines de doradas y lubinas,en palabras de su propietario,Florencio Molinero," tuvo que desarrollar una tecnología propia porque hacia los años setenta prácticamente no existía ninguna ".
Probablemente,gracias al tesón de sus impulsores,la empresa,que lleva funcionando desde el año 73 - - y acumulando pérdidas - -,espera obtener sus primeros beneficios este año.
Cupimar,también en Cádiz,propiedad entre otros de Abel Matutes,lleva a cabo investigaciones propias sobre la cría controlada de una gran variedad de peces,crustáceos y moluscos que se cultivan en las antiguas salinas de San Fernando.
La segunda ola Más al fondo están el robocarro la célula de fabricación flexible y la célula de láser robotizada.
La tercera ola está servida.
125 personas (de ellas,73 son titulados superiores e ingenieros técnicos) se encargan día tras día de mantener este espejismo surgido en el brumoso valle de Leniz.
El principal objetivo de Ikerlan es desarrollar la tecnología que necesitan las 116 cooperativas de la zona.
El propio Etxabe lo explica: " lkerlan está constituida como cooperativa de servicios,que presta a una serie de empresas.
Les transferimos tecnología a través de productos concretos,para mejorar su sistema de producción ".
Ikerlan se adelanta a las necesidades de las empresas,principalmente las cooperativas del grupo Fagor,y les of rece un servicio con tecnología punta.
Los visitantes extranjeros que acuden al valle con ánimo de entender el milagro de Mondragón se quedan sorprendidos por la capacidad del grupo para asimilar nuevas tecnologías.
El ingeniero chino Feng Si-Fa comprobó la eficacia de los cooperativistas antes de llevarse a su país equipos y maquinaria para instalar,al sur de Hong Kong,una planta productora de 200.000 frigoríficos al año.
Los responsables de la multinacional norteamericana General Electric tampoco se lo pensaron dos veces a la hora de adquirir los derechos mundiales para comercializar una máquina de control numérico desarrollada por Aurki,empresa especializada en la robotización de transformación y montaje.
Y los directivos de la firma alemana Siemens no daban crédito a sus ojos cuando en la feria de Hannover comprobaron que la planta piloto automatizada,instalada en Copreci,la cooperativa de componentes electrónicos del grupo Fagor,era capaz de fabricar dos millones y medio de electromotores para lavadoras al año y además con unos costes muy ajustados.
Segundo escenario.
Once de la mañana.
Escuela Técnica Superior de Ingenieros Industriales (ETSll) de Madrid,Departamento de Ingeniería de Sistemas y Automática.
El tablón de anuncios del Departamento despeja cualquier duda: " Micro Robots and Teleoperators Workshop "," Flexible Manufacturing Systems " Advanced Sensor Technology "," Software for Computer Control ".
Todas las convocatorias para ferias,cursos y simposios están relacionadas con la robótica y la automatización,cómo no.
Carlos Balaguer,doctor industrial,recuerda tiempos peores: " Lo primero que hicimos en robots fue crear un prototipo.
Ni siquiera teníamos financiación para su desarrollo,lo hicimos en plan artesanal,con estiles de aluminio de las ventanas y poleas hechas rudimentariamente ".
Al Robot,como le llamaban entonces,se le puede considerar pariente cercano de Gizamat.
AMR.
Estas siglas,que corresponden a las iniciales de Advanced Movil Robot,traen de cabeza a Balaguer.
El nombre no deja lugar a dudas,se trata de desarrollar,en colaboración con las empresas francesas Cea y Matra,las italianas Enea e Ital Robot y la española Casa,un robot móvil muy sofisticado en el marco del programa Eureka.
Este robot de seguridad ciudadana,en la línea del robot asumidor de riesgos que proyecta el gobierno japonés para los años noventa,estará capacitado para intervenir en casos de accidentes nucleares,de catástrofes naturales,como incendios y terremotos,y en la desactivación de explosivos en la lucha antiterrorista.
No será un robot dirigido,pues dispondrá de autonomía para tomar en cada momento las decisiones más oportunas.
Según todos los investigadores consultados,se trata del proyecto internacional más ambicioso en el que se han colado los grupos españoles que investigan estos temas.
Además del Departamento de Ingeniería de Sistemas y Automática de la ETSll,participan Ikerlan,el Instituto de Automática Industrial del CSIC y el Centro de Investigaciones Tecnológicas de Guipúzcoa.
Actualmente se encuentra en la segunda fase,la del diseño ; después,durante tres años más,vendrá la fase de construcción final.
En Sevilla,el grupo que dirige Antonio Civit Breu,del Departamento de Robótica de la Facultad de Física,tiene en cartera otro proyecto de robot móvil,que ha suscitado el interés del Ministerio de Defensa,pues en su fase final se dedicará a cortar alambradas,a buscar minas y,en general,a actuar en situaciones peligrosas.
El tema es delicado,y si finalmente se aprueba es de esperar que no acabe convertido en un robot asesino como el ALV norteamericano (Autonomic Land Vehicle),un vehículo autónomo de tierra dispuesto para maniobrar en el campo de batalla,reconocer al enemigo y destruirlo.
Ni Gizamat,ni Robot tienen memoria.
Por eso no recuerdan su árbol genealógico ; aquellos juguetes mecánicos con los que se entretenía el monarca Carlos I o los sorprendentes inventos de Leonardo Torres Quevedo,entre ellos el jugador de ajedrez y el Telekino,un sistema de control remoto basado en las ondas hertzianas.
Estas máquinas pertenecen a la historia fugaz de los androides españoles.
Pero las criaturas antropomórficas,diseñadas a imagen y semejanza de su creador,no interesan a los investigadores españoles dedicados al campo de la robótica y la automatización.
Al famoso doctor No (José No Sánchez de León),jefe de Control Automático del Instituto de Automática Industrial del CSIC en Arganda Madrid),tampoco le interesan los androides.
sabe que el futuro inmediato pertenece a la robótica inteligente," aunque - - reconoce - - hay otros muchos saltos,entre ellos el salto de la robótica a automatización integral y el salto de la robótica industrial a la robótica no industrial ".
Desde luego,Pacheco no lo era,pero estaba preparado para cualquier eventualidad.
Había pasado cuatro años entrenando en Alemania y los últimos dieciocho meses había convivido intensamente con sus actuales compañeros de tripulación: un piloto francés y un especialista de carga útil italiano.
Las últimas semanas,antes de partir,había revisado su misión tantas veces que se sentía capaz de realizarla casi con los ojos cerrados.
a astronáutica era ya una profesión.
Había dejado de ser una aventura.
Este era el tercer vuelo del Hermes con dirección a la Estación Orbital Internacional y estaba previsto que realizara otros tres el próximo año.
Soviéticos y norteamericanos bajaban y subían al espacio como si de un ascensor se tratase.
Veintiséis personas,con Pacheco,formaban parte de la primera flota de astronautas europeos.
Sólo por eso pasarían a la historia.
Pero en el Man Tended Free Flyer,el laboratorio automático europeo,al que ahora se dirigían,un módulo diseñado para realizar experimentos en microgravedad,había mucho que hacer y eso era lo único importante - - pensaba el español - -,y,además,para eso le pagaban.
España quiere tener para el año 2000 comunicación directa con los cielos.
Allá arriba hay jugosos beneficios científicos y tecnológicos ; y,por tanto,económicos.
Hay que tomar posiciones y colocar satélites,laboratorios y hombres en el espacio.
De ahí que se haya comenzado un estudio para enviar nuestro primer satélite en 1992 y nuestros primeros astronautas a finales de siglo.
Las cifras,por sí solas,hablan de esa decidida apuesta de España por la carrera espacial.
Desde 1975 hasta 1987 se invirtieron mas de 40.000 millones de pesetas en la Agencia Espacial Europea (ESA).
A partir de este año y hasta el 2000 invertiremos casi 217.000.
De los trece países miembros de la Agencia,somos el quinto en contribución y volumen de negocios.
La ESA garantiza que el dinero invertido retornará a los países miembros adjudicando contratos de fabricación y desarrollo tecnológico a las industrias,y de investigación a los centros pre parados para ello.
" Lo que pasa es que ahora,al aumentar nuestra contribución,el esfuerzo que debemos hacer es más grande,porque hay que recuperar más dinero en contratos,y carecemos del tejido industrial y científico necesario para ello.
Lo cual es grave,si tenemos en cuenta la competencia que hay en Europa ",señala Vicente Gómez,director de los programas de la ESA en el Centro de Desarrollo Tecnológico e Industrial.
Actualmente,33 empresas y once organismos públicos poseen contratos con la Agencia y participan en áreas como la electrónica,robótica,microgravedad,software avanzado,aerodinámica,nuevos materiales y medicina espacial.
Estamos lejos,muy lejos,de los líderes de la ESA - - Francia y Alemania - -,que van a invertir más de dos billones de pesetas de cara al 2000.
Las empresas españolas también participan en Arianespace,Eutelsat,Inmersat y demás consorcios y organismos que controlan y comercializan el uso de satélites,y han logrado alcanzar un mayor nivel de responsabilidad.
Hasta hace tan sólo dos o tres años,estaban involucradas en estructuras de fibra de carbono,en mecanismos de precisión,en antenas y en poco más.
Sólo podían acceder a los equipos,pero no a los subsistemas y menos aún a los sistemas.
No estaban preparadas para desarrollarlos.
Pero a partir de la Conferencia de La Haya,que tuvo lugar en noviembre del año pasado y en la que Inglaterra puso freno al aumento del presupuesto del programa científico,la participación española en los programas europeos aumentó considerablemente.
En La Haya se aprobaron los presupuestos para la estación espacial Columbus,el cohete Ariane 5 y el transbordador Hermes,y se decidió comen zar a desarrollar el Satélite Repetidor de Datos,que servirá para establecer comunicación entre ellos y los centros de control terrestres de la ESA.
En el Ariane 5 la industria española intervendrá en la estructura de fibra de carbono de la caja de equipos,en la central de conmutación y en las instalaciones de ensayo,lanzamiento e integración,situadas en tierra.
En el Hermes,que realizará tareas de servicio y mantenimiento de la Estación Espacial,tenemos un cinco por ciento de participación y nos ocupamos de diversos subsistemas.
Como el de iluminación interna de la nave,el de comunicaciones y el de simulación y ensayo del centro de control de vuelo.
Este es el gran cambio: hemos pasado de realizar equipos o estructuras a desarrollar subsistemas,a meternos de lleno en el mundo del software avanzado.
En el programa Columbus - - que consiste en un módulo presurizado enganchado permanentemente al núcleo de la Estación Internacional,más una plataforma habitable,una plataforma polar y otra más,no tripulada,que coorbitará la Estación - - participamos en un siete por ciento del presupuesto.
La industria española va a responsabilizarse en este caso de los subsistemas de atraque y amarre,de potencia y de la estructura.
En esta rápida radiografía al panorama tecnológico espacial del país,cabe señalar además la reciente puesta en marcha de un Plan Nacional del Espacio en el que n se invertirán,en principio,1.900 millones de pesetas anuales.
Los objetivos a cubrir son,por un lado crear una base industrial adecuada para sostenernos en el espacio ; y por otro,formar a los el científicos,lo que ya de por sí constituye un reto.
Según Luis Oro,director general de Innovación Científica y Tecnológica,no tenemos especialistas en ciencias espaciales.
Urge dar (a con ellos.
Aún más,y lo añade Vicente Gómez refiriéndose ahora a la industria," carecemos de especialistas en los campos de gestión de programas espaciales,de estudios de sistemas y de garantía de los productos ; tres áreas prioritarias en las que debemos incidir ".
Así están las cosas.
Sin embargo,para visitar las estrellas,conocer las galaxias o escudriñar en la barriga de los agujeros negros,hay otros caminos.
Con un buen telescopio,como el William Herschel,de 4,2 metros de diámetro - - hoy día el más potente del mundo - -,y un seeing,una calidad perfecta en la atmósfera,como la que se tiene en Canarias,es suficiente.
Así de sencillo.
La comunidad de astrofísicos en activo del país la constituyen dos centenares de personas que se distribuyen en los distintos observatorios.
En Sierra Nevada,por ejemplo,en el Instituto Astrofísico de Andalucía (IM).
En este caso,el centro y la financiación corren a cuenta de España.
pero los telescopios pertenecen a Francia e Inglaterra.
El tiempo de observación es al cincuenta por ciento,pero en 1991 se espera instalar dos telescopios de nuestra propiedad que ya se están construyendo en China.
En el IM se siguen varias líneas de investigación: astrofísica estelar,estructura galáctica y evolución estelar,astrofísica extragaláctica... " Hemos trabajado y seguimos haciéndolo,- - señala José Maris Quintana,director del IM durante trece años,- - en galaxias que tienen una interacción violenta y en galaxias con una interacción dinámicamente muy relajada.
" Pero el trabajo del IM que más impacto ha causado en la comunidad internacional es el llevado a cabo por el grupo de Instrumentación y Cálculo.
Este equipo ha construido una serie de fotómetros que se envían en cohetes sonda a alturas no superiores a los 120 kilómetros y se utilizan para determinar la composición y estructura de la atmósfera.
Los científicos del IM han tenido éxito con estos lanzamientos y han logrado desarrollar un complejo modelo de la atmósfera terrestre que resulta válido para el resto de sus colegas en el mundo.
Al igual que lo son sus modelos de las capas gaseosas de Venus y Marte.
En la actualidad estos estudios los han llevado a Júpiter y esperan llevarlos a Saturno más adelante.
Los observatorios españoles de más solera son los de Fabra,en Barcelona,Santiago de Compostela,Valencia,Lérida y,sobre todo,el de la Marina de San Fernando,en Cádiz.
Este lugar tiene una larga y excelente tradición en astronomía de posición.
Más datos: contamos con una antena de radioastronomía en Cebreros (Avila) y en la misma provincia disponemos de un tiempo de acceso a la antena de Robledo de Chavela,propiedad de la NASA.
Lo mismo ocurre con la antena situada en el pico Veleta,Granada,donde un instituto franco-alemán posee una antena de treinta metros.
Y nos queda en el tintero el observatorio de Yebes,Guadalajara,donde se dispone de dos telescopios y una antena de ondas milimétricas de dos metros y el hispano-alemán de Calar Alto,donde ocurre otro tanto,y nuestros socios ponen los instrumentos y nosotros la infraestructura.
Y por último,cabe hacer mención del observatorio de la ESA en Villafranca del Castillo,Madrid,que tiene la particularidad de que puede usarse tanto de día como de noche,incluso con nubes y con visión para los dos hemisferios.
También participamos en el programa científico de la Agencia destinado a aumentar los conocimientos actuales sobre los procesos físicos que se desarrollan en la materia fría del universo.
Las astrofísica española se vistió de gala con la creación del club internacional de Canarias en 1985.
Los frutos se recogerán la próxima década.
De todas formas,este panorama tan prometedor cuenta con una mancha importante,y es que el actual Plan Nacional de Ciencia y Tecnología no contempla presupuesto alguno para la astrofísica.
¿Curioso? Podría decirse que incomprensible.
El análisis forma parte de un completo estudio al que fueron sometidas las figuras por la Catedra de Conservación y Restauración Artística de la Facultad de Bellas Artes de Sevilla,uno de los centros pioneros a nivel nacional en el empleo de nuevas tecnologías,y en donde se forman los restauradores del siglo XXI,mitad científicos,mitad artistas.
n el aula de restauración,situada en uno de los sótanos de la Facultad,los alumnos se distribuyen en pequeños grupos que trabajan en equipo sobre diferentes obras.
La sala es todo un muestrario de las variadas formas en que el hombre puede expresarse artísticamente: pinturas,esculturas,maderas policromadas,retablos,imágenes religiosas... Todas ellas,desde que salieron de las manos del artista,iniciaron un proceso de deterioro irreversible que,tarde o temprano,las obligaría a someterse a un tratamiento de conservación o restauración.
Este interés por conservar los testimonios del pasado es tan antiguo como la propia capacidad artística humana.
Algunos autores griegos y romanos ya se mostraban preocupados por esta cuestión,y en sus escritos describen los esfuerzos que se realizaban en aquel tiempo por detener procesos como el del deterioro de los materiales empleados en monumentos y estatuas,el mismo mal de la piedra contra el que hoy luchan todos los especialistas.
Sin embargo,la ausencia de una disciplina dedicada específicamente al cuidado de las obras de arte y el consiguiente desconocimiento de técnicas e instrumentos adecuados,originó hasta comienzos de nuestro siglo que,en muchos casos,la conservación o restauración de algunas piezas se hiciera a costa del mensaje original que éstas expresaban.
Fácilmente se aprovechaba la restauración para adaptarse a las ideas religiosas o políticas del momento,o bien el restaurador estimaba más importante su trabajo que el del propio autor y enmascaraba las características originales.
Los alumnos trabajan por igual sobre grandes,superficies que sobre minúsculos detalles en los I que se hace precisa la lupa.
La minuciosidad que requieren muchas de las técnicas de restauración (por ejemplo,la reintegración de la policromía de una talla o una pintura) exige muchas horas de trabajo y así,obras que han iniciado determinados alumnos las concluyen sus compañeros de dos o tres promociones posteriores.
En una de las paredes del aula reposa un retablo de Montañés sobre el que se ha trabajado durante seis años,y cuyo proceso de restauración puede ilustrar lo complejo de este trabajo.
La obra fue destruida en 1936.
Las muestras de cerámica antes de su estudio se conservaban algunos fragmentos muy deteriorados.
Tras una labor casi detectivesca en la que se recorrieron numerosas tiendas de anticuarios,se lograron recuperar algunas piezas más,aunque apenas se alcanzó un 40 por ciento del conjunto.
tomando como referencia una fotografía anterior al daño,ésta se pasó a diapositiva y se proyectó a tamaño real,dibujándose el retablo en su totalidad en un gran tablero.
Sobre este esquema se dispusieron las piezas recuperadas.
Las figuras que se habían perdido fueron moldeadas siguiendo un criterio de síntesis,es decir,apenas esbozando las líneas del original,de forma que se hiciera legible la obra pero estableciendo una diferencia entre los fragmentos auténticos y las partes reconstruidas.
Cinco promociones de alumnos han trabajado en este retablo hasta que,por fin,este año será devuelto al Museo Diocesano,al.
que pertenecía.
/ La labor de todos los equipos está supervisada por el doctor Francisco Arquillo Torres que,tras especializarse en el ICROA (Instituto para la Conservación y Restauración de Obras de Arte) y otras instituciones similares europeas,se hizo cargo de la cátedra en 1976.
" lo primero que el alumno debe aprender " nos explica," es a distinguir entre conservación y restauración.
Es la propia obra la que nos indica el tratamiento a seguir.
A veces es suficiente con que neutralicemos el daño,sin modificar la pieza ni añadir material alguno que no sea el imprescindible para estabilizar el soporte.
En otras ocasiones,las medidas de conservación no son suficientes y hemos de restaurar,siempre fundamentándonos en documentación previa al daño,de forma que no alteremos el mensaje original del artista ".
Cuando una pieza llega a este particular " hospital " se le abre una ficha,en donde habrá de consignarse la información básica que sobre ella se conozca: tipo de obra,autor,fecha de realización,procedencia... A continuación comienza el examen de los daños.
Estos varían dependiendo de la técnica empleada por el artista y de la durabilidad y comportamiento de los materiales utilizados.
" Estos calorímetros ",señala Ferrando," están diseñados en base a un material pasivo,el uranio,y otro activo,el TMP (tetrametilpentano).
El mayor problema que se nos plantea es que si el TMP,que es un líquido,se contamina en unas pocas partes por millón deja de ser activo y se hace inservible.
Sólo puede tener contacto con el acero inoxidable y la cerámica.
Esto hace que la construcción de las cajas que lo contienen sea muy delicada.
Hasta tal extremo que el CIEMAT dedica cuatro investigadores exclusivamente para doblar y perforar los marcos,así como a fabricar los soportes de los módulos que componen estos calorímetros ".
Se montarán cuatro de estos detectores,dos a cada lado del experimento y a diferente distancia del punto de colisión de las partículas.
Estas,al estrellarse contra el uranio - - un material muy denso - -,provocarán una avalancha de nuevas partículas,que mantienen la energía de la partícula que impactó.
La avalancha llegará hasta las placas de TMP,sometidas a un campo eléctrico,donde se producirá la señal.
Las mas de 14 toneladas de uranio empobrecido y las 1.440 cajas de TMP recogerán aproximadamente el 75 por ciento de la llamada energía transversal perdida en la dirección de los haces,incrementando en un Sab de toma de tal experimento NA 36 del aire veinte por ciento la resolución de esta clase de medidas.
Se espera que esto contribuya a comprobar,entre otras cosas,la existencia - - prevista,pero aún no confirmada - - del sexto y último quark,el denominado Top.
" Se trata,en suma ",explica Antonio Ferrandc," de aislar al máximo la energía que se pierde inevitablemente - - constituida esencialmente por los neutrinos,cuya capacidad de penetración les permite atravesar la Tierra impunemente - - y así conocer mejor lo que ocurre en la región central del detector.
Prácticamente todos los detectores de partículas tienen la misma configuración,aunque cada uno de ellos ponga énfasis en un tipo de medida diferente.
Rodeando el tubo por el que viajan los paquetes de partículas se p observa una especie de tambor,de considerables proporciones.
Es la cámara de vértices,que indica por dónde se ha producido exactamente la colisión.
Todo ocurre muy deprisa: el impacto libera una enorme cantidad de energía que,inmediatamente,se e transforma en nuevas partículas.
Algunas tienen una vida muy corta y se desintegran dentro de los Límites de la cámara.
El punto,o vértice,donde se ha producido la desintegración también queda registrado,de ahí la denominación de este aparato.
Envolviendo a las cámaras de vértices actúan los calorímetros electromagnéticos,capaces de advertir la presencia de partículas cargadas o neutras que posean una naturaleza esencialmente electromagnética.
Tal es el caso de los hadrones cargados (partículas hadrónicas son aquellas que están constituidas por quarks),los electrones y los fotones.
Una verdadera nube de datos sobre la dirección,la trayectoria y la energía de estas partículas tiene su origen en estos calorímetros.
Por encima de ellos y abrazándolos,contemplamos los.
calorímetros hadrónicos.
Estos detectan y miden ya la trayectoria y la energía de todas las partículas hadrónicas,cargadas o neutras.
Aquí queda frenado,además,el resto de las partículas que pretenden escapar del detector.
Se estrellarán contra densas capas de materia y allí depositarán su energía.
de conocerse su magnitud y el tiempo transcurrido desde que se produjo la colisión hasta que la señal fue captada.
Esto suministrará información acerca de las características de la partícula que la produjo y de su punto de impacto en el detector.
En estrecha colaboración con Valencia,trabaja el grupo de Física de Altas energías de Santander.
Este,sin embargo,ha orientado sus actividades hacia el desarrollo de sistemas de simulación,diseñando y poniendo a punto el correspondiente software.
Inmersos también en el sueño / realidad de LEP encontramos a los componentes del Grupo de Física de Altas Energías de la Universidad Autónoma de Barcelona.
Desde hace algún tiempo contribuyen al análisis de los datos proporcionados por el experimento NA14 (North Area 14),en el acelerador SPPS del CERN,dedicado a estudiar las interacciones entre fotones y nucleones.
Sin embargo,su actividad principal se centra en el tercer experimento del gran anillo de LEP,llamado ALEPH.
En los laboratorios de Barcelona se construye un monitor de luminosidad,cuya misión será determinar la frecuencia con que chocan las partículas que viajan por el tubo de vacío.
" El monitor no es otra cosa que un calorímetro electromagnético ",explica Enrique Fernández.
" Está formado de varias placas de tungsteno,entre las que se intercalan capas de silicio y de plástico centelleador.
Los electrones y positrones provocan lo que llamamos una cascada electromagnética al pasar por el tungsteno.
Las partículas secundarias que forman esta cascada van a dar lugar a pequeñísimas corrientes eléctricas al atravesar el silicio y a señales luminosas al hacerlo por el plástico centelleador.
" Recientemente el grupo ha incrementado su actividad en torno al diseño y desarrollo de la estación de proceso de datos denominada FALCON,donde se analizarán las señales obtenidas en el experimento.
Se calcula que se necesitará el equivalente a unos cien ordenadores Vax / 780 funcionando constantemente durante los siete años que dure el experimento para transformar las señales que salen del detector en datos útiles.
Semejante capacidad informática,tecnológica y,sobre todo,humana no está dedicada sino a responder cuestiones tan básicas como ¿quiénes somos?,¿de dónde venimos?,¿hacia dónde vamos? Estudiando la materia interpretamos también el origen de la energía,de acuerdo con la ecuación de Einstein E = mc2.
Si el hombre logra unificar mediante estas investigaciones todas las fuerzas de la naturaleza,si demuestra que tienen un origen común,asistiremos al descubrimiento de ese Dios que tanto nos preocupa.
De esa misma fuente habría surgido también la vida.
Normalmente los cartílagos sobre los que crece el hueso se cierran hacia esta edad,determinando la altura definitiva.
Ya no se puede crecer más,y Susana queda condenada a vivir el resto de su vida como una enana en un mundo de gigantes.
Si la vida de Susana se hubiera proyectado como una película a cámara rápida,el espectador habría descubierto que Susana crecía a un ritmo desesperadamente lento,pero al mismo tiempo su cuerpo conservaba las mismas proporciones con respecto a sus demás amigas.
Entretanto,debía de haber experimentado una sensación no muy diferente de aquella que acosaba a William Grant,protagonista del film fantástico El hombre menguante,en la que se iba encogiendo,mientras el mundo crecía y se hacia más grande.
La explicación de lo que le ocurre a Susana se encuentra no muy lejos de donde vive,en un laboratorio.
Allí,un grupo de científicos trabaja en compartimentos aislados.
En uno de ellos las precauciones se extreman.
La entrada está prohibida.
Y tan sólo desde un cristal hermético contiguo,el visitante,equipado con ropa esterilizada y con los pies enfundados en bolsas de plástico,puede satisfacer su curiosidad.
Al otro lado,un científico enseña un recipiente que contiene una pasta parduzca,parecida a arena fina y oscura.
De una cámara frigorífica saca unas pequeñas bolsitas etiquetadas.
Cada una contiene la hipófisis de un cadáver,una glándula que tiene el tamaño de un garbanzo y se aloja en la base del cerebro.
Antes de llegar hasta aquí,los forenses han confirmado que la glándula proviene de un individuo sano.
Cualquier hipófisis con un historial clínico dudoso es inmediatamente desechada.
La pasta oscura extraída de las bolsitas se somete a sofisticados métodos de aislamiento y purificación.
Transcurrida una semana,todo lo que queda son unos pocos miligramos de un polvo fino.
Se trata de una proteína de 191 aminoácidos que hace que los órganos,tejidos y huesos crezcan a la velocidad adecuada ; es la hormona de crecimiento (GH).
Si se hubiera tratado a Susana con esta hormona cuando tenía tan sólo ocho años,su crecimiento habría sido normal.
El defecto se encontraba en su hipófisis que,constituyendo la fábrica natural de la GH,era incapaz de secretarla en los niveles necesarios: un mal conocido como enanismo hipofisiario.
Hasta hace pocos años,la única manera de obtener la preciosa hormona era a partir de las hipófisis congeladas de los cadáveres recién muertos.
La pequeña Susie habría requerido casi trescientos cadáveres para obtener la cantidad suficiente de hormona que garantizase un largo y continuado tratamiento.
a realidad es que no muere el número necesario de personas para hacer frente a la demanda de GH necesaria.
" La única fuente de GH hasta hace pocos años era la propia fábrica humana,la hipófisis - - dice Francisco Rubio,director de Serono España,una empresa multinacional líder en el campo de investigación,purificación y obtención de productos biológicos - -.
Ahora la ingeniería genética diseña organismos y transforma células,obligándolas a producir cantidades industriales de GH que abren un horizonte de esperanza para tratar ciertos tipos de enanismo.
Algo que todavía no ha llegado a nuestro país,pero que ha permitido disponer de cantidades suficientes de GH para investigar cómo funciona esta hormona en nuestro cuerpo.
" El proceso de crecer es enormemente complejo y se asemeja a un mecanismo de relojería,en el que unas piezas mueven a otras,activándose o desactivándose entre sí.
Todo nuestro cuerpo crece en una armonía perfecta.
Las señales que se envían de todas las partes del organismo llegan al cerebro.
Cuando hay que crecer,procesa una señal química,una especie de telegrama urgente enviado por las células nerviosas del hipotálamo a la hipófisis.
El mensaje es una pequeña proteína conocida como GRF factor estimulador de la hormona de crecimiento).
La hipófisis comienza a fabricar la hormona.
Al liberarse en la sangre,viaja hacia el hígado,donde delega la función de hacer crecer los huesos y algunos tejidos en otras proteínas.
Aunque también se ha observado que puede actuar directamente sobre las células del hueso,estimulando su división.
El GRF ha abierto una nueva vía de ataque para solucionar los problemas de crecimiento.
En muchos casos de enanismo,la anomalía reside en que el hipotálamo no es capaz de producir la cantidad necesaria de este estimulador.
" Suponiendo que la hipófisis funciona correctamente,el fallo generalmente se encuentra en el hipotálamo entre un 60 y 90 por ciento de los casos ",confirma F. Rubio.
En vez de administrar hormona GH,puede estimularse directamente a la hipófisis para que la fabrique mediante tratamiento con GRF.
Al ser una proteína pequeña,puede sintetizarse por los métodos químicos.
Su obtención no precisa de la ayuda de la ingeniería genética.
El GRF es un factor determinante que se encuentra en una posición privilegiada,al encontrar se como factor iniciador de una serie de complejas reacciones en cascada,cuyo resultado final es el propio crecimiento.
Su producción a gran escala se ha convertido en uno de los proyectos más ambiciosos de la empresa Secono España.
" cuanto más abajo te mueves en un mecanismo de cascada - - comenta Alberto Molina,refiriéndose al modelo endocrino de la GH - -,más mecanismos de control estás desconectando.
" La cantidad de GH que fabrica la hipófisis parece estar mediada por el GRF y constituye un reflejo de la cantidad de hormona que requiere el cuerpo.
Cada uno de nosotros estamos genéticamente programados para tener una altura determinada.
Que la alcancemos o no,depende de multitud de factores.
Los estudios que se han realizado en poblaciones infantiles muestran claramente la importancia de la nutrición en el crecimiento.
En las zonas más pobres,y por tanto con una alimentación de baja calidad,los niños son más bajos y crecen peor.
Un reciente trabajo realizado en colegios de la Comunidad Autónoma de Madrid dio como resultado que,para la mayoría de los grupos,la talla media es mayor en los niños que pertenecen a los estratos sociales altos y acomodados que en los de clases de economía débil.
Esta situación ficticia puede hacerse realidad en sólo una década.
Los avances en nuevos materiales son constantes y su desarrollo les dotará de sofisticadas propiedades todavía inimaginables.
Actualmente,la carrera ha comenzado: Metales,cerámicas y plásticos compiten por el puesto de honor en las aplicaciones que los actuales avances técnicos demandan.
Las llamadas cerámicas tenaces,técnicas o avanzadas son las que más expectativas generan.
La nueva Edad de Piedra está a la vuelta de la esquina.
Antes,los hombres prehistóricos utilizaban el sílex para construir sus armas.
Hoy,en asépticas fábricas se elaboran materiales cerámicos que viajan fuera de nuestro planeta como componentes de las naves espaciales,o formando parte de los circuitos electrónicos de los satélites.
La única empresa en España que ha emprendido la fabricación de cerámicas avanzadas es Ceratén.
Situada en un polígono industrial de Getafe,en las afueras de Madrid,sus instalaciones están ya - - casi listas para elaborar productos hechos con cerámica tenaz.
Por ahora en su interior solamente se pueden ver cuchillos o crisoles de distintos tamaños destinados a la industria química,con los que se esta experimentando.
Para su director,Fernando Maldonado,la puesta en marcha de nuevas tecnologías no será de hoy para mañana.
" Al principio nos dedicaremos a fabricar cerámicas oxídicas o blancas,cuyas materias primas son la alúmina de alta pureza,el óxido de circonio y el compuesto mullita circona que tendrán aplicaciones prácticas en diversos componentes de la industria metalúrgica,electrónica y herramientas.
" Los nuevos materiales cerámicos,a base de alúmina,circonio,mullita o carburo de silicio,no son recientes.
Son tan viejos como este planeta,pero su forma actual de procesarlos y el uso que les comienza a dar,pertenecen ya al siglo XXI.
Probablemente,la aplicación más llamativa de estas cerámicas tenaces está en el desarrollo de un motor de este material.
La cámara de combustión ideal de un coche será aquella que pueda trabajar a altas temperaturas de combustión sin necesidad de refrigerarse.
Las aleaciones metálicas de los actuales motores no pueden hacerlo.
Sin embargo,esto será posible cuando se desarrollen los futuros motores cerámicos.
En Estados Unidos ya existe un prototipo diesel que ha demostrado una reducción de hasta un 30 por ciento en el gasto de combustible comparado con un motor diesel convencional,y es más fiable al tener menos averías: la mitad de los fallos en un motor diesel se producen por problemas en el circuito de refrigeración.
También las turbinas de gas cerámicas,con muchas menos piezas móviles que un motor diesel,podrían trabajar a temperaturas de 1.200 y 1.500 grados,gastando menos combustible que las convencionales.
En Ceratén existe la idea de fabricar en el futuro un motor cerámico.
Para ello,han contactado con Pegaso y Derbi,e investigarán conjuntamente con la Escuela Superior de Ingenieros Industriales,centro que puede servir como lugar de ensayos.
Pero esto está todavía muy lejos.
Fernando Maldonado opina que todo el proceso de fabricación de una cerámica tenaz requiere gran precisión y aún no todas las técnicas están dominadas,como ocurre en los metales ; un número muy elevado de piezas salen defectuosas.
Todavía nadie ha conseguido fabricar motores cerámicos en serie,pero por las carreteras del Japón ya circulan deportivos como el Nissan 300 ZX con turbocompresores cerámicos,y la firma Isuzu comercializa motores diesel con cámaras de combustión y bujías de material cerámico,aunque todos ellos con fines experimentales.
a resistencia a las altas temperaturas,una de las características más apreciadas de las cerámicas,fue lo que hizo que el transbordador espacial americano las utilizara para recubrir su fuselaje y protegerlo del calor que se genera a su reentrada en la atmósfera.
Las bazas de las cerámicas avanzadas son,en primer lugar,su dureza muy superior a la del acero,que las hace resistentes a la abrasión y prácticamente imposibles de rayar.
En segundo lugar,son inoxidables y resisten los agentes corrosivos más habituales.
Después tenemos su resistencia a altas temperaturas y su excelente comportamiento como aislantes eléctricos y térmicos.
Aquellos siStemas CAD podían dibujar básicamente líneas rectas y arcos de curva,pero figuras más complicadas solían salirse del marco de posibilidades.
Además,los diseñadores tenían que esperar bastante antes de que el ordenador reaccionara a sus instrucciones ; simplemente necesitaba un tiempo de proceso demasiado largo.
Hoy,ningún usuario de sistemas CAD está dispuesto a esperar más de 0,4 segundos para obtener una respuesta a SUS órdenes.
En pocos años la velocidad de proceso se ha reducido a una décima y hasta una centésima parte.
Al mismo tiempo,los ordenadores están en condiciones de calcular y dibujar cualquier forma por complicada que sea,desde una aleta de coche al timón de cola de un avión.
A pesar de la espectacularidad de aquellos sistemas,un día me enseñaron un equipo que de golpe hizo que todo lo visto anteriormente me pareciera aburrido.
Durante una visita a una gran empresa farmacéutica suiza pude observar cómo un ordenador representaba espacialmente y a todo color una molécula de proteína,con todas sus miles de ramificaciones.
Para mayor asombro de mis ojos,la imagen se podía manipular a voluntad,volando a su alrededor,introduciéndonos en su complicada estructura,ampliándola o reduciéndola... Pero,¿a qué se debe el que los gráficos por ordenador,como se conoce genéricamente esta técnica,hayan alcanzado tan rápidamente la mayoría de edad? Fundamentalmente a tres factores: - En primer lugar,a que ya existen monitores en color de muy buena calidad,capaces de representar hasta 4.096 por 3.076 puntos (altísima resolución),y ello con hasta 16 millones de tonalidades cromáticas distintas.
- En segundo lugar,a los potentísimos chips de memoria desarrollados últimamente.
Para dibujar un objeto con la máxima fidelidad y poder manipularlo a voluntad es necesario representarlo a base de millones de puntos aislados,cada uno con su propio color,brillo e intensidad (como las reproducciones que aparecen en los periódicos).
Y para ello hace falta disponer de una increíble capacidad de memoria.
Y en tercer lugar,gracias a la vertiginosa velocidad de proceso de los actuales mi croprocesadores.
Esto ha permitido crear imágenes animadas,es decir,películas,de una espectacularidad sorprendente.
Veamos cuál es el proceso para crear una de estas películas informáticas.
Las células atacadas,atravesando la piel para extender sus efectos a otras zonas del organismo a través de la sangre (afectan además a la vista y disminuyen el nivel de hemoglobina,ocasionan derrames en los riñones,intestinos y cerebro,así como la muerte por asfixia al obstruir la tráquea o los bronquios por inflamación,hemorragia o neumonía gangrenosa.
La curación total es imposible) ; tóxicos,sin interés militar,como el ácido cianhídrico o el monóxido de carbono,ya que no producen lesiones sino que actúan sobre el sistema circulatorio y nervioso vegetativo,inhibiendo sus funciones,pero de los que no es posible obtener concentraciones eficaces al aire libre (a este grupo pertenece el Zyclon-B utilizado por los nazis para eliminar a millones de personas en los campos de exterminio,por lo que puede.
considerarse que es el producto químico causante de más muertes en toda la historia de los gases) ; finalmente,los gases letales,nerviosos,neurotóxicos y psicoquímicos constituyen el grupo más importante del arsenal químico de nuestros días.
Descubiertos durante la Segunda Guerra Mundial,son los llamados gases del futuro.
Nos ocuparemos con más detalle de éstos últimos.
Su descubrimiento proviene,paradójicamente,de las investigaciones que se llevaban a cabo en la Alemania de anteguerra en busca de insecticidas eficaces conocidos como difusibles,capaces de eliminar toda clase de insectos y parásitos mediante transmisión,a través de la sangre de los animales afectados o de la savia de las plantas atacadas,del tóxico insecticida.
El primero que se consiguió sintetizar fue el tabun,óxido de cianodimetilamonatosfN,cuyo uso militar fue propuesto al gobierno alemán por el fisiólogo Wirth.
Se trata de un líquido aceitoso que se dispersaba mediante la explosión del proyectil que lo alojaba.
El siguiente en aparecer sería el sarin,monoisopropilmetilfluN,un líquido incoloro cuatro veces más tóxico que el tabun y descubierto también por el doctor Schrader en los mismos laboratorios de la fábrica de insecticidas IG Farben.
El soman,monopinacolmetilfluroN,menos volátil que su predecesor pero aún más tóxico,fue el último de los descubrimientos alemanes en el grupo de los llamados gases nerviosos,incorporado esta vez al arsenal gracias al doctor Kuhn.
Los tres gases pasaron a llamarse respectivamente GA,GB y GD en la jerga militar.
Los gases nerviosos bloquean el enzima acetilcolinesterasa,lo que provoca una rápida acumulación de acetilcolina,la sustancia transmisora sináptica (es decir,la que establece las conexiones entre neuronas o entre éstas y los órganos),impidiendo la comunicación nerviosa.
Las consecuencias son terribles: sudoración copiosa,mucosidad abundantísima en las vías respiratorias,estrangulamiento bronquial,ceguera,vómitos y diarrea incontrolable,convulsiones,parálisis y finalmente la muerte por fallo respiratorio.
Y todo ello en el espacio de pocos minutos cuando la dosis letal (tres cuartos de miligramo para el sarin) se ha recibido por las vías respiratorias.
Si se cuenta con la protección de una máscara de gas,el aerosol penetrará a través de la piel y en ese caso la muerte puede tardar varias horas.
Ya en la posguerra,los científicos de los laboratorios británicos ICI descubrieron el agente VX.
Durante ese tiempo que el alcohol actúa en el cuerpo,se produce una auténtica metamorfosis,pues influye en todas las células del organismo.
El protoplasma de las células está compuesto de albuminoides y,como el alcohol sustrae agua a las células,éstos forman coágulos.
Este efecto devastador se puede apreciar perfectamente en las plantas: si las regamos con agua que sólo contenga un uno por ciento de alcohol,advertiremos rápidamente que las plantas perecen.
Algunos neurólogos calculan que medio litro de vino o de cerveza afectan ya a varios miles de células cerebrales,que se resienten químicamente.
Si tenemos en cuenta que el hombre tiene entre diez y cincuenta mil millones de estas células,la cifra parece ínfima,salvo que ese efecto se produzca diariamente.
Después de la sangre,es el sistema nervioso central - cerebro y médula espinal - que absorbe la mayoría del alcohol a través de dos vías: la primera es el líquido cefalorraquídeo,pues el alcohol se mezcla de inmediato con los líquidos del cuerpo ; la segunda vía es la sangre,de la que fluyen por el cerebro unos mil litros en doce horas (doscientas veces la cantidad total de sangre del cuerpo).
Veamos primero cuáles son los síntomas externos de esa invasión interior.
Todo empieza con un estímulo agradable,tras unos pocos tragos ; la alegría y excitación ya aparecen con una cantidad que va de los 250 a los 700 miligramos de alcohol contenidos en cinco centímetros cúbicos de vino.
El bebedor se siente animado,relajado,lleno de vida ; crecen sus ganas de hablar y se ríe más alto de lo habitual.
Esta alegría es ya un ligero entumecimiento cerebral,y lo primero que se perturba es la conciencia,es decir,lo último que el ser humano ha desarrollado en su evolución.
Disminuye el miedo,caen las inhibiciones,y afloran los impulsos que el hombre suele reprimir por educación.
Este Superyo,como llamaba Freud a las normas sociales y morales interiorizadas por el individuo,pierde su poder tras varios cuartos de litro.
El hombre se siente libre.
Por otra parte,con pequeñas cantidades de alcohol aumenta la fuerza bruta,porque disminuye la sensación de cansancio durante la fase de excitación.
Sin embargo,baja la capacidad de realizar tareas minuciosas,como enhebrar una aguja.
La segunda fase es un ligero embotamiento sensorial en el que cae la atención y la concentración,pero aumentan la euforia y la vanidad.
El centro de la visión está aturdido,porque los estímulos ya no funcionan,y se ve como a través de unos prismáticos mal enfocados.
El habla pasa a convertirse en un balbuceo incomprensible,porque el centro del lenguaje está bloqueado,aunque el alcohol haga creer que los sentidos están mas agudizados que antes.
No es que aumente el rendimiento,sino que disminuye la crítica.
El cerebro trabaja más despacio,y la orden ante la reacción llega retardada.
Para algunos médicos,con dos whiskys ya se produce cierta inseguridad,y con cuatro whiskys el peligro de tener un accidente es dieciséis veces superior que en estado sobrio.
Con litro y medio de vino es como si hubiéramos programado el accidente.
Pero si aún continuamos bebiendo,el cerebelo se paraliza,se perturba el sentido del equilibrio,y los músculos están con diferentes tensiones.
En consecuencia,el bebedor se tambalea.
Acto seguido,la afectada es la médula espinal: los reflejos se hacen más lentos hasta que desaparecen por completo.
La distancia de este horizonte depende únicamente de la cantidad de materia que forma su masa.
Un agujero negro con una masa similar a la de nuestro Sol tendría,por ejemplo,un diámetro de seis kilómetros ; mientras que,con una masa parecida a la de nuestra Tierra,tendría sólo uno de dos centímetros,es decir,más pequeño incluso que una pelota de golf.
Como gargantas insaciables,los agujeros negros van devorando todo aquello que rota en sus proximidades,alimentando su masa lentamente.
Cuando más devora,más se agranda ; cuanto mayor es su tamaño,tanto más puede devorar.
De todas formas esto no significa que los agujeros negros actúen en el universo como aspiradoras,absorbiendo materia de todas direcciones.
Un rayo de luz,una estrella,una nave espacial,cualquier materia o partícula cósmica que deambule por el espacio puede aproximarse sin correr peligro.
Eso sí,cuando se acerca demasiado y traspasa las fronteras que limitan ese horizonte de influencia con intención de penetrarlo caen y quedan irremisiblemente atrapados en sus redes gravitatorias,desapareciendo para siempre.
Algunos astrofísicos están convencidos de que si el agujero negro se hallase en el centro de una gran galaxia,se encontraría en un terreno de privilegio para saciar más su apetito: atraparía todo tipo de gases,materia estelar y objetos,creciendo hasta convertirse en agujeros gigantes constituidos por millones de cuerpos celestes.
Pero ¿cómo se generan los superagujeros negros? Aunque no han podido ser demostradas todavía el astrofísico norteamericano Robert Jastrow propone en la actualidad dos explicaciones teóricas: en la primera de sus hipótesis presupone que los agujeros negros se originan espontáneamente a causa de un colapso en la gravitación de una nube de gas localizada en la densidad del centro de una galaxia.
La segunda se basa en que las estrellas,al hallarse muy cercanas unas de otras en el interior galáctico,chocarían con frecuencia fundiéndose.
De ese modo resultaría un número relativamente bajo de estrellas de masa muy elevada,cuya evolución acabaría convirtiéndolas finalmente en agujero negro.
Si estos agujeros se encontrasen,el mayor devoraría al más pequeño,agrandándose progresivamente hasta convertirse en un superagujero negro.
Así pues,hasta la década de los setenta los teóricos pensaban que el único campo gravitatorio suficientemente poderoso como para formar un agujero negro sería el producido por una estrella en contracción con una masa diez o quince veces superior a la de nuestro Sol.
No parece probable,en cambio,que los mini-agujeros negros de masa pequeña se generen por una implosión causada por la influencia de la gravitación.
Un planeta como la Tierra,por ejemplo,no podría convertirse jamás en un agujero negro,ya que su gravitación no es tan potente como para superar la presión de la materia.
Por el contrario sí sería posible que en el universo inicial se hubieran originado agujeros negros de distinto tamaño.
Esta teoría fue desarrollada a partir de 1971 por el famoso físico Stephen Hawking de la universidad de Cambridge.
El científico,a pesar de verse condenado a permanecer inmóvil en una silla de ruedas a causa de una enfermedad neurológica,es considerado hoy como una de las mentes más brillantes de la astrofísica actual.
Sus razonamientos se basan en el hecho de que sólo en el momento del BigBang se produjo una concentración de toda la materia universal en el mismo lugar,al mismo tiempo y con una presión suficiente como para generar pequeños agujeros negros.
En este caso,éstos no se habrían producido por una concentración progresiva de masa,sino como fragmentos de la mate! la que componía el huevo cósmico,aquel cuerpo que concentraba toda la masa del universo antes del estallido inicial.
Para Hawking estos agujeros negros podrían tener masas inferiores a las de una estrella ; algunos el tamaño de un átomo con una masa 102 ° gramos - esto es,cien billones de toneladas - ; otros incluso el tamaño de un protón con una masa de mil millones de toneladas.
El físico británico piensa hoy que estos diminutos aunque poderosos puntos negros existen por miles de millones en el universo.
De ser válida la teoría desarrollada por Stephen Hawking,entonces también resultaría posible la hipótesis de que un miniagujero negro con una masa de cien billones de toneladas pero el tamaño de un átomo,hubiera atravesado la superficie terrestre en Tunguska sin dejar más huella que un orificio imperceptible para el ojo humano.
Sin embargo,teniendo en cuenta que con la esfera no se puede alcanzar suficiente velocidad bajo el agua y que es necesario un grupo generador y un montaje accesorio,se llegó a la determinación de disponer éstos en la parte trasera del vehículo,dándole así una forma aerodinámica,mientras que la parte delantera seguía conservando el cuerpo esférico.
Pero aún quedaba por solucionar otro importante problema.
Cuanto más descienda el sumergible,más gruesa ha de ser la envoltura resistente a la presión,hasta llegar a un punto en-que pese tanto que el vehículo pierda flotabilidad y se hunda.
Los ingenieros llegaron a la conclusión de que el acero empleado en la técnica convencional pesaba demasiado.
Había que buscar una alternativa.
Al fin encontraron un metal estable,relativamente ligero e incluso más resistente a la corrosión que el acero inoxidable: el titanio.
De este material se construyó la cámara de presión para los tripulantes del Nautile.
Con todo lo orgullosos que pueden sentirse los técnicos franceses con su trabajo,no han logrado sortear una desventaja bastante grave: para ahorrar peso,el Nautile no puede llevar más que unas pocas baterías a bordo,viéndose obligado a funcionar con una potencia de 50 kilowatios hora (kw / h),lo que se traduce en una autonomía de tan sólo trece horas.
Permanencia o profundidad,ésta es la decisión que deben tomar los constructores,pues las dos cosas no se pueden obtener al mismo tiempo.
Mientras los padres del Nautile se decidieron a favor de la profundidad,ingenieros alemanes diseñaron un sumergible que puede permanecer bajo el agua un período extraordinariamente largo: una semana,e incluso,en caso de urgencia,hasta dos semanas.
Para alcanzar este rendimiento,la firma alemana Bruker,de Karlsruhe,adaptó a su Seahorse 11 un paquete de baterías cuyo peso es equiparable al del Nautile (18,5 toneladas) y que suministra una potencia de 400 kw / h. Gracias a esta reserva de energía,el Seahorse 11 puede sumergirse,con una tripulación de cuatro a seis personas,sin necesidad de ninguna nave nodriza.
La contrapartida es la relativamente poca profundidad que alcanza: 450 metros.
Como la construcción de sumergibles tripulados suele salir muy cara por las estrictas medidas de seguridad que conllevan,muchos trabajos se confían a submarinos-robots.
Pero el dilema " permanencia / profundidad " sigue persiguiendo a los técnicos,por ejemplo en el caso del vehículo submarino francés Epaulard.
Si alguien se acerca al borde de un agujero negro,observará que el tiempo va transcurriendo cada vez más lentamente,hasta que se detiene por completo.
Lo explicaremos mejor: si pudiéramos observar desde lejos el interior de la zona de influencia de un agujero negro,un reloj situado en dicha zona daría la impresión de estar parado.
Pero un observador que penetrara en el agujero negro vería que las agujas de los relojes situados fuera del agujero negro giraban a velocidades fantásticas.
Un elemento muy apreciado en las novelas de ciencia-ficción es el viaje por el tiempo,pero ¿es una idea realista? La teoría de la relatividad permite,desde luego,viajes por el tiempo.
Aprovechando sus efectos de dilatación,se podría efectivamente viajar al futuro.
Más o menos según el efecto antes mencionado,un viaje de un año del astronauta le lleva diez años terrestres hacia adelante,o sea,le adelanta nueve años hacia el futuro.
Si se llegara a alcanzar una velocidad próxima a la de la luz,se podría realizar esta excursión al futuro casi sin limitaciones.
Pero hay un inconveniente: nuestro viajero por el tiempo no podría regresar.
Sólo son imaginables viajes hacia adelante.
La teoría de la relatividad no excluye expresamente los viajes al pasado,pero lo disparatado de semejante idea no se puede rebatir ni siquiera con las más atrevidas teorías.
Prescindiendo de los problemas técnicos,una vuelta por el pasado nos llevaría a una maraña de sucesos y situaciones paradójicas.
Supongamos que un viajero por el tiempo encuentra a su propio yo más joven,y lo mata.
¿Estaría entonces muerto el viajero? ¿Cómo habría podido emprender este viaje,si ya estaba muerto antes de comenzarlo? Esta imposibilidad de volver hacia atrás en el tiempo nos plantea otra cuestión: ¿Tuvo un comienzo? Si el tiempo es un componente de la creación física,se podría suponer que la explosión que formó el universo fue también el comienzo y el origen del tiempo.
Para averiguar esto,los cosmólogos han utilizado la teoría de la relatividad de Einstein y han descubierto que,efectivamente,con la explosión original,el BigBang,se produjo un comienzo repentino del tiempo.
No existe ningún indicio de que hubiera podido existir el tiempo antes del BigBang.
Simplemente,no existió ningún antes.
Si el tiempo tiene un comienzo,¿puede tener entonces un fin? Actualmente,los científicos no se ponen de acuerdo sobre si la expansión cósmica continuará eternamente o no.
Pero si hay en el espacio suficiente materia dispuesta cuya gravitación sea lo bastante fuerte,podría ocurrir que el cosmos detuviera su movimiento dentro de algunos millones de años antes de empezar a contraerse.
Este proceso de contracción se aceleraría entonces,hasta el desmoronamiento catastrófico total en el que todo quedaría comprimido hasta la singularidad.
Esta compresión sería la inversión del BigBang.
Con ella se terminaría el tiempo.
Entonces,el BigBang habría sido el comienzo del tiempo y el derrumbamiento,el final del tiempo.
No habría ningún después.
Un aspecto del tiempo que consideramos natural y real es que avanza ininterrumpidamente.
Los físicos no han encontrado todavía ningún motivo para poner fin a esta suposición.
No se ha descubierto que el tiempo avance a saltos o a empujones.
Por lo tanto,no existe ningún motivo para dudar de que el tiempo transcurre incesantemente y puede ser dividido de forma ilimitada,de modo que podríamos marcar en un cronómetro una cantidad infinita de rayitas divisoras.
Pero los físicos también creyeron una vez que la energía puede dividirse sin limite,antes de que la teoría de los cuantos demostrara que en su zona más baja rompe las reglas del juego y resulta cuantificada,en forma de porciones mensurables.
¿Podría ocurrir que el tiempo también estuviera cuarteado en su zona inferior? Si existen los cuantos del tiempo - podríamos denominarlos cronones - la mejor forma de imaginarnos el tiempo es como una película: durante la proyección,la película causa la impresión de desarrollarse de modo continuo,y con toda rapidez,pero si disminuimos la velocidad del proyector,podremos comprender que este transcurso fluido de imágenes es en realidad una sucesión rápida de cuadros individuales.
Aparte de estos tres tipos de radiactividad,existen otras clases de radiaciones,como los rayos X,la emisión de neutrones y la de protones.
¿Cuál es el efecto de la radiación nuclear en las personas expuestas a ella? Debemos imaginarnos las partículas y rayos gamma como microscópicos proyectiles altamente energéticos lanzados a increíble velocidad.
Cuando penetran en nuestro cuerpo colisionan violentamente contra todo tipo de células,provocando cambios en su estructura molecular.
Las lesiones concretas que producen dependen de distintos factores: el tipo de célula que se ve alcanzada y,dentro de ella,la molécula afectada,la radiosensibilidad de la célula - que depende de la fase de división en que se encuentre -,la intensidad de la radiación y el tiempo de exposición.
Las partículas alfa,grandes y pesadas,son más propensas a chocar contra las moléculas,por lo que tienen poco poder de penetración: las continuas colisiones anulan rápidamente su velocidad.
Gracias a esto,cuando recibimos radiación alfa externamente,las capas superiores a la piel absorben la energía sin que llegue a afectar a órganos importantes.
En cambio,si tenemos la mala suerte de incorporar a nuestro organismo estas partículas - ingiriendo alimentos contaminados,respirando,o a través de heridas abiertas -,los proyectiles chocarán directamente contra las células de órganos internos o de la sangre.
Como consecuencia de estas colisiones,las moléculas absorben la energía transportada por las partículas,induciendo la ionización de sus estructuras ; esto es,expulsan un electrón y se cargan positivamente.
Ello se traduce en cambios nocivos para la célula afectada,que pueden ser la inhibición de la división celular,inoperativa funcional o transformaciones estructurales.
Si se tiene en cuenta que una partícula alfa sufre unas 100.000 colisiones antes de detener su actividad y que cientos de partículas pueden impactar en un órgano,es posible hacerse una idea de las severas lesiones que es capaz de provocar la radiactividad.
Las partículas beta actúan de idéntica manera,sólo que las colisiones son menos frecuentes,al ser de menor tamaño,y por lo tanto las moléculas afectadas se encuentran más distantes entre sí.
Su poder de penetración es superior al de las partículas alfa (unos centímetros de tejido corporal).
Los rayos gamma,finalmente,desprenden electrones de los átomos con que colisionan,los cuales,a su vez,actúan como partículas beta.
El proceso es análogo con la radiación X,de protones y de neutrones.
Los daños que la radiación puede causar en el organismo vienen determinados por las moléculas que se hayan visto afectadas por las colisiones.
Cuando una partícula o rayo gamma penetra en los tejidos puede im ; pactar,o bien contra las moléculas celulares con el ya mencionado efecto de ionización,o bien contra las moléculas del medio en que las células se encuentran suspendidas,el cual está constituido en un setenta por ciento de agua.
La acción de la radiación sobre el líquido elemento da lugar a un par de iones H + y H -,más dos radicales libres H° y OH°.
Estos últimos intervienen activamente en los mecanismos de química celular,originando reacciones,uniones e intercambios con otros radicales o moléculas celulares causantes de lesiones.
Con independencia de que otras moléculas y orgánulos de una célula sufran alteraciones a consecuencia de la radiación ionizante,también puede ocurrir que una o varias partículas colisionen contra una cadena ADN (ácido desoxirribonucléico),la molécula más importante de una célula,puesto que contiene toda su información genética.
Se trata de una macromolécula filiforme compuesta por dos filamentos helicoidales enroscados entre si,a lo largo de los cuales se encuentran dispuestas ciertas partículas moleculares,llamadas bases.
La combinación de estas bases determina la información hereditaria contenida en el ADN.
Con cada división celular los filamentos quedan escindidos en dos,pasando cada uno de ellos a los núcleos de las células resultantes.
Cuando una partícula energética colisiona contra una de estas cadenas puede suceder que se rompan dos bases emparejadas,hablándose entonces de doble rotura de filamento,en cuyo caso la célula queda incapacitada para reproducirse y termina muriendo.
Debido a ello tampoco tiene mucho sentido establecer que una determinada dosis máxima de radiación es inocua para los seres humanos.
Estos baremos sólo indican que la incidencia de la radiactividad es significativa en el conjunto de la población (cuando por ejemplo aumentan los casos de cáncer) a partir de ciertos niveles.
Síntomas claros de enfermedad por radiotoxemia únicamente se conocen gracias a las dolorosas experiencias de Hiroshima y Nagasaki,y más recientemente por los accidentes de Sellafield y Chernobil: durante la primera y parte de la segunda semana transcurre una fase de trastornos no característicos,comunes a muchas enfermedades y sobre todo las infecciosas,como falta de apetito,debilidad,excitabilidad,dolores de cabeza,náuseas y vómitos.
Ligeros aumentos de la temperatura hacen aparición acto seguido.
Viene entonces la fase de síntomas característicos de enfermedad por radiotoxemia,con fiebre,úlceras en boca y garganta,diarreas sanguinolentas,hemorragias internas,así como - si el paciente ha sobrevivido las dos primeras semanas - pérdida del cabello.
Según la intensidad de la radiación y su localización (no es lo mismo una exposición a cuerpo entero que en una sola zona),el enfermo puede llegar a morir en el plazo de unas horas a varias semanas.
Y en cualquier caso,si no sobreviene el fallecimiento en los meses siguientes y el paciente logra recuperarse,sus expectativas de vida habrán quedado sensiblemente reducidas.
La masiva destrucción de células se manifiesta especialmente perniciosa en el intestino delgado y la médula ósea.
Las paredes intestinales se vuelven rápidamente permeables al no ser abastecidas constantemente por nuevas células ; al cabo de un tiempo se rompen por numerosos puntos con el resultado ya descrito.
El daño en la médula ósea es más grave aún,pues aquí es donde se renuevan las células de la sangre.
Cuando la médula interrumpe su trabajo a causa de la destrucción de sus células,acaban por desaparecer los glóbulos blancos,los soldados del sistema inmunológico que nos protege contra las infecciones.
Por último también mueren los trombocitos,responsables de la coagulación de la sangre,por lo que cualquier hemorragia resulta fatal.
Sin embargo,la medicina ha encontrado un remedio desesperado para los enfermos graves de radiotoxemia: el trasplante de médula ósea,una delicada operación que también se practica en casos de leucemia o cáncer de sangre.
Por fortuna,esta espantosa enfermedad aparece únicamente con dosis de radiactividad extremadamente altas,a las que seguramente jamás nos veremos expuestos.
Lo que realmente nos importa e interesa es conocer los niveles de radiación normales a los que nos vemos sometidos cotidianamente y saber si pueden ser perjudiciales para nuestra salud.
Desde siempre hemos sufrido una cierta radiación natural o de fondo,la cual proviene de diversas fuentes: el espacio exterior,la corteza terrestre,los materiales de construcción de nuestras casas y hasta nuestro propio organismo.
Es precisamente esta constante presión radiactiva la que nos ha obligado a desarrollar a lo largo de miles de años mecanismos de reparación celular.
A la radiación natural hay que añadir desde hace poco una carga adicional de radiactividad que recibimos por tratamientos médicos (exploraciones radiológicas y radioterapia),precipitación de partículas provenientes de pruebas nucleares en la atmósfera en los años cincuenta,aceleradores de partículas para experimentos científicos,escapes de centrales nucleares y hasta de las esferas luminosas de los relojes.
De este grupo la mayor parte corresponde a la medicina nuclear.
Expresado en porcentajes,del total de radiactividad soportada,un sesenta por ciento es de origen natural y un cuarenta por ciento de origen artificial.
Si esta explicación no fuera suficiente,existe otra que nos permitirá entenderlo sin por ello vernos obligados a abandonar nuestro puerto de mar.
Subamos ahora al barco dueño del mástil en el que antes nos hemos fijado,levemos anclas y naveguemos fuera de puerto,alejándonos de la costa.
Llegará un momento en que,si miramos hacia la orilla,no veremos única mente los mástiles de los barcos,sino también las casas y edificaciones columpiando su reflejo en la superficie del mar.
Basándose en esta imagen,Gold establece un ejemplo como explicación: " Imaginemos que en lugar de estar viendo un mástil y una casa,vemos frente a nosotros una línea vertical que separa una superficie completamente negra,a la izquierda,de otra completamente blanca,a la derecha.
Ambas superficies se extienden infinitamente hacia los lados y hacia arriba.
Pues bien,al fijarnos en la zona blanca situada a la derecha de la l (nea divisoria,ver (amos que su reflejo en el agua es completamente blanco,pero a lo largo de la línea que divide ambas zonas aparecerían algunos puntos negros,de forma que en vez de una recta definida aparecería como una imagen quebrada entre el blanco y el negro.
El serpenteo de las imágenes de mástiles y casas sobre el agua obedece a las mismas causas ".
Esta extraña propiedad reflectiva del agua es sólo una parte,pero no toda,de su capacidad para distorsionar el sentido visual tal y como es concebido por el ser humano.
Gold ha continuado con sus elucubraciones sobre el tema,pero desde un punto de vista diferente al nuestro.
La idea es la siguiente: desde ciertos ángulos,todo lo que vemos en el agua es el reflejo de lo que hay sobre ella.
Si cambiamos el ángulo,podremos ver parte de lo que hay de bajo,incluso algún pez ; pero ese pez que estamos viendo ¿Cómo nos ve a los habitantes de tierra firme? La pregunta en cuestión no ha preocupado sólo a Gold.
Muchos son los pescadores de caña que se la hacen a la hora de intentar atrapar alguna presa.
Cualquier pescador que adivine la presencia de un pez en el agua tomando como punto de referencia,por ejemplo,los círculos concéntricos que éste produzca en la superficie,tiene el problema de adivinar cuál es el lugar más adecuado para lanzar el anzuelo,de modo que el pez pueda verlo bien.
Tomemos el caso concreto de una mosca artificial usada como cebo.
Este tipo de cebo tiene la particularidad de que no es arrojado dentro del agua por el pescador,sino que es depositado por éste con toda suavidad sobre la superficie,para que se asemeje en todo lo posible a una mosca auténtica que se posa sobre el agua.
Per ¿cuál es el punto desde el cual el pez tiene una mejor visión del cebo? Robert Harmon y John Cline,campeones de pesca con caña de Chicago (EE. UU.),han dedicado arduas horas de investigación al respecto,quizá movidos por el deseo de que piquen más fácilmente ; su conclusión ha sido que el mejor punto para dejar caer la mosca en cuestión es exactamente encima del pez.
Cuanto más distanciada caiga,más difícil será para éste identificarlo como comida debido a la distorsión ocasionada por los rayos del sol al encontrarse con la superficie del agua.
Para hacernos una idea de esta distorsión tomemos el ejemplo de un rayo de sol que caiga oblicuamente sobre el agua con un ángulo de cincuenta grados ; parte de la luz será reflejada fuera del agua en el mismo ángulo,formando lo que ya conocemos como reflejo ; pero el resto de la luz es desviada al entrar en el agua y su ángulo es ahora de 35 grados.
Según el ángulo con que un rayo entre en el agua,será más o menos desviado.
Teniendo en cuenta esta refracción,un pez en el agua verá las imágenes mas distorsionadas cuanto más lejos estén de él.
La distorsión aumenta a medida que los rayos están mas próximos al horizonte hasta llegar al punto máximo,donde ese rayo que cae a cincuenta grados se desvia 48,7.
El único rayo que no sufre desviaciones de ningún tipo es el que entra verticalmente en el agua.
El pez asume que el objeto que se encuentra en la superficie del agua se halla al final del rayo de luz,sin tener en cuenta la distorsión,por lo que tiende a verlo más arriba de donde realmente está.
Su campo de visión de la superficie es como un cono invertido descrito por el ángulo de luz que incida en su retina.
Los pescadores Harmon y Cline han bautizado este cono de visión con el nombre de ventana,una ventana cuyas dimensiones dependen de la profundidad a la que nade el pez ; cuanta más profundidad,mayor campo de visión.
Pero lo más curioso es cómo varían los puntos de vista de este pez según la profundidad.
Tomemos como punto de referencia un pescador de pie en la orilla y caña en ristre: un pez que nade a ochenta centímetros de profundidad vería la figura humana del tamaño de un rascacielos,con una cabeza minúscula que descansa sobre un cuerpo enorme apoyado en dos pies.
Esa inteligencia humana hay que entenderla desde los mecanismos básicos de la evolución,de la que se deriva aquélla.
La fuerza evolutiva más poderosa está en nosotros y es el haber logrado el éxito en la reproducción,la fitness de Darwin o adaptación.
El número de descendientes de un individuo es la medida más determinante de su importancia evolutiva,y cada uno determina en parte las transformaciones evolutivas en la medida en que engendra vida y la deja tras de sí.
Esto llevó en el hombre a que el cerebro humano influyese mucho más en la reproducción que en el caso de otros animales.
Y en consecuencia,el hombre posee una conducta de reproducción totalmente diferente.
Si los hombres somos capaces de pensar,eso se lo debemos en primer lugar a la estrategia de reproducción tan especial que idearon nuestros antepasados.
A esa estrategia debemos que el cerebro humano se haya desarrollado así y que el resto de nuestro cuerpo se haya adaptado a ese desarrollo pasito a pasito.
Hoy sabemos que nuestra conducta está movida por la conjunción entre nuestra inteligencia y nuestros hábitos y costumbres ; es decir,está movida por la cultura.
Pero ésta descansa sobre los instintos básicos,y uno de esos impulsos más simples es la sexualidad.
Los seres vivos más pequeños de nuestro planeta no se reproducen sexualmente ; con una sencilla división celular,un organismo unicelular puede generar millones de descendientes,todos idénticos a él.
Sin embargo,en el hombre existe la sexualidad y tiene un tremendo papel en la sociedad humana.
Ya hemos dicho que en la lucha por la supervivencia todo gira alrededor de lograr el éxito en la reproducción.
El organismo unicelular lo tiene fácil,pero ¿qué pasa si se transforma un poco el medio ambiente? Sus descendientes se encontrarán en grave peligro,precisamente porque son todos iguales.
La especie se extingue cuando no se produce la adaptación adecuada o cuando ésta no es posible ; y la adaptación es más fructífera cuando hay más variantes en una especie.
Y eso es lo que se consigue por reproducción sexual: que hay muchas variantes,lo que favorece la su pervivencia incluso en condiciones diferentes.
En nuestros días la reproducción sexual es prácticamente universal.
El papel de la hembra es el principal,y el del macho,el secundario.
Basta un solo macho para fecundar a muchas hembras.
Por eso los machos son baratos en este aspecto,y en el proceso de selección natural la elección que hace la hembra cobre mayor peso.
Como macho y hembra desean engendrar tantos descendientes como puedan,se fecundan todas las hembras posibles.
Para las hembras lo importante es encontrar una pareja fecunda,y han de hacer de todo para encontrar el macho óptimo para la reproducción.
Pero ¿cómo puede la hembra distinguir cuál es ese macho? Depende de la estrategia del macho.
La especie humana también tiene unas diferencias anatómicas y fisiológicas que se deben a su peculiar estrategia sexual ; hombres y mujeres son muy diferentes a machos y hembras del reino animal.
Lo más específico es que en las mujeres no existe ningún síntoma externo de la ovulación.
En casi todos los primates y mamíferos existen muestras muy claras de la ovulación que pueden reconocer ambos sexos.
La ovulación o bien desencadena una serie de señales externas,o produce una atracción imperceptible que llama la atención del macho para emparejarse.
La atracción que siente el macho hacia la hembra está regida por la ovulación.
Lo curioso de la mujer es que en ella no se puede establecer con exactitud,ni siquiera con instrumentos muy avanzados,cuál es el momento en que ésta ovula.
¿Cual es la razón? Todo el mundo sabe que para que se produzca un embarazo el acto sexual se tiene que producir poco antes o después de la ovulación.
El niño,el ser mas desprotegido de la naturaleza no sobreviviría sin el cuidado continuo de los padres,lo que sólo es posible cuando la pareja masculina ayuda.
Es precisamente en esto donde reside la estrategia de la mujer: encontrar un hombre dispuesto a ayudarle con los retoños durante largo tiempo.
Por esta razón,la atracción sexual durante la ovulación se va difuminando,mientras que la unión de pareja cobra importancia.
Pero ¿esto es sólo una suposición? No,se trata de una teoría comprobada.
Los hallazgos que he podido ver personalmente fueron encontrados en los últimos quince años en el Este de Africa y corresponden a nuestro antepasado más antiguo,el Australopithecus afarensis,que vivió por Tanzania y Etiopía hace tres o cuatro millones de años.
Este antepasado,aunque tenía el cerebro del tamaño no muy superior al del chimpancé,era bípedo,caminaba sobre dos patas,como nosotros.
Sin embargo,el bipedismo no se debe a una necesidad de manejar herramientas,como se quiso explicar: el Australipithecus afarensis apareció más de un millón y medio de años antes que las primeras herramientas de piedra.
Entonces ¿por qué andaban erguidos antes de que su inteligencia estuviera lo suficientemente desarrollada para dar el gran paso hacia el hombre? El Australopithecus se puso sobre dos patas para poder agenciarse comida,transportarla y elaborarla con sus manos.
Esta provisión fue,sin duda,la tarea encomendada a la pareja masculina del Australopithecus.
Así la hembra tenía una gran ventaja para lograr el éxito en la reproducción,la supervivencia de su cría.
Otro indicio que demuestra esto es la mandíbula del Australopithecus: en ambos sexos los colmillos son iguales,lo que no ocurre,por ejemplo,con los zambos,cuyos machos tienen prominentes caninos para rivalizar en los torneos.
Cuando Michael Faraday (1791 - 1867) exponía sus hallazgos en electromagnetismo,generación y transformación de electricidad,los fundamentos,en suma,de la gran industria de energía moderna,se cuenta que el gran político liberal británico William Gladstone le preguntó:   ¿Y para qué va a servir todo eso? Todavía no muy seguro de sus descubrimientos,Faraday respondió: - No se preocupe,Sir: ya verá como se le puede aplicar un impuesto.
Muchos impuestos han caído desde entonces sobre la energía eléctrica en mérito de sus innumerables aplicaciones que han caracterizado a nuestro siglo.
Dándole a un interruptor,el obrero de una fábrica moderna pone instantáneamente a trabajar fuerzas insospechadas ; el agricultor y el ganadero,pueden contar con tantos esclavos como quiera en la plantación moderna ; el ama de casa poner a sus órdenes todos los aparatos que desee para sustituir al servicio de antes.
La energía es el índice de nuestra riqueza: los Estados Unidos son más ricos que la India,por ejemplo,porque consumen más kilowatios por cabeza.
A su vez,es también el índice de nuestra dependencia moderna.
Las " crisis de energía ",nos han dejado a merced de quienes la controlan o de la tecnología misma como cuando,un apagón,sume a ciudades y a comarcas enteras en una inmóvil oscuridad.
Las formas de energía se fueron adoptando según su conveniencia inmediata,su rendimiento y rentabilidad financiera.
La pugna es muy antigua entre unas y otras.
nicamente nos llega la que obtiene otras ventajas: preservar la ecología y la supervivencia humana,que se siente súbitamente amenazada cuando ocurren accidentes catastróficos como el de Chernobyl en la URSS.
Después de los primeros usos mineros e industriales del vapor,la distribución fue la primera conveniencia que se buscó para la energía.
Aunque llevar a cada casa,a cada ciudadano,la fuente de energía,era casi impracticable,un escocés llamado William Murdoch quien,a comienzos del siglo XIX iluminó primero su propia casa y luego una fábrica textil,sugirió llevar el gas mediante tuberías a las calles de una ciudad.
La idea fue recibida con ruidosa oposición por excéntrica,peligrosa y antihigiénica,según se adujo: algunos médicos advirtieron gravemente que la gente se aficionaría a salir a la calle,al aire húmedo de la noche,a pillar resfriados y se pondría enferma.
A fines del siglo XIX,cuando se trataba de hacer lo mismo con la electricidad,el argumento era de que las calles iluminadas serían más seguras,pues intimidarían a los delincuentes.
A Thomas Alva Edison (1847 - 1931) se atribuye el primer sistema práctico de distribución de energía eléctrica para el alumbrado.
La primera red,se inauguró en 1882 desde la central de Pearl Street en Nueva York.
Casi de inmediato surgió una pugna sobre cuál era la forma más apropiada de energía eléctrica para su uso práctico y,por consiguiente,cuál era la que más dinero produciría a las compañías explotadoras.
El sistema de Edison operaba con corriente continua,cuyas limitaciones se hicieron pronto evidentes.
Tenía a su servicio al inventor croata Nikola Tesla (1856 - 1943),al que Edison ofreció 50.000 dólares si resolvía sus problemas.
Como Tesla veía los circuitos antes incluso de construirlos,modificó con éxito dinamos y motores.
Sin embargo a la hora de la prometida recompensa,Edison se limitó a subirle diez dólares su jornal de la semana.
Tesla se despidió en el acto y después de un año de infructuosos intentos consiguió asociarse con un financiero,y constituir la Tesla Electrical Company,de la que Nikola fue el presidente y cuya sede se abrió a tres manzanas de la de Edison.
Se desencadena la competencia entre la corriente continua y la alterna.
Tesla registra patente tras patente,y atrae la atención de George Westinghouse,propietario de una compañía independiente,que le of rece un millón de dólares por sus invenciones y un dólar de derechos por cada caballo de potencia.
Nikola inventa furiosamente motores alternos,su célebre bobina Tesla,sus transformadores de alta tensión.
En el caso de que un niño naciera con tan sólo ese escaso veinte por ciento,podría incluso sobrevivir sin problemas.
Dormiría y se despertaría regularmente,bebería,podría incorporarse,bostezaría y lloraría,como cualquier bebé.
Sin embargo,su comportamiento y su modo de conducta serían puramente instintivos,ya que carecería de inteligencia,conciencia y capacidad de comunicarse mediante un lenguaje articulado y lógico.
Es precisamente la corteza cerebral la que nos hace humanos,seres pensantes.
Pero esta poderosa masa gris todavía nos reserva más sorpresas.
Además de sus numerosos pliegues,la corteza cerebral está dotada de pequeñas fisuras,algunas de ellas tan profundas que delimitan claramente el cerebro en cuatro lóbulos,dos a la derecha y dos a la izquierda.
Estas regiones del " mapa cerebral " fueron comparadas durante mucho tiempo con ignotos continentes.
Ni siquiera los más eminentes científicos sabían al principio qué ocurriría en ellos.
Sin embargo,al estudiar los procesos físicos de las lesiones cerebrales,empezaron a ser localizadas algunas de las funciones de las distintas regiones.
Así,los científicos - comprobaron que el lóbulo frontal es el de formación más reciente en-el proceso evolutivo del ser humano.
Es el encargado de todos los movimientos voluntarios y quien da las ordenes para que se ejecuten todas las formas posibles de movimiento,desde mover los músculos precisos para el habla hasta la acción de correr o saltar.
Es en la zona prefrontal de este lóbulo en donde se hallan las funciones de la actividad mental,muy importantes en la determinación de nuestra personalidad.
Detrás del lóbulo frontal se encuentra el parietal,a cuyas órdenes están las principales áreas de recepción de las sensaciones táctiles.
En la parte posterior de la corteza cerebral se halla el lóbulo occipital,encargado de recibir todas las imágenes recibidas por los,ojos ; es pues el responsable de la visión.
Por último,el lóbulo temporal es quien se encarga de interpretar | las informaciones captadas por los oídos.
Todo ello,sin duda,debió | cambiar forzosamente muchas de las teorías sobre el cerebro que habían estado consideradas como inamovibles.
El mayor descubrimiento fue sin duda el que se refiere a las dos partes del cerebro.
Separados por una profunda fisura,aparecen dos hemisferios aparentemente iguales pero con unos trabajos y cometidos muy diferenciados: en el hemisferio izquierdo trabajan el lenguaje y la lógica,mientras que en el derecho se hallan los centros responsables de la imaginación y la creatividad (ver MUY n. ° 22).
Este descubrimiento resultó apasionante para la comunidad científica,ya que por fin se empezaba a completar el mapa cerebral.
No obstante,aún quedaban algunas lagunas por resolver.
Con gran decepción se comprobó que tan solo es posible localizar con exactitud los puntos de la corteza cerebral en los que se hallan cometidos tan simples como los movimientos o las percepciones sensoriales,pero todo lo relacionado con el pensamiento continúa sin ser delimitado con precisión.
Nuestro cerebro está formado por miles de millones de células nerviosas,llamadas neuronas,cuyos apéndices filiformes - axones y dendritas - pueden ponerse en contacto y comunicarse con otras células.
Sin embargo,la corteza cerebral no es un simple puzzle en donde cada pieza tiene una misión propia y específica.
Se trata más bien de un perfecto,y también complejo,sistema de redes de intercomunicación.
Por ejemplo,en la función del habla existen indicios de que no sólo participan los conocidos centros de lenguaje,sino también algunas zonas más alejadas e incluso las partes más " antiguas " del cerebro.
Cuando en 1985 un equipo de investigadores alemanes y daneses publicaron esta conclusión,numerosos especialistas mostraron su sorpresa y su perplejidad.
¿Cómo era posible que las zonas más antiguas del cerebro,que en la historia de la evolución pertenecen a nuestros predecesores animales,fueran también las responsables del lenguaje humano? Una virtual respuesta se encontraría en que en estas regiones residen las emociones,y éstas desempeñan un importante papel en el acto de hablar.
Un grupo de científicos norteamericanos presentaron a un grupo de personas diversos tonos musicales que debían clasificar espontáneamente como " iguales " y ano iguales ".
Las tomografías cerebrales mostraron que el hemisferio derecho fue el encargado de esta tarea diferenciadora.
Sin embargo,cuando los individuos tuvieron que señalar en una escala la mayor o menor elevación del sonido,fue el hemisferio izquierdo,responsable de la ejecución del lenguaje y la lógica,quien cumplió a la perfección tal cometido.
En cualquiera de los dos casos,uno u otro hemisferio no permanecieron inactivos.
Mientras que el hemisferio derecho consiguió más rápidamente una impresión global,el izquierdo se dirigió más hacia el reconocimiento de detalles.
Queda claro,pues,que los hemisferios no trabajan de forma aislada,sino que ambos se apoyan mutuamente.
Se trata de dos cualificados especialistas que se convierten en una unidad genial.
Podríamos enumerar todavía una gran cantidad de investigaciones sobre el cerebro,pero siempre nos encontraríamos con el mismo problema: los científicos tienen la misión de explicar el trabajo del cerebro humano,pero la gran dificultad estriba en que no saben a ciencia cierta qué ocurre en él y cómo funciona exactamente.
Los ultramodernos y sofisticados aparatos tan sólo pueden indicarnos - lo cual ya es mucho - en qué parte del cerebro se producen los pensamientos.
Sin embargo,sigue siendo un misterio insondable cómo se genera esta actividad intelectual,es decir,el pensamiento.
Las sustancias químicas que actúan como mensajeras desempeñan un papel importantísimo en todos los procesos del pensamiento.
Estas sustancias son las encargadas de establecer los contactos entre los miles de millones de células del cerebro.
Hasta hace poco tiempo los científicos creían que tan sólo existían dos tipos de sustancias transmisoras: las que establecen el contacto y las que lo cortan.
Pero algunos descubrimientos recientes hablan de que también existen sustancias que aumentan o amortiguan las señales.
Actualmente,los científicos han encontrado por lo menos dos docenas de sustancias transmisoras diferentes,y todavía se siguen descubriendo otras nuevas.
Los cometidos de algunas de ellas son bastante conocidos.
La serotonina,por ejemplo,es una sustancia hormonal que actúa de intermediaria sobre el sistema parasimpático,componente del sistema nervioso.
La vasopresina actúa sobre las arterias y desempeña un importante papel en la memoria ; y la dopamina,ejerce su influencia sobre los movimientos.
Las exposiciones universales comenzaron a prepararse en 1798 pero hasta la de 1889 no hubo ningún síntoma,nada que perdurara a través del tiempo y se convirtiera realmente en algo universal.
Ese año,el tema central de la Exposición giró en torno a las estructuras de hierro.
Y Gustav Eiffel,con su ya mítica torre,dejó sentado qué es lo que se podía hacer tan sólo con hierro.
En esta ocasión,el leiv motiv es la " Era del Descubrimiento ".
Colón fue la primera persona que demostró que la Tierra es redonda y abrió una nueva era de comunicaciones marítimas.
El aventurero derrumbo uno de los más grandes tabúes de la época y dejó un nuevo continente,un universo al alcance de las manos.
Un nuevo mundo que alimentó a la vieja Europa durante centenares de años.
El homenaje está más que justificado.
La " Era del Descubrimiento " se explorará en tres etapas: el mundo antes de 1492,desde entonces hasta nuestros días y el futuro.
Pero la Expo 92 contará también con áreas para el Pabellón de España y de los demás países ; para la Administración y los Servicios y para una sección que se ocupará de exponer por donde van los tiros del futuro del Hombre,la Tierra y el Cosmos.
Uno de los aspectos más interesantes de la Expo 92 será el arquitectónico.
Hace pocas semanas el jurado concedió la medalla de oro " ex aequo " al arquitecto argentino Emilio Ambasz y al ingeniero español José Antonio Fernández Ordoñez,hermano del ministro español.
¿Curiosa coincidencia,no les parece? El premio se decidía entre trece profesionales a los que se había pedido previamente que presentaran proyectos.
El escenario definitivo se ubicará en la isla de la Cartuja y saldrá de los estudios realizados por la pareja ganadora.
Fernández Ordoñez propone construir una gran esfera armilar que lleve incorporados los últimos descubrimientos astronómicos y la tecnología más vanguardista.
Dentro de la esfera girarían los planetas y lunas del sistema solar y el público podría acceder a este universo flotante.
Se podría pasear por los confines del cosmos con la misma facilidad que por la ribera d Guadalquivir.
En opinión de Fernandez Ordoñez,la esfera armilar simboliza la herencia de Colón y otro de los grandes hitos c la aventura humana: el que la sonda Voyager 1,enviada al espacio en 1977,llegue Plutón,a la periferia de nuestro sistema solar,en 1991.
En resumen,la propuesta del ingeniero se basa en argumentos prácticos y esté con una pincelada simbólica: la creación de la gigantesca esfera armilar.
Por parte,el arquitecto Emilio Ambasz resume así su proyecto: " Se pretende una simbiosis entre arquitectura y naturaleza,entre edificio y espacio verde ".
Ambasz apuesta por un cambio más profundo y espectacular en la fisonomía de la isla de La Cartuja.
En principio,piensa que hay que crear tres lagunas artificiales.
Y,con la tierra que se excave para formar las lagunas,propone levantar tres colinas que además de darle otro aspecto a la isla,servirían como apoyo al estadio de deportes y al " Teatro del Mundo " ; un espacio concebido para,las actividades culturales.
El argentino sugiere que los pabellones de los distintos países y temas descanse sobre balsas que floten en las lagunas.
Su idea es que aquello se convierta en una minúscula Venecia o Cachemira,con ferry en lugar de góndolas o barcazas.
Según sus estudios,con el sistema de los ferry se podría transportar a 200.000 pasajero diariamente.
Lo que no está nada mal si tenemos en cuenta que se prevé la visita d 17 millones de personas desde abril hasta el doce de octubre,día de la clausura.
Emilio Ambasz ha convertido el agua en algo esencial para la Expo 92.
Quiere las lagunas y un sistema de bombeo ininterrumpido que impida que las aguas se estanquen.
Y quiere niebla artificial.
¿Cómo producirla? Un exmeteorólogo norteamericano,Thomas R. Mee,tiene la solución.
Su niebla artificial se utiliza fundamentalmente en invernaderos y en los campos como protección contra las heladas.
Pero si distribuye el sistema a través de unas pérgolas,diseñadas para dar sombra al lugar - piensa Ambasz -,la niebla se sostiene en el aire sin descender al suelo creando una sensación de frescor en el ambiente y reduciendo la temperatura entre 6 y grados.
Algo que en pleno verano andaluz es muy de agradecer.
Todavía quedan seis años y muchas dificultades pero,ya lo ven,el asunto está en marcha.
Ahora sólo cabe que Barcelona obtenga la organización de las Olimpiadas.
Por esta razón,los científicos compiten últimamente con otras teorías al margen para explicar los efectos de las fuerzas gravitatorias.
En el caso concreto de los cálculos de Mercurio,primeramente se consideraba al Sol totalmente esférico.
Más tarde,sin embargo,los físicos norteamericanos Robert Dicke y Mark Goldenberg descubrieron en 1967 un ligero achatamiento del astro en sus polos.
Este aplanamiento podría justificar un siete por ciento ese desplazamiento adicional en el perihelio del planeta.
Una segunda teoría presupone un desplazamiento adicional de 39 segundos de arco,por la curvatura del espacio,más otros cuatro segundos a causa del achatamiento solar.
Las controversias podrían resolverse si la NASA llevara a cabo a principios de los noventa su proyecto Starprobe.
Los norteamericanos tienen previsto lanzar una sonda espacial especialmente protegida para que se aproxime al astro hasta una distancia de dos diámetros solares.
A través del seguimiento riguroso de la nave,habrán de ponerse de manifiesto las desviaciones ocasionadas por la fuerza de gravedad solar.
Por el momento la teoría de la relatividad de Einstein sólo ha podido ser comprobada a través de unos reflectores de láser colocados sobre la Luna durante el proyecto Apolo,gracias a los cuales se pudo calcular la distancia entre nuestro satélite y la Tierra con un margen de error de sólo treinta centímetros.
Con el tiempo será el análisis de los cálculos los que permitirán constatar si la masa pesada y la masa inerte de la Tierra son efectivamente idénticas.
Mientras tanto,los científicos que colaboran con la Universidad de Stanford tienen el proyecto de efectuar un curioso experimento en 1992.
Se proponen poner en órbita alrededor de la Tierra un satélite con cuatro pequeñas esferas de cristal del tamaño de una pelota de ping-pong.
Las esferas son tan perfectas que podrán servir como giroscopio de gran precisión.
Se prevé también que,una vez alcanzada su órbita alrededor de nuestro planeta,unos chorritos de gas que se encuentran en sus cápsulas accionarán el movimiento giratorio de las bolas.
Más tarde,para realizar cálculos y mediciones constantes de las esferas en rotación sin que sus movimientos se vean alterados,las bolas serán enfriadas hasta los dos grados por encima del cero absoluto (273 ° centígrados).
En consonancia con las predicciones de la teoría de Einstein,la curvatura del espacio alrededor de la Tierra obligará a estas esferas a realizar un giro completo de peonza sobre su eje,durante el tiempo que dure su recorrido orbital.
Sin embargo los físicos de Stanford no descartan la posibilidad de que los resultados no concuerden con las hipótesis de Einstein.
En 1827,al final de una conferencia en la Sorbona,el eminente químico Jean Dumas se vio abordado por una mujer,extremadamente agitada porque su marido se había vuelto loco.
Quería " hacer cuadros con unas lentes y una caja ",según decía.
- Como hombre de ciencia,¿cree usted que eso es posible?,preguntaba la ansiosa mujer.
- No,por lo que hoy sabemos no puede hacerse - le dijo Dumas -,pero yo no digo que sea siempre imposible.
El loco era Louis-Jacques Mande Daguerre,un pintor parisino,bailarín y acróbata,hombre muy emprendedor e imaginativo.
Con unos amigos había montado un " Diorama " que entusiasmaba al público parisiense.
Daguerre había oído hablar de un litógrafo llamado Joseph Nicéphore Niépce,que en 1824 había conseguido reproducir en una superficie de asfalto - ¡tras ocho horas de exposición! - una línea de dibujo.
Le escribió una carta,le fue a visitar y consiguió,al fin,su ayuda y colaboración,sobre experiencias con placas de yoduro y cloruro de plata.
Daguerre persistió por su cuenta y el 7 de enero de 1839 pudo presentar los primeros " daguerrotipos ",unas imágenes del Puente María que dejaron impresionados a los miembros de la Academia de Artes y Ciencias por su fidelidad y detalle.
Pero el 25 de aquel mismo mes y año,apenas tres semanas más tarde,se hacía en Londres otra presentación ante la Royal Society.
Su inventor,William Henry Fox Talbot,advertía que su procedimiento era totalmente independiente del francés,pues se basaba en el proceso negativo-positivo que caracteriza desde entonces a lo que se llamó fotografía,nombre aparentemente sugerido por Sir John Herschel,hijo del famoso astrónomo.
Del griego fotos (luz) y grafos (dibujar),Talbot describía en su informe a la fotografía como un proceso por el cual se consigue que los " objetos naturales se delineen por sí mismos sin la ayuda del pincel del artista ".
Tal vez hubiera debido decir que era la luz la delineante,pero lo relevante entonces y ahora era la cualidad mecánica,automática del procedimiento: la Naturaleza dibujándose a sí misma sin intermediario.
La fotografía estaba muy lejos entonces del fantástico desenvolvimiento que seguirá ininterrumpidamente a lo largo de los siglos XIX y XX desde aquellos indecisos comienzos,pero allí estaba lo esencial de su propósito: dar una imagen de la realidad.
Inicialmente y aún hasta nuestros días la práctica de la fotografía implica un sistema óptico y un medio sensible a la luz para recoger y fijar imágenes.
El desarrollo de estos elementos básicos hasta el estado actual de la tecnología fotográfica,ha requerido una combinación muy compleja y extendida de ciencia,ingeniería y arte a las que se han dedicado incontables recursos.
Ha de tenerse en cuenta que la tecnología fotográfica original es,en esencia,una " técnica madre " de la que van surgiendo con los años innumerables tecnologías derivadas y,subsiguientemente,tan vastas y complejas como aquélla,tal y como ha ocurrido,por ejemplo,con la fotografía del movimiento - el cinematógrafo -,del que habrán de seguir,eventualmente,las de la transmisión de imágenes y el formidable complejo técnico de la televisión.
Todas ellas conservan,empero,el fundamental principio de la imagen delineada por sí misma.
Cada una de estas adiciones tiene su momento de inicio,comenzando por los primeros perfeccionamientos de las emulsiones y del medio,placa,film,etcétera,así como los de la mejora de lentes y de la mecánica de captación de la imagen,que van ocurriendo ya en el siglo XIX.
Algunos de ellos " hacen época ",como la emulsión de gelatina que introduce George Eastman en 1878 y que le lleva a la película en 1884 y,finalmente,a la " Kodak " en 1888,con la cámara que hacía fotos apretando simplemente un disparador y que puso la fotografía al alcance de todo el mundo.
Ello no sin disputas de patentes y pleitos como el que planteó el clérigo Hannibal Goodwin,que falleció antes de obtener justicia por su invención del rollo,después de doce años de litigio.
Existen cuatro tipos productores de malaria: P. Malariae,P. Ovale,P. Vivax y P. Falciparum.
Todos ellos afectan al hombre,aunque cada uno lo hace de modo diferente ; el Falciparum es el de efectos más letales.
Estos protozoos son transmitidos al hombre por la picadura del mosquito de la especie Anófeles,del que existen cuatrocientos tipos diferentes,aunque sólo sesenta son responsables de la transmisión de la malaria.
a infección se produce cuando el aguijón del mosquito penetra en la piel humana en busca de sangre ; en ese momento,cientos de parásitos entran en la corriente sanguínea de la persona mordida,y rápidamente se abren paso hacia el hígado,donde los anticuerpos no los pueden destruir.
Allí permanecen varios días,durante los cuales su número se multiplica,y después entran de nuevo en la corriente sanguínea invadiendo las células y continuando con su reproducción.
Al cabo de algunos días comienzan a aparecer los síntomas: una anemia generalizada,acompañada de fiebre y escalofríos ; sudor,dolor de cabeza,vómitos y dilatación del bazo.
La malaria no siempre resulta mortal,sobre todo si se trata adecuadamente en cuanto aparezcan los primeros síntomas,pero es corriente que sus secuelas nunca sean completamente erradicadas del organismo de quien la ha padecido alguna vez.
Es el caso de la P. Vivax,que es la variedad más resistente a la terapia,y que puede permanecer crónica en el organismo.
Paradójicamente,la forma de combatir la malaria fue descubierta mucho antes de que se lograra determinar el origen de la enfermedad.
Desde el siglo xvll se utilizaba en América del Sur la corteza del árbol de la quina como remedio contra la malaria.
En 1700 fue aislada la quinina,su principio más activo,y desde entonces ha sido universalmente utilizada como el medio más eficaz para prevenirla,o curarla en los casos en que ello sea posible.
Por fin,en 1898,el doctor británico Ronald Ross consiguió determinar que los mosquitos de la especie Anófeles eran los transmisores de la enfermedad.
Ello permitió desarrollar un nuevo sistema de lucha: el ataque contra los mosquitos.
La utilización masiva de DDT demostró ser el sistema más seguro,junto con la instalación de mosquiteras y otras barreras en las zonas habitadas por los insectos transmisores.
Durante la Segunda Guerra Mundial,ante la perspectiva de que las fuerzas aliadas sufrieran un considerable número de bajas debido al paludismo durante la campaña del Pacífico,se llevó a cabo un intenso programa de prevención: las zonas infestadas de mosquitos fueron fumigadas con DDT,y los soldados tomaban diariamente drogas preventivas,al tiempo que se mantenían unas estrictas normas de higiene.
Los resultados cumplieron plenamente las expectativas,pues en una zona tan propensa a la enfermedad como es Okinawa,sólo se registraron dos casos de paludismo en cuatro meses.
AL mismo tiempo,comenzaron a descubrirse derivados químicos de la quinina,como la pirimetamina,la mefloquina y el más utilizado en la actualidad,la cloroquina.
Todo ello contribuyó a crear un clima de optimismo mundial,y a presumir que la eliminación completa de la malaria era sólo cuestión de tiempo,y no de mucho.
Pero en los últimos años han aparecido inconvenientes que han hecho resurgir el problema,ahora con carácter de urgencia.
La principal raíz del problema tiene dos bifurcaciones: en primer.
lugar,los mosquitos han ido desarrollando una progresiva resistencia al DDT y otros insecticidas ; y en segundo lugar,los parásitos de la malaria también han desarrollado una considerable inmunidad a las drogas utilizadas para combatirlos.
Cuentan con inteligencia artificial,con una capacidad informática superior,sobre todo en lo que se refiere al aprendizaje,toma de decisiones y comunicación.
La inteligencia artificial es el gran reto de la robótica y de ella depende en gran parte su futuro.
Terregator.
Los científicos de la Carnegie-Mellon University (CMU) quieren construir el " Terregator ",un vehículo que puede viajar por sí mismo,sin conductor.
El aparato fue encargado por el Ministerio de Defensa norteamericano y se quiere utilizar en operaciones y zonas peligrosas para el ser humano: centrales nucleares,bajo el agua,en otros planetas o en los campos de batalla,para realizar averiguaciones allí donde no pueden enviarse vehículos tripulados.
El " Terregaton " cuenta con una cámara de vídeo que filma la imagen del entorno y la transforma en señales eléctricas.
Un " convertidor digital analógico " permuta de nuevo estas señales en algo comprensible para el ordenador,El robot recoge así la información de su entorno,sabe dónde están los bordes de la carretera,los obstáculos,las piedras,la sombra de los árboles y hasta las grietas y baches del firme.
Sin embargo,todavía pasarán unos años hasta que el " Terregator " pueda conducir realmente un automóvil.
Aún es una caja poco diestra y pesada que se arrastra despacio y penosamente por un patio de la universidad.
Robots que Oyen,palpan y ven.
Este robot también tiene ojos,pero funcionan de forma diferente a los del " Terregaton ": ve las formas de los objetos de la casa.
Las amas de casa pueden estar especialmente contentas con él.
Se trata de una máquina que aspira el polvo y friega el suelo sin chocar con los muebles ni dañar las paredes.
La única pega es que pesa media tonelada.
Tan sólo es apto para trabajar en viviendas muy resistentes y de grandes dimensiones.
Mientras en " Terregator " calcula la distancia de diferentes objetos superponiendo dos imágenes estereoscópicas - como es el caso del ser humano -,el robot casero utiliza los oídos para calcularla.
El robot emite impulsos ultrasónicos hacia una pared,por ejemplo,y determina la distancia que le separa de ella atendiendo al tiempo que tardan los impulsos en regresar a su oído.
Es un proceso similar al utilizado por el ecómetro o el radar.
Hace unos años Polaroid sacó al mercado una cámara que enfocaba y medía la distancia al objeto automáticamente y los investigadores del CMU decidieron colocar unos telémetros similares en el cuerpo del robot casero.
A partir de entonces,el robot puede enviar constantemente ondas ultrasónicas a su alrededor,al tiempo que registra en una especie de mapa la distancia que le separa de las paredes y obstáculos de una habitación.
Para situar las esquinas de la habitación,simplemente alarga las paredes de su mapa hasta que se cortan.
Además,y también ayudado por el mapa,busca calles por las que poder moverse sin tropezar.
El sonar le ayuda a mantener la orientación.
El doctor Hirzinger ha inventado un sensor de momentos-fuerza que puede palpar por sí solo las fuerzas y movimientos en todas direcciones.
Lo que permite por ejemplo programar al robot para que se quede quieto de inmediato en cuanto tropieza con un objeto en un lugar concreto.
También puede ser utilizado para reconocer automáticamente los contornos de las cosas.
Por ejemplo,puede identificar y almacenar la primera pieza de una larga serie y de esa forma,como ya conoce el perfil de todas las demás,es capaz de localizarlas y trabajar con mayor rapidez.
El sentido del tacto es vital para un robot ; sólo con él es posible una reacción inteligente en condiciones variadas.
Si,vamos a suponer,se produjera el desplazamiento de una pieza en su trabajo,los robots no inteligentes continuarían trabajando en el vacío como estúpidas marionetas.
Continuarían realizando los mismos movimientos indefinidamente.
Según el doctor Hirzinger,un sensor táctil aporta una cualidad absolutamente nueva a la técnica robótica.
La mayoría de los robots son grandes y,sobre todo,pesados.
Si fueran ligeros,podrían desplazarse con facilidad o vibrar y balancearse.
Perder su posición en suma.
Así no podrían llevar a cabo su labor con precisión.
Pero si en los lugares claves del robot - piensa el investigador alemán - llevara incorporados sensores táctiles,no tendrían por qué ser tan voluminosos y pesados.
Los sensores corrigen y compensan los desplazamientos y variaciones de las máquinas.
Las manos de los robots.
Como siempre,el modelo es la mano humana.
Nuestra mano tiene unos 20 grados de libertad (posibles direcciones de movimiento) mientras que las de los mejores robots de la actualidad cuentan tan sólo con ocho.
La mano regula prácticamente todo mediante los sensores táctiles ; por eso es tan flexible.
También es blanda.
Pero éste no es el caso todavía de la mayoría de las pinzas de los robots ; habrá que esperar a un futuro no demasiado lejano para verlo.
Sin embargo,ya se trabaja en la piel electrónica que cubrirá al robot y le permitirá reconocer por el tacto los objetos que deba manipular.
La idea es medir no sólo el tamaño y la forma sino su temperatura,suavidad y vibración también.
No hemos de olvidar tampoco que hay robots tan diestros que pueden manejar un huevo con su pinza,servir martinis,interpretar piezas musicales al piano o jugar al ajedrez Es otra la impresión pero hay ya bastante camino recorrido.
Existen muchos otros sistemas adicionales que aportan a los robots más capacidades.
No sólo hay sensores de visión,audición y tacto ; los hay también inductivos,sensores que pueden localizar,por ejemplo,agujeros en zonas metálicas.
Los robots de las cadenas de montaje se sirven de ellos para meter tornillos en los agujeros.
En unos años podremos conversar con los robots y quién sabe si los utilizaremos para enseñar a nuestros hijos.
Si nos dejan dormir,dormimos,pero si se nos impidiera mantener esta fea costumbre nos convertiríamos en superhombres.
Edison y los antropólogos que esgrimieron tales teorías estaban en un error que los años y las investigaciones posteriores pondrían de manifiesto.
El ser humano necesita dormir para funcionar y vivir,y si no duerme... Si no duerme su vida se convierte en un drama.
Lo saben millones de personas que padecen de insomnio: entre un veinte y un veintinco por ciento de la población lo sufre ocasionalmente,y entre un doce y un quince por ciento tiene serios trastornos relacionados con el sueño.
Pero quienes demostraron científicamente por primera vez la gravedad de la privación del sueño fueron los médicos del Walter Reed Institute of Research,de Washington,a raíz de la Segunda Guerra Mundial.
En las campañas bélicas,los soldados se veían a veces obligados a no dormir durante días,y los médicos del citado instituto pusieron en marcha un estudio para averiguar las consecuencias que esto podría acarrear.
Un grupo de soldados que se prestaron voluntarios para el experimento fueron encerrados en una sala de hospital con comida,bebida,juegos y distracciones.
Podían incluso beber café sin límite,pero debían mantenerse despiertos durante 98 horas.
A partir de las treinta horas,algo más de un día,los soldados empezaron a tener dificultades con la visión: las sillas cambiaban de tamaño,los objetos pequeños se desplazaban,algunos veían halos de niebla alrededor de las lámparas.
Al cabo de noventa y ocho horas - tres días y dieciocho horas - los soldados empezaron a alucinar ; unos se veían envueltos en telarañas,de las que querían de sembarazarse ; otros veían un perro inexistente.
Todos habían perdido la noción del tiempo y se hallaban dominados por un desequilibrio anímico general.
Lloraban o reían sin motivo,soñaban despiertos,no distinguían sueño de realidad.
Las 98 horas se cumplieron y,terminado el experimento,los soldados pudieron dormir.
Bastaron diez o doce horas para que todos despertasen perfectamente normales y descansados.
El experimento llegó aún más lejos con el norteamericano Peter Tripp,que estuvo doscientas horas sin dormir,bajo vigilancia médica.
Pasadas cien horas,Tripp encontró en los más espeluznantes delirios: veía a su médico vestido con un traje de gusanos peludos ; el traje del doctor que le atendía era de paño inglés.
Poco después abría un cajón y salía despavorido,porque veía que de el salían llamaradas.
Pasadas 150 horas no sabía quién era,ni dónde estaba.
Llegó por fin el noveno y último día ; Tripp fue examinado por un neurólogo vestido con un traje no muy a la moda que le pidió que se echara para ser reconocido.
El alucinado paciente le tomó por un enterrador compinchado con los médicos,cuyo propósito era enterrarle en vida.
Peter Tripp pudo por fin descansar,tras tan patética experiencia,y recobrar la normalidad después de trece horas de profundo sueño.
Para los científicos,las alucinaciones tenían ya una explicación.
Habían visto que el cerebro fabricaba,al cabo de dos días completos sin dormir,una sustancia alucinógena muy parecida al LSD.
Pero la mayoría de los insomnes no han dado con una solución eficaz a la larga.
Los Centros del Sueño,cada vez más extendidos,tratan de buscarla.
Allí,primero se pregunta al paciente por sus costumbres y forma de vida ; si no se encuentran causas orgánicas,se busca en el lado anímico.
Muy frecuentemente el conflicto queda al descubierto en esta conversación,pero si esto no ocurre,se propone al insomne que pase dos o tres noches en el Laboratorio del Sueño.
Una vez en su cuarto,al paciente se le conecta toda una maraña de electrodos,unas plaquitas de plata: dos en la frente para la toma de tierra,dos bajo la barbilla para controlar las corrientes eléctricas de la musculatura de la deglución ; dos junto al rabillo del ojo para los movimientos oculares.
La frecuencia e intensidad de estos movimientos durante las fases del sueño son muy indicativas.
También se le ponen electrodos en orejas,cráneo,piernas,sobre el pecho,la nariz,la boca y lóbulo de la oreja derecha.
Así se registran,entre otras cosas,las corrientes del cerebro,los movimientos convulsivos de las piernas,la actividad cardíaca,la frecuencia de la respiración y la saturación de oxígeno en la sangre.
El paciente se echa a dormir encerrado en su habitación,y una cámara de video y un micrófono registran cualquier incidencia que se produzca dentro,a lo largo de la noche.
A pesar de toda esta parafernalia,los pacientes suelen dormir mejor aquí que en su propia cama.
A la mañana siguiente le hacen análisis de sangre y un electroencefalograma,y sus resultados vienen a sumarse a los cientos de metros de papel que ofrecen el poligrama del sueño.
Una persona normal suele registrar una escalera: desde el escalón más alto,el estado en que la persona está despierta,se descienden cuatro escalones,cada uno correspondiente a un estado del sueño.
el estado 1,de adormecimiento,se baja al estado 2,considerado como el inicio del sueño propiamente dicho ; aquí transcurre más o menos la mitad del tiempo total del sueño de una persona.
De aquí se pasa al sueño más profundo,que son los estados 3 y 4. Al cabo de una hora u hora y media después de que la persona se adormece,se suele alcanzar el estado REM (Rapid Eye Movement,movimiento rápido de los ojos).
Un durmiente sano suele pasar por cuatro o cinco estados REM,de diez a treinta minutos cada uno.
En estas fases las corrientes del cerebro se parecen a las del estado de adormecimiento,pero varía la profundidad del sueño.
Son también las fases en que se desarrollan los sueños,preferentemente ; el hombre suele tener erecciones,y la mujer presenta la vagina húmeda ; aumenta también la presión arterial y se acelera el corazón y la respiración.
El mecanismo que origina el sueño sigue sin conocerse,así como el órgano anatómico que lo produce.
Sin embargo se va sabiendo algo más sobre los aspectos fisiológicos y bioquímicos del sueño.
También parece claro que existen estructuras,como el hipotálamo,con una importancia directa en los períodos de sueño y vigilia,y ciertas teorías mantienen que existe un sistema capaz de producirlo situado en la protuberancia.
También parece claro que existen cambios humorales durante el sueño - el córtex,por ejemplo,libera una cantidad mucho mayor de neurorreguladores - y que se modifica la producción de otros tipos de sustancias.
Se ha visto que,si se libera serotonina en exceso,aumentan los períodos de sueño normal y disminuyen los estados de sueño paradójico.
Con base a la serotonina,esa sustancia mensajera del cerebro que juega un papel importante en los estados de sueño y vigilia,se ha empezado a utilizar un producto para dormir llamado triptófano.
Es químicamente la fase previa de la serotonina.
Este producto,que carece de los nefastos efectos secundarios que acarrean los otros somníferos,es un aminoácido componente de la albúmina.
Se encuentra en el chocolate,en la leche y en los plátanos,pero ya se suministra en el mercado sin necesidad de presentar una receta.
Sin embargo,el triptófano no es efectivo para todas las personas.
Algunos pacientes suelen preguntar angustiados si pueden morir por falta de sueño.
La respuesta es no ; hasta los casos más graves duermen por término medio cuatro horas por noche,aunque ellos no lo crean.
Cuando llegan a comprender esto,la primera batalla contra el monstruo del insomnio está ganada,porque el objetivo número uno de los médicos es que el paciente vuelva a recuperar la total confianza en su sueño.
En 1904 Fleming era asesor de la compañía británica de Guglielmo Marconi y,en busca de un detector más eficiente decidió servirse de lo que había observado.
En una lámpara muy semejante a las de la luz introdujo el filamento (cátodo) y la placa (ánodo),y así montó la primera válvula termoiónica,más tarde llamada simplemente diodo,un rectificador electrónico que se aplicó inicialmente a la recepción de radio.
La invención no pareció gran cosa al principio,pero dos años más tarde,en 1906,Lee de Forest (1873 - 1961),un prolífico inventor norteamericano,tuvo la idea de añadir un tercer elemento,una rejilla,entre el cátodo y el ánodo,convirtiendo el diodo en un triodo,que añadía una nueva propiedad a la rectificación: la de la ampliación.
Al igual que en el diodo,la corriente de electrones circula en el triodo del filamento a la placa,pero la rejilla,en el medio,la regula o modula de tal manera que el voltaje de aquel flujo depende de ella.
En otras palabras: la corriente de la rejilla impone,por decirlo así,su imagen o perfil a la corriente entre cátodo y ánodo.
Y mientras la corriente entre ambos puede ser tan grande como se quiera,la de la rejilla puede ser tan débil como la de una lejana señal de radio.
La ubicuidad de la válvula electrónica contribuyó,tal vez,a hacer pensar en algo más cómodo y flexible.
Las válvulas consumían mucha electricidad,requerían cierto tiempo de calentamiento y,pese a su perfección,todavía fallaban.
En Bell Laboratories,el centro de la AT & T,había un experto en radio llamado George Southworth que tuvo la curiosa idea de volver treinta años atrás y estudiar más de cerca el olvidado descubrimiento de Ferdinand Braun ; cuéntase que le costó su tiempo dar con un viejo bigote de gato en las tiendas de segunda mano,pero al final,reconstruyendo una vieja galena,pudo comprobar que,efectivamente,era muy superior a la válvula (el triodo) en la detección de altas frecuencias.
Esta comprobación llevó al estudio de puntos de contacto con diversos materiales,entre ellos una barrita de silicio,que en 1939 reveló la sorprendente propiedad de que cada extremo rectificaba en direcciones opuestas,lo que incitó a los investigadores Russel Ohl,John Scaff y Henry Theurer a bautizarlos p (positivo) y n (negativo).
Con estos primeros pasos,relativamente modestos,comenzaba una de las más persistentes y brillantes investigaciones del siglo,una de cuyas implicaciones científicas apenas podían sospechar sus iniciadores.
El estudio de los que más tarde se llamarían semiconductores estaba por entonces en plena forma en Alemania (Walter Schottky) e Inglaterra (Neville Mott),y era en cierto modo una lógica secuela de la mecánica cuántica formulada pocos años antes.
Apenas concluida la guerra,reanudaban la investigación en Bells un grupo de físicos bajo la dirección de William B. Shockley,Walter H. Brattain y John Bardeen,que se dedicaron durante dos años a observar efectos y a formular teorías en torno a " estados de superficie " y " efectos de campo ",nuevos términos de una ciencia emergente.
Shockley era el imaginativo del equipo y Bardeen el matemático profundo.
Tras muchos esfuerzos frustrados (que eran otros tantos " peldaños hacia la creatividad ",como explicaba Shockley,pues les ayudaban a comprender las propiedades de los semiconductores),en noviembre de 1947 Bardeen y Brattain experimentaban con una muestra de germanio con un puntito de oro y un agujerillo para introducir un bigote de wolframio.
Un chispazo accidental estropeó el agujerillo y Brattain,en plena frustración,se limitó a acercar el bigote al oro,esperando obtener un flujo de electrones.
Y así ocurrió,en efecto,pero en dirección contraria a la esperada: los electrones salían del semiconductor de germanio en lugar de entrar.
Por una nota de Brittain se sabe que tanto él como Bardeen se percataron inmediatamente del fenómeno y allí mismo acuñaron los términos emisor y colector.
Días más tarde,con una nueva versión del chisme,con el oro y el wolframio mucho más cerca,obtuvieron una amplificación.
El 23 de diciembre estaban seguros de su descubrimiento y lo demostraron a sus compañeros.
Uno de ellos,J. R. Pierce,les ayudó,según Brattain,a dar un nombre al nuevo dispositivo: después de pensar en sus propiedades,se le ocurrió resumirlas en transresistencia y de ahi,abrevió a transistor.
Más tarde,el notable invento,haría posible los circuitos integrados con millares de transistores,condensadores y resistencias concentrados en un chip del tamaño de una uña ; proporcionaría a los sordos ayudas imperceptibles,a los médicos nuevos instrumentos de diagnóstico,a los naturalistas medios de seguir la emigración animal ; lograría su inicial objeto de conmutar llamadas telefónicas e innumerables aplicaciones.
En palabras del propio William Shockley: " sin él,el mundo y la historia moderna serían radicalmente diferentes ; no habría,por ejemplo,industria del ordenador a la escala que conocemos,no habría satélites de comunicaciones y no habría viajes del hombre a la Luna.
N Ocurre a veces que un gran invento no se reconoce como tal hasta pasados muchos años desde su invención,cuando su aplicación práctica cobra realmente sentido.
Este es el caso del contador de partículas radiactivas,aparato descrito por primera vez en 1928 en una revista especializada por Hans Geiger,profesor de física experimental en la universidad de Kiel (RFA),junto con su ayudante Waíter Muller.
El artículo,que apenas ocupaba tres páginas de extensión,tenía este prosaico título " El tubo contador de electrones ".
La reseña publicada por los dos investigadores resumía el resultado de veinte años de trabajo en el laboratorio.
Casi seis décadas después,los aparatos para la medición de radiaciones ionizantes,derivados todos en mayor o menor grado del tubo de Geiger-Muller,se han convertido en instrumentos de uso común en muchas facetas de la actividad humana.
Aunque fundamentalmente todos sirven para lo mismo,es decir,medir los parámetros y magnitudes de la radiactividad,existen cientos de modelos diferentes,diseñados cada uno para una aplicación específica.
Según su principio de funcionamiento se puede establecer una clasificación general en cuatro apartados: tubos Geiger-Muller,cámaras de ionización,contadores de centelleo y contadores semiconductores.
Un contador Geiger clásico consta básicamente de un cilindro metálico de unos diez centímetros de longitud y dos de sección que contiene argón a baja presión y a lo largo de cuyo eje discurre un hilo delgado de wolframio o acero.
Tubo e hilo,aislados entre sí convenientemente,conducen una corriente eléctrica de entre 1.000 y 1.500 voltios,ligeramente inferior a la necesaria para que se produzca una descarga eléctrica entre el electrodo positivo o ánodo (el hilo) y el negativo o cátodo (el tubo).
El electrón es conducido mediante campos eléctricos a través de un amplificador llamado fotomultiplicador.
En él el electrón originario va rebotando sucesivamente contra una serie de electrodos metálicos llamados dinodos,provocando en cada choque nuevas expulsiones de electrones que refuerzan la señal.
También aquí se produce un efecto avalancha.
El flujo de electrones significa electricidad: al final del amplificador se mide el impulso eléctrico resultante,cuya intensidad será tanto mayor cuanto más energética fuera la partícula o cuanto gamma detectado por el cristal.
Los principales campos de aplicación del contador de centelleo son,por un lado,la medición exacta de los rayos gamma,y por otro,la cuantificación de la energía radiante.
Cualquier material radiactivo emite partículas y cuantos gamma con una energía determinada.
Si se conecta al contador de centelleo un clasificador electrónico,llamado en el lenguaje de los técnicos (analizador multicanal),se puede deducir,a partir de la intensidad energética de los impulsos entrantes,la proporción de los distintos elementos radiactivos en el material analizado.
El precio de este instrumento de medición está de acuerdo con su altísima precisión: un buen contador de centelleo vale más o menos un millón de pesetas,a las que hay que añadir otras 700.000 pesetas para el analizador multicanal.
Más caros todavía resultan los contadores semiconductores.
También éstos trabajan con un cristal especial en el que las partículas radiactivas desencadenan determinados procesos.
Sin embargo,aquí no se producen destellos,sino que el corrimiento de electrones hacia el ánodo del cristal semiconductor se puede medir directamente.
Estos aparatos contadores se consideran el non plus ultra en exactitud y precisión,entre otras cosas porque son capaces de silenciar el ruido térmico de fondo (el calor producido por el movimiento de átomos y grupos de átomos) al trabajar a muy baja temperatura.
Esta cualidad exige mantener el instrumento constantemente refrigerado con nitrógeno líquido.
Un equipo de este tipo viene a costar unos 7 millones de pesetas.
Naturalmente,pocos particulares estarán interesados en comprar un aparato de estas características,y no en último lugar debido a su altísimo precio: el manejo de un gran equipo de cuantificación de radiaciones ionizantes requiere amplios conocimientos de física atómica,amén de una especialización técnica adecuada.
Resulta imposible encontrar un finlandés que no haya tomado una sauna en algún momento de su vida ; es una costumbre extendida hoy por todo el mundo,y son muchas las personas que la utilizan para gozar de un mayor bienestar físico y una relajación que les permita olvidarse de las preocupaciones.
Pero en Finlandia,su país de origen,la sauna es algo más que una costumbre ; es una institución.
En este hermoso y frío país hay casi millón y medio de saunas para una población que no llega a cinco millones de habitantes.
Están repartidas por todas partes: en casas de campo,hoteles,edificios de apartamentos,fábricas y bancos.
El ritual de la sauna no ha cambiado en sus dos mil años de existencia ; una sauna es,fundamentalmente,una cabina con paredes de madera donde,por medio del va por de agua,se alcanzan elevadas temperaturas que someten a sus ocupantes a una fuerte transpiración que elimina toxinas corporales y produce una sensación de tranquilidad y bienestar.
Pero en muchos lugares aún se asocia la palabra sauna con sufridos métodos de adelgazamiento,o con locales llenos de bellas señoritas de dudosa reputación.
Ninguna de las dos cosas es cierta,y la segunda,en concreto,es la que causa mayor indignación entre los ciudadanos finlandeses.
Es cierto que el cuerpo sufre en la sauna una pérdida de peso que oscila entre quinientos y 2.500 gramos,pero ese peso se recupera en menos de 24 horas.
En segundo lugar,el sexo y la sauna son conceptos totalmente aislados para los finlandeses.
No existen saunas colectivas mixtas,a menos que sus usuarios sean miembros de la misma familia.
Ningún finlandés utilizaría una sauna para prácticas sexuales,ya que,como ellos mismos dicen,(hay lugares más apropiados).
El origen de la sauna es algo confuso.
Hay teorías que consideran posible su aparición entre los pueblos eslavos de la Europa oriental,que en sus migraciones la habrían llevado hasta el territorio hoy conocido como Finlandia.
Pero en las civilizaciones griega y romana ya existían instalaciones similares: se calcula que en el año 300 antes de Cristo había en Roma 856 baños públicos y 15 termas.
Los turcos tenían otra variante llamada Hammam donde se provocaba el sudor por medio de aire caliente,pero sin humedad ; e incluso los indios norteamericanos y aztecas han disfrutado de baños de vapor.
En un principio,la sauna finlandesa tuvo usos muy diversos.
Su caldeado ambiente era también utilizado para elaborar malta,secar el lino o curar la carne.
Asimismo,servía como sala de partos,pues era el lugar más caliente y limpio de la casa.
Así,los finlandeses pasaban en la sauna los primeros minutos de su vida... y a veces también los últimos,pues otra aplicación de la sauna era utilizarla como lugar para lavar a los muertos.
Los enfermos también solían ser trasladados al interior de la sauna para su curación.
La migración constante de estos pueblos aficionados al baño de vapor extendió la sauna por el continente,y en el año 1200 era ya conocida en toda Europa.
Hasta el año 1600 los europeos fueron grandes aficionados a la sauna,aunque con ciertas variantes en la tradición original: Por ejemplo,era práctica común que hombres y mujeres se bañasen juntos,aunque quizá para que la higiene no degenerase en bacanal,las mujeres vestían una especie de delantal y los hombres unas calzas cortas.
Para proteger la cabeza del excesivo calor,se utilizaban sombreros de paja.
(Una costumbre que actualmente ya no existe,pues en la sauna,para evitar molestias,se entra completamente desnudo.
) Tan honorables precauciones,sin embargo,sirvieron de muy poco,y las actividades dentro de la sauna fueron degenerando hasta el punto de que la salud y el bienestar pasaron a segundo plano.
Como consecuencia,los baños de vapor quedaron prohibidos en toda Europa en el siglo XVIII,con la excepción del pueblo finlandés.
La sauna primitiva era un agujero excavado en la tierra en una de cuyas esquinas se colocaba una pila de piedras.
Estas se calentaban a altas temperaturas,y se las rociaba con agua para producir el vapor.
Con el tiempo,la sauna pasó a ser una construcción de madera de un solo piso,con suelo de tierra y una estufa para calentar las piedras.
Durante el proceso de calentamiento,la estufa llenaba la sala de humo,y el bañista esperaba fuera.
Cuando las piedras estaban calientes se ventilaba la habitación,y al entrar el usuario persistía en el ambiente un olorcillo a humo que acabaría siendo un elemento típico de la sauna.
Es lo que se conocía como una sauna de humo,y aunque hoy son más frecuentes las estufas eléctricas para calentar las piedras,muchos puristas afirman que una sauna sin olor a humo no es una sauna como Dios manda.
La madera utilizada para la construcción de las paredes y bancos de la sauna suele ser de pino o abeto.
La vieja estufa de humo,conocida como kiua,es difícil de encontrar hoy día,pero las piedras siguen ahí ; deben formar siempre parte de una sauna,no sólo para producir calor,sino también para proporcionar al bañista la humedad necesaria.
Igualmente es imprescindible un buen sistema de ventilación,pues el cuerpo humano aumenta su consumo de oxígeno en la sauna hasta un veinte por ciento.
El aire debe renovarse unas seis veces cada hora,y la temperatura en el interior debe oscilar entre ochenta y cien grados centígrados.
El grado de humedad requerido es entre cuarenta y setenta gramos de vapor de agua por metro cúbico de aire.
Un dicho finlandés afirma que " En la sauna debes comportarte como en la iglesia ",y en efecto existen unas normas elementales que deben seguirse con atención.
Una sauna puede tomarse solo,en familia,entre amigos,o incluso en una reunión de negocios.
Pero la primera regla es tener tiempo para tomarla.
Las prisas y la sauna no congenian bien.
Los exploradores llaman a este fenómeno polvo de diamantes,por el centelleo y el azote que producen los cristales cuando se levanta el viento.
Pero ¿por qué ese predominio de la estructura hexagonal? Unos sostienen que se debe a los átomos de azufre en las moléculas de agua ; otros que se trata en realidad de una estructura triangular,debido a la construcción en tres átomos de la molécula de agua.
En el fondo,la estructura es hexagonal sólo en apariencia,como resultado de la unión de dos núcleos helados unidos en una fase inicial,que acaban creciendo juntos y dan copos de seis puntas,auténticos cristales siameses.
El meteorólogo Charles Knight,del National Center of Atmosphere Research - Centro Nacional de Investigación Atmosférica - de Boulder,Colorado,lleva años estudiando la relación entre la forma de los cristales y el tipo de clima y la época en que caen de las nubes.
" Para la aparición de nieve - dice Knight - son necesarios en cualquier caso núcleos helados ; núcleos en torno a los cuales se forman los cristales de nieve.
En su mayor parte son partículas de arcilla o de polvo.
Pero también pueden ser cristales de sal minúsculos,de un turbulento incendio.
U hollín,alquitrán y dióxido de azufre provenientes de la contaminación industrial o del incendio de un bosque.
Incluso las bacterias pueden - como acaba de descubrir Steven Lindow,de la Universidad de Winsconsin - llegar a provocar precipitaciones de nieve.
)) Una de las aplicaciones prácticas de la investigación de Lindow y Knight es la producción de nieve artificial con bacterias.
La nieve artificial se produce con aire a presión y gotas de agua enfriadas a - 10 ° C. Pero si se emplea la bacteria Pseudomonas Syringae,ésta produce en su membrana exterior una albúmina que favorece la formación de cristales.
Ya se emplean en la actualidad estas bacterias y consiguen que la nieve cristalice a - 3 ° C,lo que significa un ahorro enorme.
Una estación de esquí de tamaño medio necesita de cien a quinientos kilos de bacterias para convertir en una temporada cuatrocientos litros de agua en nieve artificial.
¿Existe algún peligro con las bacterias? Los científicos sostienen que no ; son tan vulnerables a los rayos gamma que no pueden propagarse expuestos a la luz.
Si usted es un fanático de la belleza de estos cristales,y quiere atrapar uno para siempre,he aquí el método de Vincent J. Schaefer,del Centro de Investigación de Ciencias de la Atmósfera de Albany,Nueva York.
Con una solución del uno al dos por ciento de polivinilo en una al 1,2 de dicloroetileno consiguió un plástico líquido que se solidificaba con rapidez.
Colocó los cristales de nieve bajo el portaobjetos del microscopio inmersos en esta mezcla y cubiertos con otra gota de la misma.
Los cristales dejaban así una impresión perfecta.
" Envueltos como un bocadillo entre las dos finas capas de plástico,los cristales se derriten poco después y desaparecen del todo.
Así queda una huella permanente en su estructura en el plástico,que se puede conservar sin problemas.
" Además,la relación empuje de los motores / peso del avión quedaría drásticamente reducida,con la consecuencia de una menor aceleración y maniobrabilidad.
Por último,almacenando mucho combustible,se gasta mucho más,precisamente por cargar con ese peso extra.
La idea de abastecer de combustible aviones en pleno vuelo no es nueva.
Los primeros ensayos se realizaron en los años veinte con objeto de establecer nuevos récords de permanencia en el aire.
Aquellos primitivos sistemas se fueron perfeccionando paulatinamente,pero siempre necesitaban el concurso de un operario que viajara en el avión receptor.
La aparición del primer caza a reacción con un solo tripulante obligó a un replanteamiento general de la cuestión.
En 1949 se probó con éxito el aprovisionamiento en vuelo mediante sonda y cono-ancla,el sistema más utilizado todavía hoy en la mayoría de las fuerzas aéreas.
En este procedimiento el avión nodriza arrastra una manguera flexible,rematada por un cono estabilizador - parecido a una pelota de badmington - con un dispositivo de anclaje en su interior.
El avión receptor despliega una sonda alargada y la introduce en el cono a una velocidad relativa de cuatro a nueve kilómetros por hora.
Ahora ya puede comenzar el trasiego de combustible.
as principales ventajas de este método son que el nodriza puede servir a dos receptores simultáneamente y que el dispositivo puede instalarse en aviones de aprovisionamiento pequeños.
El segundo sistema utilizado hoy en día,llamado flying boom o aguilón volante,fue desarrollado a principios de los años cincuenta.
En este caso todas las maniobras de acoplamiento corren a cargo del avión nodriza.
Desde una cabina en la parte inferior del fuselaje,dos operarios especializados se encargan de desplegar un aguilón telescópico rígido dotado de dos pequeños alerones estabilizadores hasta hacerlo coincidir con la abertura de entrada combustible en el avión receptor.
Una vez acoplado el aguilón,comienza el trasvase.
Tal procedimiento también tiene sus ventajas: el piloto del receptor no tiene que mantener el rumbo estable.
Los países que disponen de aviones cisterna propiamente dichos,es decir,que sólo se pueden utilizar para ese fin,pueden contar con los dedos de una mano.
Sin embargo,existen kits de reaprovisionamiento en vuelo acoplables a cualquier avión,y aviones de doble cisterna - carguero que poseen casi todos los ejércitos relativamente fuertes.
España utiliza cisternas-carguero KC - 130 para reaprovisionar a los Harrier de la Armada a los Mirage F-l del Ejército del Aire.
Imaginó la bóveda como si estuviera sostenida por arcos transversales apoyados en pilastras,entre las que pintó lunetas con los precursores de Jesucristo y gigantescos profetas y sibilas.
Sobre los capiteles de los pilares aparecen los jóvenes desnudos en violentas posturas.
Las escenas del Génesis llenan los tramos de la bóveda.
El resultado se dice que fue el producto de la simbiosis artística entre Miguel ´ngel y Julio ll,tándem que dio origen a una estrecha colaboración y a abundantes trifulcas.
Habrá de pasar un cuarto de siglo hasta que Miguel ´ngel,con 62 años,pinte el Juicio Final," stupenda meraviglia del secolo nostro ",en palabras de Vasari.
El trabajo,realizado bajo Pablo III,lo inició en 1536 y fue descubierto ante el público en 1541.
Tanto en el Juicio Final como en la bóveda,empleó la técnica del fresco.
La mayor dificultad reside en que se ha de ejecutar rápidamente,trocito a trocito,pues hay que aplicar el color antes de que se seque el revoque (capa de cal y arena).
El color penetra en el poroso revoque,uniéndose a él ; una vez seco,el color consigue toda su fuerza.
Miguel ´ngel se tuvo que enfrentar a no pocos problemas: por la mañana sólo podía ser revocada la parte del muro que le fuera a dar tiempo a pintar a lo largo del día ; el revoque sobrante debía ser retirado.
Tenía que calcular el efecto de los colores después del secado y memorizarlos,pues cambian notablemente ; los retoques debían ser evitados,pues en el revoque seco los colores adquieren otro matiz y no son tan resistentes (a pesar de todo ahora han salido a la luz algunos retoques).
Por último,Miguel ´ngel tenía que pintar las figuras torcidas,pues,por las curvaturas del techo,una figura normal parecería desde el suelo horriblemente deformada.
Los frescos se han mantenido en muy buen estado,hay pocas grietas y las sedimentaciones de cal producidas por el agua se pueden quitar sin notable daño.
¿Por qué entonces se planteó una restauración? En primer lugar,el Vaticano contaba con la oferta de una cadena de televisión japonesa,la Nippon Television Network,que aportaba los 450 millones de pesetas necesarios para el rejuvenecimiento de los muros: 1.200 metros cuadrados,aunque bien conservados,no exentos de corrosión.
A cambio de esto,la NTN se reservaba en exclusiva todos los derechos de reproducción de imágenes de las fases de restauración.
Luego se planteó la polémica sobre si ésta era o no conveniente ; una inexperta y no suficientemente cuidadosa restauración podría estropear para siempre una obra de arte.
e dio el paso,y por estos motivos: en primer lugar,los frescos eran ya difícilmente reconocibles por la suciedad acumulada ; el hollín de millones de velas había oscurecido la pintura ; la grasa de las velas y el incienso se añadían al velo de suciedad,al igual que los vahos de la respiración de los visitantes.
Por último,la mella de la climatología.
Aún había más: al limpiar el polvo que se realizó en los anos treinta se vio que un conservador del siglo XVIII había aplicado una capa protectora de cola animal,acelerando el oscurecimiento de los frescos.
El último motivo que puso en marcha a los hombres de la limpieza del Vaticano fue el descubrimiento de un nuevo producto limpiador.
El AB 57 es una misteriosa mezcla del Instituto Central de Restauración italiano que,entre otras cosas,tiene bicarbonato sódico,bicarbonato amónico,DesogenantihongosycarN.
Todo esto va diluido en agua destilada.
Según los defensores de la restauración,el mágico AB 57 quita la suciedad,pátina y cola animal sin afectar apenas a los colores.
Pierre Giorgio Bonetti y Mauricio Rossi son los hombres de la limpieza química,los restauradores que,con un pincel,van aplicando el gelatinoso limpiador de pocos en pocos centímetros cuadrados.
Luego dejan que actúe durante tres minutos y lo retiran con una esponja impregnada en agua destilada.
Esperan veinticuatro horas y repiten la operación.
El profesor Colalucci,restaurador jefe del Vaticano,supervisa la labor.
Los trabajos de restauración se iniciaron en dos frescos de la antesala de la capilla.
" Fue maravilloso ver aparecer las antiguas pinturas " - comenta Gian Luigi ¿Cuáles fueron los conocimientos que Auguste Piccard obtuvo durante sus viajes en globo acerca de la naturaleza de la atmósfera? Por lo pronto,comprobó que la temperatura descendía aproximadamente seis grados por kilómetro de altura.
Al observar las formaciones nubosas,constató que ninguna nube sobrepasaba una determinada altura,de lo que concluyó que existe un punto en el que se agota la fuerza ascensional para el aire húmedo.
Según sus observaciones,por encima de los doce mil metros no existían movimientos verticales de aire.
Lo que observara Piccard en aquel entonces desde su cápsula de aluminio ha sido comprobado en repetidas ocasiones.
En la denominada troposfera,es decir la capa atmosférica que va hasta una altura de 12.000 metros,tienen lugar todos los fenómenos relacionados con el tiempo: ahi se producen los ciclones y los anticiclones,se forman las nubes para luego desvanecerse,van y vienen los vientos.
Pero por encima de esta capa,el tiempo atmosférico es siempre igual.
La capa llamada estratosfera llega hasta,aproximadamente,cincuenta kilómetros de altura.
En ella no hay nubes ni tienen lugar movimientos verticales de masas de aire.
Hoy día se sabe que en la atmósfera media ocurren cantidad de fenómenos interesantísimos.
Por ejemplo,la temperatura,que había descendido en la troposfera hasta,aproximadamente,60 ° centígrados bajo cero,vuelve a subir hasta alcanzar de nuevo,a una altura de cincuenta kilómetros,los 0 ° centígrados.
Además de los vientos que mueven las masas de aire en direcciones horizontales,aparecen también ondas de distintas clases.
Las más conocidas son las ondas.
acústicas,con una frecuencia de oscilación de pocas milésimas de segundo.
De naturaleza algo distinta son las ondas de gravedad,cuya extensión es influida principalmente por la gravitación de la Tierra.
Su frecuencia de oscilación varia entre cinco minutos y muchas horas,y son importantes portadores de energía (en cualquier caso,no tienen nada que ver con las ondas de gravitación de Einstein).
¿Cómo se ha llegado a averiguar todas estas cosas? Existen,en principio,dos métodos diferentes para conseguir información sobre determinadas zonas de la atmósfera.
Uno de ellos consiste en medir distintos valores in situ,mientras que en el segundo,los valores se estudian a distancia.
En la práctica se utilizan ambos métodos.
En el primer caso,los instrumentos de medición deben ser transportados de alguna manera hasta el lugar en que tienen lugar los diferentes fenómenos a estudiar,con ayuda,por ejemplo,de globos o cohetes.
No obstante,estas mediciones sólo proporcionan una imagen momentánea de los fenómenos que ocurren en una determinada zona de la atmósfera.
Las ondas electromagnéticas generadas por la emisora y concentradas por la antena son enviadas en una determinada dirección.
En su recorrido a través de la atmósfera,una pequeña parte de las ondas emitidas se refractan o reflejan a causa de las irregularidades existentes,por ejemplo,ondas de gravedad o acústicas.
Esta parte de emisión reflectada es registrada por un receptor de alta sensibilidad,que proporciona informaciones acerca de la zona en que tienen lugar las citadas irregularidades y también sobre la velocidad de las mismas.
Pero el viento,las ondas y el tiempo atmosférico no es lo único relevante.
Al me nos de la misma importancia son los procesos químicos que tienen lugar en las capas superiores,pues,gracias a ellos,es posible la existencia de vida en la Tierra.
Nuestro planeta se halla rodeado,a una altura de quince a treinta kilómetros,por una capa de ozono de vital significado para nosotros por dos diferentes razones.
Por una parte,dicho envoltorio gaseoso actúa como unas gigantescas gafas de sol que filtran la peligrosa luz de los rayos ultravioleta.
Además,como consecuencia de esta función absorbente,la capa se calienta,impidiendo de ese modo que descienda en exceso la temperatura de la superficie terrestre.
El ozono es un alótropo del oxígeno,es decir,se trata del mismo elemento pero bajo otra forma.
Mientras que el oxígeno que respiramos tiene dos moléculas,el ozono tiene tres (03).
Las moléculas de ozono se originan,a partir del oxígeno,por efecto de la radiación ultravioleta del Sol.
El ozono se disgrega cuando existen determinados catalizadores químicos,como,por ejemplo,óxidos ozoicos o hidrógeno,volviendo de nuevo a su estado natural,el oxígeno.
En el transcurso de los milenios,el equilibrio entre las moléculas de ozono generadas y las disgregadas se ha mantenido,por lo que la concentración de ozono es siempre la misma.
Pero resulta que recientemente,hace sólo unos diez años,se ha descubierto que existe un catalizador no natural para la destrucción del ozono: los hidrocarburos de flúor y cloro,que,aparte de en ciertos procesos químicos,se utilizan como gas impulsor en los sprays.
A nivel mundial se liberan al aire unas 600.000 toneladas por día de estos productos.
Los ciudadanos se inquietan: ¿significa eso que estamos deteriorando poco a poco la capa de ozono,dejando penetrar así la mortal radiación ultravioleta del Sol hasta la superficie terrestre? Aunque nadie ha conseguido ni comprobar ni desechar esta hipótesis,lo que sí es un hecho comprobado es que sobre la Antártida se ha abierto en años recientes un gigantesco agujero en la capa de ozono.
Por otra parte,los químicos han averiguado que el dióxido de carbono,en colaboración con otras sustancias que contaminan la atmósfera,provoca una serie de reacciones que llegarían a reparar la famosa ozonosfera.
No obstante,no es posible hacer afirmaciones definitivas acerca de estos complejos procesos.
Hace falta continuar estudiando la estratosfera con la mayor exactitud y durante un tiempo prolongado.
Ya hemos visto que en las capas inferiores de la atmósfera los procesos mecánicos son los que tienen mayor importancia,mientras que en las capas medias serían las aleaciones químicas las de mayor relevancia.
Sin embargo,en las capas superiores el primer plano lo ocupa un fenómeno bien distinto.
Aquí,las moléculas de gas se encuentran casi totalmente disgregadas en iones y electrones,dada la alta radiación energética.
Por ello,esta capa recibe el nombre de ionosfera.
La información es todavía digital,en forma de valores numéricos.
Hay que efectuar ahora su transformación en valores analógicos en el reproductor de compact disc,porque los amplificadores de nuestras instalaciones domésticas de alta fidelidad suelen trabajar con tecnología analógica.
La transformación digital - analógica sigue un proceso similar,aunque con signos previos invertidos,al de la analógico - digital.
Pequeños rectángulos,que parecen sostener como columnas la bóveda 11 de las ondas,forman una vibración continuada,con senos y crestas.
Una ley acústica dice: para detectar una vibración,la velocidad de exploración debe ser el doble que la de vibración de la onda.
En la exploración digital se - producen 44.100 vibraciones por segundo,suficientes para detectar los más finos matices del sonido.
Incluso los tonos más altos de la escala,audibles para el ser humano,tienen una vibración que sólo llega a la mitad de dicha velocidad de exploración.
Pues bien,cada una de estas vibraciones recibe un número propio que debe ser codificado digitalmente a través de impulsos de corriente.
Dicha codificación corresponde a unas claves especialmente creadas para el sistema CD.
Este lee los signos resultantes de la longitud de los pits,descifra la codificación digital y detecta simultáneamente los errores dé transmisión,corrigiéndolos mediante funciones automáticas de cálculo.
La corrección de errores es una diferencia esencial respecto a los discos tradicionales.
Y lo mismo sucede con el mecanismo de giro.
Un plato convencional puede girar más de prisa o más despacio,alterando sustancialmente la reproducción del sonido.
Eso es impensable en un CD: su mecanismo de giro no conoce oscilaciones porque el ritmo del sistema electrónico proporciona un flujo de datos uniforme que se mantiene con exactitud.
Después de este recorrido por las entrañas del CD,queda claro que este revolucionario sistema de reproducción del sonido tiene muy poco que ver con los tocadiscos y discos convencionales.
El compact disc,sin embargo,no es un invento reciente (MUY 25).
Su diseño original se remonta a 1972 y la casa Philips - inventora del sistema - no lo comercializó en todo el mundo hasta 1982.
Tras la adopción de un estándar único por todos los grandes fabricantes mundiales,se puede decir que la tecnología CD ha reanimado espectacular mente el mercado de la música grabada siendo el año que acaba de termina - 1986 - el de consolidación definitiva de sistema.
La competencia entre Philips! las grandes marcas japonesas - Matsushita y Sony - se plantea ahora en el terreno de las prestaciones accesorias: programación,reducción de tamaños,sistemas portátiles... pero el aficionado a la música acepta el sistema CD como el más perfecto que existe hoy día de cara a la reproducción sonora.
Y quienes lo escuchan por primera vez sucumben instantáneamente a sus encantos.
Sólo para el año 1987 se prevé un aumento de la producción de discos CD hasta 450 millones de unidades en todo el mundo (en 1986 fue de 200 millones).
Hasta ahora se calcula que el sistema de rayo láser sólo ha podido sustituir la vieja aguja reproductora en tres de cada cien hogares de la CEE (un sistema nuevo no se considera definitivamente implantado en el sector de la música hasta que no alcanza una cuota de penetración del seis por ciento).
Parece claro,sin embargo que ambos sistemas están condenados coexistir durante las próximas décadas.
pesar de que las grandes casas discográficas ya lanzan todas sus grabaciones en versión LP,CD y cassette,el alto precio de los compact disc - que disminuye de año en año - los mantiene todavía en un nivel de expansión que puede calificarse de minoritario.
Una cosa es segura: en el campo de tecnología audio,el CD representa un hito.
La calidad de sonido que consigue es más parecido a la perfección que los ingenieros de audio han logrado fabricar has la fecha.
Un auténtico placer para el oído.
UN VIENTO HELADO,QUE SE filtra por los miles de huecos del montacargas,acompaña al grupo de científicos hasta el interior del hoyo.
Unos segundos más y ya habrán llegado.
Es un momento tenso.
El montacargas se detiene.
Ninguno se atreve a pronunciar palabra.
Intentan contener la respiración.
Nadie del grupo había estado antes allí,a pesar de haber trabajado durante años en el experimento.
En sus miradas se intuye una mezcla de orgullo y consternación.
No es para menos.
En el centro de la caverna,se eleva un octógono de acero de gran tamaño que parece mirarles con sus inmensas puertas laterales abiertas de par en par.
Es un electroimán de 8.500 toneladas - - con más metal en sus entrañas que toda la torre Eiffel - - envuelto en una red de centenares de kilómetros de cables.
Pero lo que mas sobrecoge a los científicos es lo que no se puede ver.
A pesar de su tamaño,el imán sólo es la cubierta,el envoltorio del L3.
" Cuando por primera vez extendimos los planos sobre la mesa para estudiar a fondo el proyecto - - confiesa Ting - - supe que sus dimensiones iban a ser mayúsculas ".
Los que le conocen afirman que la voz de Ting normalmente es monótona y sin aristas,incluso cuando se ve obligado a dar una reprimenda.
Ahora,sin embargo,vacila ante el periodista: " Pero hasta que no lo ves - continúa - no te das cuenta de cuán... grande... realmente es... Bajé aquí hace un par de días y pensé.
. es una locura.
No...,no puede ser de este tamaño.
Y entonces me percaté - - comenta sonriente - - de que otros experimentos por ahí son prácticamente de la misma envergadura y están buscando lo mismo que nosotros... la masa.
" Los profanos en la materia confundimos habitualmente el peso con la masa.
Pero son dos conceptos bien distintos.
Coja un ladrillo.
Lo primero que notará es que pesa.
Si lo arroja con fuerza hacia alguien,lo puede herir.
Ahora,machaque el ladrillo con un martillo y recoja todos los trozos para pesarlos en una báscula.
Descubrirá que son tan pesados como el ladrillo original.
Aunque su forma sea diferente,su sustancia no cambia.
Imagínese que se traslada al espacio exterior con otro ladrillo.
La primera cosa que notará es que ya no es tan pesado.
Si su imaginación le ha llevado hasta la Luna,le costará seis veces menos mantenerlo entre sus manos.
Parece tan inofensivo como una pluma.
Pero no se fíe de las apariencias.
Ni se le pase por la cabeza arrojárselo a alguien,porque este ladrillo light todavía es un peligro en potencia.
Puede no pesar nada,pero... uf!... continúa ahí.
Sigue teniendo volumen y sustancia.
Y no ha perdido ni pizca de su masa.
Mientras que el peso puede variar,la masa permanece inmutable.
El peso es la medida de la fuerza de la gravedad.
Si decide hacer una dieta porque los michelines empiezan a deteriorar su línea y la báscula del baño roza los noventa kilos,sepa que este número no es más que un modo de indicar cuánto la Tierra,cariñosamente,le presiona sobre la báscula.
Si lo que le obsesiona son esos kilos de más,no hace falta someterse a una sufrida dieta... ¡Váyase a vivir a la Luna! Nada más llegar habrá adelgazado 75 kilos,aunque para su desconsuelo los michelines seguirán en su sitio.
Los científicos saben mucho más acerca del peso que de la masa.
Hasta hace bien poco no tenían ni la menor pista sobre su origen.
Las cosas han cambiado,o por lo menos se intenta que cambien.
Por fin,el pasado mes de agosto,el colosal L3 se puso en marcha.
La carrera para atrapar la partícula de Higgs daba comienzo.
Una cacería que se ha convertido en una de las mayores y más costosas en la historia de la ciencia.
A ambos lados del Atlántico,centenares de científicos y técnicos han trabajado codo a codo durante toda una década para poner a punto el L3,la máquina que será capaz de detectar los bosones de Higgs.
Ya llevan gastados más de 70.000 millones de pesetas y no han hecho más que empezar En su construcción han participado 39 instituciones de toda Europa,EEUU URSS,China,India y Japón.
El equipo de 450 físicos del L3 es uno de los seis que hoy,en todo el mundo,esperan resolver el misterio de la masa.
Pero,¿por qué este interés repentino por encontrar el bosón de Higgs? Sus aplicaciones,en el caso de que se lograra hallarlo,son absolutamente desconocidas.
Aunque claro,eso pasa siempre al principio.
Curiosamente,los grandes descubrimientos en física parecen a primera vista de poca utilidad.
Hoy,el electromagnetismo alimenta nuestras bombillas,computadoras y refrigeradores.
En el siglo pasado parecía tan inútil e inservible que el laboratorio de James Clerk Maxwell - - el primer hombre que reveló los secretos de esta fuerza - - casi cierra por problemas económicos.
" ¿Cuál es el uso de la electricidad? ",preguntó el primer ministro Israeli.
" Señor,algún día - - contestó Maxwell - - lo comprobará ".
¡Si aquel gobernante levantara la cabeza!... No hay ninguna razón para pensar que la comprensión de la masa vaya a ser diferente.
LA BSQUEDA DE LA MASA Comenzó - - al menos indirectamente - - durante los años 50,cuando los físicos,incluido el teórico Peter Higgs,empezaron a unificar teorías.
Se empeñaron en demostrar que las cuatro fuerzas fundamentales en la naturaleza - - la atracción gravitatoria,la interacción electromagnética,la interacción fuerte y la débil - - eran diferentes manifestaciones de una sola.
Cada una de ellas cuenta con un agente invisible,una partícula subatómica,que provoca los efectos que nosotros vemos.
Son los llamados,de forma genérica,bosones gauge.
Los fotones,por ejemplo,transportan la fuerza electromagnética en forma de ondas de radio,de luz,de rayos X,gamma... Otros,los mesones,rigen la interacción fuerte que mantiene unidos los núcleos de un átomo.
Los científicos aún no han visto gravitones,las hipotéticas partículas que transportarían la gravedad.
Y hasta hace bien poco,ningún físico se había topado con un bosón W y Z,que rigen la interacción débil,una misteriosa fuerza crucial en el proceso del deterioro de la radiactividad.
Higgs,hombre tímido,poco hablador y excesivamente modesto,trató de imaginar qué es lo que podría ocurrir si un tipo especial de campo - - uno que él mismo se había imaginado - - existiera en la realidad.
Un campo es una región en el espacio que está sometido a la influencia de una fuerza.
Sin embargo,como sucede con todos los seres,conforme el cableado - - o sistema nervioso - - se complica,la conducta se hace menos rígida,ya que el edificio puede seleccionar una acción entre varias posibilidades.
Razón por la cual se aplica la inteligencia artificial o los sistemas expertos a las grandes edificaciones.
Pero no sólo por su actividad nerviosa puede ser considerado como un ser vivo.
Bajo control cerebral,el edificio inteligente debe poseer un sistema circulatorio por el cuál,en vez de sangre,transita el agua de la calefacción ; un aparato respiratorio formado por todos los conductos de aire acondicionado ; una actividad muscular que permite abrir o cerrar válvulas u otros mecanismos ; así como una especie de sentidos,formados por sensores y controladores que permiten al ordenador conocer en todo momento la situación del edificio.
Esta analogía no es más que la aplicación de la alta tecnología electrónica a la arquitectura,una idea que comenzó a tomar cuerpo a principio de los ochenta en Estados Unidos,cuando se abarató la técnica informática.
Tanto en Estados Unidos como en Japón se denomina smart building al edificio de oficinas que incorpora esta tecnología ; smart house,cuando se refiere al hogar.
Los franceses acuñaron la palabra domótica,concepto que integraba la informática y la construcción de viviendas.
En España,se ha impuesto el término edificio inteligente,aunque también es corriente el uso de domótica referida a la casa unifamiliar.
(Ver recuadro).
El edificio inteligente está emparentado con una corriente arquitectónica que se conoce con el nombre de high-tech y que se caracteriza por el muro cortina,fachadas con una escueta piel de vidrio y aluminio.
Otro elemento característico sería el falso techo relleno de instalaciones por donde circulan todo tipo de cables y conductos de aireación.
Aunque en España el edificio high-tech tuvo su máxima expresión hace algunos años,en la actualidad está dejando paso a un movimiento paneuropeo llamado neotradicional,que utiliza otra vez materiales clásicos como el hormigón y el acero,pero que los combina con otros modernos gracias a sofisticados métodos de fijación.
El ejemplo más elocuente de esta tendencia tradicionalista es el nuevo edificio IBM-España o Casa Rosada,como ya la conocen popularmente los madrileños.
Pero en todo el mundo sigue floreciendo el estilo high-tech.
Los ejemplos más conocidos son el Beaubourg o centro Georges Pompidou en Paris y el Lloyd's Bank de Londres.
O el ya mitificado Hong Kong Bank de la colonia británica diseñado por Norman Foster,máximo exponente de este movimiento y que,además,es autor de proyectos españoles como el metro de Bilbao o la Torre de Comunicaciones de Barcelona.
Entre las anécdotas más destacables de este banco está la de recurrir a un geomántico chino para salvaguardar la tradición del fungshui en los edificios de Hong Kong.
Esta técnica,entre ocultista y pintoresca,confiere a las construcciones propiedades mágicas como " buenas " o " malas vibraciones ".
Entre otras extravagancias,el geomante o fungshuiman recomendó que los dos leones de bronce que guardaban la entrada del antiguo edificio fueran levantados al mismo tiempo y se colocaran en la misma posición en las nuevas oficinas,al concluirse años después sin que en ningún momento se enfrentaran su miradas.
Esta concesión a la cultura tradicional le ha valido al edificio el beneplácito de la sociedad china,que asegura que el rascacielos de Foster tendrá éxito porque tiene un buen fungshui.
Pero los edificios inteligentes son aún una minoría.
Es posible que no pasen de cien en todo el mundo.
Sin embargo,es un mercado en expansión que los prospectores cifran en siete mil millones de dólares para 1990 y doce mil millones para 1995.
Como casi todo tipo de previsiones en alta tecnología,éstas se quedarán cortas.
La demanda supera con creces la oferta.
Del centenar construido,Estados Unidos se lleva la palma en cuanto a ventas,con un sesenta por ciento de este mercado,mientras que Japón el trece y Europa el 27 por ciento.
El negocio está servido.
La compañía telefónica japonesa tiene en plantilla un equipo de al menos doscientos arquitectos,cuya única misión es pensar e idear nuevas neuronas y prestaciones para estos edificios.
En ese comercio participan importantes empresas niponas como NEC,Fujitsu e Hitachi.
Calculan que en torno a los edificios inteligentes se moverán 300.000 millones de dólares en diez años.
Las definiciones de edificio inteligente son variadas.
Para algunos son simplemente edificios equipados con electrónica de alta tecnología.
Esta es la tesis que sigue la National Academy of Sciences,en Washington,para decir que es un " edificio realzado electrónicamente " (electronically enhanced).
Según Francisco Linares,director completamente informen las necesidades.
el técnico del Proyecto del Nuevo Edificio IBM,una construcción inteligente " es aquella cuya gestión de servicios está totalmente informatizada y cuyos,usuarios tienen a su disposición un sistema completo e integrado de comunicaciones tanto en el exterior como en el interior.
" El nuevo cuartel general en España del | Gigante Azul,como se conoce en ambientes financieros a la multinacional de informática IBM,es el edificio más inteligente de nuestro país.
Un recorrido por él nos servirá para comprender la eficacia de esta tecnología electrónica.
El inmueble,un rascacielos tumbado según la expresión de uno de sus directivos,esta proyectado para que resuelva Inteligentemente tres funciones: una de servicios como la regulación y control del aire acondicionado,calefacción,iluminación,ascensores,seguridad,etcétera.
Otra,la que se refiere a la propia gestión del edificio,como por ejemplo los aspectos de mantenimiento preventivo o la planimetría - - absolutamente informatizada - - que dispone de una base de información para modificar la infraestructura según las necesidades.
Para facilitar esta flexibilidad,todos los tabiques son de pladur o cartón-yeso.
De esta manera,una nave compartimentada en despachos puede hacerse diáfana,si así lo requiriera la gestión del local.
Y la tercera función sería la ofimática,que soporta,sobre todo,el correo electrónico.
" Las ventajas son puntuales para los empleados ",asegura Francisco Linares.
" Ya nadie escribe notas a máquina o las dicta a una secretaria para mandarlas de un departamento a otro vía botones.
Se envían a la persona vía pantalla,quien luego se la encontrará al encender la suya.
Cómo y cuándo hacerlo es lo mas difícil,lo verdaderamente complicado.
A lo largo de la evolución,y casi a golpe de martillo,los seres vivos - - y en especial todos aquellos que soportan el continuo azote de un depredador - - han diseñado infinidad de estrategias para evitar al adversario.
Se han convertido en auténticos profesionales de la picardía,el engaño y el timo.
Muchos animales reaccionan ante situaciones de peligro y tensión de forma aparentemente agresiva.
Pero en la mayor parte de los casos no es más que una artimaña para asustar o mantener a raya al enemigo.
Es lo que algunos científicos llaman una demostración de amenaza o bluff,un amago de intimidación que jamás llega a cumplirse.
os que recurren al bluff son,nada más ni nada menos,que inimitables actores en el   escenario natural cotidiano.
Una farsa,que,según los biólogos,viene justificada por la exaltación de un fenómeno relativamente frecuente en la naturaleza,el mimetismo (ver MUY 96).
En unas ocasiones simulando otras personalidades,y - - en otras - - adoptando rasgos extraordinariamente vistosos que les deforman.
Una mariposa vestida de negro y blanco descansa despreocupada sobre la corteza de un árbol.
Se trata de una mariposa gitana,inquilino frecuente en nuestros bosques.
Ha notado la presencia de un temible depredador.
Sin perder un instante despliega sus alas superiores para dejar a la vista otras más pequeñas coloreadas de rojo intenso.
El atacante,como hipnotizado,fija su mirada en tan vivo color y,sin dudar un instante,se avalanza hacia el.
La mariposa ha logrado llevar al enemigo a su terreno Por esta vez se ha librado de ser digerida en el estómago de un desconocido.
Aunque ha perdido la pequeña ala derecha,sus partes vitales como la cabeza,el tórax y las mismísimas alas superiores han salido bien paradas.
Un color ha salvado la vida a esta mariposa.
Es lo que el etólogo Wolfgang Wickler llama defensa cromática pasiva.
Muchos animales encuentran un refugio seguro envolviéndose en colores.
Una forma de inducir en los enemigos una sensación de rechazo,quizás queriendo advertir de un peligro latente,aparentando - - en multitud de ocasiones - - lo que no se es.
Así,la agresiva y falsa serpiente de coral rayada (Scathiodontophis venustissus) se disfraza con los colores y dibujos de su compañera y respetada serpiente de coral rayada (Micurus nigrociatus),altamente venenosa.
Algunos aningidos - - esbeltas aves tropicales de cuello largo y delgado,y pico largo y recto - - imitan como nadie el movimiento de las serpientes.
Una de ellas es la aninga.
Ante Y el ataque de un enemigo - - en vez de huir - - comienza,tranquilamente,a extender su interminable cuello,a la vez que lo contornea simulando ser una terrible serpiente.
Otra forma de disuadir al enemigo es hinchando o desplegando alguna parte del cuerpo.
La tarántula agiganta el cuerpo gracias a su forro peludo,el pez fugu mostrando su espinosa piel inflada,el clamidosaurio desplegando su intimidatorio collar,la cobra extendiendo su capucha.
. ¿Quién no ha visto a un gato enfurecido erizando el pelo o - - ya en menos ocasiones - - a un sapo intimidándonos con sus pulmones abultados como un globo a punto de estallar? Numerosos lagartos exóticos y aves adoptan esta actitud.
Cuando los polluelos del búho real se sienten amenazados,ahuecan el plumaje y literalmente lo inflan,logrando duplicar o triplicar su volumen.
Hay que ser muy osado para enfrentarse a esas pelotas monstruosas que miran desafiantes.
Aumentar de tamaño es,sin lugar a dudas,una forma efectiva de intimidar al adversario.
Mayor peligro para el atacante revisten los animales que se defienden mediante alguna sustancia química.
Tienen para hacer huir al agresor.
Aguijón,cuernos,substancias corrosivas... ES el recurso más cómodo y arriesgado,abandonándose a la suerte.
Se precisa ser muy buen actor y contar con unos nervios de acero.
Esto explica la creencia popular de que " la víbora pica después de muerta ".
Algunas serpientes,como la culebra de collar (Natrix natrix),fingen estar muertas si Su farol - - imitan como nadie a las venenosas víboras - - no ha resultado efectivo.
Para que el adversario pierda interés - - ya que a muchos no le apetece llevarse a la boca algo que está muerto - - se tienden inertes en el suelo,inmóviles,boquiabiertas y con la lengua colgando.
Lo mismo le ocurre al curioso oposum,un animal perteneciente a la familia de las zariguellas,que cuando se ve inmerso en una situación limite de peligro se desploma al suelo como lo haría una damisela victima de un repentino desmayo.
Un notable ejemplo de pasividad absoluta es el erizo,un insectívoro de torpes y lentos movimientos que decide arroparse en Su coraza protectora cuando es molestado por un zorro,un ave de presa o el hombre.
Sus 16.000 agujas córneas - - espinas de 2 a 2,5 centímetros de longitud - - están perfectamente afiladas y entrelazadas le convierte en una muralla infranqueable.
En la naturaleza existen multitud de ejemplo de animales que han decidido defenderse de forma pasiva,protegiendo su cuerpo con armas afiladas.
Algunas de ellas llegan a tener medio metro de longitud,como las que escudan a los puercoespines.
Los erizos de mar resultan bastante peligrosos para aquel que les pise descuidadamente.
Lagartos del género Cordylus y Phrynosoma se libran de una muerte segura,cuando SuS espinas se clavan en la garganta de un depredador al intentar tragárselos.
O los peces equipados con púas.
El cielo Impone.
Cuando el publico que abarrota la sala de proyecciones del Planetario de Madrid se sienta,y comienzan a bailar por el firmamento simulado las primeras estrellas,el silencio es unánime.
No hay ojos para ver tanta belleza.
Hasta los niños,tan alborotadores ellos,enmudecen.
La cúpula tiene tan sólo 17,5 metros de diámetro,pero son suficientes para que el extraño proyector,situado en el centro de la sala,lance contra el techo abovedado nada menos que 9.000 fulgurantes estrellas.
Desde la alfombra terrestre únicamente alcanzamos a ver las mas brillantes,las de primera,segunda y hasta de sexta magnitud,pero cuando los telescopios se aproximan mas allá,el espectáculo celeste es otro.
Hay tantas que marea.
Las dimensiones del cosmos,vistas desde aquí,son tan enormes como la propia imaginación y cubren esa sospecha,a veces aterradora,de que no estamos solos,que no somos los únicos.
Tres años acaba de cumplir el Planetario de Madrid.
Desde su inauguración,cerca de 900.000 personas - mayoritariamente chavales - - han aprendido a situar las estrellas Vega y Arturo,la Vía Láctea,las constelaciones,los planetas y el cinturón de asteroides en el mapa ce leste.
Es uno de sus méritos.
Pero en España hay más planetarios: uno,futuro,en Castellón,dos en Barcelona,y otro mas en La Coruña,aunque son pequeños en comparación con éste y con el que se construirá próximamente en Pamplona.
Estas instalaciones son el resultado de la curiosidad que ha tenido siempre el hombre para resolver y explicar el enigma que mueve la maquinaria de los días y las noches,de los planetas y sus satélites.
Los primeros astrónomos o vigilantes del universo vieron dibujados en la fisonomía de las estrellas los rasgos de sus mitos y dioses.
A Arquimedes se le adjudica la creación del primer planetario.
Y al emperador Adriano,ya en el siglo ll,el segundo,del que se tienen noticias por viejas crónicas.
Pero el primero que reprodujo fielmente una noche estrellada en una cúpula pintada de blanco - a modo de pantalla - fue el ingeniero alemán Bauersfeld.
Lo hizo cuando los hermanos Lumiere ya habían dado el pistoletazo de salida a la poderosa industria cinematográfica.
Corría el año 1923,y en tan sólo dos años 50.000 personas fueron a contemplar aquel insólito invento.
Una muestra mas de que la tortícolis no ha sido un enemigo importante de ese vicio humano y prehistórico de mirar hacia arriba para ver el infinito.
Bauersfeld utilizó una esfera de cincuenta centímetros que guardaba en su interior una lámpara de tungsteno de 200 watios y 31 proyectores cónicos.
Estos con tenían un campo de estrella hexagonal o pentagonal.
Junto a la esfera,el ingeniero situó los proyectores de los planetas.
Su sistema contaba asimismo con dos tipos de giro: el polar y uno perpendicular a la ecliptica,lo que le permitía mostrar la evolución de los planetas a lo largo del año.
Hoy día,los planetarios del mundo presentan espectáculos dignos de la fantasía de Steven Spielberg o George Lucas.
nicamente la técnica puede poner limites a su imaginación.
Pero la técnica actual da mucho de si.
Por ejemplo,permite seguir la estela fugaz de los cometas,el desplazamiento súbito de los meteoritos o los apacibles eclipses.
" La gente quiere saber y se le deben explicar todas las noticias que depara la actualidad ; quieren ver en su planetario todos los acontecimientos estela res.
Quieren saber qué es una estrella y cómo se produce una supernova.
Le fascinan cosas cómo los agujeros negros o las tormentas de estrellas fugaces.
La sonda espacial Voyager acaba de llegar a Neptuno y también quieren saber cómo se ven las cosas desde allí ",señala Asunción Sánchez,directora del Planetario madrileño.
No es fácil recrear estos y otros efectos ópticos sobre el techo abovedado.
Cualquiera de los espectáculos audiovisuales necesita de tres a seis meses antes de su puesta de largo.
Además,los planetarios,hablando ya del mundo,suelen producir y mostrar programas sobre temas variados: viajes al interior del átomo,excursiones al momento mágico y sobrenatural del bigbang,erupciones volcánicas,vida extraterrestre,etcétera.
Todo con el realismo y la proximidad que alcanzan los más modernos ordenadores.
¿Cómo se consigue? Detrás del espectáculo hay equipos de físicos,astrofísicos,ópticos,informáticos,fotógrafos,ilustrado res,escritores,músicos y actores.
Esto en cuanto a la materia gris,porque en cuanto a tecnología... El Planetario,que surgió bajo los auspicios del alcalde Enrique Tierno Galván,cuenta con un proyector central modelo RFP DP3 Space Master - - de la empresa germano oriental Carl Zeiss Jena - que logra recrear el cielo desde cualquier punto de la Tierra,en el pasado,en el presente o en el futuro.
El equipo también dispone de 61 proyectores adicionales para diapositivas y otros 30 para efectos especiales.
Todos ellos están controlados por un ordenador central,que se halla conectado a su vez a dos más que manipulan el sonido y las diapositivas.
Algunas imágenes,las de tipo panorama,no pueden proyectarse de frente sino sólo de lado,en sentido oblicuo,de manera que los técnicos se ven obligados a distorsionar previamente las fotografías para conseguir los efectos deseados.
Es un proceso complejo,al igual que lo es el de los efectos especiales,que se consiguen,sobre todo,por medio de la luz.
Con pequeños trucos luminosos se pueden reproducir con realismo explosiones cósmicas y nebulosas en expansión.
El movimiento de las estrellas y cometas lo realiza un grupo de espejos móviles automatizados sobre los que se proyectan las imágenes,y con zooms motorizados que logran transmitir la sensación de velocidad y de viaje.
Los espectadores que asisten al paso siempre espectacular de un cometa no saben,por ejemplo,que se necesitan al menos tres proyectores para conseguir que la bola inyectada en gas se mueva en el vacío.
El corazón de los también llamados teatros espaciales es el proyector central ; y el cerebro,obviamente,el ordenador que convierte la cadena de transparencias en una película sin fin.
Su esencia,la materia prima,es la diapositiva,pero una lluvia de nuevos sistemas amenaza con acaparar,al menos,parte de su reino.
Entre ellos,el videoláser es el que apunta mas aptitudes.
Asunción Sánchez quiere traerse dos o tres de estos proyectores: " Las técnicas de videoláser van a permitir diseñar efectos especiales por ordenador y almacenarlos en un disco.
Instalando dos o tres proyectores en la cúpula,ahorras muchas instalaciones de trucaje de generaciones anteriores.
" Con el videolaser basta cambiar el disco,aprovechando la velocidad de los ordenadores,para utilizar tal o cual efecto que se encuentre almacenado en la memoria.
podrán conseguir efectos tales como ver la Tierra pequeñita y de repente gigante,que una estrella explote,que una galaxia empiece a girar y además se lance materia hacia fuera,y otras cosas de ese estilo.
" Otro sistema,todavía rudimentario,pero que puede suponer un salto cuantitativo en este sentido,es el conocido por Digestón Se trata de un aparato que proyecta sobre la cúpula,por medio de un ojo de pez,gráficos generados por ordenador.
Resulta ideal para crear simulaciones.
Supongamos que nos encontramos en la orilla del lago.
Pero no hace falta recurrir a complicadas ecuaciones para imaginarse cómo una masa puede doblar un rayo de luz que pase a su lado.
Nos bastará con realizar un experimento imaginario que respete los postulados del Principio de la Equivalencia.
En primer lugar necesitamos dos laboratorios con paredes de cristal situados en el Sol.
En cada uno de ellos deberá situarse un observador que conozca bien el Principio de la Equivalencia,el cual determina que los procesos físicos que tienen lugar en un sistema de referencia en caída libre se desarrollan exactamente del mismo modo que si no existiera ninguna gravitación.
Nuestros laboratorios deberán tener un par de propiedades especiales.
Su construcción ha de ser perfecta,con los ángulos rectos en todas las esquinas y sus paredes exactamente paralelas entre sí.
Estas condiciones habrán de mantenerse iguales a lo largo de todo el experimento,por lo que necesitamos una guía que funcione sin ninguna fricción y que proporcione libertad de movimiento vertical a los laboratorios,pero manteniéndolos siempre paralelos.
Una vez que los observadores han entrado en sus respectivos laboratorios,los lanzamos desde la superficie solar hacia arriba mediante unos cohetes,con la misión de interceptar un rayo de luz que pase junto al Sol.
Cuando por fin desconectamos los cohetes,los laboratorios se encuentran en caída libre y pierden rápidamente velocidad por efecto de la gravitación.
Hemos elegido sus velocidades iniciales con una exactitud tal,que el laboratorio número uno quede parado sobre el Sol exactamente en el momento en que el rayo de luz penetre por su pared de cristal izquierda.
La velocidad del segundo laboratorio también será exacta mente la requerida por nuestro experimento.
Deberá quedar parado sobre el Sol un poquito más tarde,justo cuando el rayo de luz,después de atravesar el laboratorio número uno,alcance la pared de cristal izquierda del laboratorio número dos.
El observador uno debe registrar en qué punto y con qué ángulo ha penetrado la luz en su laboratorio.
El rayo atravesará el laboratorio y saldrá por la pared de cristal derecha con el mismo ángulo con que penetró por la izquierda.
Al observador situado en el primer laboratorio,esto le parece totalmente natural,puesto que su sistema de referencia (el laboratorio) se encuentra en caída libre y no siente ninguna gravedad.
En consecuencia,la luz se mueve en línea recta y por lo tanto deberá formar el mismo ángulo en la pared izquierda y en la pared derecha.
El observador situado en el segundo laboratorio llegará a conclusiones similares.
También él verá que el rayo de luz entra a través de la pared de cristal con un ángulo determinado y vuelve a salir luego con exactamente el mismo ángulo.
La sorpresa llega luego,cuando ambos laboratorios descienden otra vez hacia el Sol y los observadores comparan sus datos.
Entonces comprueban que han medido diferentes ángulos para el rayo de luz.
El ángulo de entrada en el laboratorio dos tenía una inclinación mayor hacia abajo que el ángulo de entrada en el laboratorio uno.
Después cavilar un rato,ambos se dan cuenta de por qué esto es así.
Ciertamente,cuando les llegó el rayo de luz,ambos laboratorios se encontraban en reposo en relación con el Sol.
Sin embargo,fueron dos momentos diferentes.
Cuando rayo de luz llegó al laboratorio dos,el laboratorio uno ya había comenzado a mandar otra vez hacia el Sol y había alcanzado una cierta velocidad.
Por lo tanto,el observador situado en el laboratorio número dos vio cómo penetraba en su laboratorio la luz que procedía de un sistema de referencia que se encontraba otra vez en movimiento.
Por eso,el ángulo de entrada de la luz estaba desviado hacia abajo cuando el rayo llegó al laboratorio dos.
La causa de esta aberración es un fenómeno que conoce cualquiera que haya caminado rápidamente bajo la lluvia man teniendo en vertical sobre su cabeza un paraguas abierto.
Cuanto más rápido se mueve uno,tanta mayor inclinación parecen tener las gotas de agua que caen de manera que acabamos mojándonos las canillas.
También la luz está sometido a este tipo de aberración,sólo que la desviación es muy pequeña porque su velocidad es muy alta.
Resultado del experimento: el observador situado en el laboratorio número dos afirma que el rayo de luz ha sido ligeramente desviado hacia el Sol.
Repetimos el experimento una y otra vez a lo largo de toda la trayectoria del rayo de luz y al final pedimos a los observadores de ambos laboratorios que sumen todas las insignificantes desviaciones,encontraremos que la desviación total de un rayo de luz que pase rozando la superficie del Sol es de 0,875 segundos de arco.
Así pues,queda perfectamente claro que este valor no varía,lo mismo si lo calculamos según la Teoría de la Gravitación de Newton - - como hizo Soldner - - o según el Principio de la Equivalencia,como en este experimento ficticio.
Sin embargo,a pesar de esta coincidencia,Einstein continuó pensando,hasta que en noviembre de 1915 duplicó la predicción.
Por aquella fecha había rematado ya su Teoría General de la Relatividad y comprobó que las nuevas ecuaciones daban por resultado una desviación de aproximadamente 1,75 segundos de arco,y no 0,875,es decir,justo el doble.
¿Ocurrió esta duplicación por casualidad? ¿Es que estaban totalmente equivocados los cálculos anteriores? Nada de eso.
Los cálculos estaban bien hechos,pero en ellos no se había tenido en cuenta una circunstancia importante,que únicamente podía ser aportada por las ecuaciones de la Teoría General de la Relatividad: la medida de la curvatura del espacio.
Cuando hablábamos del Principio de la Equivalencia en el segundo capitulo de esta serie,dedujimos que el espacio - tiempo tiene que ser curvo,pero no pudimos averiguar nada sobre la cuantía de tal curvatura espacio temporal,y ni mucho menos sobre la curvatura del propio espacio.
Y precisamente es de esto de lo que se trata,de la curvatura del espacio.
a Relatividad General predice que en las cercanías de cuerpos con gravitación,el espacio está curvado.
Así podemos verlo en los triángulos pequeño y grande del diagrama de la página 126.
El triángulo pequeño,más alejado del Sol,no está curvado.
Efectivamente,la suma de sus ángulos interiores.

