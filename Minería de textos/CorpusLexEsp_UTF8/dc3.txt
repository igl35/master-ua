Si admitimos como cierta la idea antropológica que afirma que la inteligencia aparece,en la escala evolutiva hacia el hombre,cuando hay un ser capaz de crear sus propias herramientas para cada necesidad,nunca ha sido el hombre tan humano como en los modernos descubrimientos de nuevos materiales.
El hombre ha ido aprendiendo,desde los tiempos más remotos,a utilizar los recursos y materiales que la naturaleza le brindaba.
Poco a poco aprendió a efectuar modificaciones sobre ellos,como en las aleaciones metálicas,pero sólo recientemente se ha producido una auténtica revolución con la creación de nuevos materiales,cada vez mejores,en mayor número y más adaptados a las propiedades específicas que en cada caso se les demandan.
Nace así una nueva rama científica intrínsecamente interdisciplinar: La Ciencia de Materiales.
Dentro de este campo,una importantísima propiedad en nuevos materiales orgánicos es la conductividad eléctrica,incluida la superconductividad.
Hasta hace relativamente pocos años,no era imaginable el hecho de que un compuesto orgánico mostrase propiedades conductoras como un metal.
Esto es debido fundamentalmente a la falta,en las sustancias orgánicas,de la banda electrónica parcialmente llena que caracteriza a los metales.
Como es sabido,la mayor parte de los compuestos orgánicos muestran valores de la conductividad eléctrica muy bajos,lo que les sitúa dentro del grupo de las sustancias eléctricamente aislantes.
Este cuadro,sin embargo,ha cambiado drásticamente en los últimos años y la ciencia-arte de la síntesis química ha conducido a una nueva clase de materiales moleculares y poliméricos con propiedades eléctricas semejantes a los metales.
Términos tales como " metales moleculares "," metales sintéticos " y " metales orgánicos " son frecuentes en este nuevo campo de estudio surgido en la interfase entre la Química,la Física y la Ciencia de los Materiales (véase la tabla 1).
Con propósito de claridad y sistematización,clasificaremos los " metales orgánicos " en los tres tipos fundamentales que actualmente se conocen: a) Polímeros orgánicos covalentes ; b) Metalomacrociclos poliméricos ; c) Complejos de transferencia de carga.
Suponen los primeros (a) el empleo de polímeros derivados de acetileno o heterociclos a los que se somete a " dopado ".
Es éste un término tomado de los semiconductores metálicos de modo un tanto libre,pues en los polímeros orgánicos conductores exige la adición de cantidades notables de " agente dopante ",a veces con importantes cambios estructurales en el polímero.
Como veremos,se logra con ello un sistema conductor.
El tipo (b) consiste en la formación de complejos de metalomacrociclos,generalmente de naturaleza porfirínica.
Su carácter quasi-unidimensional les permite conducir electrones a lo largo de la dirección de su eje.
El tercer tipo (c) de materiales orgánicos conductores,en los que se encuadra la labor investigadora de los autores,son los llamados complejos de transferencia de carga o,abreviadamente,C. Se pretende con esta denominación definir la transferencia parcial de un electrón de una molécula dadora a una molécula aceptora formando un cristal de tipo iónico en el que los aniones o los cationes,o ambos simultáneamte sean grupos químicos de alguna complejidad.
Desde que la ciencia supo poner a servicio la electricidad,los metales ha sido,con sus inconvenientes de coste,peso o corrosión,los conductores eléctricos disponibles.
Una excelente manera sería disponer de conductores con la ligereza,flexibilidad y moldeabilidad de los plásticos.
La primera,y más simple,solución en este sentido fue la adición a un polímero,un plástico,de fibrillas o partículas metálicas o de otra sustancia conductora,como el grafito.
Pero junto a estos conductores de matriz polimérica,llamados extrínsecos (" composites " o materiales compuestos) por ser una segunda sustancia la que proporciona la conductividad,se está consiguiendo desde hace unos años un tipo mucho más interesante: los conductores poliméricos intrínsecos,en los que es el propio polímero el responsable de la conductividad.
Sin embargo,incluso los polímeros con estructura policonjugada son aislantes.
Como máximo,el poliacetileno de alta cristalinidad puede alcanzar una conductividad de.
Hace poco más de una década,empleando la síntesis de Shirakawa de poliacetileno,se preparó por primera vez un polímero orgánico que presentaba una conductividad eléctrica comparable a la de los metales.
La clave de este logro se basaba en el gran aumento de la conductividad que se producía al exponer el poliacetileno a la presencia de un agente oxidante (" dopado "),llegándose a valores de 200.
Supone el procedimiento de Shirakawa la polimerización del acetileno con catalizador de Ziegler-Natta,lo que permite obtener películas de polímeros de aspecto y brillo metálico.
Tales láminas,con espesores controlables desde 1 a varios mm,permiten lograr fácilmente el " dopado " del polímero (fig.
1).
De este modo,el trans-poliacetileno puro,que es a lo sumo semiconductor (9),alcanza una conductividad metálica.
Como es sabido,en efecto,el trans-poliacetileno es un polímero semiconductor debido a que la banda ocupada más alta está completamente llena al solaparse todos los orbitales pz de la cadena hidrocarbonada.
La pérdida de electrones producida en el " dopado " daría lugar a una banda parcialmente llena en la que sería posible el movimiento de cargas.
Sin embargo,esta teoría de bandas no justifica el valor tan elevado de la conductividad para el caso del poliacetileno y otros polímeros a causa,fundamentalmente,de que el número de spines libres,medido por la técnica de resonancia de spin electrónico,es mucho menor del que cabía esperar para justificar la conductividad observada.
Schrieffer y Heeger,de la Universidad de Santa Bárbara en California,sugieren la formación de solitones (básica mente carbocationes deslocalizados).
En aquellos polímeros conductores para los que no es posible la formación de solitones se ha propuesto la formación de polarones (iones-radical) y bipolarones (dicationes cuyas cargas se hallan en una misma región del polímero).
A pesar de estos argumentos,los aspectos teóricos que justifican la elevada conductividad en polímeros orgánicos es hoy un tema abierto De hecho,el proceso de " dopado " no suele ser completamente uniforme en el polímero aunque el grado de control es muy elevado utilizando el método electroquímico.
Por otro lado,la influencia de la especie dopante en el proceso de conducción aún no está totalmente esclarecida.
Hasta ahora estos compuestos orgánicos poliméricos conductores presentan el problema de su escasa estabilidad,ya que en el caso del poliacetileno dopado,la degradación en presencia de aire comienza al cabo de una o dos horas.
Muy recientemente H. Naarmann de los laboratorios de BASF,en Ludwigshafen,Alemania,ha conseguido sintetizar,por un nuevo método,películas de poliacetileno de alta calidad,lo que ha llevado a un material más cristalino que no presenta defectos de carbonos Sp3 en su estructura.
La casi desaparición de zonas amorfas impide ahora el ataque del oxígeno del aire,con lo que el poliacetileno dopado sintetizado por el método de Naarmann es estable al aire durante varias semanas.
Junto con este aumento en la estabilidad,la conductividad aumenta sorprendentemente hasta 147 000 S. cm - ',lo que le sitúa en un valor incluso mayor que el cobre en relación a su peso.
Estos resultados parecen también sugerir que el carácter aislante del poliacetileno no es una propiedad intrínseca,sino probablemente el resultado de una alta densidad de defectos en el polímero y una baja cristalinidad.
Por lo que respecta al problema de la baja solubilidad,que dificulta el proceso,Grubbs ha logrado llegar a poliacetilenos solubles mediante un enfoque químico distinto: la polimerización con apertura de ciclo de ciclooctatetraenos sustituidos.
Los sustituyentes que soporta ahora la cadena polimérica incrementan notablemente la solubilidad del polímero conductor,si bien al precio de una conductividad menor,del orden de 350.
Otros polímeros también han mostrado valores metálicos de la conductividad (véase la tabla 2).
A pesar de las dificultades que aún quedan por resolver,la aplicación de polímeros conductores en la fabricación de baterías es una de las posibilidades de estos nuevos materiales.
Resulta esto del descubrimiento por Heeger,McDiarmid y Nigrey,que encontraron Compuestos polimericos conductores.
El poliacetileno no es ya el único polímero conductor (Tabla 2).
Junto a él han surgido otros no menos importantes.
El polisulfuro de fenileno,aunque no conjugado,es en estado puro un excelente aislante (1016) pero al " doparlo " alcanza una conductividad de 1 y presenta la ventaja de ser el primer polímero conductor soluble y termomoldeable.
El polipirrol,por su parte,se obtiene dopado en una sola etapa por polimerización del pirrol electroquimicamente con BF4 - como contraión y es un polimero conductor de considerable estabilidad,tanto térmica como frente al aire.
Resultados similares se alcanzaron más recientemente con el politiofeno.
que los polímeros pueden ser dopados electroquímicamente de modo reversible,con lo que una película de polímero,con un electrólito apropiado,puede actuar como electrodo en una batería.
Las pruebas realizadas utilizando poliacetileno dopado como electrodo en diferentes estados de oxidación o bien utilizando litio en uno de los electrodos,son muy prometedoras.
De hecho,dos compañías,Bridgestone y Varta,en colaboración con ISF han patentado baterías con ánodo de litio-aluminio y cátodo de polímero conductor,polianilina y polipirrol respectivamente.
Son muchas las otras posibles aplicaciones de los polímeros conductores tales como su uso como conductores eléctricos,antiestáticos,electrodos,sensores,etc. Recientemente,Yoshino ha patentado un procedimiento para moldear piezas conductoras de poli (3 - alquiltiofenos),lo que supone un notable avance,si bien estos tipos de polímeros son térmicamente sensibles,no porque se degraden,sino porque se pierde el " dopado " a temperaturas elevadas.
El conocimiento de la estructura de estos materiales así como el fenómeno de la conductividad,es lo que conducirá a polímeros más estables y,por consiguiente,a su posible comercialización en las respectivas aplicaciones.
El enorme esfuerzo que se está vertiendo en todo el mundo en el estudio de estos nuevos materiales,cuenta también en nuestro país con programas de investigación desarrollándose en varias Universidades y en el CSIC.
Como se ha visto en el apartado anterior,los polímeros orgánicos dopados presentan serias dificultades a causa fundamentalmente de su baja estabilidad térmica y química,lo que por el momento limita sus aplicaciones prácticas.
Una clase de compuestos comparativamente más estables son los metalomacrociclos,es decir,complejos metálicos que contienen un sistema macrocíclico como ftalocianinas o tetrabenzoporfirinas,coordinado con un metal de transición capaz de dar hexacoordinación,como por ejemplo Fe,Co,Cr o Ru.
Estos metalomacrociclos polimerizan formando un apilamiento entre las distintas moléculas,con la distancia interplanar adecuada (fig.
2).
El solapamiento de los orbitales moleculares n permite,tras el proceso de dopado,una deslocalización de los electrones a lo largo del sistema macrocíclico dando lugar a materiales conductores quasiunidimensionalesN.
En algunos casos también se ha podido observar conducción por contacto entre los átomos metálicos contenidos en los macrociclos.
Sin embargo,los complejos de metalomacrociclos no siempre cristalizan de este modo,y puede no darse el solapamiento de orbitales,perdiéndose así uno de los requerimientos necesarios para la conductividad.
Estos compuestos son,en consecuencia,sustancias aislantes con valores de conductividad del orden de 10 - 11.
No obstante,sometidos a un dopado oxidante muestran a menudo aumentos sustanciales de la conductividad.
Así por ejemplo,la ftalocianina de níquel,tras ser " dopada " con yodo,eleva su conductividad de 10 - " a 0,7 S. cm - '.
Ahora bien,aun logrando el metalomacrociclo polimérico " dopado " y con la cristalización apropiada,no se puede conseguir un control sobre la distancia interplanar de las unidades moleculares paralelas,lo que sería esencial para asegurar el correcto solapamiento entre orbitales responsables de la conductividad.
En busca de la solución a este problema,desde hace algunos años se han desarrollado los polímeros metalomacrocíclicos denominados " Chich Kebabo o " Schaschlik ",que en castellano podríamos traducir por " brocheta ".
Se trata,en efecto,de introducir un ligando unido al metal del metalomacrociclo,lo que permite la formación de especies poliméricas en las que la distancia entre macrociclos (fig.
2) viene fijada por el ligando L. Se llega así a la formación de compuestos metalomacrocíclicos unidos por ligandos puente en los que es posible variar la naturaleza tanto del sistema macrocíclico (aunque ftalocianinas y tetrabenzoporfirinas han sido las más estudiadas) como del átomo metálico central.
Por otro lado,el uso de ligandos de distinta naturaleza (átomos o moléculas) permite un perfecto control sobre la distancia interplanar.
Además,según los ligandos utilizados,éstos pueden o no intervenir en el proceso de conducción.
Por otro lado,Hanack y sus colaboradores,en Alemania,han demostrado que si se utilizan metales de transición como Fe,Ru,Os,Co,Rh,Mn o Cr,que imponen una configuración octaédrica,y se utilizan moléculas orgánicas con sistemas n como ligandos,es posible una conducción adicional a lo largo del eje central en estos polímeros.
En algunos casos se ha conseguido llegar a propiedades semiconductoras sin realizar el proceso de dopado.
Los compuestos así obtenidos presentan,además de propiedades semiconductoras en ausencia de oxidación externa (" dopado "),una elevada estabilidad térmica que abre un gran campo de estudio de nuevos materiales con propiedades conductoras y semiconductoras.
Recientemente,J. Simons ha descrito un procedimiento alternativo para conseguir una disposición de macrociclos de ftalocianinas en apilamientos sin necesidad de ligandos puente.
Consiste en la presencia de largas cadenas hidrocarbonadas en posiciones periféricas del macrociclo,lo que conduce a la formación de mesofases discóticas y al apilamiento de los macrociclos.
Esta disposición incrementa la interacción metal-metal o bien entre orbitales n de los distintos macrociclos,facilitando el transporte de carga.
Así,se han conseguido mesofases discóticas con derivados de ftalocianinas tanto con átomo metálico en su interior como en ausencia de metal.
Sin embargo,los valores de la conductividad eléctrica observados no son especialmente elevados.
Algunos de los metalomacrociclos anteriormente descritos han encontrado ya aplicaciones prácticas: su adición a los " films " de materiales plásticos les confiere la suficiente conductividad para evitar que se carguen eléctricamente,evitando los serios problemas técnicos que ello conlleva.
El primer metal orgánico,descrito en la bibliografía fue un complejo de bromo-perileno con una pequeña conductividad ().
Sin embargo,la búsqueda de conductores orgánicos no se desarrolló hasta el descubrimiento del potente aceptor de electrones tetraciano-p-quinodimA (TCNQ) en 1960 y el potente dador electrónico tetratiafulvaleno (TTF) en 1970,lo que condujo en 1973 a su combinación para formar el complejo n-molecular dadoraceptor (TTF-TCNQ) (fig.
3),con una conductividad semejante a los metales (véase la tabla 1).
Este descubrimiento clave fue origen de un gran esfuerzo investigador,que provocó la búsqueda de nuevos complejos orgánicos de transferencia de carga (CTC) que pudieran igualar o incluso mejorar al TCNQ-TTF.
Desde entonces se ha suscitado un enorme interés por el diseño,síntesis y propiedades de los CTC conductores.
Este estudio ha permitido determinar algunos de los requisitos necesarios para convertir una colección de moléculas desorganizadas en una disposición eléctricamente conductora.
En primer lugar,las moléculas han de estar dispuestas espacialmente próximas y con entornos cristalográficos y electrónicos similares.
Esta situación se consigue cuando moléculas planas y conjugadas cristalizan en apilamientos segregados de dadores (D) y aceptores (A),consiguiendo de este modo una de las condiciones necesarias para que pueda darse la conductividad.
Lamentablemente,el apilamiento mezclado que contiene de forma alternativa moléculas dadoras (D) y aceptoras (A) en cada apilamiento,es en general el modelo termodinámicamente favorecido.
Como especies dadoras se han utilizado metales alcalinos,aminas,alquenos ricos en electrones,heterociclos o moléculas aromáticas ; entre las especies aceptoras,átomos de halógeno,quinonas,alquenos deficientes de electrones,heterociclos o moléculas aromáticas,han sido los más estudiados.
El segundo requisito para el estado metálico molecular es que las moléculas deben estar en un estado de oxidación fraccionado.
Es decir,las moléculas han de tener la banda de valencia fraccionalmente ocupada.
Parte de esta exigencia es análoga a la norma clásica de que en un metal,la banda ocupada más alta tiene que estar sólo parcialmente llena,lo que corresponde en los CTC a una transferencia electrónica dador-aceptor parcial.
De hecho,la combinación de moléculas fuertemente dadoras y aceptoras no garantiza la formación del complejo,ya que puede producirse una transferencia total de carga formando especies (D + IA-I) que se comportan eléctricamente como sustancias iónicas aislantes (aislantes Mott-Hubbard).
En consecuencia,en los compuestos orgánicos conductores conocidos en los que ha sido evaluado el grado de transferencia de carga (),se ha encontrado que ésta es menor de la unidad,y generalmente en torno a 0,5.
Por consiguiente,es condición necesaria que 0 " d " 1. Además del grado fraccional de transferencia de carga,ésta tiene que estar deslocalizada a lo largo del apilamiento.
Cuanto más próximas se encuentren las moléculas en el apilamiento,mayor será el solapamiento de los orbitales entre ellos y,en consecuencia,mayor será la deslocalización de la carga.
En los casos estudiados esta distancia está en el rango de 3,2 - 3,5 angstroms.
De otro lado,las propiedades conductoras de los materiales moleculares se ven muy afectadas por las interacciones entre los sistemas electrónicos y las vibraciones del retículo cristalino,llegando en el caso extremo a producirse a bajas temperaturas (próximas al cero absoluto) una distorsión del retículo (transición de Peierls) acompañada de una transición del comportamiento metálico a aislante.
En este sentido,los metales orgánicos tipo CTC muestran en general una acusada dependencia de la conductividad con la temperatura.
Así,a temperatura ambiente,el TCNQ-TTF tiene un valor de 500 (el correspondiente valor para el cobre es 6.105).
A medida que la temperatura disminuye,la conductividad aumenta hasta alcanzar a = 10 a 59 K (fig.
3).
Sin embargo,por debajo de 59 K a cae bruscamente y la sustancia se comporta como un semiconductor,a consecuencia del mencionado " efecto Peierls ",en cierto modo análogo al efecto Jan-Teller.
Una segunda e importante característica de la conductividad del complejo TCNQ-TTF es su carácter fuertemente anisotrópico.
En efecto,la conductividad a lo largo de los tres ejes del cristal es muy diferente (en una relación 500: 5: 1),de modo que puede considerarse monodimensional.
Estas interesantes y peculiares propiedades electrónicas del TCNQ-TTF encuentran su explicación en una situación electrónica y cristalina que cumple los necesarios requisitos.
La planaridad de las moléculas hace que puedan aproximarse mucho en la dirección perpendicular a su plano molecular,tanto en el apilamiento segregado de moléculas de TNCQ como en la columna paralela de moléculas de TTF,a lo largo de cuyos ejes se produce el movimiento electrónico responsable de la conductividad.
La pequeña distancia interplanar entre moléculas adyacentes (3,17 angstroms para TNCQ y 3,47 angstroms para TTF) permite una gran interacción entre los OM n de moléculas vecinas,es decir la formación de una banda,un rango restringido de energías permitidas para los electrones en el sólido.
En efecto,cuando un número de átomos (como en los metales o semiconductores) o de moléculas (en nuestro caso de los metales orgánicos) se unen para formar un sólido cristalino,los estados electrónicos se mezclan y forman bandas.
El llenado de estas bandas es análogo al principio Aufbau para átomos.
Si las bandas están totalmente llenas y hay un salto de energía hasta la siguiente banda vacía,sería necesario un gran aporte energético para que los electrones pudiesen desplazarse y dar lugar a conducción eléctrica (fig.
4 A).
Al no ser esto posible,la sustancia es aislante.
Por otro lado,una banda completamente llena conduce a propiedades semiconductoras.
Pero si la banda ocupada más alta está parcialmente llena,existen estados electrónicos vacíos infinitamente próximos al nivel de Fermi (al estado ocupado más alto),de modo que esos electrones pueden tomar parte en la conductividad eléctrica en esta banda de conducción.
Al igual que en los metales,en esta situación,la conductividad (a) disminuye como consecuencia de las vibraciones reticulares,lo que explica también el efecto,antes mencionado para el complejo TCNQ-TTF,de aumento de 1 al disminuir la temperatura y,con ella,las vibraciones reticulares.
Así,la alta conductividad de " metal orgánico " en CTC sólo se produce cuando,de las tres situaciones posibles de empaquetamiento y transferencia de carga (fig.
4 B),los apilamientos segregados van acompañados de transferencia parcial de carga,lo que implica la existencia de moléculas neutras junto con otras ionizadas.
Si la transferencia de carga es total (1 electrón por pareja dador-aceptor),todas las moléculas están ionizadas y,como máximo,pueden lograrse semiconductores,ya que el movimiento electrónico entre moléculas a lo largo del apilamiento supondría la formación de especies doblemente cargadas en el dador o el aceptor,lo que implica una barrera energética a causa de la repulsión electrónica: Una situación desfavorable que no se produce al existir moléculas sin ionizar,en el caso de la transferencia parcial,pues únicamente se forman iones monocargados que son,además,particularmente estables como consecuencia.
La conductividad eléctrica es una propiedad típicamente metálica,mientras que las sustancias orgánicas son aislantes.
Los valores de las conductividades eléctricas (véase la tabla 1) muestran que las sustancias típicamente orgánicas como el antraceno presentan valores de la conductividad muy bajos (a = 1016 Scm - '),por lo que se incluyen dentro del grupo de sustancias aislantes.
Otros compuestos orgánicos,como el cis poliacetileno,muestran valores de conductividad superiores (a = 109 Scml) y muestran propiedades de semiconductores.
La tabla también recoge otros compuestos pomméricos como el trans pobacetileno,pobfenileno o polipirrol que,sometidos al proceso de " dopado ",muestran unos valores de a próximos al de los metales.
Por último,un pequeño grupo de sustancias,como el complejo CNQ-ITF,presentan una conductividad eléctrica comparable a la de los metales.
de la deslocalización de carga a una norma canónica aromática por resonancia en el anión-radical del TCNQ y del catión-radical del TTF.
Es justamente el valor relativo del potencial de oxidación del dador y del potencial de reducción del aceptor lo que garantiza que la transferencia de carga no sea total.
Se cumple con ello el requisito de conductividad por movimiento de carga de molécula cargada a molécula neutra a lo largo del apilamiento.
Es obvio,por tanto,que un camino apropiado para mejorar la conductividad sería el empleo de moléculas dador y aceptor que tuviesen valores de potenciales lo más apropiados para lograr la transferencia parcial de carga en el grado óptimo.
Se trata,pues,de modificar los potenciales redox de dador y aceptor para modular el grado de transferencia de carga,y con este propósito se ha sintetizado un considerable número de moléculas dadoras y aceptoras con estructuras " diseñadas " para lograr mejores CTC conductores.
En particular,son tres los objetivos: - - Transferencia de carga D-A entre 0 - - Interacciones entre apilamientos segregados que impidan cambios de fase capaces de provocar la transición de estructura conductora a aislante.
- - Incremento de la polarizabilidad molecular.
A ello se ha dedicado un gran esfuerzo investigador en los últimos años.
Por lo que respecta a la parte dadora del complejo conductor,la mayor parte de nuevas moléculas dadoras sintetizadas han mantenido el sistema de anillo de 1,3 ditiol del TTF y se han estudiado derivados con una estructura extendida con enlaces o como el tetrametiltetratiafulN (TMTTF),o un sistema extendido como por ejemplo el dibenzotetratiafulvalN (DBTTF) o,finalmente,la sustitución del átomo de azufre por otros heteroátomos como selenio o teluro (fig.
5).
Por otra parte,la síntesis de nuevas moléculas aceptoras ha tenido que superar las dificultades de obtención de derivados del TCNQ,pero durante los últimos años se ha podido sintetizar un elevado número de moléculas aceptoras (fig.
5) con una amplia gama de valores de la afinidad electrónica.
En general,la sustitución en el esqueleto monocíclico de TCNQ da lugar a complejos con propiedades conductoras inferiores a las del TCNQ,por la menor planaridad de la molécula,lo que dificulta el apretado apilamiento necesario para el solapamiento n responsable de la conductividad.
Aunque menos estudiados,los análogos de TCNQ fusionados con anillos aromáticos presentan un interés especial.
De estudios teóricos pudo concluirse que la extensión del sistema 11 en estos compuestos conduce a una disminución de la repulsión intramolecular de tipo Coulomb lo que habría de favorecer la formación de complejos de transferencia de carga.
Los datos espectroscópicos,y en algún caso el difractograma de rayos X,indican que estas moléculas no son totalmente planas y se encuentran bastante deformadas.
Esta distorsión molecular ejerce probablemente una mayor influencia que los efectos electrónicos de extensión del sistema.
Figura 7. Desde un punto de vista estructural estos compuestos son semejantes a los metales orgánicos ya comentados y forman apilamientos segregados en zig-zag formando una cadena quasi-unidimensional.
Sin embargo,el hecho más importante ahora es el contacto existente entre átomos de selenio,cuya distancia es menor que la suma de los radios de Van der Waals,con lo que en este caso estos compuestos pueden ser conductores bi o tridimensionales.
Este tipo de nuevos compuestos conductores resultan,por tanta,de la oxidación parcial producida por el reactivo inorgánico sobre el dador orgánico.
De hecho,los cristales se obtuvieron electroquimicamente con Bu4NaO4 en 1,1,2 - tricloroetano y el TMTSF se oxidó en el ánodo a TMTSF + con lo que cristaliza la sal suPerconductora.
En consecuencia,la presencia de anillos aromáticos fusionados con el anillo de TCNQ conduce a valores del primer potencial de reducción menores y,con ello,a una menor capacidad para la formación de CTC conductores.
Supone esto una conclusión importante en el sentido de que,a pesar de la ventaja electroquímica que supone,la fusión de sus anillos aromáticos origina una excesiva distorsión de la planaridad molecular.
Una alternativa para lograr una menor repulsión coulómbica,y al propio tiempo modular el potencial redox,consiste en la síntesis de moléculas aceptoras con sistemas n extendidos que contengan anillos heterocíclicos.
En ello trabaja nuestro grupo de investigación (fig.
6).
La presencia en la molécula,de un sistema heterocíclico n-deficiente o excedente permitirá así modular el carácter aceptor de la molécula.
Una alternativa para lograr moléculas aceptoras eficaces se refiere al fragmento electronegativo de la molécula capaz de deslocalizar la carga.
Usualmente,como hemos visto,es un grupo dicianometileno quien cumple este papel en el TNQ y análogos,y se ha mantenido en la mayoría de moléculas aceptoras sintetizadas con sistemas n extendidos.
Sin embargo,desde hace algún tiempo también se utiliza como agrupación funcional en la molécula aceptora el grupo cianimino (=N-CN) con el que recientemente se han sintetizado un ya elevado número de nuevas moléculas aceptoras,que junto con los dadores apropiados,han conducido a nuevos compuestos orgánicos conductores.
Una última línea de trabajo dirigida a la obtención de metales orgánicos,en uno de sus aspectos más interesantes,pretende soslayar la dificultad de conseguir los complejos a través del empleo de un solo componente: una molécula mixta dador-aceptor (D-A) en lugar de una molécula dadora y otra aceptora.
Su conductividad no requiere ahora un complejo intermolecular,pues la molécula contendrá en sí misma,los dos fragmentos en una única estructura.
Son moléculas capaces de transferencia electrónica por sí mismas,de la parte dadora a la aceptora,de modo,además,que se tiene una estequiometría D / A prefijada.
Por razones sintéticas,caben dos tipos fundamentales de moléculas en este sentido D-A-D y A-D-A,que podrían conducir a sistemas del tipo.
El acceso a tales moléculas viene dado a partir de sistemas quinoides en los que uno o dos anillos aromáticos capaces de actuar como dadores se hallan unidos al esqueleto de TCNQ pero sin participar en su conjunción.
La posibilidad más inmediata es situar estos anillos dadores como sustituyentes a través de un carbono saturado.
Un avance importante en la investigación de metales orgánicos se produjo con la síntesis del complejo TCNQ que a 13 kbar de presión y 10 K,presenta una rJ = 105 (S. cm) - l. A presión ambiente la transición metal-aislante se produce a 42 K. La observación de la extremadamente alta conductividad eléctrica a baja temperatura,y el hecho de que a 42 K la transición estaba asociada exclusivamente con el apilamiento del TMTSF (es decir con la molécula dadora),hizo que Bechgaard sintetizarse un nuevo tipo de sales de transferencia de carga con TMTSF como dador y aniones inorgánicos,en lugar de una molécula orgánica,como aceptores.
En 1979 se obtienen las sales (TMTsF),X (X = una a 105 y,al someterlas a presión para evitar la distorsión de Peierls,condujo a la primera observación de superconductividad (9 K) en un conductor orgánico con una temperatura de transición a superconductor de 1,4 K. Poco después,en el (TMTsF) 2 ClO4 la superconductividad fue observada a presión ambiente,y desde entonces hasta hoy la superconductividad se ha detectado en otros compuestos orgánicos (fig.
7).
Aunque sin alcanzar superconductividad,se ha logrado también obtener sales conductoras mediante el enfoque contrario: la reducción de una molécula orgánica aceptora con una especie inorgánica actuando como dador electrónico,tal como un metal alcalino,que puede ser también reemplazado por una base orgánica,como una amina.
Finalmente,una sal de gran interés es BEI,que con una transición de 4 K alcanza ya una superconductividad en un rango de temperaturas utilizable.
Parece claro que la intensa investigación que se está desarrollando en superconductores orgánicos,además de los beneficios prácticos que supone,aportará luz sobre el fenómeno general,tan importante como complejo,de la superconductividad.
El intenso trabajo que se está dedicando en este momento a los conductores orgánicos,hasta el punto de que se publican revistas monográficas sobre el tema,está justificado por las enormes aplicaciones que son posibles para estos nuevos materiales (fig.
8).
La posibilidad de obtener materiales conductores solubles y termoplásticos tienen una valor tecnológico y práctico que va más allá de su interés meramente científico,desde el uso como antiestáticos o recubrimientos para el blindaje electromagnético de equipos electrónicos,hasta el empleo como electrodos no corroíbles en las nuevas baterías.
El empleo de compuestos orgánicos fotoconductores juega ya un importante papel en las técnicas reprográficas (electrofotografía).
La naturaleza de muchas de estas aplicaciones de alta tecnología hace que los costos,hasta ahora altos,de los nuevos materiales orgánicos conductores puede ser particularmente cierto para los superconductores orgánicos,dado el costo y complejidad actual de los superconductores conocidos,y muy especialmente si se confirma la superconductividad a temperatura relativamente alta de algunos compuestos de transferencia de carga.
En 1791,año en que nació el egregio científico inglés Michael Faraday,aún sonaban los ecos de la reciente revolución francesa (1789).
Este acontecimiento histórico marca de hecho el principio del fin de la Ilustración,movimiento intelectual caracterizado por un racionalismo utilitarista,que se extendió por toda Europa,pero que en Francia alcanzó su máxima expresión con el enciclopedismo.
Lavoisier,uno de los máximos exponentes de esta corriente intelectual,moría guillotinado cinco años después de estallar la revolución,lo cual era una demostración clara de que la Ilustración estaba agonizando.
No obstante,la figura de Faraday parece contradecir esta realidad.
En efecto,este científico,al igual que sus antecesores " ilustrados ",cultivó distintos campos del saber,principalmente la física y la química,realizando descubrimientos que marcaron,sin lugar a dudas,el desarrollo posterior de la ciencia.
Michael Faraday nació el 22 de setiembre de 1791 en Newington,Surrey,una población que en aquel entonces formaba un núcleo separado de la gran ciudad de Londres.
Michael era uno de los diez hijos del señor Faraday,un humilde herrero que había recalado en Newington,proveniente de Yorkshire,en busca de mejor fortuna.
La fortuna,sin embargo,no debió sonreírle puesto que en 1805,cuando Michel tenía ya 14 años,la familia tuvo que emigrar hacia el norte de Londres.
Allí,y para colaborar económicamente con los gastos familiares,nuestro protagonista se empleó como aprendiz en la casa de un encuadernador.
El encuadernador,que a su vez era propietario de una librería,permitió,e incluso alentó la curiosidad de Faraday por los libros,en especial aquellos dedicados a temas científicos,facilitándole la lectura de monografías y otras publicaciones.
Su curiosidad le llevó a asistir a las conferencias de carácter científico que,periódicamente tenían lugar en la Royal Institution.
Uno de los conferenciantes que más le impactó fue Humphry Davy,célebre químico y físico inglés,que a la sazón era profesor de química de la Royal Institution.
El interés por este conferenciante fue lo que le indujo a pedir a Davy que le admitiese como ayudante particular suyo.
Tocado en su vanidad,Davy lo aceptó de buen grado en su laboratorio.
En consecuencia,Faraday se despidió de su trabajo como encuadernador y,a pesar de que su salario en su nueva ocupación iba a ser bastante inferior,se incorporó de forma inmediata a las tareas de laboratorio.
Todo ello ocurría en 1813,justo el año en que Davy debía realizar un viaje por Europa.
Ello lo aprovechó este último para llevarse a Faraday,el cual,dadas las circunstancias,actuó más como criado que como ayudante científico.
A pesar de esta humillación que tuvo que soportar Faraday y que de hecho sería el germen de futuros problemas personales que enturbiarían sus relaciones con Davy,la gira por el continente,visitando los más prestigiosos laboratorios del momento,resultó de mucha utilidad para el joven Faraday el cual se dio cuenta de cuáles eran las cuestiones científicas que se debatían en el momento en los círculos científicos de Europa.
Ya de vuelta de la gira europea,Faraday se puso a trabajar en el laboratorio con total y absoluta dedicación.
Sus primeros trabajos fueron dentro del campo de la química continuando la tarea iniciada por Davy.
Al principio,Faraday se dedicó a la búsqueda de materiales resistentes a la corrosión.
En 1820 preparó hierro inoxidable y se interesó en el papel estabilizante que ejercía la adición de pequeñas cantidades de ciertos metales en el acero.
Tres años más tarde,Faraday sintetizó dos compuestos hasta entonces desconocidos,formados por la combinación de átomos de cloro y carbono en dos de ellos,y de iodo,carbono e hidrógeno en el otro.
En este mismo año,1823,llevó a cabo la licuación del cloro,lo cual desencadenó la envidia de Davy quien consideraba que la gesta de Faraday no era más que una consecuencia de su trabajo anterior.
Ello originó un enfriamiento en las relaciones entre Davy y Faraday,lo cual aprovechó Faraday para desacreditar uno de los inventos más estimados de Davy,la denominada lámpara de Davy,un instrumento de seguridad cuyo fin era evitar las explosiones de grisú en las minas,y a la que Faraday señaló la existencia de algunos defectos.
Faraday,no solamente licuó el cloro sino que también hizo lo propio con otros gases,tales como el dióxido de carbono,el ácido sulfúrico y el ácido bromhídrico,ideando métodos para disminuir la temperatura a valores muy bajos.
En este sentido,Faraday fue el primero en obtener valores negativos de temperatura en las escala Farenheit.
En realidad,puede considerarse que Faraday fue un precursor de la criogénesis,que es aquella rama de la Física dedicada a las técnicas de producción,mantenimiento y aplicación de temperaturas muy bajas,y para lo cual generalmente se utilizan gases licuados.
Dos años más tarde de lograr la licuación del cloro,Faraday realizó su única incursión de toda su carrera en la química orgánica.
Su contribución a esta área del conocimiento fue,no obstante,crucial puesto que logró aislar por primera vez el benceno,o como él lo denominó," bicarburato de hidrógeno ",siguiendo una típica nomenclatura inorgánica que es la que él estaba acostumbrado a emplear.
En esta época,Faraday conoció en los locales de la Royal Institution a John Frederic Daniell,conocido químico inglés,inventor,entre otras cosas,de la llamada pila Daniell,que fue la primera pila impolarizable.
Además de cultivar una gran amistad con Faraday,gracias a la cual Daniell en 1839 le dedicó su libro Introducción al estudio de la filosofía química,desarrolló también con él una larga y fructífera colaboración científica.
Faraday y Daniell se reunían a menudo,y con ellos también lo hacía el físico inglés Charles Wheatstone,a la sazón profesor de Filosofía experimental en el King's College de Londres.
Wheatstone se dedicaba a la electricidad,siendo inventor del denominado puente de Wheatstone,que sirve para medir resistencias.
Estas conversaciones fueron ciertamente muy fructíferas para el trabajo futuro de Faraday,en relación al estudio que condujo acerca de la influencia de la electricidad en los procesos químicos.
Precisamente en este campo,Faraday siguió el camino iniciado por Davy sobre un curioso fenómeno observado por éste,en el cual y de forma sorprendente se liberaban metales haciendo pasar corrientes eléctricas por las correspondientes sales metálicas fundidas.
Gracias a las experimentaciones realizadas por Faraday y a las conversaciones mantenidas con Daniell y Wheatstone,pudo enunciar,en 1834,sus famosas Leyes de Faraday,mediante las cuales se puede predecir cuantitativamente la masa de sustancia transformada por el paso de una determinada cantidad de corriente eléctrica.
Y no sólo esto,sino que introdujo los términos electrólisis,electrólito,electrodo,ánodo,cátodo,anión y catión,utilizados desde entonces y aceptados universalmente.
La trascendencia del trabajo de Faraday en este campo,del cual depende una buena parte de la industria química actual,ha llevado a considerar a muchos que,de hecho,con él se inicia la Electroquímica.
Una buena parte de su actividad científica,la dedicó a la Física,y en concreto al estudio de las relaciones entre el magnetismo y la electricidad.
De hecho,tras el descubrimiento realizado por el físico danés Hans Oersted en 1820,quien consiguió desviar una aguja magnética al hacer pasar una corriente eléctrica por un conductos paralelo a aquélla.
Faraday se propuso llevar a cabo el fenómeno inverso,es decir,generar una corriente eléctrica por la acción de un imán.
Un año después del descubrimiento de Oersted,Faraday ideó un dispositivo consistente en un alambre rígido,pero que podía girar libremente sobre un gozne,el cual tenía uno de sus extremos sumergido en un fondo de mercurio.
Situando un imán con uno de sus polos cerca del gozne y haciendo circular una corriente por la varilla,se producía el libre giro de ésta.
Este descubrimiento,en el que las fuerzas magnéticas y eléctricas actuaban conjuntamente para producir un movimiento continuo,dio mucha fama a Faraday.
Ello no obstante dio lugar a que estallasen definitivamente sus relaciones con Davy,pues éste consideraba que Faraday se había apropiado de la idea,surgida a raíz de una conversación entre aquél y William Hyde Wollaston (químico y físico inglés,descubridor del paladio y del rodio).
Lo cierto es que las tentativas de Davy y de Wollaston por obtener mecánica a partir de la electricidad y del magnetismo fueron infructuosas.
A pesar de la fama que le dio este descubrimiento,Faraday no estaba aún satisfecho,puesto que lo que él quería era inducir una corriente eléctrica a partir de la influencia de un imán.
Con este objetivo estuvo trabajando durante los siguientes años y,aunque por el momento no logró resultados positivos,no cejó en su empeño.
Al final,en el año 1831,Faraday recibió el justo premio a sus esfuerzos.
Enrolló una bobina (A) de alambre alrededor de una sección de un anillos de hierro dulce.
La bobina A disponía de una llave que,accionándola,podía abrir o cerrar el paso de una corriente que circulaba por sus espiras y que era suministrada por una batería.
Por otro lado,otra bobina,la B,que disponía de un galvanómetro,estaba enrollada alrededor de otra sección del anillo de hierro.
Con este dispositivo,Faraday observó,sorprendentemente,que bien al abrir o bien al cerrar el circuito de la bobina A,y sólo en estos precisos instantes,se generaba una corriente transitoria en la bobina B. Había logrado,pues,su propósito de generar una corriente eléctrica,aunque no continua.
Para tratar de explicar este fenómeno,y siendo muy deficiente su formación en matemáticas,Faraday imaginó que actuaban unas líneas de fuerza magnética.
Puesto que el mismo fenómeno se producía cuando un imán se introducía o se sacaba del interior de una bobina,Faraday supuso que la generación de corriente eléctrica era una consecuencia del cambio en el flujo de las líneas de fuerza magnéticas.
De hecho,Faraday pudo visualizar este campo de líneas de fuerza,esparciendo limaduras de hierro sobre un papel.
Éstas,en presencia de un imán,se disponían a su alrededor,formando dibujos regulares que marcaban el sentido de las fuerzas magnéticas.
Los conceptos de líneas de fuerza y de campo magnético,introducidos por Faraday,fueron desarrollados matemáticamente medio siglo después por su discípulo,el físico escocés James Clerk Maxwell,quien al formular sus ecuaciones (las ecuaciones de Maxwell),sentó las bases con las que se fundamenta el electromagnetismo.
Una vez descubierto el fenómeno de la inducción eléctrica,que todo debe decirse,fue observado también,y de forma simultánea por el físico norteamericano Joseph Henry,Faraday se propuso inducir corriente eléctrica,pero no de forma transitoria,si no continua.
Para ello adoptó a sus propósitos un experimento diseñado por el científico catalán Francesc Aragó (véase " Francesc Aragó,científico y político ",Mundo Científico,n° 55,febrero,1986),mediante éste último produjo lo que él denominó magnetismo de rotación,consistente en imantar una pieza de hierro,gracias al giro constante de una rueda de cobre situada debajo.
Lo que hizo Faraday fue girar la rueda de cobre de forma que su borde pasase por entre los polos de un imán permanente.
Con ello lograba cortar las líneas de fuerza magnética y generar,en consecuencia,una corriente en la rueda de cobre que subsistía mientras durase el movimiento giratorio de la rueda.
Faraday había inventado,nada más y nada menos,que el primer generador eléctrico.
Todo ello sucedía en el año 1831,justo cuando Faraday cumplía cuarenta años.
Una vez demostrada la interrelación entre la electricidad y el magnetismo,Faraday se empecinó en demostrar que la luz y el magnetismo también estaban relacionados.
Ello lo consiguió en el año 1845,al descubrir que se producía la rotación del plano de polarización de la luz en presencia de un campo magnético.
En estos experimentos Faraday utilizó el " vidrio pesado " producido por él mismo veinte años atrás,fruto de sus investigaciones sobre vidrios ópticos.
Ya en el ocaso de su carrera,y siguiendo con su idea de la búsqueda de las relaciones que pensaba que existían entre todas las fuerzas que actuaban en la Naturaleza,se propuso buscar los puntos de contacto entre la fuerza gravitatoria y la electricidad.
No obstante en 1858,ya cansado física e intelectualmente,desistió de este propósito y se retiró de su actividad científica.
Faraday se casó a los treinta años con Sarah Barnard,y de su matrimonio no tuvo ningún hijo.
Ello le permitió dedicarse por completo a su trabajo,no aceptando la asistencia de ningún colaborador,realizando él mismo los experimentos que diseñaba.
A pesar de la fuerte oposición de Davy,Faraday ingresó como miembro a la prestigiosa Royal Society londinense en el año 1824.
En el año siguiente fue ascendido a director de laboratorio y,en 1826,empezó a impartir clases magistrales a los miembros de la propia institución.
En 1833 fue nombrado profesor de la Royal Society.
Fue un gran divulgador,que logró hacer interesar a los jóvenes por la ciencia,gracias a sus famosas lecciones navideñas,que también se impartían en los locales de la Royal Society.
Al igual que su coetáneo Daniell,incorporó a sus lecciones los denominados experimentos de cátedra para ilustrar sus explicaciones.
Fruto de sus investigaciones y de sus enseñanzas,Faraday publicó diversos libros,entre los cuales destacan,Manipulaciones químicas (1827),Investigaciones experimentales en electricidad (1839 - 1855),Investigaciones experimentales en Física (1859),etc... Faraday pertenecía a la secta religiosa de los sandemanitas,los cuales se caracterizaban por eludir toda clase de vanidades humanas.
Ésta fue la razón por la que rehusó medallas y distinciones,rechazando incluso el apetecido cargo de presidente de la Royal Society por el que suspiraban todos los académicos de la época.
También rehusó participar en la elaboración de armas químicas durante la guerra de Crimea,a pesar de las prebendas que le ofrecieron.
Tan sólo aceptó una vez la invitación de la reina Victoria a cenar en palacio.
Ello,no obstante,le valió su excomulgación de la secta,y sólo con una larga y severa penitencia pudo rehabilitarse.
Como ya se ha indicado anteriormente,en 1858 se retiró a vivir cerca de Hampton Court,población situada cerca de Londres,y aunque todavía seguía pensando acerca de cuestiones científicas,su salud le iba venciendo,hasta que el 25 de agosto de 1867 le sobrevino la muerte.
Aunque la vida de Faraday,como la de cualquier ser humano,fue breve,su legado científico fue vasto y aún es vigente.
Así por ejemplo,en los libros de texto de Física y de Química son numerosas las referencias a su trabajo,figurando su nombre en unidades,instrumentos,leyes o fenómenos físicos,como el cilindro de Faraday,las leyes de Faraday,el efecto Faraday,la jaula de Faraday,el faradímetro,el faradio,las corrientes faradaicas...,que es en definitiva un merecido homenaje hacia esta gran figura de la ciencia.
Falleció en Roma Sebastián Izquierdo,uno de los escasos españoles del siglo XVII que realizaron aportaciones de relieve al desarrollo de la metodología científica y de las matemáticas.
Nació en la localidad albaceteña de Alcaraz en 1601 e ingresó en la Compañía de Jesús el año 1623.
Fue profesor de filosofía y de teología en los colegios jesuiticos de Alcalá de Henares y de Murcia desde 1641 hasta 1661,fecha en la que fue nombrado asistente del Propósito General de su orden.
En consecuencia,se trasladó a Roma,donde residió los veinte últimos años de su vida.
En 1659 apareció su más importante obra,titulada Pharus scientiarum.
Su contenido,como han demostrado los estudios de Ramón Ceñal,corresponde a un escolasticismo abierto a las corrientes modernas.
La finalidad principal del libro es exponer una " scientia de scientia ",en el sentido de la tradición de inspiración luliana.
Dicha tradición había tenido una gran influencia entre los miembros de la Academia de Matemáticas de Madrid,a finales del siglo anterior,y continuó pesando durante la centuria barroca a través de varias vías,una de las cuales culminaría en la obra de Leibniz.
Izquierdo entiende expresamente el " ars generalis " como " método científico " e intenta " restaurar,ilustrar,completar y liberar de defectos " al Organon aristotélico.
Considera que es incompleto porque solamente reglamenta la facultad intelectiva,siendo necesario hacerlo también con la memoria,la imaginación y la experiencia sensorial.
Sobre esta base tradicional,Izquierdo incorpora varios supuestos centrales de la ciencia moderna.
Distingue entre la física y la metafísica como grandes órdenes del saber,defiende que la experiencia y la observación son los fundamentos de todo conocimiento y propugna que su desarrollo y exposición se realicen " more geométrico et arithmetico ".
Acepta las reglas de Bacon,pero estima que sus ocho modos pueden completarse con " otros muchos casi infinitos '.
Propone diez " instrumentos " del conocimiento científico: observación,composición,división,definición,locación,combinación,argumentación,traslación,memoración y tradición.
La combinación es el más útil para construir una " scientia de scientia ",cuyo objeto es " el conocimiento asequible al hombre ".
Por todo ello,la " Disputatio XXIX.
De combinatione " - recientemente reeditada y traducida al castellano por Ceñal - es la parte más importante del Pharus scientiarum.
Izquierdo ofrece en ella una exposición bastante completa de la combinatoria que incluye algunas exposiciones originales.
Entre estas últimas,Víctor Navarro ha destacado una formulación para las variaciones (Vq = Cq P4).
una regla correcta para obtener las combinaciones con repetición y,sobre todo,una de las primeras aplicaciones del triángulo aritmético al cálculo combinatorio.
Esta última aportación,que realizó con independencia de Pascal,la expuso en los siguientes términos: " 17.
Mas para hallar los ternarios,los cuaternarios,los quinarios y los demás agregados,y aun también para los mismos binarios,de cualquier número dado de términos,sobre los cuales nada se encuentra en los Autores,sea la regla general y muy excelente la siguiente.
Hágase una tabla de cuadrados,tal como la construida a continuación ; y en su primera columna transversal A póngase la progresión natural aritmética de los números 1.2.
3.4.
5,etc. Después,en la segunda columna,también transversal,B póngase la unidad en todos sus cuadrados.
Asimismo,en la tercera columna transversal C,comenzando del número 2. póngase también la progresión natural,2.3.
4.5,etc. A su vez,en la primera columna descendente D,comenzando desde la unidad del segundo cuadrado,hágase descender la progresión natural aritmética,1.2.
3.4.
5,etc. Hecho esto,los números que se han de poner en los cuadrados de la segunda columna descendente E,se hallarán de esta manera.
Súmense el cuarto número de la columna D y el tercero de la columna E,que son 3 y 3,y póngase la suma (6) en el cuarto cuadrado de la columna E. El cual número 6,sumado,a su vez,con el número del quinto cuadro de la columna D,que es el 4,dará el número 10.
que es el que se ha de poner en el quinto cuadrado de la columna E ; y así se procederá después indefinidamente,sumando siempre el último número hallado de la columna E con el número del cuadro inmediatamente inferior de la columna D. y poniendo la suma de ambos inmediatamente debajo del mismo de la columna E y junto al mismo número de la columna D,que han sido sus sumandos.
Del mismo modo se hallarán los números que se han de poner en los cuadrados de la tercera columna descendente F,esto es,sumando el número cuarto de la segunda columna E con el tercero de la columna F,y colocando la suma,que es el número 10,en el cuarto cuadrado de la misma columna F,y así después descendiendo por los cuadrados sucesivos.
Del mismo se procederá en todas las columnas siguientes.
Con lo cual se podrá extender la tabla hasta el infinito.
Una vez construida de la manera dicha,desde los cuadrados de la primera columna descendente trácense líneas transversales o cuasi diametrales hasta los cuadrados de la primera columna transversal A,comenzando desde el cuarto cuadrado de la columna D,como puede verse en la tabla.
Y así toda su construcción estará acabada para hacer patente nuestro propósito.
..
19.
El uso de esta tabla (la cual,como dijimos,puede extenderse indefinidamente) es el siguiente.
Sea el número dado de términos,por ejemplo,8,del cual se quiere hallar el número de quinarios,que se pueden hacer de él: búsquese el número 8 en la primera columna descendente D,y el número S en la primera columna transversal A,y véase en qué cuadrado de los que debajo del número S descienden por su misma columna concurra la línea diagonal trazada desde dicho número 8: ese cuadrado dará el número,de los quinarios,que se pueden hacer de ocho términos,que son 56.
Del mismo modo se hallarán los binarios,los ternarios,los cuaternarios,etc.,posibles de cualquier número dado en términos: buscando siempre en la columna D el número de términos,y en la columna A el número designante (exponente) la combinación buscada: si es el de los binarios,el número 2,si el de los ternarios,el número 3,si el de los cuaternarios,el número 4,etc.,y viendo en qué cuadrado de la columna descendente bajo el número designante la combinación concurra la línea diagonal trazada desde el número de los términos: tal cuadrado dará el número de las combinaciones.
" La " Disputatio de combinatione " fue utilizada por científicos españoles del siglo XVII de mentalidad renovadora,como Juan Caramuel y Tomás Vicente Tosca.
También lo hicieron otros extranjeros,entre ellos el alemán Athanasius Kircher,que sirvió de puente para que Leibniz conociera el libro de Izquierdo,que citó en varios escritos posteriores a la publicación de su obra De Arte Combinatoria (1666).
Izquierdo reconoce las limitaciones del método que ha expuesto.
De los conceptos de " universalidad suma " no pueden deducirse las " verdades particulares ",que deben ser investigadas por las " ciencias concretas ".
Por otra parte,encuadra esta epistemología en una concepción del microcosmos humano contenido en el macrocosmos del universo y con la comunidad política como intermedio.
Ya en Roma,Izquierdo publicó un Opus theologicum (" Escritos teológicos ") en dos volúmenes (1664 - 1670) que incluye algunos estudios de cuestiones de filosofía natural.
Uno de los objetivos que persigue la agricultura española es el de satisfacer la demanda de productos agrarios de la sociedad procurando mantener el equilibrio en la balanza de pagos del comercio exterior agrario.
Alcanzar este objetivo no resulta fácil,como nos lo demuestra la realidad cotidiana.
En España hemos pasado de una balanza del comercio exterior agrario con un saldo positivo de unos 100 - 200 millones de dólares en 1961 - 1965 a un saldo negativo de 1000 millones de dólares en 1974 - 1978.
Ello ha sido la consecuencia lógica del aumento de la renta percápita de la población,que ha motivado un cambio estructural de la dieta media,cuyo rasgo más característico ha sido el aumento del consumo de productos de origen animal y una reducción del consumo de alimentos vegetales.
Satisfacer las necesidades de proteínas motivadas por el cambio del hábito alimentario que ha experimentado la sociedad española en las dos últimas décadas exige un aumento de la producción agraria equivalente al 50 por ciento,aumento que,al no haberse alcanzado en el interior,ha obligado a aumentar las importaciones de productos agrarios,que han provocado el desequilibrio negativo en la balanza de pagos.
Resulta paradójico que un país que tiene unos 20 millones de hectáreas cultivables y,de ellas,2.700.
000 de regadío como es el nuestro,posea una balanza comercial agraria deficitaria.
Para reducir este déficit se impone,por un lado,aumentar las producciones de maíz y de soja al objeto de disminuir los volúmenes que se importan y,por otro,aumentar las exportaciones.
Para alcanzar estos objetivos se precisa,en primer lugar,corregir las deficiencias inherentes a toda agricultura tradicional como es la española y,después,tratar de aumentar los rendimientos unitarios mediante la introducción de nuevas técnicas de cultivo y haciendo un uso más racional de los fertilizantes,fitosanitarios y tecnologías disponibles.
De acuerdo con la evolución de otros países,parece evidente que el desarrollo agrario del nuestro sólo podrá alcanzarse a través de una profunda transformación de las tecnologías de la producción agraria.
Ello exige realizar los trabajos necesarios para poner en marcha las tecnologías más avanzadas,disponer de una estructura de extensión agraria con capacidad para acercar esas tecnologías a los agricultores y aumentar considerablemente el capital que se destina al desarrollo de la producción y comercialización agraria de acuerdo con las nuevas tecnologías.
Desde la década de los sesenta se están realizando en España esfuerzos por conseguir una ordenación sistemática y eficaz de la investigación y de los servicios de extensión agraria,así como del mecanismo de interconexión entre ambos.
Todo ello es básico para trazar una política agraria eficiente.
Con objeto de contribuir a esta política,los autores (P. Cuñat,del Instituto de Agroquímica y Tecnología de Alimentos de Valencia,A. Aguilar,de la Estación Experimental " La Mayora " de Caleta de Vélez,y V. García,del Centro de Edafología de Tenerife) tratan aquí de apuntar algunas ideas sobre las necesidades de investigación.
Para ello,toman como base de partida la situación actual de la producción agrícola de exportación española ; en colaboración posterior se ocuparán de los problemas prioritarios para concluir apuntando las acciones,actividades o estudios que deberían emprenderse.
La precocidad de algunas producciones agrícolas derivadas de la situación meridional de la península ibérica y de las islas Canarias ha motivado nuestra especialización en el cultivo de frutas y hortalizas para la exportación.
En estas áreas se ha dado prioridad a las producciones de exportación.
Cuando se examinan los datos relativos a las horas de insolación eficaz que inciden en España y se comparan con los otros países europeos se aprecia,en términos generales,una situación de privilegio,pues la mayor parte del país goza de unas 250 () - 3000 horas de sol por año.
La temperatura media anual de la zona mediterránea,donde se halla ubicada la mayor parte de la producción agrícola de exportación,oscila entre los 15 y 19 grados centígrados ; la suavidad de los inviernos permite el cultivo de hortalizas al aire libre o,en todo caso,bajo una protección ligera contra los vientos fríos.
Algunas zonas del área mediterránea,sur y este de la península,así como las islas Canarias,están prácticamente exentas de heladas.
Junto a estas condiciones de insolación eficaz y temperaturas suaves,francamente favorables para una producción agrícola de exportación,se dan otras,como son: escasa pluviometría y humedad y excesiva evaporación,que han de corregirse para evitar sus perniciosos efectos sobre los cultivos.
La pluviometría de la zona del mediterráneo español es escasa,inferior a 400 milímetros anuales y,además,irregular y estacional (primavera y otoño).
Esta escasez,estacionalidad e irregularidad hacen necesario que la producción agrícola se lleve a cabo en regadío.
Las humedades relativas medias oscilan entre el 35 por ciento,que puede darse en algunas zonas de Andalucía en pleno verano,y el 75 - 80 por ciento en otras del litoral levantino.
En general,la evaporación media anual del área mediterránea es mayor que la precipitación.
Según los índices de Thornthwite,este área es de clima semiárido y en algunas partes árido.
Afortunadamente,toda la región posee buenas reservas de aguas subterráneas ; los habitantes han sabido adaptarse al medio y,perforando el suelo,han aprendido a proveerse del agua necesaria para el regadío.
El mercado español de frutas y hortalizas está en plena expansión desde el año 1960,aproximadamente.
Pero las corrientes comerciales existentes en este mercado entre España y los otros países europeos tienen,prácticamente,un solo sentido cuyo origen está en España.
Como una parte considerable de las exportaciones agrícolas españolas se dirigen a los países del Mercado Común,se prevé que la entrada de España en la Comunidad Económica Europea ha de repercutir favorablemente para la producción hortofrutícola de nuestro país.
La integración de España en Europa ha de contribuir a equilibrar la balanza comercial agraria.
Mas,para ello,es necesario disminuir los costes de producción de los principales cultivos de exportación,lo cual requiere a su vez que mejoremos nuestra tecnología agrícola.
El cultivo de los cítricos en nuestro país es de una gran importancia económica.
España es el primer productor de cítricos de la cuenca del mediterráneo y el tercer productor mundial.
Si se analiza la evolución de las exportaciones de cítricos españoles,se ve que durante el período 1930 - 1955 España proporcionaba el 63 por ciento de los frutos consumidos en Europa ; en cambio,durante el período 1955 - 1972 este porcentaje representó solamente el 40 por ciento,a pesar de que el consumo de frutos cítricos en Europa mantuvo una tendencia a la alza.
La competencia de los otros países exportadores del mediterráneo aumentó notablemente ; y así,en los períodos citados,la exportación desde Marruecos pasó del 3,4 por ciento al 16,3 por ciento del consumo europeo y la de Israel pasó del 12,7 por ciento al 16,9 por ciento,en los mismos períodos.
Por fortuna,desde 1972 la participación española en el consumo europeo de frutos cítricos se mantiene prácticamente constante.
En la actualidad,frente a un consumo estabilizado de unas 3.000.
000 toneladas métricas,España mantiene su posición ; seguimos siendo el primer país exportador de frutos cítricos con unas 1.500.
000 toneladas,aproximadamente.
Ante la estabilización del consumo y la creciente competencia de otros países,en España debemos esforzarnos por producir frutas de mejor calidad,que satisfagan las exigencias del consumidor.
para mantener nuestra posición en Europa.
El cultivo de los cítricos en España ocupa una extensión aproximada de 200.000 hectáreas.
La producción anual oscila alrededor de las 2.800.
000 toneladas,de las cuales 1.900.
000 corresponden a naranjas,650.000 a mandarinas,200.000 a limones y unas 10.000 a pomelos.
De esta producción se destinan unas 2.500.
000 toneladas a la comercialización en fresco,de las cuales se exportan 1.500.
000 - 1.600.
000 toneladas ; unas 750.000 - 850.000 se reservan para el consumo interior.
La producción restante se destina a la industrialización en forma de zumos y conservas.
El valor estimado de las exportaciones de cítricos españolas se cifra en unos 50.000 millones de pesetas.
En lo concerniente a las hortalizas de exportación,las regiones españolas ribereñas del Mediterráneo junto con las islas Baleares y Canarias constituyen la zona productora más precoz de Europa.
El valor de las exportaciones de hortalizas es de unos 12 - 13.000 millones de pesetas.
En la ilustración superior se relacionan,por orden de importancia decreciente,las principales hortalizas exportables,que representan en conjunto el 95 por ciento,aproximadamente,de las exportaciones.
Las cinco primeras (tomate,patata,melón-sandía,cebolla y pepino) suponen alrededor del 80 por ciento.
Les siguen en importancia las judías verdes y las alcachofas y,después,lechuga y calabacín con unos 500 millones de pesetas ; a partes prácticamente iguales y a gran distancia del grupo primero,los pimientos y berenjenas.
El cultivo de pimientos y berenjenas está adquiriendo importancia digna de consideración en Canarias,desde donde salen,aproximadamente,el 40 por ciento del total de las exportaciones españolas.
Somos el primer productor de melón del mundo con unas 700.000 toneladas anuales.
El 90 por ciento de los pepinos exportados procede de Canarias.
La superficie dedicada al cultivo de la mayoría de las especies consideradas se mantiene prácticamente constante ; sólo el tomate,judías verdes y alcachofas muestran una tendencia creciente año tras año.
Conviene señalar que el 40 por ciento del tomate de invierno se exporta desde Canarias.
La superficie dedicada al cultivo de esta especie en España aumenta a un ritmo medio anual de 1500 hectáreas,estimulado,principalmente,por el aumento de las exportaciones.
Durante los últimos diez años,la superficie dedicada al cultivo de judías verdes ha aumentado a un ritmo de unas 700 - 800 hectáreas por año,si bien,durante los tres últimos años,la superficie cultivada se mantiene constante ; la de alcachofas crece a un ritmo de 1000 hectáreas por año ; el aumento de la superficie de cultivo de estas dos especies se debe,sobre todo,al incremento del producto industrializado,que también se dedica en un alto porcentaje a la exportación.
La exportación de alcachofas en nuestro país se ve favorecida por la precocidad con la que se presenta en el mercado.
Se exporta desde noviembre hasta abril ; la producción española en esta época concurre al mercado sin competencia,pues la producción francesa empieza en mayo,que es cuando finaliza el ciclo anual de esta especie en España.
El valor de las exportaciones de frutos no cítricos asciende a unos 10.00011.
000 millones de pesetas.
Las frutas exportables más importantes pueden agruparse en: uva de mesa y pasas,frutos secos,frutas de hueso,frutas de pepita y frutas tropicales.
El valor de las exportaciones de las frutas pertenecientes a los grupos citados representa el 90 - 95 por ciento del valor total de las exportaciones de frutas no cítricas.
El grupo más importante es el de los frutos secos ; entre éstos,el almendro es,con mucha diferencia,la especie que participa en mayor proporción en tales exportaciones.
La superficie dedicada al cultivo del almendro crece con cierta irregularidad año tras año ; durante los diez últimos,se ha dulcificado su superficie de cultivo.
España es el primer país productor de almendra,seguido en importancia por Estados Unidos,Italia,Grecia y Turquía.
Dentro del grupo de los frutos secos,el avellano es otro a destacar por el valor de la producción exportable ; las exportaciones de avellana crecen con bastante regularidad y durante el año 1977 alcanzaron los 900 millones de pesetas ; el cultivo de esta especie está también en expansión,creciendo a un ritmo medio anual de casi 1000 hectáreas.
A los frutos secos les sigue en importancia la uva de mesa y pasas con una producción exportable que representa unos 2500 - 3500 millones de pesetas.
La superficie destinada al cultivo de esta especie está prácticamente estabilizada,o en ligera regresión.
A gran distancia del valor de las exportaciones de frutos secos o uva de mesa se halla el de las frutas de hueso y de pepita que,en conjunto,representan unos 1500 millones de pesetas con una variación anual muy fuerte,función de las producciones de los países del Mercado Común.
La superficie destinada al cultivo de frutales de hueso y de pepita en el país está en franca regresión y se prevé una reducción importante de las superficies cultivadas.
Las frutas tropicales exportables,excepción hecha del plátano que compite en desventaja con la producción de las otras regiones productoras dada la carestía de su cultivo en Canarias,están representadas por el aguacate y el chirimoyo ; hasta la fecha,no han tenido una incidencia importante en las exportaciones agrícolas debido,sin duda,al escaso volumen producido ; sin embargo,se prevé un fuerte incremento en el futuro inmediato,dado el fuerte ritmo con el que los agricultores de Andalucía oriental y Canarias están realizando nuevas plantaciones (unas 1000 hectáreas anuales de las dos especies),estimulados por la buena acogida de estos frutos en los países del Mercado Común.
Las exportaciones de aguacate desde Canarias pasaron de unas 30 toneladas en 1975 - 76 a 250 toneladas en 1978,lo que representa un incremento superior al 700 por ciento.
Por último: flores y plantas vivas.
La superficie dedicada a la floricultura en España se eleva a unas 2500 hectáreas,unas 900 de las cuales se cultivan bajo protección (invernaderos o abrigos de plástico).
Al cultivo de claveles se destinan unas 930 y 530 hectáreas al de rosas ; las 1000 restantes se reparten entre el cultivo de plantas vivas y flores distintas de claveles y rosas.
La floricultura constituye uno de los sectores más jóvenes y prometedores de nuestra producción hortofrutícola de exportación ; durante los diez últimos años la superficie de cultivo de flores y plantas vivas se ha multiplicado,aproximadamente,por tres,aumento que no tiene parangón en ningún sector agrícola de España,y aún estamos muy lejos de alcanzar las cotas de producción de países como Francia,Italia,Bélgica,Holanda,Israel y otros.
El valor de las exportaciones de flores y plantas vivas ascendió en 1977 a 1550 millones de pesetas,siendo el capítulo de plantas vivas el más importante con 780 millones de pesetas ; le siguen a gran distancia las rosas,con 310 millones,y los claveles,con 220 millones de pesetas ; los 240 millones de pesetas restantes exportados se reparten entre otras flores y hojas y ramas ornamentales.
(P. Cuñat,A. Aguilar y V. García.
) La teleinformática progresa a pasos agigantados.
La utilización de los computadores en cualquier lugar donde se desarrolle la actividad humana es recurso que nos acompañará durante los próximos años,como un feliz resultado del maridaje entre las técnicas de procesado de la información y de las de comunicación de datos.
El avance de la técnica y el desarrollo de la tecnología facilitan la creación de un consumo masivo que,merced al aumento de la producción,ocasiona un abaratamiento de los costes,lo que a su vez incide sobre las primeras completando la espiral.
Si este principio es válido en la mayoría de las actividades económicas,lo es doblemente en el campo de la electrónica en general y en el de la informática en particular.
Vaya,pues,por delante el hecho de que también la teleinformática avanza por la gran rentabilidad de sus productos.
Alguien afirmaba con ironía no hace mucho que,además de un excelente área de inversión,la teleinformática constituye una interesante herramienta para mejorar la calidad de la vida.
Nada más cierto a nuestro entender.
JUICIO.
El videotex es la primera manifestación doméstica de la teleinformática,esto es,el primer servicio pensado para acceder desde el hogar,mediante un terminal y a través de un medio público de comunicación,hasta una información almacenada en un computador remoto.
Explicaremos brevemente algunos aspectos técnicos de los elementos que conforman un servicio videotex.
Nos referiremos en primer lugar a las características generales de un sistema teleinformático para completarlas,a continuación,con las específicas de un sistema videotex.
La utilización de un computador desde un lugar remoto implica,al menos,el empleo de tres tipos de elementos: el computador,al cual se desea acceder,un elemento terminal que facilite el acceso,y,por último,un medio de comunicación que permita alcanzar a los dos tipos de sistemas mencionados.
Por lo que se refiere al computador,poco tenemos que decir aquí ; sólo recordar que deberá estar dotado tanto del soporte físico como del soporte lógico necesarios para realizar sus operaciones.
Las funciones básicas de este computador consistirán,por una parte,en el tratamiento de la información que le corresponda y,por otra,en la gestión del diálogo con los distintos terminales que le estén asociados.
Habitualmente,el acceso a un computador se realiza a través de un terminal que suele estar provisto de un teclado,similar al de una máquina de escribir,y de una unidad de presentación de la información,ya sea una impresora ya sea una unidad electrónica de visualización,generalmente un tubo de rayos catódicos (una pantalla).
A este tipo de terminal se le suele llamar alfanumérico,debido a que la información que maneja consta fundamentalmente de letras,números y un conjunto de otros símbolos ortográficos.
Todos ellos reciben la denominación genérica de caracteres ; éstos se manipulan en forma binaria según un determinado código o alfabeto.
La comunicación entre un terminal y un computador se funda básicamente en la transmisión,entre dichos elementos,de los códigos correspondientes a los caracteres que constituyen las informaciones que desean intercambiar.
Si dicha comunicación se realiza a través de una línea telefónica,cuyas características,como es natural,han sido previstas para la transmisión de la voz en la banda de frecuencias de 300 a 3200 hertz,será necesario proceder a una adaptación para que se puedan transmitir a través de ella informaciones binarias.
Dicha adaptación consistirá.
en el origen,en una operación de modulación apropiada,con objeto de transformar la señal digital en otra analógica capaz de transmitir por la línea ; en el destino se precisará realizar la operación contraria,denominada demodulación,a fin de recuperar nuevamente la información digital.
Se conoce por modem el equipo que realiza dichas operaciones,modulación y demodulación.
Partiendo,pues,de las características generales de un sistema teleinformático,estamos en condiciones de comentar aquellas que son específicas de un sistema videotex.
Como veremos,éstas serán consecuentes con los objetivos que se pretende cumpla el nuevo servicio teleinformático.
Dado que el videotex está orientado hacia el gran público,debe ser ante todo: sencillo de manejar,atractivo en lo que se refiere al acceso a la información y económico en su coste para un gran número de posibles usuarios.
El servicio videotex permite el acceso a información de uso e interés general,ya sea suministrada por organismos públicos o por promotores privados,y,en general,pretende proporcionar cualquier servicio que pueda ser facilitado por un computador como,por ejemplo,la transmisión de mensajes entre usuarios,el acceso a medios de cálculo,etcétera.
Al igual que en cualquier otro sistema teleinformático,en un servicio videotex intervendrán también: un computador al que se desea acceder,un terminal que facilite el acceso y un medio de comunicación que materializa el enlace.
El computador contendrá la información a la que es posible acceder,es decir,la base de datos ; además,deberá realizar las operaciones de atención a los terminales,así como las de tarificación y facturación de los servicios prestados a los usuarios.
En los terminales es donde van a influir más las características a las que hemos hecho alusión antes.
Como unidad de presentación de la información se utiliza un receptor de televisión,el televisor en color doméstico,que permitirá presentar la información de forma atractiva en uno de los ocho tonos que es posible obtener combinando los tres colores fundamentales de la pantalla: rojo,azul y verde.
El teclado para la realización de las consultas,de simple y fácil manejo.
puede acercarse al tipo que figura en los modernos teléfonos de teclas y que contienen los números del 0 al 9 y los símbolos y * ; doce símbolos en total a través de los cuales se desarrollará el diálogo del usuario con el sistema para acceder y seleccionar la información deseada.
El teclado puede tener una forma similar a la de un mando a distancia de cualquier televisor.
Para la adaptación a la línea telefónica será necesario utilizar un equipo formado por un modem,que permita además compartir la línea con el teléfono y utilizarla indistintamente para uno u otro tipo de comunicación,aunque en cada caso de forma exclusiva.
Por lo que se refiere a la presentación de las informaciones sobre la pantalla existen distintas soluciones.
Aquí vamos a ocuparnos de la que quizá se use en el sistema videotex español,que coincide a grandes rasgos con la empleada en el sistema prestel,desarrollado y en servicio actualmente en Gran Bretaña.
Recordemos que se trata de representar en la pantalla del televisor textos y figuras.
La solución utilizada,conocida por sistema de presentación alfamosaico,parte de la división de la pantalla en una matriz de celdas,de 40 columnas por 24 filas,lo que da un total de 960 elementos ; en cada uno de ellos podremos representar un símbolo alfanumérico o un símbolo gráfico,o bien una parte de ellos.
En la adopción de un sistema videotex,surge el problema de la elección del conjunto de símbolos que se desea que puedan representarse en la pantalla del aparato.
En el caso de España,el alfabeto debe ser el necesario para permitir escribir correctamente en cada una de las cuatro lenguas del estado: castellano,catalán,vascuence y gallego,utilizando letras mayúsculas y minúsculas.
Por lo que se refiere a los símbolos gráficos,el conjunto seleccionado debe permitir la composición de dibujos que resulten agradables al usuario.
En un sistema videotex típico,el número total de símbolos tanto alfanuméricos como gráficos que pueden llegar a representarse oscila en torno a los 300,que podrán aparecer en diferentes formas y colores combinados con distintos fondos.
La comunicación entre el computador y el terminal consistirá en la transmisión de las informaciones necesarias para componer con la ordenación deseada las 960 celdas que forman cada pantalla.
Para configurar cada celda,habrá que transmitir el código del símbolo que se desea representar en ella,su forma,color,fondo,etcétera ; todo ello convenientemente codificado.
El control de las funciones de decodificación de estas informaciones,la presentación en la pantalla,y la atención al teclado del usuario corresponderá a un equipo electrónico controlado por un microcomputador [véase " Microcomputadores ',por Antonio Alabau y Joan Figueras ; INVESTIGACION Y CIENCIA,septiembre de 1977: páginas 92101],que deberá añadirse a los circuitos que actualmente existen en un receptor de televisión.
Cierto es,pues,que la teleinformática progresa a pasos de gigante,que el videotex es su primera manifestación doméstica y que la evolución no se detendrá aquí.
Alguien lo dijo: " en la mayoría de las casas existe un televisor,en muchas hay también un teléfono,hacedlos trabajar conjuntamente y habréis creado la base de un poderoso servicio '.
(Antonio Alabau.
Universidad Politécnica de Barcelona.
) Se publicó la primera edición de la obra de Juan Luis Vives.
De disciplinis,uno de los principales textos epistemológicos del Renacimiento.
La relación de la obra de Vives con las ciencias de la naturaleza y sus aplicaciones puede entenderse adecuadamente,considerándola desde la perspectiva de su programa educativo.
' Una lectura cuidadosa de los escritos de Vives - ha afirmado Noreña - conduce al convencimiento de que sus últimos doce años de intensa reflexión filosófica estuvieron motivados por un creciente interés de precisar las implicaciones filosóficas de la reforma educativa.
" El principal resultado de dicha reflexión fue precisamente la publicación de de disciplinis.
Desde nuestro punto de vista,entre los supuestos pedagógicos de Vives hay que destacar su insistencia en la necesidad de un conocimiento directo y práctico de las cosas de la naturaleza.
La observación de las " res naturae " accesibles a los sentidos debe comenzar.
en el inicio mismo de la formación escolar,con las más sencillas,sin que entonces sean oportunas ' las controversias,sino la silenciosa contemplación de la naturaleza ".
De acuerdo con esta línea,Vives recomienda el estudio de la cosmografía y la historia natural,de la filosofía natural,de la agricultura y de la medicina.
Esta última,en especial.
Ia considera como el modelo de " arte " o saber práctico,dedicándole la exposición más detenida y aguda de su tratado De disciplinis.
" Cuando,hija de la experiencia.
controlada por el juicio agudo,el arte de la medicina hubo nacido y crecido - afirma en ella Vives - su corrupción primera y la más maligna y cruel de todas le provino de la carencia de práctica,de experiencia y de criterio filosófico.
Muchos son los que confían exclusivamente en los experimentos personales,y muchas veces en los ajenos también,pobres y desarmados de juicio y de esta observación universal,y creen que los experimentos en torno a dondequiera tienen valía,siendo así que ellos aprovechan en alguna enfermedad específica de mujer y dañan en la de un hombre,y no conviene el mismo tratamiento a un niño y a un viejo,a un bilioso o a un pituitoso,ni en invierno como en verano,ni en España lo mismo que en Rusia.
Otros,por el contrario.
bien abastados de preceptos filosóficos,pero vírgenes de toda práctica,sin experiencia de realidades,aplican la mano a las curaciones,siendo así que la parte principal de aquel arte no consiste en la inteligencia y retención honrada de lo que está escrito,sino en la habituación a sus aplicaciones,que cuando se llevan a efecto y obra,más que del teórico es cosa del práctico.
" Pero como sea que cada cual se tiene a sí mismo por muy recomendada la salud,para con la cual,por el descomedido amor que la tenemos,sentimos debilidades y temores sobrado indulgentes,ese amor,conjugado con el miedo,quita todo discernimiento de lo útil y de lo nocivo.
Ello hace que no hay hombre alguno que anuncie que va a hacer algo efectivo para nuestra salud,que no le prestemos muy despierta atención,tanto más cuando la enfermedad aprieta y nos aflige con sus acerbos y ardientes escozores.
Desilusionados y defraudados más de una vez,volvemos al mismo punto,olvidados del desengaño primero ; si ya no es,por ventura,alguno escarmentado por sus propias decepciones y por las ajenas,reacciona en odio de la profesión,teniendo del arte un peligro mayor o que,acosado por la desesperación,prefiere arrostrar una manifiesta crisis mortal que confiarse en manos de una profesión tentada con suceso infeliz por él y por otros ; pero de estos hombres hay pocos.
Todos los otros,en su casi totalidad,dan crédito a todo el que promete curaciones,y en él depositan su confianza ; por eso esta profesión se hizo gananciosísima.
¿Qué no pagará el doliente a quien le libró de un sufrimiento? ¿Habrá,por ventura,alguno tan desatinado que,estando en peligro de muerte,piense en economías y escaseces? ¿Para qué ocasión reserva el dinero quien está en trance de inmediata desaparición? Piel por piel y todo cuanto el hombre tiene dará por su vida,como dice Job.
Donosamente Filipo,padre de Alejandro,como padeciese de una luxación de clavícula y un médico le pidiera una inmediata entrega de dinero: ' Toma - le dijo - todo cuanto quieras,pues tú tienes la llave '.
Pero en la antigüedad esta profesión rendía mucho más que en esos tiempos nuestros ; yo creo que porque parecían mayores y más admirables sus servicios.
Pero aún hoy día no faltan a los médicos sus buenos ingresos.
Un médico podrá mantenerse y comer en el burgo más podrido,en la aldea más perdida y soledosa,donde ni siquiera sonó el nombre de ninguna de las otras artes.
Esta ganancia tan obvia y disponible alucinó a muchos que,arruinados y desesperados,se acogieron a esta profesión como a una última y sagrada áncora y como a la segura arena en el naufragio.
" Menos penetrantes son los comentarios de Vives sobre la filosofía natural,ya que,como los demás humanistas,no llegó a comprender el significado de las críticas nominalistas a la dinámica aristotélica tradicional.
También era una actitud generalizada entre los miembros del movimiento humanista el desinterés que manifiesta por el cultivo de las matemáticas puras.
Plenamente consecuente con estos supuestos fue la valoración positiva de la técnica por parte de Vives,en abierto enfrentamiento con la tradición platónica y aristotélica que despreciaba los oficios manuales y las " artes mecánicas " como ocupaciones propias de los siervos.
Paolo Rossi ha destacado varios pasajes de esta obra de Vives en el contexto del cambio de la imagen social de las relaciones entre ciencia y técnica.
Recoge,en primer lugar,sus exhortaciones a los estudiosos a prestar seria atención a los problemas técnicos,a " las artes y descubrimientos humanos en lo que toca y atañe a la alimentación,al vestido,a la vivienda ; en esta tarea le ayudarán los tratadistas de agricultura y los que estudian la naturaleza y las propiedades de las hierbas y los animales y los que trataron de arquitectura... Las artes de tracción animal en que andan mezclados el caballo,el mulo.
el buey y toda suerte de vehículos,así como el arte vecina de la navegación ".
El hombre culto " no tenga empacho de acudir a las ventas y a los obradores,y preguntar y aprender de los artesanos las peculiaridades de su profesión ; porque de muy atrás los sabios desdeñaron apearse a este plano y se quedaron sin saber una porción incalculable de cosas que tanta importancia tienen para la vida ".
Vives,sin embargo,no se limita a recomendar el estudio de estas cuestiones únicamente por su utilidad práctica.
Rossi ha puntualizado también que el pensador valenciano considera que la técnica proporciona un conocimiento directo de la naturaleza.
superior al puramente especulativo.
Hablando de los escolásticos,afirma que ' tienen una virginal inexperiencia de estas cosas y de esta naturaleza,mejor conocida por los labradores y artesanos que no por ellos,filósofos tan grandes.
Enojados con esa naturaleza,que ignoraban,fantaseáronse otra a base de bagatelas,de sutilezas,de aquellas zarandajas que nunca Dios creara,que se denominan formalidades,relaciones,ideas platónicas y otras monstruosas invenciones que no entienden los mismos que las engendran,quienes,puesto que no pueden otra cosa,al menos la autorizan con un nombre de sonido y dignidad,llamándola metafísica.
Y si ocurre que alguno tiene un ingenio desconocedor de esa naturaleza o propenso a las fantasías o a los sueños delirantes,ése dicen que tiene talento metafísico ".
EL sodio es el ión dotado de carga positiva (catión) más abundante en la mayoría de los biotopos.
En muchos casos,sobrepasa varias veces la abundancia o concentración de potasio.
En la composición media de la corteza terrestre.
Las cantidades de sodio y potasio son parecidas.
Ahora bien,debido a que los minerales sódicos sufren una descomposición más rápida que los potásicos,la solución resultante de la meteorización de las rocas es más rica en sodio que en potasio.
A este factor hay que añadir que el potasio es fijado más fuertemente por las arcillas y acumulado preferentemente por los organismos vivos.
En virtud de este proceso,el sodio es arrastrado a los mares,donde se acumula en grandes cantidades.
La evaporación del agua de los mares y los factores que han intervenido en la formación de los suelos han originado así ambientes muy diversos para la vida,en punto a concentraciones de potasio y sodio.
En contraposición a la diversidad de los biotopos definidos en función de-las concentraciones de potasio y sodio que muestran,las células de los organismos vivos presentan una gran homogeneidad: contienen,casi exclusivamente,potasio,y a una concentración bastante más elevada que la del medio externo [véase la tabla de la página 75].
Son excepción a esta regla algunos organismos que viven en ambientes de alta salinidad sódica.
En presencia de altas concentraciones de sales externas,la supervivencia de un organismo depende de su capacidad para mantener en sus células una solución más concentrada que la del medio circundante.
Sólo cuando el medio interior es más concentrado,el agua pasará del exterior al interior y las células podrán crecer.
A fin de mantener una elevada concentración de solutos,los organismos que viven en biotopos de alta salinidad acumulan cloruro de sodio.
aunque en todos los casos las células concentran fuertemente el potasio del medio.
Se ha discutido mucho en torno a la función del potasio en las células de los seres vivos.
El requerimiento de potasio por parte de algunos enzimas hizo pensar inicialmente en que las necesidades de potasio podrían obedecer a una exigencia para la actividad de esas proteínas.
Sin embargo,la función enzimática del potasio podría ser secundaria y desarrollada durante la evolución,precisamente por la abundancia de potasio y por su dinámica durante el crecimiento de la célula.
En relación con los cationes celulares,un hecho indiscutible es su papel como neutralizadores de las cargas de los aniones (iones portadores de carga negativa).
Las cargas negativas de los aniones pueden ser neutralizadas por protones,pero anión y protón pueden reaccionar para dar una molécula sin carga,un ácido.
La cantidad resultante de protones libres dependerá de las características del anión y nos definirá la acidez del sistema.
La concentración de protones en el agua pura es lo - 7 molar (pH = 7),y la de los líquidos celulares muy próxima.
La célula contiene gran cantidad de aniones de ácidos orgánicos y de ésteres de ácido fosfórico,para que el protón neutralizara estos aniones la concentración del protón debería ser 10 ' (Ph = 2) aproximadamente.
Pero no ocurre tal.
En los organismos vivos de nuestro planeta el catión potasio es el que neutraliza eléctricamente estas cargas,aunque el sodio puede contribuir significativamente en algunos casos en que existe una fuerte deficiencia de potasio.
La abundancia de potasio en la célula ha condicionado probablemente la evolución de muchos sistemas fisiológicos,quedando implicado el potasio en la regulación de importantes procesos celulares.
Por ejemplo.
el ribosoma contiene una gran cantidad de ácido ribonucleico,cuyos restos de fosfato están neutralizados por potasio.
Si se sustituye el potasio del ribosoma por sodio,Ia estructura del ribosoma resultará afectada,debido a que los dos cationes no son iguales,y alterada,consecuentemente,la síntesis de proteínas.
En el caso de una sustitución extensa,la síntesis de proteína se detendrá totalmente,según pudo comprobarse hace ya muchos años en mutantes de Escherichia que eran incapaces de retener el potasio cuando se suspendían en un medio con sodio y sin potasio.
Pero si la sustitución de potasio es sólo parcial,Ia síntesis no se detendrá totalmente,y en este hecho se basa el mecanismo de actuación de algunos virus.
Cuando un virus infecta a una célula,se altera el metabolismo normal,dirigiendo entonces el virus algunas de las funciones metabólicas.
Uno de los primeros acontecimientos después de la infección vírica es la inserción,en la membrana celular,de unas proteínas especiales de la cápside del virus que modifican los flujos de cationes,afectando a las concentraciones internas de potasio y sodio.
A los nuevos niveles de estos cationes,la síntesis de las proteínas del virus está favorecida frente a la síntesis de las proteínas de la propia célula.
Además de neutralizar las cargas negativas de la célula y regular la función enzimática,el potasio actúa como soluto implicado en la regulación osmótica en algunos organismos.
Obviamente,el alto contenido en potasio hace que el catión contribuya de un modo significativo en el establecimiento del potencial osmótico de cualquier célula ; además,algunas bacterias poseen la capacidad de variar el contenido en potasio.
en respuesta a la exposición a medios de baja actividad de agua.
Así,cuando la bacteria crece en un medio con baja actividad de agua se promueve la acumulación de potasio en mayor extensión que cuando crece en medios diluidos.
favoreciendo el paso del agua hacia el interior de la célula.
La función del sodio en las células vivas y los requerimientos de sodio son más complejos que los de potasio En general,puede decirse que el sodio no es indispensable para la célula.
aunque puede reemplazar y suplir parcialmente las funciones del potasio Sin embargo,la abundancia de sodio en muchos biotopos ha determinado que este catión quedara implicado en numerosos procesos de transporte.
como luego veremos.
Hay que tener presente que los animales con circulación cerrada (es decir.
dotados de una red de vasos) mantienen en sus líquidos extracelulares una alta concentración de sodio.
y que sus células se encuentran en condiciones similares a aquellas que viven en biotopos con alto contenido en ese mismo catión.
La universalidad de la tendencia a concentrar potasio,que alcanza a todas las células de los organismos vivos.
no se cumple en lo relativo a los mecanismos que realizan esa función.
No conocemos todavía con precisión los mecanismos que.
a lo largo de la evolución.
se han diferenciado para la acumulación de potasio.
Se admiten.
no obstante,tres mecanismos básicos que.
en general,corresponden a los de las bacterias,los vegetales y los animales Al igual que la mayoría de los organismos unicelulares que viven en medios muy diluidos y con marcada diferencia de las células animales.
Las bacterias tienen una gran impermeabilidad pasiva y están dotadas de mecanismos que les permiten concentrar el potasio varias decenas de veces.
La primera especie en la que se establecieron con claridad los mecanismos de los transportes de potasio y sodio fue Streptococcus faecalis,una bacteria láctica que no respira y que obtiene toda la energía para su metabolismo a partir de la fermentación de la glucosa a ácido láctico La incapacidad de respirar distingue a S faecalis de otras bacterias en cuyas membranas celulares se asienta un sistema respiratorio que.
durante la oxidación del sustrato.
expulsa protones al exterior de la misma [véase Cómo fabrican ATP las células ".
por P. C. Hinkle y R. E. MacCarty.
INVESTIGACION Y CIENCIA,mayo,1978].
En S. faecalis la energía para concentrar al potasio procede del ATP (trifosfato de adenosina) obtenido en la fermentación ; se trata.
empero,de un proceso secundario.
pues el primer paso para la acumulación es la extracción de protones.
En la membrana de S. faecalis existe un enzima de relativa complejidad que,al consumir ATP.
excreta protones al exterior de la misma.
La actividad característica de este enzima cuando se obtiene en tubo de ensayo es la hidrólisis del ATP,por lo que se la conoce por el nombre de ATPasa [véase ' Mecanismo molecular de la transducción de energía ",por Emilio Muñoz y J. Manuel Andreu,INVESTIGACION Y CIENCIA,febrero,1979].
La extracción de protones por la ATPasa genera un potencial eléctrico negativo en el interior de la célula.
que atrae hacia dentro cualquier compuesto con una carga positiva neta.
Es necesario tener presente que.
dada la magnitud de una bacteria y la capacitancia de la membrana.
el movimiento de un número muy reducido de protones genera un potencial eléctrico elevado En respuesta al potencial eléctrico.
el potasio se mueve hacia el interior de la bacteria.
pues en la membrana existe un transportador que reconoce al potasio y le franquea el paso a través de la misma.
La entrada del potasio permite más salida de protones hasta satisfacer las necesidades de la bacteria.
En el caso de una concentración muy baja de potasio en el exterior.
el ión se acumulará por difusión desde el exterior hasta que el potencial eléctrico correspondiente a su distribución iguale el potencial eléctrico máximo que puede obtenerse con la ATPasa.
El proceso de acumulación de potasio en las bacterias que respiran es semejante al expuesto para S. faecalis,siendo la única diferencia el origen del potencial eléctrico.
Como mencioné antes,en el proceso de la respiración se extraen protones hacia el exterior de la membrana.
generando este proceso un potencial eléctrico.
utilizado para la síntesis de ATP,el transporte y otros procesos.
La atracción de la carga eléctrica negativa sobre el protón determina que éste se mueva hacia el interior de la célula a través de la ATPasa.
Lo que genera ATP.
Simultáneamente.
Ia atracción de la carga eléctrica sobre el potasio determina su acumulación.
El esquema hasta ahora presentado para el transporte del potasio en bacterias constituye sólo una parte de una realidad mucho más compleja que aún no se conoce con detalle.
En Escherichia coli,bacteria que ha sido utilizada con gran intensidad como modelo biológico,se han aislado numerosos mutantes para el transporte de potasio.
Estos mutantes han permitido establecer la existencia de dos sistemas distintos para su transporte: el Kdp y el TrKA.
El sistema Kdp tiene una gran capacidad para acumular potasio.
permitiendo a la bacteria crecer en concentraciones extraordinariamente bajas de potasio (10 - molar).
El sistema TrKA tiene mucha menos afinidad por el potasio que el sistema Kdp.
pero es muy efectivo a concentraciones de potasio en el orden milimolar.
Estos dos sistemas corresponden a mecanismos completamente diferentes: mientras el sistema TrKA promueve la acumulación del potasio en respuesta al potencial eléctrico,el sistema Kdp actúa de una forma singular,pues obtiene la energía directamente del ATP,hidrolizando este compuesto para promover la acumulación del potasio.
Este sistema en tubo de ensayo tiene la capacidad de hidrolizar ATP.
por lo que también es una ATPasa.
Aunque se desconoce la forma en que el sistema Kdp transporta el potasio,existe la posibilidad de que algún anión o catión sea transportado simultáneamente,en el mismo sentido o en sentido contrario,ignorándose por tanto el valor de la carga neta movida.
En las células de los hongos y de las plantas,los mecanismos implicados en la acumulación de potasio no están tan caracterizados como en las bacterias,ni completamente confirmados los procesos propuestos.
La información más importante se ha obtenido con hongos,fundamentalmente Neurospora crassa y Saccharomyces crevisiae.
Sin embargo.
hay numerosas pruebas a favor de la existencia de mecanismos comunes con las células de los vegetales superiores.
En común con lo descrito para las bacterias,las células vegetales también mantienen un potencial eléctrico,negativo en el interior de la célula.
El punto más conflictivo se centra sobre el mecanismo mediante el cual se genera este potencial,aunque gran número de datos indican que una ATPasa existente en la membrana bombea protones al exterior.
como sucede en las bacterias.
Hay que hacer notar que esta ATPasa no es la misma que la ATPasa bacteriana,de la que se diferencia netamente por sus propiedades bioquímicas.
e ha mencionado antes que la vida se S desarrolla sobre medios en los que existe sodio.
incluso en concentraciones considerablemente más altas que las de potasio.
La existencia de un potencial eléctrico como origen inmediato de la acumulación del potasio indica claramente que el sodio también será atraído al interior de la célula.
Aun suponiendo una baja permeabilidad al sodio,y una gran capacidad de discriminación del transportador del potasio frente al sodio.
Ia acumulación de éste sería indefectible.
La mayor o menor permeabilidad sólo sería un factor cinético que retrasaría más o menos el correspondiente equilibrio termodinámico.
Desde un punto de vista teórico.
Ia única vía para que no se acumule sodio es la existencia de mecanismos especiales para su extracción.
La extracción del sodio,simultáneamente con su entrada,impediría que se alcanzase el equilibrio termodinámico,estableciéndose antes un equilibrio cinético entre la entrada y la salida.
La necesidad teórica de mecanismos encargados de la extracción del sodio ha recibido confirmación experimental en algunas bacterias,encontrándose los mecanismos implicados en la extracción de este catión.
En todos los casos estudiados,la extracción del sodio ocurre por un mecanismo que introduce protones a la vez que saca aquél.
El movimiento de los dos cationes tiene lugar simultáneamente.
y no puede darse uno de ellos por separado.
El transporte que tiene lugar de esta forma se denomina antiporte,y antiportador,el sistema de la membrana que lo realiza.
Desde un punto de vista energético,el movimiento del sodio con el protón hace que,en la salida del sodio.
haya que considerar simultáneamente el potencial electroquímico de los dos cationes.
Si la estequiometría del proceso es uno a uno,no se mueve ninguna carga neta ; por tanto.
el proceso será independiente del potencial eléctrico y el sodio podrá salir en contra de una mayor concentración del mismo en el exterior celular si el Ph exterior es inferior al del interior.
En este caso.
el potencial del protón para entrar arrastraría el sodio hacia fuera y,en el equilibrio,el cociente de las concentraciones dentro y fuera de los dos cationes sería inverso.
Cuando la estequiometría del proceso es mayor de uno.
esto es,cuando penetra en el interior de la célula más de un protón por cada sodio que sale,la salida del sodio estará también ayudada por el potencial eléctrico de la célula.
El potencial eléctrico,al arrastrar hasta el interior el exceso de carga positiva del protón,extrae el sodio.
Evidentemente,en este último caso la extracción del sodio será efectiva aun cuando la concentración de protones no sea mayor en el exterior que en el interior.
No se sabe a ciencia cierta cuál es la estequiometría del proceso de extracción de sodio en E. coli.
Podría ser.
incluso,variable: uno a uno (1: 1),cuando el medio exterior es ácido.
y mayor que uno cuando es alcalino.
En las células animales.
Ia acumulación de potasio y la extracción del sodio tiene lugar mediante un proceso completamente diferente de los descritos.
No hay en ellas movimiento de protones,y los movimientos de potasio y sodio se realizan simultáneamente por una bomba que consume directamente ATP.
Esta ATPasa se estimula con potasio y con sodio.
por cuya razón se denomina ATPasa (Na + + K +).
El mecanismo de actuación de la bomba de potasio y sodio en las células animales es también un proceso de antiporte ; cada ATP consumido supone la salida de tres sodios y la entrada de dos potasios.
El resultado,además del movimiento de los cationes.
es la creación de un potencial eléctrico,negativo en el interior de la célula.
Debido a la permeabilidad de las células animales a los cationes,se establece un flujo continuo de entrada y salida de sodio y potasio,que determina los niveles celulares de los cationes.
En condiciones de equilibrio,la ATPasa bombea el sodio y el potasio en contra de sus correspondientes potenciales electroquímicos,en tanto que ambos cationes se difunden pasivamente en sentido contrario.
Podemos diferenciar esos flujos mediante el uso de algunas drogas,por ejemplo,el esteroide cardiotónico ouabaina.
utilizado por algunas tribus africanas para envenenar flechas ; la droga inhibe la actuación de la bomba sin afectar los flujos pasivos,destruyendo el equilibrio natural de los cationes en la célula.
La incorporación de la ATPasa (Na + + K +) en vesículas de fosfolípidos ha permitido conocer con detalle la colocación del enzima en la membrana.
La ATPasa se extiende de un lado al otro de la misma con una orientación determinada.
Por ejemplo,la ouabaina inhibe sólo desde la cara exterior de la membrana,en tanto que el ATP se une al enzima por la cara interna.
La ATPasa (Na + + K +) no tiene ninguna relación con la ATPasa bacteriana,de la que se distingue fácilmente en razón de sus propiedades bioquímicas.
Su posible relación con la ATPasa de hongos o con el sistema Kdp de E. coli es discutible.
La ATPasa (Na + + K +) está constituida por un polipéptido de un peso molecular aproximado a 100.000 y una glicoprotema con un peso aproximado a 50.0 () 0. El sistema Kdp de E. coli está constituido por tres subunidades,de las cuales dos son similares a las descritas,siendo la tercera más pequeña.
La ATPasa encontrada en hongos parece tener una sola subunidad con un peso molecular aproximado a la subunidad mayor de la ATPasa (Na + + K +).
La ouabaina,potente inhibidor de la ATPasa de animales,no influye en las otras dos ATPasas que estamos considerando ; el vanadato sí es un inhibidor de la ATPasa animal y de la ATPasa de hongos.
Es evidente que estos datos,por sí solos,no prueban ni una relación funcional,ni una relación evolutiva.
Se impone ahondar más en el conocimiento de estos sistemas,antes de que podamos abordar el problema en su globalidad.
Los movimientos de cationes expuestos no son los únicos flujos de cationes que tienen lugar en las células de los seres vivos.
El sodio y el protón toman parte en la acumulación de otros sustratos.
Todas las células de los seres vivos concentran algunos productos que les son necesarios para su metabolismo y que se encuentran en concentraciones bajas en el medio externo.
Aunque este tipo de transporte concentrativo consume energía,muchos transportes concentrativos no utilizan energía química,como la que se obtiene por hidrólisis de ATP u otro compuesto químico similar.
El mecanismo más común para los procesos de transporte concentrativo es la utilización del potencial eléctrico de membrana y el gradiente de concentración de iones.
Al hablar anteriormente de antiportes,observamos que.
utilizando la diferencia del potencial electroquímico del protón a ambos lados de la membrana.
el sodio podía ser extraído de la célula aunque en el exterior hubiera una concentración superior a la interior.
Muchos compuestos necesarios para la vida se concentran en la célula por un proceso basado en las mismas consideraciones energéticas.
La diferencia con el proceso de extracción de sodio estriba en que.
en estos casos,catión y sustrato se mueven conJuntamente en un proceso que se conoce con el nombre de simporte.
Así se transportan azúcares,aminoácidos.
vitaminas,bases para los ácidos nucleicos.
etcétera.
Algunos años atrás se consideraba que en las células animales los procesos de simporte tenían lugar con el catión sodio.
en tanto que en las células de las bacterias y de los vegetales estaba implicado el protón.
Esta generalización resultó ser demasiado simple.
A medida que progresó la investigación.
se fue descubriendo que algunas bacterias marinas también utilizaban el sodio.
Recientemente se han encontrado muchos datos que prueban que en las bacterias los simportes con sodio son generales.
En plantas y hongos las pruebas de simportes con sodio son escasas ; siendo uno de esos ejemplos el de Saccharomyces cerevisiae,donde el transporte del fosfato puede realizarse con sodio Desde un punto de vista teórico hay que tener en cuenta que tanto las bacterias como los hongos poseen mecanismos para la extracción del sodio (probablemente también las plantas),por lo que pueden reciclar el catión y por tanto utilizar simportes con sodio.
En el caso de las células animales,el problema es distinto.
pues poseen bomba de sodio [la ATPasa (Na + + K +)],pero no bomba de protones (son excepción las células de algunos tejidos),dependiendo la extracción del protón de los mecanismos de regulación del Ph. Cuando se consideran organismos pluricelulares,el papel del potasio y del sodio se complica mucho más.
pues a las funciones que los cationes desempeñan en las células hay que añadir un buen número de funciones intercelulares que utilizan los gradientes o los flujos de potasio y sodio.
En las plantas,la absorción y la selección de cationes se realizan en las raíces ; la composición de las células de la planta refleja la del medio.
existiendo excreciones de sodio sólo en las plantas adaptadas a ambientes salinos.
En los animales el problema no se resuelve con tanta simplicidad,pues la composición de los líquidos extracelulares es netamente diferente de la de la célula.
, manteniéndose constante gracias a mecanismos de excreción.
El contenido en sodio y potasio de los líquidos extracelulares se parece al del agua de mar de distintas épocas.
En los animales terrestres,la composición se asemeja a la de los mares en el período precámbrico,cuando se supone que aparecieron probablemente los animales con circulación cerrada.
La salinidad sódica de los mares ha ido aumentando desde entonces.
tanto en valor absoluto como en relación con el potasio,y muchos animales marinos se han adaptado a esta variación aumentando el contenido en sodio de los líquidos extracelulares.
En cualquier caso,es evidente que aunque el sodio se encuentra en cantidades muy bajas dentro de la célula,los animales necesitan sodio para sus líquidos extracelulares.
Los requerimientos de sodio y potasio en las plantas difieren considerablemente de las necesidades que muestran los animales con circulación cerrada.
Las plantas que no están adaptadas a suelos salinos no requieren sodio (podrían ser excepción las plantas con fotosíntesis C - 4) y en ellas el potasio cumple las funciones celulares ya mencionadas: neutralización de cargas.
osmorregulación,transporte de solutos.
control de Ph,etcétera.
El potasio de la solución del suelo es transportado por las células corticales de la raíz a su interior por un proceso de cambio con protones que probablemente responde a lo que se expuso para hongos De las células corticales el potasio se mueve radialmente,célula a célula,hasta los vasos del xilema,por un proceso no del todo conocido.
Del líquido vascular e intercelular,cada célula transporta el catión para sus propias necesidades.
En determinadas condiciones,la concentración de potasio en la solución del suelo puede ser muy baja,bien por haber sido muy lavado el suelo o bien por el cultivo del mismo.
En condiciones de cultivo,el potasio es exportado por las cosechas disminuyendo las reservas disponibles para la planta,pues el potasio de los minerales del suelo no está disponible para la planta.
La carencia de potasio,sobre todo si es intensa,tiene obviamente múltiples efectos para las plantas.
Sin embargo.
en carencias parciales,el crecimiento de la planta es el primer afectado en muchas especies,debido a deficiencias en la osmorregulación.
La falta de potasio disminuye el potencial osmótico de la célula y,aunque la planta moviliza azúcares para compensar este defecto,por debajo de un determinado nivel,la carencia de potasio afecta al alargamiento de los entrenudos.
como antes vimos,el papel del sodio en los animales de circulación cerrada difiere absolutamente del que pueda desempeñar en los demás seres vivos.
La composición constante de los líquidos extracelulares ha determinado que,a lo largo de la evolución.
se hayan conformado sistemas de relaciones intercelulares basadas en la presencia del sodio extracelular.
Por ello.
en los animales,el sodio no sólo está implicado en las funciones celulares a las que antes aludimos (osmorregulación,transporte,actividad enzimática,etcétera),sino además en otras que,como el impulso nervioso,dan unidad regulando el comportamiento de las células de diferentes órganos y tejidos.
El impulso nervioso se presenta merced a la modificación de la permeabilidad a los cationes sodio y potasio.
Lo que ocasiona cambios en el potencial eléctrico a través de la membrana.
En los animales,la dieta provee las cantidades necesarias de potasio y sodio para mantener el nivel de potasio en las células y de sodio en los líquidos extracelulares Una dieta normal contiene cantidades importantes de potasio,pero puede ser muy baja en sodio si la dieta es exclusivamente vegetal.
La función renal regula con precisión los niveles de sodio y potasio en el plasma,reteniendo eficazmente el sodio cuando la dieta es baja en este catión.
Al contrario de lo descrito para los vegetales,y por las razones expuestas.
en los animales sólo se presentan carencias de sodio y potasio en condiciones patológicas.
Cuando esto se produce.
bien por disfunción de los mecanismos de regulación o cuando,por razones físicas.
se registra una disminución o aumento de los fluidos corporales afectando a las concentraciones de sodio y potasio.
la función muscular y la nerviosa son las primeras afectadas.
La información que durante los últimos años se ha venido acumulando sobre la función de los cationes alcalinos pone cada vez más de manifiesto el papel central que estos cationes tienen en un número muy elevado de procesos.
Un ejemplo indicativo de que existen estas funciones,aún no establecidas con claridad,lo tenemos en los efectos descritos para otro catión alcalino,el litio.
El litio es un catión que se encuentra en la naturaleza en cantidades pequeñas en algunos minerales,particularmente silicatos,y en pequeñas cantidades también en el agua de mar.
Químicamente,el litio comparte ciertas propiedades con el sodio y es de suponer que su actuación en los seres vivos se produzca por interferencia con los procesos en los que toman parte el sodio o el potasio.
La existencia de aguas minerales que contienen cantidades apreciables de litio y la reputación de que sus propiedades terapéuticas se debían a la presencia del catión atrajo un considerable interés sobre el mismo.
La mayoría de las propiedades terapéuticas atribuidas al litio no han podido ser demostradas.
pero en cambio su actuación como agente psicoterapéutico está fuera de toda duda,siendo el primer agente psicofarmacológico utilizado.
El efecto del litio sobre el temperamento y su valor en el tratamiento de la manía depresiva pone de manifiesto,por encima de la valoración clínica del catión,el papel que en este proceso pueden desempeñar los cationes alcalinos.
En vegetales hallamos otro ejemplo que puede ser indicativo de la misión que los cationes alcalinos pueden cumplir en la morfogénesis.
Si a una planta de Bidens pilosa L. se le corta la yema terminal cuando sólo tiene los dos cotiledones.
una de las dos yemas axiales se desarrolla para sustituir a la yema terminal eliminada.
Si orientamos una serie de plantas de Bidens con un cotiledón hacia la derecha y el otro hacia la izquierda.
y cortamos la yema terminal,la mitad de las plantas desarrollarán la yema de la derecha y la otra mitad la de la izquierda.
Esta respuesta aleatoria puede controlarse.
sin embargo,si efectuamos un ligero traumatismo sobre uno de los cotiledones.
Pinchando uno de los cotiledones,las plantas desarrollarán,en un 65 por ciento.
Ia yema del lado contrario.
La señal de la herida se transmite muy rápidamente desde el cotiledón herido ; si se cortan ambos cotiledones un minuto después de herir uno.
Ia respuesta no variará de la obtenida con las plantas sin cortar los cotiledones.
Pero si se riegan las plantas con cloruro de litio.
incluso a concentraciones tan bajas como lo - 7 molar,las plantas responderán como si no hubieran sido heridas ; es decir.
cada yema con el cincuenta por ciento de probabilidades.
El desarrollo de nuestro conocimiento sobre las funciones de los cationes alcalinos.
y la revelación de sus mecanismos de transporte.
con toda seguridad,nos ayudarán a comprender aspectos hoy en día desconocidos de la fisiología de los seres vivos.
Sin embargo.
la aplicación más inmediata de estos conocimientos es terapéutica.
Los microorganismos.
tanto eucariotas como procariotas.
tienen mecanismos encargados del transporte de cationes que difieren considerablemente de los que tienen los animales.
Estas diferencias abren el camino para el desarrollo de drogas que.
afectando a los primeros no incidan en los segundos.
Hay que tener presente que ningún microorganismo podría prosperar en los fluidos extracelulares de los animales si se bloquearan sus mecanismos para el transporte de los cationes alcalinos.
El objetivo general de la reflexión epistemológica de Vives fue,en efecto,rechazar la dialéctica de los escolásticos como fundamento objetivo de la verdad o falsedad de los conocimientos científicos.
Como dijo Ernst Cassirer," La verdadera meta que Luis Vives se traza y persigue por doquier consiste en emancipar a las ciencias empíricas de la metafísica y de la lógica metafísicamente concebida ".
A falta de una base filosófica general,las diferentes disciplinas tienen que estructurarse a partir de los fenómenos que recoge la experiencia.
Vives utiliza el término experimentum.
pero solamente para designar la observación empírica.
El concepto de experimentación no aparece en sus obras,ni siquiera de modo implícito.
Tampoco contribuyó a una formulación más precisa de los métodos inductivos.
Se limitó,en el fondo,a recomendar una cuidadosa observación de la realidad como alternativa general de las especulaciones escolásticas.
Frente a la literatura panegírica,empeñada en convertir a Vives en precursor " de planteamientos epistemológicos plenamente modernos,hay que recordar que,como dice Cassirer,en su pensamiento ' se mezclan todavía indistintamente las fecundas y positivas sugerencias con las objeciones y postulados confusos ".
Lo que no puede negarse es la gran influencia que sus planteamientos ejercieron en el desarrollo de la renovación científica.
La arqueología,a pesar de ser una ciencia joven,tiene ya una interesante y apasionada historia y estamos obligados a escribirla antes de que se pierdan recuerdos y noticias de los que todavía,directa o indirectamente,hay gentes que pueden dar fe.
Sin perjuicio de tratar del tema en otra ocasión de manera más amplia,nos limitaremos a poner de manifiesto unos cuantos hitos que jalonan la historia de la arqueología en España entre los años 1850 y 1900,a lo largo de cuyas décadas esta rama del saber ha sobrepasado la etapa de la " Anticuaria ",nombre con que se designaba en el siglo pasado,para llegar a ser una verdadera ciencia con metodología y contenido propios.
En el período de un siglo.
Ia arqueología ha pasado,de ser una parcela que apenas si era cultivada por unos cuantos aficionados y por muy escasos hombres de ciencia,a ser un campo en el que desarrolla su actividad un elevado número de investigadores sólidamente preparados,gracias a cuyo esfuerzo el horizonte histórico y cultural del mundo se ha ampliado de una manera insospechada para quienes protagonizaban su cultivo hace unos pocos lustros ; pero no hay que olvidar que esta situación,altamente esperanzadora,no se ha alcanzado sin esfuerzo.
Muy esquemáticamente,voy a consignar algunas de las fechas y los hechos que considero más importantes,así como los nombres de algunas de las personas que las protagonizaron entre 1850 y 1900.
Hemos de empezar por Basilio Sebastián Castellanos,fundador de la Academia Anticuaria Española y luego de la Academia Española de Arqueología y Geografía,entidades que,aunque de vida efímera,sirvieron para aglutinar y animar a quienes sentían análogos afanes.
Fue Castellanos el primer profesor de arqueología en España y autor de un Compendio elemental de Arqueología,que,a pesar de los errores que en él se dan,reflejo del nivel científico que por entonces había en España en el campo que nos ocupa,fue fuente de información para los estudiosos del momento.
Su polifacética formación le llevó también a ser Director del Museo Arqueológico Nacional,cargo desde el que impulsó los estudios arqueológicos.
Casiano del Prado,en 1848,había reconocido la importancia de los hallazgos de útiles paleolíticos en las terrazas del Manzanares y los había clasificado adecuadamente.
Marcelino de Sautuola,en 1875,reconocía la " Cueva de Altamira ",cuyas pinturas descubriera su hija en 1879,y tuvo el mérito singular de que,de inmediato,se percató de la importancia de aquel descubrimiento con el que alumbraba al mundo el arte rupestre paleolítico,y en sus Breves apuntes sobre algunos objetos prehistóricos de la provincia de Santander,aparecido en 1880,publicó por vez primeras el plafón principal de Altamira y clasificó acertadamente sus pinturas.
Este descubrimiento produjo tal sorpresa en los medios científicos de España y de fuera que no sólo fue acogido con escepticismo,sino que las más burdas calumnias se cernieron sobre él hasta el extremo de ser totalmente silenciado en congresos científicos internacionales.
En favor de los puntos de vista de Sautuola,únicamente se levantó la voz de Juan Vilanova y Piera,catedrático de la Universidad de Madrid,quien,frente a la postura adoptada por los más prestigiosos investigadores españoles y extranjeros,se erigió en paladín de la tesis de Sautuola.
Tesis que,tras fuerte resistencia,y gracias a hallazgos semejantes que por entonces se produjeron en Francia,acabó por imponerse y recibió el definitivo espaldarazo científico cuando Cartailhac,pontífice de la prehistoria en Francia y tenaz contradictor de la tesis de Sautuola,acabó reconociendo noblemente su error en su famoso trabajo Mea culpa d'un sceptique,publicado en el año 1902,sin que Sautuola,muerto en 1888,alcanzara la satisfacción de conocer el arrepentimiento de su más importante y tenaz adversario científico.
Algunos años después,el conde de Vega del Sella descubriría y clasificaría adecuadamente importantes yacimientos del Paleolítico,utilizando en el estudio de los mismos la más rigurosa metodología.
Mientras tanto,llegaban al S. E. peninsular los hermanos Enrique y Luis Siret,cuyos trabajos fueron especialmente decisivos para el conocimiento y adecuada valoración de las culturas de la Edad de los Metales: Los Millares,Almizaraque,El Oficio,Tres Cabezos,La Pernera,el Argar,etcétera,figuran entre sus principales aportaciones.
En 1894 se clasifica y valora,por Vilanova y Rada y Delgado,la " Cerámica de Ciempozuelos ",cuya expansión europea pronto quedaría confirmada ; por su parte,Góngora,en sus Antigüedades Prehistóricas de Andalucía,daba puntual relación acerca del hallazgo de la cueva de los Murciélagos,que había tenido lugar en Albuñol (Granada) en 1831,en la que entre otras piezas se encontró una diadema de oro cuyo ingreso en el Museo Arqueológico de Granada tuvimos ocasión de gestionar cerca de los Padres Jesuitas de Granada que la conservaban en su poder.
La " cultura del Algar ",con sus típicos enterramientos en tinajas o en cistas,se perfila con caracteres bien definidos en la estación epónima excavada por Siret y en otros yacimientos del S. E.,entre los que hay que mencionar la necrópolis de San Antón de Orihuela (Alicante),que excavó Santiago Moreno en 1872 y el P. Furgús posteriormente.
En 1887 tuvo lugar el descubrimiento del sarcófago antropoide de Cádiz,con cuyo hallazgo la arqueología fenicia en España quedaba representada por una pieza de especial significación.
De épocas más avanzadas,se descubren las esculturas del Cerro de los Santos y del Llano de la Consolación en Montealegre (Albacete) entre 1859 y 1900,con lo que se abre un nuevo capítulo para la historia de nuestra escultura antigua.
Se descubre,también,el busto de la ' Dama de Elche ",y al evocarlo no pueden dejar de acudir a nuestro recuerdo los nombres de Manuel Campello Escaple,a quien en 1965,octogenario ya,se le distinguió con la Cruz de Alfonso X el Sabio,por haber sido el descubridor material de la Dama,Manuel Campello Antón,a cuya posesión pasó por ser propietario del terreno donde se encontró,a Pierre Paris,quien llevó a cabo su compra y la trasladó al Museo del Louvre,a Pedro Ibarra,quien defendió y gestionó la compra de la Dama por el Estado,sin que por desgracia sus gestiones se vieran correspondidas.
A pesar de su intervención en la compra y exilio de la Dama,el nombre de P. Paris hay que pronunciarlo con respeto,pues a él se debe el primer intento de sistematización de la Cultura Ibérica,y concretamente de su cerámica,de la que tanto se escribiría después poniéndose de manifiesto los errores en que P. Paris había incurrido al emparentarla con las cosas micénicas.
Por su parte,G. Bonsor-el pintor inglés captado por la arqueología de España - exploraría las márgenes del Guadalquivir y especialmente los alrededores de Mairena y de Carmona,alumbrando una serie de yacimientos cuyo estudio supuso importantísimas aportaciones para nuestra arqueología romana y prerromana.
En 1881,Saavedra ratificaría noticias que ya habían consignado los historiadores del siglo XVI en relación con la ubicación de Numancia,sobre cuyo emplazamiento no quedaría ninguna duda tras las excavaciones llevadas a cabo en 1877 por Rada y Delgado,Olózaga y Fernández Guerra,a pesar de lo cual no ha faltado un sabio extranjero que,con notorio desenfado.
se ha asignado el honor de haber descubierto los vestigios de la inmortal ciudad.
Rodríguez de Berlanga estudiaría los Bronces que contienen las " Leyes de Málaga y Salpensa ",encontradas casualmente en 1851,y cuyos textos son fundamentales para el estudio y conocimiento del régimen municipal romano.
El estudio que de estos textos hizo Rodríguez Berlanga,modélico para su época,sirvió además para llamar la atención hacia ellos de Mommsen y de Hubner ; este último quedaría definitivamente vinculado a nuestra ciencia y. fruto de la dedicación,fue la publicación del Corpus Inscriptionum Latinarum y de los Monumenta Linguae Ibericae y de La Arqueología de España,obras todas llenas de erudición cuya vigencia todavía conservan.
Al hallazgo de los Bronces de Málaga siguieron los de Osuna,Bonanza y otros más que enriquecieron el Museo Loringiano y dieron lugar a doctas monografías que contribuyeron a llamar la atención de los estudiosos hacia la arqueología hispano-romana,en uno de cuyos yacimientos más señeros - Itálica - Demetrio de los Ríos hizo excavaciones que luego continuaría Hungtinton en 1898 para acrecentar las colecciones de la Hispanic Society por él fundada en Nueva York.
El descubrimiento,también casual.
del Tesoro de Guarrazar,acaecido en 1859,sería un poderoso acicate para el estudio de las artes aplicadas españolas.
La venta al extranjero de parte del mismo,a la que luego se sumaría la de la Dama de Elche,sirvieron para espolear el interés por la defensa de nuestro patrimonio cultural.
Se vieron aquellas en parte reparadas cuando estas piezas retornaron a España en virtud de un convenio con el Gobierno Francés el año 1941.
La creación del Museo Arqueológico Nacional,en 1867.
cuya inauguración tendría lugar en 1871,el funcionamiento de varios museos arqueológicos provinciales puestos bajo la custodia del cuerpo facultativo de archiveros bibliotecarios y arqueólogos,creado en 1858,o bajo la tutela de las comisiones provinciales de monumentos,y la actuación de varias sociedades arqueológicas que con carácter provincial o local se crearon entonces,supuso el disponer de una serie de instituciones en las cuales se recogían y exponían los testimonios más insignes de nuestras pasadas culturas.
Paralelamente,se hicieron algunos intentos de sistematizar nuestra arqueología,con diferente fortuna,pero que en cualquier caso son exponente del nivel que alcanzó nuestra ciencia entre 1850 y 1900,que si no fue muy elevado en su conjunto,tuvo,sin embargo,singulares aciertos.
Y sobre todo sirvió para que por desinterés no se perdieran algunos importantes hallazgos que tuvieron lugar en aquellas décadas,entre los que se cuentan " La bicha de Balazote ",el " brazalete " de oro de Estremoz,o de Bauer,encontrado en 1872,o los " toros de Coxtig " (Mallorca),enigmático conjunto de cabezas y astas de toro,de bronce,que se encontraron casualmente en 1895.
Con estas aportaciones y hallazgos alcanzamos el alborear del siglo xx,en cuyos primeros lustros asistiremos al logro de nuevas e importantes conquistas,las cuales,como había sucedido en la etapa anterior,van a contribuir a que se perfilen con trazo firme nuestros estudios.
De eslabones entre una y otra etapa sirven nombres tan prestigiosos como el Marqués de Cerralbo.
P. Fita,Alcalde del Río.
Manuel GómezMoreno,Mélida,Obermaier,Breuil,Cabre,Schulten... etc.,la reseña de cuya labor ingente,rigurosa y eficaz,será objeto de ulterior tratamiento.
Fueron ellos auténticos pioneros y paladines de una ciencia que,si empezó balbuceante,contradictoria y un poco empíricamente,pronto,gracias al esfuerzo y al tesón de sus cultivadores,comenzó a perfilarse como una rama de la historia con método y contenido propios y con unos resultados científicos evidentes pese a las vacilaciones,tanteos y hasta limitaciones que todavía se echan a ver en algunas de las parcelas que cultiva,pero sobre las cuales se proyecta cada día mayor claridad,Lo que explica el enorme atractivo que la arqueología ha despertado a todos los niveles.
(Gratiniano Nieto Gallo.
) Investigando los sistemas de afloramiento Entre los días 2 de marzo y 17 de abril de este año,se llevó a cabo,frente a las costas de California,la campaña oceanográfica OPUS 1,la primera de un nuevo programa de estudio de los fenómenos de afloramiento (designado en inglés con las siglas OPUS,de ' Organization of Persistent Upwelling Structures ").
En este programa,coordinado por R. C. Dugdale,de la Universidad de Southern California,participan científicos de diversos laboratorios y universidades de Estados Unidos y del Instituto de Investigaciones Pesqueras de Barcelona (España).
Se denomina afloramiento al ascenso de aguas relativamente profundas hasta la zona superficial del mar (" Las áreas oceánicas más productivas ",de Ramón Margalef y M. Estrada ; INVESTIGACION Y CIENCIA,octubre de 1980).
Este fenómeno,que ocurre principalmente en la costa oeste de los continentes y en las zonas marinas ecuatoriales,reviste especial importancia porque el agua profunda es rica en nitratos,fosfatos y otras sales nutritivas necesarias para el crecimiento del fitoplancton,que las consume en la capa superior,iluminada,de la columna de agua.
Aunque las causas del afloramiento sean complejas,los factores principales se resumen en la distribución de corrientes y el arrastre del agua superficial mar afuera causado por vientos de dirección adecuada.
El afloramiento no ocurre de modo homogéneo a lo largo de la costa.
Podemos imaginarnos un sistema de afloramiento como compuesto por una serie de centros de afloramiento intenso (plumas),separados por áreas donde el movimiento ascendente del agua es más débil.
Las plumas suelen ocupar una posición persistente,aunque su extensión y la velocidad de ascenso del agua pueden presentar fluctuaciones en respuesta a ciertas variables,intensidad de los vientos locales por ejemplo.
El programa OPUS está dedicado a un estudio detallado de las plumas de afloramiento ; varios subprogramas o componentes se ocupan de diferentes aspectos físicos,químicos y biológicos del problema.
Por ejemplo,el componente dedicado al estudio del fitoplancton se propone la definición de los motivos de distribución de fitoplancton y sales nutritivas en un centro de afloramiento,la investigación del papel de los mecanismos físicos y biológicos en la producción de estas distribuciones y la evaluación de la importancia de las plumas en relación con la composición y distribución del fitoplancton en el conjunto de la región de afloramiento.
La campaña OPUS I tuvo sobre todo un carácter piloto ; su finalidad principal era explorar la zona de trabajo,hacer una identificación preliminar de las escalas más importantes de variabilidad del sistema estudiado y optimizar de acuerdo con ellas la estrategia de muestreo.
Durante la campaña,se realizaron dos tipos de trabajo: secciones hidrográficas,en que se ocupaban una serie de estaciones situadas en una línea perpendicular a la costa,y recorridos,en que se barría sistemáticamente el área de estudio,con objeto de hacer mapas de la distribución superficial de diversos parámetros.
Durante parte de la campaña,se contó además con el apoyo de un avión que sobrevolaba la zona y mediante sensores remotos elaboraba mapas de temperatura superficial del agua y concentración de clorofila.
En las estaciones hidrográficas se tomaba agua de diversas profundidades y se hacían determinaciones de salinidad,oxígeno,sales nutritivas y clorofila ; además,se tomaban muestras para el análisis de las poblaciones de fito y zooplancton y se realizaban perfiles verticales continuos de temperatura y salinidad con una sonda CTD y de fluorescencia (parámetro que proporciona una estimación de la concentración de clorofila) con un fluorómetro especial.
Durante los transectos se aspiraba agua de superficie con una bomba y se registraban en continuo temperatura,concentración de varias sales nutritivas y fluorescencia.
Las secciones hidrográficas pusieron de manifiesto los gradientes perpendiculares a la costa,típicos de una zona de afloramiento: concentraciones de sales nutritivas elevadas cerca de la costa y decrecientes al alejarse de ella y bajas concentraciones de clorofila en la zona costera con tendencia a aumentar hacia los bordes de la pluma.
Durante la última fase de la campaña,se llegaron a detectar concentraciones de clorofila de 25 miligramos por metro cúbico (como referencia,se puede indicar que los valores usuales en el Mediterráneo,en época de estratificación y en zonas no afectadas por vertidos de aguas residuales,no suelen llegar a 0,5 miligramos por metro cúbico).
Dentro de las líneas generales mencionadas,los mapas pusieron de manifiesto una gran heterogeneidad espacial de los parámetros físico-químicos y biológicos.
La elaboración e integración de los datos acumulados durante una campaña oceanográfica es un proceso laborioso que ocupa a los científicos participantes en el proyecto un tiempo mucho más largo que el necesario para realizar las mediciones a bordo.
En el caso de OPUS 1,aún es pronto para aventurar conclusiones,pero todo hace suponer que los resultados obtenidos representarán una aportación importante en el estudio de la estructura y función de los sistemas de afloramiento.
(Marta Estrada.
) El cuerpo negro browniano En 1905,Albert Einstein publicó tres artículos fundamentales: sobre el movimiento browniano,el efecto fotoeléctrico y la teoría especial de la relatividad.
En otro escrito clásico,publicado doce años más tarde,analizó la interacción mecánico-cuántica de la radiación electromagnética con las moléculas de un gas.
Ese escrito.
de 1917,- estableció el concepto de cuanto de - energía electromagnética (más tarde - denominado fotón),dotado de una energía y un momento determinados.
Para analizar los efectos de la radiación sobre las moléculas del gas,Einstein se apoyó en sus conclusiones sobre el movimiento browniano y el efecto fotoeléctrico.
Pero es curioso constatar que no tomó en consideración su trabajo sobre relatividad especial,ya que sólo consideró el caso en que las moléculas del gas se mueven a velocidades no relativistas,es decir,pequeñas en comparación con la velocidad de la luz.
Nadie cayó en la cuenta de tal omisión hasta 1979,cuando Timothy H. Boyer,de la Universidad de Nueva York,la puso sobre el tapete.
Además,arguía que si Einstein hubiese partido de las hipótesis relativistas correctas,entonces su análisis le hubiera llevado a una inquietante incoherencia entre la mecánica cuántica y la mecánica estadística.
El escrito de Einstein de 1917 era la culminación de los primeros años de la mecánica cuántica,iniciada cuando Max Planck analizó el intercambio de energía entre la pared de una cavidad de un cuerpo sólido y la radiación contenida en dicha cavidad.
La pared de una tal cavidad es el mejor emisor de todas las longitudes de onda de la radiación electromagnética.
La gráfica de la intensidad de la radiación respecto de la longitud de onda coincide con la correspondiente gráfica para un cuerpo negro perfecto ; esta gráfica se llama el espectro del cuerpo negro.
La forma del espectro del cuerpo negro depende de la temperatura de la pared de la cavidad ; se trata,pues,de una familia de gráficas,una para cada temperatura.
La determinación de un procedimiento general para deducir la familia de gráficas a partir de bases teóricas era uno de los problemas más acuciantes de la física de las postrimerías del siglo pasado y principios de éste.
En octubre de 1900,Planck anunció que había desarrollado una única fórmula,coherente con los datos observados.
Planck procedió al análisis de la interacción de la radiación con la pared de la cavidad.
Supuso que la pared estaba formada por resonadores submicroscópicos tales,que cada uno de ellos oscilaba a una frecuencia característica y que emitía y absorbía radiación en dicha frecuencia.
La distribución de energía entre los resonadores de la pared viene descrita por la fórmula de mecánica estadística llamada distribución de Boltzmann.
A fin de que los resonadores pudieran estar en equilibrio térmico con la radiación,Planck se vio obligado a concluir que los resonadores podían emitir y absorber energía sólo en cantidades múltiplos enteros de hv,siendo v la frecuencia del resonador y h una constante universal,que ahora se conoce como la constante de Planck.
Sin embargo,Planck limitó cautamente sus consideraciones sobre la cuantización de la energía a los intercambios entre los resonadores materiales y el campo de radiación,sin sacar conclusiones sobre la energía de la radiación propiamente dicha.
En 1905,Einstein propuso que la radiación actuaba como si estuviese constituida por un número finito de cuantos de energía localizados de valor hv.
Durante los doce años subsiguientes,Einstein extendió su teoría corpuscular,argumentando que la luz no sólo se comportaba como si estuviese constituida por partículas,sino que de hecho lo estaba.
En su escrito de 1917,Einstein detalló la interacción cuántica entre material y radiación.
Con él la mecánica cuántica había completado un círculo: Planck había empezado con la ley de radiación del cuerpo negro y había deducido la distribución de Boltzmann de la energía para osciladores materiales,mientras que Einstein,por contra,dedujo la ley de radiación de Planck a partir de la distribución de Boltzmann.
Einstein consideró un gas formado por moléculas hipotéticas en estado de equilibrio término con la radiación del cuerpo negro.
Se supone que para cada molécula sólo son posibles dos niveles de energía.
Una molécula en el nivel inferior de energía puede absorber radiación cuya frecuencia,multiplicada por la constante de Planck,sea igual a la diferencia de energía entre los dos niveles.
Análogamente,una molécula en el nivel superior puede caer espontáneamente al nivel inferior emitiendo radiación de la misma frecuencia.
Einstein supuso que las moléculas están sometidas a un movimiento browniano,como las partículas suspendidas en un medio líquido.
Cada molécula se mueve en línea recta hasta que su trayectoria se ve alterada por una colisión.
Los intervalos entre dos choques son aleatorios y por ende la partícula recorre un camino en zigzag.
En sus cálculos no relativistas,Einstein consideró que las moléculas presentaban una velocidad pequeña y una masa grande en comparación con el equivalente másico de la energía de radiación.
En consecuencia,la velocidad de una molécula apenas cambia por el efecto de la emisión o absorción de un fotón.
El análisis de Boyer de estas ideas,que apareció en el Physical Review,sugería que la descripción de Einstein no podría ser extendida a velocidades moleculares relativistas.
Boyer argumentaba que,bajo supuestos relativistas,la distribución de Boltzmann de la energía de las moléculas no podía reconciliarse con la distribución de la energía de la radiación dada por la ley de radiación de Planck.
Puesto que la teoría especial de la relatividad debe ser válida dentro de toda teoría física,el hallazgo de Boyer implicaría una grave incoherencia entre una ley de la mecánica estadística y una ley de la mecánica cuántica.
La ley de radiación de Planck depende de la temperatura de los resonadores oscilantes,pero es independiente de cualquier otra propiedad,como por ejemplo la masa.
Boyer concluyó que la distribución de Boltzmann de la energía de partículas relativistas podría ser coherente sólo con una ley de radiación que dependiera de la masa de las partículas.
Otros tres análisis,con resultados bastante distintos,han aparecido ahora en el Physical Review.
Sus autores son Uri Ben-Ya'acov,de la Universidad de California en Santa Bárbara,Asher Peres,del Instituto de Tecnología Technion-Israel,y Charles H. Braden,Ronald F. Fox y Harold A. Gersch,del Instituto de Tecnología de Georgia.
Estos investigadores señalan que ciertas precauciones son necesarias en el proceso de extender el análisis de Einstein a partículas rápidas.
Por ejemplo,Boyer adoptó la suposición de Einstein de que una molécula se mueve inercialmente,o sin aceleración,en la emisión o absorción de un fotón,que funciona bien para las velocidades pequeñas investigadas por Einstein.
Sin embargo.
si las velocidades son grandes,la sacudida que la molécula experimenta como resultado de la emisión o absorción de un fotón no se puede despreciar,ni siquiera cuando la energía intercambiada en el sistema de referencia de la partícula es pequeña en comparación con la masa de la molécula.
En otras palabras,el modelo de Boyer viola el principio de conservación del momento.
También viola el principio de conservación de la energía,y no tiene en cuenta el hecho de que la masa de una partícula cambia como resultado de la emisión o absorción de la energía de un fotón.
De acuerdo con las tres interpretaciones recientes,no se halla ninguna incoherencia entre la teoría especial de la relatividad,la mecánica cuántica y la mecánica estadística cuando se parte de suposiciones relativistas apropiadas.
De hecho,tanto a altas como a bajas velocidades,la distribución de Boltzmann de la energía de las moléculas de un gas implica la ley de radiación de Planck.
Aunque la conclusión de Einstein es válida para velocidades relativistas,podemos preguntarnos por qué no investigó él tales velocidades.
Aunque esencial,no imprescindible Gen regulador Los genéticos están asistiendo a un asombroso combate de boxeo.
Se valen de sondas moleculares para eliminar un gen de un ratón y seguir la forma en que dicha pérdida afecta al organismo.
En 1992,Lawrence A. Donehower y Allan Bradley,de la Facultad Baylor de medicina,la han emprendido contra el p53,gen que parecía ser un peso pesado en el desarrollo embrionario.
Para sorpresa de casi todos,con el gen fuera de combate el ratón aún se mantiene en pie.
Lo que ha obligado a replantearse el papel de p53.
Este descubrimiento y otros experimentos recientes,algunos relacionados con el gen Rb,están arrojando nueva luz sobre la regulación del crecimiento celular,y hay quien empieza a cuestionar la hipótesis de la irreemplazabilidad de los genes durante el desarrollo y su intervención en el cáncer.
La excitación que ha provocado tal observación radica en el hecho de que p53 y Rb son genes supresores de tumores.
Las personas y animales que no tienen esos genes,o que los tienen dañados,tienden a desarrollar tumores.
Los individuos que heredan sólo uno de los dos genes pS3 normales manifiestan el síndrome de LiFraumeni,una predisposición al cáncer de mama y de otros muchos tejidos.
Las personas con sólo una copia de Rb suelen desarrollar retinoblastomas,tumor retiniano que da nombre al gen,Rb.
Esos fenómenos y otros de parejo tenor determinaron que bastantes biólogos celulares consideraran genes " maestros " el p53 y el Rb,encargados de guiar el desarrollo y crecimiento de las células durante la formación armónica de un embrión.
Sin ellos,razonaban,un embrión se convertiría en una masa desorganizada,y nunca vería la luz.
Ahora bien,cuando Donehower y Bradley consiguieron dejar fuera de combate al gen p53 de ratón,los animales sobrevivieron y presentaban al nacer un aspecto de lo más sano.
Cierto es que más del 70 por ciento desarrollaron tumores en los seis primeros meses.
Por lo que parece,pues,el gen p53,aunque no resulta indispendable durante el desarrollo temprano,sí lo es para impedir la manifestación de tumores en una edad más avanzada.
A Michael B. Kastan el caso no le tomó por sorpresa.
Kastan,de la facultad de medicina de la Universidad Johns Hopkins,presentó,en agosto del año pasado,pruebas de que p53 actúa como " controlador " o revisor de la integridad del ADN.
Descubrió que,tras recibir una dosis de rayos gamma,las células normales detienen su desarrollo,para dar tiempo a la intervención de los mecanismos encargados de reparar los daños producidos por las radiaciones.
Según Kastan,la proteína pS3 actuaría de " señal que detiene la síntesis de ADN hasta que el daño es reparado ".
Y añade: " Si p53 no funciona falta la señal y las células siguen replicando su ADN dañado.
" Al acumularse los daños,algunas células sufren cambios que las transforman en cancerosas.
Experimentos realizados en los laboratorios de Geoffrey M. Wahl,del Instituto Salk de Estudios Biológicos en San Diego,y Thea D. Tlsty,de la Universidad de Carolina del Norte en Chapel Hill,han verificado que p53 impide al menos un tipo de anormalidad genética común en los tumores.
Wahl,sin embargo,no apoya la interpretación de Kastan sobre el papel de p53.
Más que a los daños genéticos,p53 respondería a la carencia de ciertos metabolitos esenciales para la replicación y la función de los genes.
Bert Vogelstein,de la Johns Hopkins,un pionero en el estudio de p53,sostiene que ni los experimentos con ratones ni los demás trabajos resuelven el problema fundamental del papel de p53 en embriones y tumores.
Por ejemplo,el ratón que carece de p53 puede haberse desarrollado correctamente porque otros genes realizaron el trabajo de p53.
" Muchos genes considerados cruciales para el desarrollo normal pueden ser eliminados,y el ratón no se ve afectado ",insiste Vogelstein.
" Los mamíferos tienen una increíble capacidad para tolerar pérdidas de genes específicos.
" Bradley duda que p53 sea genéticamente redundante.
Quizá sólo tenga una función altamente específica ; su ausencia podría limitarse a causar deficiencias muy específicas.
En muchas células,prosigue razonando,que posean incluso la proteína pS3,ésta podría carecer de función.
" Si tenemos ADN basura,¿por qué no podemos tener ARN y proteínas basura?,se pregunta.
Sea como fuere,la eliminación de pS3 no destruye a los embriones de ratón.
No ocurre lo mismo con Rb.
Bradley,Robert A. Weinberg,del Instituto Whitehead,adscrito al Tecnológico de Massachusetts,y Hein te Riele,de la Universidad de Amsterdam,han comprobado que los ratones que carecen de Rb mueren antes del decimosexto día de desarrollo embrionario.
Falla en ellos la diferenciación del sistema nervioso y de los glóbulos rojos.
Los ratones con un solo gen Rb se van formando con normalidad,pero suelen morir,a edad temprana,de tumor cerebral.
Este tipo de resultados también plantea interrogantes sobre el papel de Rb.
Su falta no crea problemas al embrión hasta los 13 días de desarrollo.
¿Quién regulaba el ciclo celular hasta entonces? Rb se expresa en todas las células del cuerpo.
¿Por qué sólo unos pocos tejidos se desarrollan anormalmente en ausencia del gen? Estas son sólo algunas de las preguntas que guían las investigaciones sobre Rb y pS3.
Es preciso profundizar más para determinar con precisión cuándo,por qué y cómo se activan los dos genes.
Para Vogelstein,gran parte de las investigaciones se dirigirán hacia los genes y proteínas que son controlados por Rb y pS3.
Los trabajos sobre pS3 podrían tener una relación directa con el tratamiento o prevención del cáncer.
Kastan sugiere que,en terapias futuras,los pacientes podrían recibir dosis baja de radiación,para suspender transitoriamente el crecimiento de los tejidos normales.
Como las células cancerosas,carentes de pS3,crecerían sin hacer caso alguno,un tratamiento con agentes químicos adecuados las mataría selectivamente.
Donehower sugiere también que los ratones sin pS3,por su especial sensibilidad,podrían ser útiles para probar la carcinogenicidad de algunas sustancias.
Da la impresión de que,a pesar del golpe encajado,estos ratones podrían ser todavía campeones.
¿Hay materia oscura en las galaxias espirales? la velocidad de rotación del gas que se encuentra en la periferia ù de las galaxias espirales es,al menos ù hasta donde se ha podido observar,, independiente de la distancia a que,se halle del centro galáctico (es decir,su curva de rotación es plana).
I J. L. Garrido,M. M. Membrado,- E. Florido y quien esto firma,de las universidades de Granada y Zaragoza,publicamos recientemente un artículo en Nature en el que sosteníamos.
que este hecho sorprendente puede tener una causa magnetohidrodinámica ; su explicación,pues,no requeriría hipótesis más atrevidas,como la - de la existencia de una materia os - cura de naturaleza desconocida.
Nuestro grupo de investigación viene dedicándose,desde hace unos anos,- al estudio del campo magnético de las galaxias espirales en sus regiones periféricas.
Habíamos comprobado que el campo puede alabear,ensanchar,estirar y arrugar en acordeón (" corrugar ") los discos galácticos.
¿No podría aumentar también su velocidad.
de rotación? Si fuese así,una de las - pruebas más sólidas de la existencia ,.
de materia oculta precisaría de una r seria revisión.
Valía,pues,la pena averiguarlo,sirviéndonos,además,de r las mismas técnicas empleadas en la - investigación de tales distorsiones.
a ¿En qué pruebas se funda la hipótesis vigente sobre la materia obscura? La principal prueba observacional es la curva de rotación plana de - las galaxias espirales.
Si la autogravitación sólo estuviese compensada - por la fuerza centrífuga,la curva de,rotación debería ser kepleriana para e grandes radios galactocéntricos y disminuir en razón inversa al cuadrado del radio (según rl / 3,de la misma - forma que disminuye la velocidad de 1 - traslación de los planetas al aumentar el radio de su órbita.
Aceptada " esta premisa y dado aquel hecho,s cabe inferir que la masa de las galaxias decuplica quizá la masa visible existente en las estrellas y el gas intergaláctico.
Así,la masa de nuestra galaxia sería,de admitirse la hipótesis de la materia oculta,de 1012 s masas solares,en vez de las masas solares que se suponía hace n poco más de una década.
- Conviene no olvidar lo siguiente: para un radio muy grande,la velocisdad de rotación se determina a través del efecto Doppler,que se mide en alguna raya de emisión del gas interestelar,casi siempre en la de 21 centímetros del hidrógeno atómico.
Sin embargo,a esas enormes distancias galactocéntricas,de unos 105 anos-luz,la población estelar es inobservable ; vale decir,es imposible medir el desplazamiento Doppler de ninguna raya de absorción de origen estelar.
Nadie ha comprobado,por tanto,que la curva de rotación estelar no sea de tipo kepleriano,ni,por tanto,que el gas y las estrellas de la periferia giren con la misma velocidad angular.
Otras pruebas que respaldan la hipótesis de la materia oscura se basan en la aplicación del teorema del virial (tocante al movimiento promedio de un sistema de muchos cuerpos) a galaxias elípticas y cúmulos de galaxias,si bien adolecen de serios problemas observacionales.
Técnicas recientes permiten calcular la densidad media del universo de otra manera (basándose en la determinación del campo de velocidades en una fluctuación de densidad con respecto al flujo de Hubble).
Cuando se aplican,se obtiene una densidad que centuplica la densidad de la materia visible y se aproxima al valor de la densidad crítica (por encima de la cual el universo sería cerrado,y justo con ella,plano).
Además de las mencionadas pruebas observacionales,la hipótesis de la materia oscura cuenta con razones teóricas de peso.
La nucleosíntesis primordial del helio y otros elementos ligeros,que tuvo lugar algo más de un segundo después de la gran explosión,debió realizarse en un medio con una densidad tal,que hoy ésta debería ser aún unas 10 veces superior a la visible.
Hay que tener en cuenta que esta densidad,obtenida a través de la contrastación entre modelos teóricos y observaciones,es de tipo bariónico (tipo de partículas al que pertenecen protones y neutrones).
Por otra parte,el modelo inflacionario del universo predice que la densidad de éste ha de ser igual a la crítica,y los físicos de altas energías mantienen que hay partículas inobservables.
Ciertas teorías sostienen,además,que el disco galáctico sería inestable de no hallarse inmerso en un halo oscuro esférico de gran masa,si bien estas teorías parecen requerir más masa oculta que la deducida a partir de la curva de rotación.
Pero también los campos magnéticos pueden explicar la curva de rotación plana en la periferia de las galaxias espirales.
El campo magnético de una galaxia se comporta en el medio interestelar de forma muy distinta de como lo hace en el laboratorio.
El hecho de que pueda permanecer sin disiparse un tiempo muy superior al de la vida del universo es,quizá,su propiedad más sorprendente.
En un medio de la turbulencia del medio interestelar,su comportamiento reviste complejidad mayor.
Los movimientos convectivos multiplican la intensidad del campo,pero cuando éste es muy intenso,se disipa por difusión turbulenta.
Sin embargo,el caos turbulento conduce al establecimiento de un campo medio con estructuras geométricas sencillas.
A este efecto,descubierto en el magnetismo de la atmósfera solar,se le llama efecto dinamo.
El " modo " geométricamente más simple que pueda generar el efecto dinamo en una galaxia es el materializado en nuestra vecina M31,la galaxia de Andrómeda.
donde las líneas de campo magnético son círculos en torno al eje de rotación de la galaxia.
(El campo es azimutal).
La elegimos para probar nuestra hipótesis.
Además,su cercanía permite disponer de mediciones más numerosas y precisas,que facilitan la comprobación de los resultados.
En otras galaxias la distribución del campo magnético es menos simple,pero n también en ellas la componente azimutal del campo es importante o incluso dominante.
De ahí que nos limitásemos a estudiar el efecto de la componente azimutal del campo magnético en la rotación galáctica.
El magnetismo produce dos tipos - de fuerza sobre el medio interestelar: - a) la originada por un gradiente de densidad de energía magnética,que actúa de presión y tiende a uniformar el propio campo y el medio magnetizado que lo sustenta,y b) la que podría denominarse " inercial magnética ",que con frecuencia tiende a extender filamentosamente el plasma a lo largo de las líneas de campo.
En el caso de un campo magnético azimutal en un disco galáctico,la primera fuerza,aunque influye,opera hacia fuera.
Esto se debe a que,en la periferia del disco,el campo magnético disminuye con el radio galactocéntrico,ya que debe conectar con el campo magnético intergaláctico,mucho más débil.
Nos centramos en la fuerza inercial magnética.
Apunta ésta hacia dentro,reforzando la acción de la gravedad.
El equilibrio requerirá,pues,que la fuerza centrífuga sea mayor que la necesaria para que haya un equilibrio puramente gravitatorio.
En la práctica,los cálculos son más complicados,pues hay que tener presentes los potenciales creados por el sistema estelar y el gas,cuya obtención no es trivial,además del gradiente de presión magnética.
Se obtiene entonces una ecuación diferencial,que no presenta graves problemas numéricos.
Un hecho resaltable de esta fuerza de inducción magnética es que no depende de la densidad del medio al que se aplica ; adquiere así importancia creciente conforme nos alejamos del centro y se convierte en fuerza dominante en las partes más externas.
Obtuvimos la intensidad del campo magnético que producía la tensión magnética necesaria para mantener la alta velocidad de rotación no kepleriana del gas periférico.
A 100.000 años-luz del centro se precisan unos 5 x I o - 6 gauss.
Este valor es compatible con las observaciones del continuo de radio.
Las supernovas de la población joven del disco producen una gran cantidad de electrones relativistas,que al moverse en trayectorias helicoidales en torno a las líneas del campo magnético emiten radiación de sincrotrón,contribuyendo al espectro continuo de radio en ondas decamétricas.
Este hecho permite el cotejo directo de las observaciones con el campo calculado.
Efectuado,el resultado fue muy satisfactorio,dentro,por supuesto,de los inevitables errores observacionales y de interpretación,y decisivo para concluir que la explicación magnética de la rotación galáctica es muy razonable y compatible con las condiciones que dicta la observación.
Los valores altos del campo magnético en un disco deben tener como consecuencia secundaria una serie de efectos observables.
En primer lugar,han de producir un ensanchamiento progresivo del disco según nos alejamos del centro,dando a las galaxias vistas de perfil el aspecto de una corbata de pajarita.
Este efecto se ha venido atribuyendo a una gravitación decreciente en un disco de agitación turbulenta constante.
Sin embargo,algunos autores consideran que no puede darse cuenta cuantitativamente de este efecto mediante semejante mecanismo.
Por otra parte,el ensanchamiento del disco por efectos magnéticos no alcanza fácilmente el equilibrio,produciéndose inestabilidades de Parker,que distorsionan el disco y le dan una forma ondulada (" corrugada ").
Nuestro equipo en la isla de La Palma ha obtenido mediante el telescopio Isaac Newton,de 2,5 metros,imágenes (publicadas por Florido,Battaner,M. Prieto,E. Mediavilla y M. L. Sánchez Saavedra) que muestran claramente el fenómeno.
Hay que decir,sin embargo,que tales ondulaciones podrían deberse a otros fenómenos,como explosiones de supernova frecuentes o la caída de nubes gaseosas del halo.
En definitiva,cabe la posibilidad de que se deba a un efecto magnetohidrodinámico la curva de rotación plana de las galaxias espirales,lo que pone en entredicho la prueba observacional más clara en favor de la materia oscura.
(E. Battaner,del departamento de física teórica y del cosmos,de la Universidad de Granada.
) Sangre y vampiros Disolución de coágulos Infarto de miocardio,embolias pulmonares y apoplejías se cuentan entre las causas habituales de muerte.
Débense a alteraciones vasculares,provocadas por coágulos que taponan venas o arterias.
En mi laboratorio de la empresa Schering AG Berlin,hemos encontrado,analizando la saliva de murciélago vampiro,cuatro activadores del plasminógeno distintos de cuantos se conocían: no bloquean la coagulación,aunque sí impiden la formación de trombos,a menudo mortales.
Los murciélagos excitan la fantasía del hombre,más que ningún otro mamífero.
En torno a ellos se han forjado leyendas,prácticas supersticiosas y hábitos de comportamiento.
Se les relaciona con el conde Drácula y otros ilustres chupadores de sangre,protagonistas de narraciones centenarias y,en las últimas décadas,de numerosas películas.
Sorprendentemente no existen en Europa murciélagos bebedores de sangre que pudieran haber inspirado la superstición popular.
Los auténticos vampiros los conocieron los conquistadores europeos en las regiones tropicales del Nuevo Mundo.
Sus narraciones sobre ellos se fundieron en el siglo XVII con las leyendas europeas sobre muertos vueltos a la vida.
Los vampiros constituyen una subfamilia de los murciélagos,que consta de varios géneros.
El vampiro común (Desmodus rotundus) se ha especializado en la succión de la sangre.
Ahora bien,la observación atenta de su conducta durante la nutrición nos enseña que,frente a lo que el vulgo cree,no succionan la sangre de sus víctimas.
La lamen tras retirar,gracias a sus afiladísimos incisivos,una finísima porción de piel,imperceptible para la presa.
De ahí que ya en los años treinta se sospechara la presencia en su saliva de una substancia que bloqueara la coagulación.
Christine Hawkey,de la Sociedad Zoológica Londinense,se dedicó en el decenio de 1960 a la investigación sistemática de esa hipótesis.
Halló en la saliva de murciélagos una substancia que presentaba actividad fibrinolítica,esto es,que disolvía la fibrina del coágulo.
En 1974,Terence Cartwright,adscrito también a la Sociedad Zoológica Londinense,aisló a partir de la saliva del vampiro una substancia activadora del plasminógeno.
Activadores del plasminógeno los hay en todos los vertebrados estudiados.
Transforman el precursor enzimático plasminógeno,muy abundante en la sangre,en su forma activada,la plasmina.
Esta es una enzima proteolítica más,como la tripsina,quimiotripsina y elastasa,que se segregan en el jugo intestinal.
La plasmina disuelve muy bien la fibrina,proteína responsable de mantener la estructura del coágulo.
La plasmina recuerda,en su operación,al agente de tráfico: impide el taponamiento de los vasos y el consiguiente bloqueo del suministro de componentes sanguíneos indispensables para los órganos.
En el organismo sano,sólo se forma fibrina a partir de fibrinógeno allí donde se presentan hemorragias ; la fibrina teje entonces una malla junto a las plaquetas presentes,taponando la salida de sangre.
Determinadas alteraciones patológicas de las paredes internas de los vasos - - arterioesclerosis o patologías venosas - - propician la aparición de coágulos en el interior de los vasos,que pueden resultar en infarto de miocardio,embolia pulmonar o apoplejía.
En la década de los cincuenta se intentó combatir los coágulos patológicos mediante activadores del plasminógeno.
Se empezó recurriendo así a la estreptoquinasa,una proteína de estreptococo.
Se utilizó luego la uroquinasa (abreviado PA u),un activador del plasminógeno presente en la orina humana.
Formada en los riñones,coopera en la liberación por las vías urinarias de precipitados de fibrina.
La terapia con estreptoquinasa o con uroquinasa implica,empero,ciertos riesgos.
Dado que son precisas grandes dosis,se corre el riesgo de desencadenar una activación generalizada de plasminógeno en todo el cuerpo,lo que comportaría la destrucción de proteínas coagulantes imprescindibles.
En el peor de los casos se producirían hemorragias incontrolables.
A principios de los ochenta,se pusieron grandes esperanzas en el activador de plasminógeno tisular (abreviado PA-t).
Lo sintetizan y liberan las células del endotelio vascular.
Fue,además,una de las primeras proteínas que se sintetizaron,en 1983,a través de los nuevos métodos de la bioingeniería,lo que posibilitó su fabricación industrial.
PA-t,que presenta una elevada afinidad por la fibrina,ofrecía la ventaja,creíase,de actuar sólo en presencia de ésta,manteniendo incólumes el resto de proteínas sanguíneas.
Pero la investigación clínica se encargó de demostrar que apenas si mediaba diferencia entre su aplicación y los tratamientos con estreptoquinasa.
Probablemente,las grandes dosis de PA-t requeridas anulaban la selectividad por la fibrina y provocaban idénticas complicaciones vasculares.
Así las cosas mediados los ochenta,emprendí junto a Alejandro Alagón,de la Universidad de México,la tarea de continuar los trabajos de Cartwright.
Comprobamos la insólita selectividad por la fibrina que presentaba una substancia de la saliva de los vampiros.
Nuestros colaboradores del Schering AG aislarían,desde finales de 1987,las substancias fibrinolíticas y determinarían su secuencia de aminoácidos.
Nosotros clonamos los genes e introdujimos en una línea celular especial (proviene del ovario de un hámster),para que se sinteticen las proteínas correspondientes.
La primera sorpresa fue el descubrimiento de la existencia de cuatro activadores de plasminógeno.
Se parecen bastante al PA-t humano,pero también presentan diferencias características.
Al igual que otras enzimas implicadas en la coagulación sanguínea,el PA-t se estructura en módulos característicos - - dominios - -,descritos por el bioquímico danés Staffan Magnusson a mediados de la década de los setenta.
Se reconoce un dominio en dedo de guante,un dominio de crecimiento (parecido al factor de crecimiento epidérmico,EGF),dos dominios " kringel " (denominados así por su semejanza con un dulce danés de ese nombre) y un dominio proteasa (similar a la tripsina).
Los cuatro activadores del plasminógeno de la saliva de vampiro presentan diferencias substanciales respecto a esta estructura.
Para poder desarrollar su actividad,el PA-t tiene que escindirse en un lugar muy preciso por la plasmina,que él mismo genera ; con ello se reduce bastante su selectividad por la fibrina.
En cambio,la actividad de los activadores de plasminógeno del vampiro en ausencia de fibrina se aproxima a cero.
Esperábamos que esta sorprendente selectividad por la fibrina (experimentalmente se ha demostrado que es hasta 500 veces superior a la del PA-t) implicase mejores propiedades farmacológicas.
A diferencia de los tratamientos con PA-t,en los experimentos en animales con dosis terapéuticas de DSPA (abreviatura de activadores de plasminógeno de la saliva de Desmodus) no se observó destrucción alguna de fibrinógeno.
Tal destrucción constituye un importante indicador de la activación sistémica de plasminógeno que queremos evitar con un nuevo tipo de medicamento contra los coágulos sanguíneos.
Se requiere que sólo se destruya la fibrina.
¿Cómo justificar esa insólita selectividad para la fibrina,que ofrece posibilidades extraordinarias para el desarrollo de un nuevo fármaco que disuelva los coágulos? Abordaremos la respuesta desde un enfoque bioquímico y desde una óptica bioevolutiva.
La base molecular de la limitada selectividad para la fibrina que presenta el PA-t ha sido objeto de estudio por distintos grupos de trabajo.
Y aunque se disputa sobre los resultados,existe acuerdo en que los dominios en forma de dedo y los dominios " kringel " cooperan en la unión con la fibrina.
De ahí que la mayoría de los investigadores hayan considerado responsables de la selectividad a estas regiones.
Los resultados de nuestros estudios señalan,sin la menor ambiguedad,que una forma mutada del DSPA,creada por ingeniería genética,cuyo único elemento es un proteasa,muestra intensa selectividad hacia la fibrina,esto es,se manifiesta activa sólo en su presencia y no se une a ella.
Por cuyo motivo,el mecanismo de la selectividad para la fibrina es independiente del mecanismo que la liga.
Las razones del primero hay que buscarlas en la influencia alostérica de la proteína sobre el DSPA.
Vayamos con la explicación bioevolutiva.
¿Por qué la saliva de los vampiros no impide la coagulación.
a diferencia de otros parásitos que también se alimentan de sangre? La respuesta estriba en que estos murciélagos no succionan la sangre,como la sanguijuela y muchos insectos,sino que la lamen.
La economía de su nutrición sería deficitaria si escapara más líquido del que el murciélago pudiera beber.
Lo mejor es que se siga sintetizando fibrina para impedir una hemorragia importante.
Cuando sea preciso para la toma de alimento,ya se disolverá la fibrina.
Los cuatro activadores del plasminógeno de la saliva de vampiro tienen esta función en la toma de alimento por parte del animal - - función muy distinta de la desarrollada por la uroquinasa (el activador de plasminógeno sintetizado por los riñones) y del PA-t,liberado por el endotelio vascular.
Su misión no es la destrucción masiva de coágulos,vitales e imprescindibles en heridas,sino la destrucción paulatina de fibrina en relación con la curación de la herida y con inflamaciones.
De donde se desprende el interés de dichos activadores de plasminógeno en la terapia de procesos de coagulación patológicos.
La idea de substancias extrañas contra substancias propias tiene numerosos ejemplos en la historia de la medicina.
Clásico es el del alcaloide vegetal morfina ; al contrario de la endorfina y los opiáceos que nosotros mismos sintetizamos,se revela como un potente calmante.
Durante siglos y con fines médicos se han extraído substancias de las plantas,cuya estructura conocemos desde hace ya algunos decenios.
Las substancias proteicas no entraron en nuestro bagaje hasta la introducción de las técnicas de la ingeniería genética.
Con su empleo ha sido posible investigar proteínas que se presentan en el organismo en concentraciones ínfimas,disponer de ellas en cantidades prácticamente ilimitadas y describir su secuencia aminoacídica.
Los activadores de plasminógeno de la saliva del vampiro común Desmodus rotundus son una parte infinitesimal del tesoro de principios activos naturales de empleo polivalente en medicina.
(Wolf-Dieter Schleuning.
director del Instituto de Biología Celular y Molecular de investigación farmacológica de Schering AG de Berlín.
) El quid está en la pregunta oportuna.
Una industria produce cada día una cantidad ingente de componentes eléctricos,de los que probablemente algunos adolecen de defectos de fabricación.
Quizá puedan probarse muchas piezas a la vez: una señal luminosa se encenderá cuando todas sean correctas ; el piloto apagado denunciará que al menos una es defectuosa ; la búsqueda entonces deberá prosegir! ¿Tiene sentido esta forma de proceder? Si el conjunto total se divide simplemente en dos subgrupos,y si el resultado en ambos subgrupos es " defectuoso ",entonces la utilidad de todo el procedimiento es muy reducida.
Si todas las piezas fueran defectuosas,el número de pruebas necesarias sería mucho mayor que con el procedimiento individual: habría que examinar todos los subgrupos posibles antes de convencerse de que efectivamente todas las piezas eran defectuosas.
Está claro,sin embargo.
que este caso extremo carece de interés.
En la práctica habrá sólo un número limitado de piezas estropeadas,y habrá,pues,subgrupos que sí pasarán la prueba.
Y en cuanto uno la pase,nos habremos ahorrado tantos análisis individuales como elementos tenga.
Este mismo principio vale naturalmente,de nuevo,para cada uno de los subgrupos más pequeños en que seguiremos dividiendo los subgrupos que no pasen la prueba ; en todo momento se nos presenta la posibilidad de que uno de ellos la pase,con lo que todos sus componentes quedarán aceptados en bloque.
Evidentemente,más se ahorrará cuanto más pequeña sea de entrada la proporción de piezas o muestras defectuosas en el conjunto total.
Ya en 1960,el matemático Peter Ungar,del Instituto Courant de la Universidad de Nueva York,demostró el siguiente resultado: si la probabilidad de que una pieza sea defectuosa es menor que (3 -) / 2 (0,382 aproximadamente),entonces,con un procedimiento de agrupación adecuado.
podrá siempre conseguirse que el número de pruebas necesarias para localizar las piezas defectuosas sea menor que el requerido con pruebas individuales.
Pero las estimaciones sobre la amplitud de la búsqueda optimizada son más difíciles de lo que cabría esperar.
Sólo muy recientemente ha habido alguien capaz de dar un paso importante en este sentido.
Eberhard Triesch,en colaboración con Ingo Althofer,ha efectuado una estimación del número de pruebas que hay que llevar a cabo cuando el número de piezas defectuosas no es superior a cierta cota máxima r. Si r = 1,cada prueba consiste entonces en especificar un subconjunto y decir si la pieza defectuosa se encuentra en él.
Ese caso especial del problema aparece ya en un conocido juego de niños: un niño piensa un objeto determinado y el resto de sus compañeros deben averiguar cuál es formulando el número mínimo de preguntas de tipo sí-no.
El pasatiempo consistente en identificar una moneda falsa de entre una gran cantidad de monedas legales mediante el mínimo número de pesadas con una balanza ordinaria de dos platos pertenece a la misma familia de problemas.
Sea un conjunto de seis objetos,de los que dos son defectuosos,pero sin que sepamos cuáles,y hay una prueba capaz de determinar la presencia en un subconjunto dado de dos objetos defectuosos.
De entre los segmentos que se obtienen uniendo 6 puntos de todas las maneras posibles,hay por tanto que escoger uno.
En estas circunstancias no tiene sentido dividir exactamente en dos el conjunto de 6 puntos y hacer luego la prueba con cada uno de los dos triángulos resultantes (izquierda),ya que entonces la prueba sólo actuaría sobre 3 de los 15 casos posibles.
En el juego de las preguntas,el tramposo que responde con la única intención de dificultarnos lo más que se pueda el hallazgo de la palabra oculta respondería " no " a la pregunta correspondiente.
La estrategia más inteligente consiste en probar con un subconjunto de 4 puntos (seis segmentos posibles,derecha).
Si la respuesta es negativa quedarán aún nueve lados,entre los cuales se encontrará el buscado ; para hallarlo se precisarán,de acuerdo con la teoría de la información,al menos cuatro preguntas (ya que tres preguntas bastan para distinguir 23 = 8 objetos solamente).
De esta manera,se necesitarán en total cinco preguntas,esto es,una mas de las necesanas para hallar una cosa entre IS (pues 24 = 16).
bién la cuestión,que poco tiene que ver con los juegos de niños,de colocar una entrada nueva en una lista alfabéticamente ordenada de palabras ya existente.
Aquí el objeto a buscar es la palabra inmediatamente detrás,o delante,de la cual deberemos situar la nueva.
En todos estos ejemplos el arte está en formular preguntas inteligentes o - - lo que es lo mismo - - en elegir subconjuntos adecuados del conjunto total.
Así,quien en el juego de las preguntas sabe ya que el objeto buscado es un animal,hará bien en no preguntar: ¿es un elefante?,sino en dividir el reino animal en grupos amplios (terrestres,anfibios,peces,etc.) para reducir luego la búsqueda a uno de estos grupos.
Si uno quiere averiguar el número de preguntas que son necesarias en el peor de los casos o diseñar una estrategia oportuna para tal circunstancia,debe plantearse el caso de tener ante sí a un contrincante tramposo,quien no ha pensado ningún objeto en concreto y siempre contesta a la pregunta que le hacemos con el único norte de ponérnoslo tan difícil como pueda.
Habrá siempre,por supuesto,un objeto al menos que pueda corresponder a todas las preguntas que se hayan formulado ya.
Una estrategia que pueda triunfar contra un contrincante de semejante pelaje ha de estar diseñada para lo peor.
Este es el principio del mini max de la teoría de juegos: se trata de minimizar el espectro de posibilidades que den el máximo conflicto.
Cada frase ha de formularse de suerte tal que el contrario se vea obligado a elegir entre dos subconjuntos de parejo tamaño.
De lo contrario,si dividimos las posibilidades en " elefantes " y " no elefantes ",daremos al contrario las máximas posibilidades en su labor obstruccionista: " no,no es un elefante ".
Si los dos subconjuntos son aproximadamente iguales dificultamos la estrategia del contrario.
Con esta estrategia se puede en teoría,con 21 preguntas,identificar un objeto de entre 221 objetos - - eso es,de entre más de dos millones.
En general,se tiene que el número k de las preguntas que es necesario formular para llegar al objetivo es igual (redondeado por arriba) al logaritmo en base dos del número n de objetos totales.
Esto se corresponde con un teorema de la teoría de la información: para numerar un conjunto de n elementos - - eso es,para caracterizar individualmente cada uno de ellos - - se precisan números binarios de k cifras.
La información buscada comporta,por tanto,k bits,es decir k preguntas sí-no.
Cuando r es mayor que 1,el problema se torna más complicado.
Para empezar,las soluciones posibles no son ya simplemente los elementos del conjunto a escrutar de partida,sino cualquier subconjunto de r elementos suyos.
Por otra parte,aunque ahora se siga la estrategia de dividir el conjunto total en dos subconjuntos,éstos deberán ser iguales no en número de objetos sino en el de posibilidades de solución que cada uno contenga.
En tercer lugar,en ambos subconjuntos de la partición efectuada pueden hallarse objetos defectuosos,lo que hace difícil valorar el resultado de la prueba.
A la vista de todas estas dificultades,el resultado de Althofer y Triesch es sorprendentemente favorable: cuando r es mayor que 1 hacen falta,por supuesto,más pruebas que cuando r es 1 ; sin embargo,el número de pruebas adicionales,/ r,depende sólo de r y no del número total de objetos.
Puede incluso demostrarse que ' l / r no varía si el test empleado se sustituye por otro menos sensible.
Se desconoce en general el valor preciso de l / r. Triesch y Althofer han demostrado,sin embargo,que podemos aproximarnos a su valor dentro de un cierto margen.
Peter Damaschke ha conseguido demostrar que en el caso r = 2 basta con una única pregunta suplementaria.
Althofer y Triesch no prueban su resultado ofreciendo una estrategia concreta ; su demostración no es constructiva.
Búsqueda del quark.
Es más fácil encontrar la belleza que la verdad.
La belleza,quark también llamado " fondo ",fue descubierta hace casi veinte años,pero la verdad,por otro nombre " cima ",no ha podido ser encontrada en ninguna parte del cosmos,excepto,quizá,en el Laboratorio Fermi del Acelerador Nacional en Batavia,el Fermilab.
Hace pocos meses,los físicos de allí han registrado dos " sucesos interesantes " que podrían ser señales del quark cima,dice Melvyn Shochet,portavoz del detector colisionador del Fermilab.
" Para que pudiésemos proclamar el descubrimiento del cima,necesitaríamos probablemente de cinco o 20 sucesos,para cada uno de sus rios,modos de desintegración ",añade.
El cima permanece como el único de los seis quarks cuya existencia no ha sido confirmada.
La mayor parte de la materia - - esto es,los protones y los neutrones - - está formada por los quarks " arriba " y " abajo ".
Otros sabores - - la extrañeza,el encanto y el fondo - - se producen sólo en los aceleradores de partículas y,tal vez,en estrellas densas y de gran masa.
El cima,si existe,probablemente no ha hecho aparición desde el explosivo y caliente nacimiento del universo.
" Algunos dirían que el descubrimiento del quark cima sería algo así como un anticlímax,pues tenemos muy,muy fuertes razones para creer que existe y conocemos su masa dentro de un cierto intervalo ",afirma el premio Nobel Steven Weinberg.
de la Universidad de Texas en Austin.
" Quizá dé la impresión de que no es Importante que sepamos con precisión la masa del quark cima.
Pero la verdad es que encierra el máximo interés.
" El quark cima proporcionaría una pista esencial para saber por qué las partículas tienen las masas que tienen.
En particular,a los físicos les desconcierta que cada partícula fundamental posea dos compañeras que le son iguales en todos los sentidos excepto en la masa.
Por ejemplo,el quark fondo responde a las fuerzas débil,fuerte y electromagnética del mismo modo,casi,que los quarks extraño y abajo,pero el fondo tiene una masa 25 veces mayor que el extraño y 700 veces mayor que el abajo.
Para explicar por qué unas partículas se hallan dotadas de mayor masa que otras,se han ideado varias teorías,la más simple de las cuales es el mecanismo de Higgs.
De acuerdo con ella,tal y como la carga eléctrica de una partícula está relacionada con la intensidad con que interacciona con los campos electromagnéticos.
interesante,los investigadores de Fermilab y sus sistemas de computación deben buscar entre miles de millones de sucesos,en algunos de los cuales aparecen cientos de partículas.
Entonces,si encuentran uno que se parezca a la desintegración de un quark cima,deben probar que no ha sido producido por una docena de procesos que pueden imitar la señal del quark cima.
" Cuando se encuentra sólo un suceso ",dice Shochet," no hay manera de determinar si es o no es el quark cima ".
El detector colisionador del Fermilab ha registrado recientemente un posible suceso de quark cima,y el Zero,el más reciente de los dos gigantescos instrumentos del Fermilab,ha observado un segundo.
Cada uno de los sucesos consiste en una cascada de partículas que puede ser la consecuencia de la desintegración de un quark cima y su contrapartida de antimateria,el quark anticima.
Cada una de estas dos partículas se desintegra en un quark fondo y una partícula W,más conocida por ser la transmisora de la fuerza débil.
Entonces,el quark fondo se desintegra,y produce chorros de partículas más corrientes.
La partícula W se desintegra bien sea en un electrón,bien sea en el hermano de ésta,el muon.
Así,lo que realmente se ha medido en Fermilab es un electrón y un muon energéticos y varios chorros.
Pero cabe la posibilidad de que estas partículas se generen por la desintegración de algo que no sea el par cima-anticima.
Las observaciones del Fermilab incluyen otra pequeña prueba que avala la hipótesis del quark cima.
El quark fondo producido en la desintegración del cima puede juntarse con otro quark y formar una partícula estable.
Esta partícula,que se moverá a una velocidad cercana a la de la luz,viajará unos pocos milímetros antes de romperse en chorros de partículas más ligeras.
A fin de identificar esta señal,los expertos del Fermilab han añadido un " detector de vértice " al detector colisionador ;.
" Una medida precisa de la masa del quark cima nos acercaría a la respuesta de las preguntas: ¿Existe una partícula de Higgs? ¿Cuál es su masa? ¿Qué tipo de experimentos debemos hacer para encontrarla? ",explica Weinberg.
Pero,¿por qué es tan difícil crear quarks cima,con lo sencillo que es hoy hacer que aparezcan quarks fondo? En primer lugar,porque son pesados.
Los experimentos del Fermilab muestran que el quark cima tiene al menos tanta masa como un átomo de plata y más de 20.000 veces la del quark arriba.
Se producen allí partículas de tanta masa lanzando protones contra antipartículas suyas.
La colisión libera 1,8 billones de electronvolts de energía,que podrían bastar,o no,para generar quarks cima.
La segunda razón de que el quark cima eluda la detección es su extrema inestabilidad.
Nadie espera que el quark cima se mantenga más de una billonésima de billonésima de segundo.
Se desintegra dando un surtido de partículas secundarias,que pueden entonces ser detectadas.
Para encontrar un solo suceso integran en el centro del detector y partículas que se rompen una pequeña distancia más allá.
El detector de vértice debería permitir a los investigadores identificar los quarks fondo sin ambiguedad,y así hacer más fácil el reconocimiento de un suceso de quark cima.
Uno de los dos sucesos presentados como una desintegración de quark cima parece mostrar chorros formados más allá del centro del detector,lo que sugiere la desintegración de un quark fondo.
Pero los investigadores del Fermilab aún no tienen suficiente experiencia con el nuevo detector de vértice como para estar seguros de sus mediciones.
" Lo que realmente necesitamos es una.
gran cantidad de colisiones a analizar,de modo que produzcamos suficientes objetos como éstos y podamos ver la señal por encima del ruido de fondo ",comenta Shochet.
Les sobran razones para andar con,cautela.
En 1985 investigadores del CERN,el Laboratorio Europeo de Física de Partículas,anunciaron el descubrimiento del quark cima,cosa que más tarde resultó ser falsa.
Hará s unos tres años,los del Fermilab registraron un candidato a quark cima ,.
, pero las pruebas no fueron concluyentes.
Sin embargo,ahora puede,a pero sólo puede,que los físicos conozcan pronto la verdad.
EL año pasado,un tsunami en la isla indonesia de Flores producía la muerte de más de 1600 personas y causaba 700 bajas el terremoto de El Cairo.
Desastres moderados a pesar de todo,si los comparamos con el ciclón que asoló Bangladesh a fines de abril de 1991,con un saldo de 133.000 víctimas entre muertos y desaparecidos.
Un año antes,otro terremoto iraní acabó con la vida de 40.000 personas.
A esos desastres naturales debiéramos añadir los producidos por la técnica,si bien en el segundo caso la proporción con respecto al total de fallecidos es mínima.
En efecto,si excluimos las guerras,los grandes desastres de origen técnico son casi tres órdenes de magnitud menores que los máximos naturales.
El terremoto de Shensi,ocurrido en China en 1556,se ha estimado que mató a 830.000 personas ; el escape de gas tóxico de Bopal provocó unas 1500 bajas ; 1403 personas se ahogaron en el hundimiento del Titanic en 1912 al chocar el transatlántico con un iceberg.
Aunque el derretimiento de Chernobyl en 1986,que liberó cincuenta veces más radiación que la bomba atómica de Hiroshima,podría cambiar esta valoración.
De acuerdo con ciertos cálculos,podría ser el responsable de 14.000 a 475.000 muertes por cáncer.
Los desastres naturales de mayor alcance están desencadenados por terremotos,tifones e inundaciones,con cifras de varios cientos de miles de víctimas mortales en cada caso.
Así,el colapso del Krakatoa en 1883 generó un tsunami que produjo 36.417 muertes.
Los deslizamientos subsecuentes al terremoto chino de Kansu en 1920 provocaron otras 100.000.
Pero las víctimas mortales no son el único aspecto social de los desastres naturales.
Por cada muerte hay que contar,además,entre S y 10 heridos que requieren atención hospitalaria y unas 50 personas que pierden su hogar y deben ser realojadas.
En los episodios mayores,se desarticula la administración pública y los pueblos afectados se quedan sin organización política ni medios económicos para hacer frente a la catástrofe.
El riesgo de desastres naturales,computado por el total anual de víctimas mortales,no es sin embargo el principal.
Los accidentes automovilísticos en los países de la Comunidad Europea suponen casi 50.000 muertos y millón y medio de heridos ; otro tanto puede decirse de las enfermedades infecciosas y lacras sociales (alcoholismo o tabaquismo).
En España,la media anual de víctimas por peligros naturales en el período 1989 - 1991 fue de 1110 al año ; las víctimas en accidentes automovilísticos rozaron las 6000.
El impacto económico de los desastres naturales tampoco iguala las pérdidas causadas por otros riesgos.
De acuerdo con nuestros propios datos,los daños totales mundiales del bienio 1990 - 1991 fueron de 46.820 millones de dólares,es decir,en torno al 0,24 % del Producto Bruto mundial.
Su incidencia en el PIB de los países desarrollados es de este mismo orden (0,20 % en España) ; pero su efecto en la economía de las regiones más afectadas es mucho mayor,cifrándose por encima del 2 por ciento,un auténtico freno para la promoción de estas naciones subdesarrolladas.
Ante semejante situación,se entiende el interés de las Naciones Unidas por reducir la incidencia de los desastres y la consagración del decenio 1990 - 2000 a ese objetivo.
El cumplimiento de esto exige dos tipos de actuaciones: preventivas y en emergencias.
Las estrategias preventivas buscan reducir el riesgo,un concepto nacido en el siglo XVIII a propósito de la seguridad del transporte marítimo,que se mide por las pérdidas potenciales humanas o económicas en una región.
Las medidas preventivas resultan más eficaces que las tomadas en situaciones de emergencia.
Hoy,carece de sentido que las políticas nacionales se muevan al compás del último desastre o de la inercia inversora,ya que las estrategias pueden diseñarse con criterios objetivos.
La actuación preventiva debe centrarse en los factores de riesgo: peligrosidad,vulnerabilidad y exposición.
Es inmediato que un movimiento sísmico en el desierto no implica riesgo alguno porque no hay exposición de personas o bienes materiales ; en cambio,otro menor en un casco urbano destruye inmuebles y acarrea la muerte de miles de personas.
De acuerdo con su expresión formal,llamamos riesgo al producto de la probabilidad de ocurrencia de un desastre por la vulnerabilidad en tanto por uno y la exposición.
Si la exposición se indica mediante la población involucrada,se calculará el riesgo social en víctimas potenciales por año ; si se expresa en valor de los bienes expuestos,se calculará el riesgo económico en unidades monetarias al año.
A escala global,hemos de comenzar el análisis por la distribución de riesgos entre los diversos tipos de peligros naturales,que varía de acuerdo con la región estudiada.
En esta geografía del riesgo,Africa,excluido el Magreb,presenta un riesgo muy pequeño de terremotos destructores,frecuentes en el Himalaya o los Andes ; sin embargo,tiene un riesgo importante en plagas de langosta y sequías.
En Estados Unidos el número mayor de bajas lo producen las tormentas eléctricas.
En España,el riesgo social de mayor cuantía se relaciona con los naufragios en temporales marítimos (99 víctimas en el período 90 - 91),que supera al derivado de inundaciones (12 en el mismo período) ; sin embargo,los peores desastres pueden ser los causados por terremotos destructores,que se presentan,en promedio,cada cien años.
La distribución espacial se representa en mapas de peligrosidad,una pieza básica en la preparación del inventario de riesgos y gestión de emergencias.
Así,el análisis de la distribución geográfica de los ciclones y tifones nos indica su concentración en las zonas tropicales adyacentes a océanos de aguas cálidas.
Entre los volcanes activos,los más peligrosos por su mayor índice de explosividad se concentran en el " Cinturón de Fuego del Pacífico ",en América occidental y Asia oriental.
Si comparamos ahora el área afectada por erupciones volcánicas,alrededor de 1,5 millones de kilómetros cuadrados,con la superficie expuesta a movimientos sísmicos,de unos 15 millones de kilómetros cuadrados,y comparamos las víctimas mortales,advertiremos que guardan una relación similar: 260.000 en el caso de los volcanes y 2.500.
000 en los terremotos.
(Datos éstos correspondientes al período 1600 - 1980.
) La distribución geográfica permite imaginar el tamaño del desastre,cuyo análisis estadístico arroja datos de interés evidente.
Los episodios con 10.000 muertos o más a principios del decenio significaron,en el mundo,más del 85 % del total de víctimas ; correspondieron al terremoto de Irán en 1990 y al ciclón de Bangladesh en 1991.
Dos desastres de un total de 114.
Esta ha sido la tendencia observada hasta ahora.
Lo que refuerza la idea de que la estrategia internacional debiera centrarse en regiones históricamente castigadas: Asia,Iberoamérica y el Mediterráneo,donde coinciden peligrosidad elevada y alta población expuesta.
La distribución temporal de los desastres depende del tipo de riesgo predominante.
Hemos observado que,en China,salvo terremotos,la frecuencia de desastres y la distribución de víctimas mortales corrían paralelas a la distribución de precipitaciones propia de climas monzónicos,con máximas en el verano ; en España,se siguen pautas de climas mediterráneos,con máximos en otoño invierno.
En ambos casos,la razón estriba en la preponderancia de los riesgos meteorológicos en los años analizados.
El análisis temporal debiera servir de criterio para la organización de los medios de actuación en emergencia.
El incremento de víctimas y pérdidas económicas registrado a lo largo del siglo se achaca a veces a una mayor peligrosidad de los desastres naturales.
Karnik,tras analizar los terremotos ocurridos en los últimos cien años,no encontró datos que justificaran la hipótesis,aunque no debemos olvidar la existencia de épocas de vacío sísmico,de unos 1000 años en China y casi 200 en Turquia.
El motivo real de los desastres reside en el crecimiento demográfico (sobre todo en Asia e Iberoamérica),concentrado además en ciudades con edificaciones e infraestructuras vulnerables.
El aumento observado en los daños económicos obedece a una mayor renta percápita,que se ha multiplicado por 3,5 en los países en vías de desarrollo y por 6 en los desarrollados.
El crecimiento de los daños tiene,pues,una causa social,no natural.
Para analizar el riesgo económico hemos de seguir pautas similares a las adoptadas en el estudio del riesgo social,si bien la fiabilidad de los datos económicos y su representatividad suelen ser menores.
Las tasas de cobertura por seguros en los países desarrollados pueden rebasar incluso el 50 % ; en el Tercer Mundo no alcanzan el 20 %.
identificados y caracterizados los objetivos,procede definir y evaluar las medidas técnicas de reducción del riesgo.
Un primer grupo de éstas sirve para reducir el riesgo menguando la peligrosidad potencial.
No siempre es posible ; de hecho,dos de las causas principales de desastres,los terremotos y los ciclones,no admiten este tipo de medidas.
Carecemos de medios técnicos adecuados para rebajar directamente la peligrosidad de un ciclón,una borrasca marítima o una " gota fría " como las que se forman en el Mediterráneo español a fines del verano y en otoño ; podemos reducir,sin embargo,la peligrosidad de uno de sus efectos: las inundaciones.
Por otra parte,los terremotos destructores,con focos a más de 10 kilómetros de profundidad y gigantescas potencias (el terremoto de San Francisco de 1906 desarrolló 24 ergios en 6 segundos),se sustraen por ahora a nuestra capacidad técnica.
Casi otro tanto puede afirmarse de las erupciones volcánicas ; aunque seamos capaces de predecirlas en casi la mitad de los casos,no podemos intervenir en las cámaras magmáticas o las chimeneas,lo que no obsta para que actuemos sobre algunas coladas de lava,regándolas,desviándolas o encauzándolas con voladuras,como se ha hecho a veces en erupciones del Etna en Sicilia.
Las inundaciones son el fenómeno natural que admite mayor número de medidas preventivas con actuaciones en cauces y cuencas hidrográficas.
Una inundación se caracteriza por el hidrograma,que expresa la relación entre caudales medidos en una sección del cauce y tiempo ; presenta en los casos sencillos un pico,algo retrasado con respecto al de la precipitación.
Cuanto mayor sea el caudal de pico,mayor será la altura alcanzada por el agua,que puede llegar a desbordarse.
Las actuaciones conducentes a disminuir el caudal de pico se denominan medidas de laminación.
Pueden construirse presas de laminación o mixtas (también para abastecimiento,regadíos y generación hidroeléctrica),actuar sobre la cuenca hidrográfica para rebajar el coeficiente de escorrentía (cociente de la fracción del agua que escurre sobre el terreno respecto al total caído) o aumentar el almacenamiento subsuperficial en el suelo del agua caída.
Las presas requieren inversiones importantes ; una típica de la España mediterránea cuesta unos 5000 millones de pesetas y almacena unos 60 hectómetros cúbicos,equivalentes a la escorrentía anual de una cuenca de unos 200 kilómetros cuadrados.
Gestionadas en períodos de intensas precipitaciones con sistemas informáticos,son una valiosa herramienta de laminación en los cauces principales.
Su rendimiento se mide en términos de reducción de frecuencia de las inundaciones.
Dado el criterio seguido para la avenida máxima de diseño,frecuentemente la que corresponde a un período de retorno de 500 años,es posible,sin embargo,que no ofrezca protección suficiente para precipitaciones de excepcional intensidad.
Las actuaciones en las cuencas vertientes,pensemos en la repoblación forestal,se miden a través de su influencia en la reducción de la escorrentía.
Igual que en el caso precedente se recurre a modelos hidrológicos informatizados,que aquí dan la reducción de frecuencia de los desbordamientos,y en definitiva la merma de peligrosidad.
La influencia de la vegetación sobre la escorrentía,positiva,decrece con la intensidad y duración de la lluvia.
Las prácticas tradicionales de cultivo,así el abancalamiento,son beneficiosas.
Pero no se olvide que la reforestación y la construcción de presas alteran los usos del suelo y pueden encontrar oposición social.
Las canalizaciones debilitan la peligrosidad por inundaciones en núcleos urbanos.
Al aumentar la sección de paso y disminuir la rugosidad natural,esas obras permiten el flujo de un mayor caudal pico sin desbordamiento.
La urbanización intensa de las cuencas vertientes eleva el coeficiente de escorrentía y restringe la capacidad de almacenamiento del suelo,produciendo un aumento de los caudales de pico.
Este fenómeno,observado en Sao Paulo,ha motivado el diseño de un tratamiento integral de la cuenca para reducir las inundaciones.
Asimismo,los deslizamientos y desprendimientos rocosos pueden en algunos casos estabilizarse gracias a la ingeniería geotécnica,con lo que su peligrosidad se anula.
En resumen,la acción de técnicas reductoras de la peligrosidad se traduce en una disminución de la probabilidad de ocurrencia del desastre y de su intensidad o severidad.
Puede acometerse también la reducción de la vulnerabilidad,expresada en porcentaje o en tanto por uno destruido de un bien o afectado en una población expuesta.
A diferencia de las medidas de reducción de peligrosidad,la mayoría de los riesgos naturales admiten siempre algún tipo de actuación de esta clase.
Así,el hundimiento de techos por caída de piroclastos y cenizas en el entorno de los volcanes activos,observado recientemente en el Pinatubo,se hubiera evitado con un diseño reforzado para hipótesis de espesor de piroclastos e impactos.
En el caso de las inundaciones,pueden realizarse edificaciones con plantas bajas exentas o construir sobre rellenos en las zonas inundables.
La acción del viento queda recogida en los códigos constructivos.
La propia vulnerabilidad de las edificaciones en zonas con deslizamientos o hundimientos lentos de gran dimensión puede paliarse con cimentaciones sobre losa armada o pilotes (deslizamiento),y arriostramiento de pilares,siempre que el movimiento no exceda de un umbral.
Quizá la invención más espectacular contra la vulnerabilidad fue la del pararrayos por Benjamin Franklin en 1752.
Desgraciadamente,la carencia de escalas objetivas de vulnerabilidad constituye uno de los mayores obstáculos para el análisis objetivo de los riesgos naturales.
En todos los casos,la eficacia de las medidas depende del período de retorno,del " peligro de diseño " adoptado.
que ha venido calculándose a partir de series de datos aplicando distribuciones estadísticas,de tipo exponencial,propias de valores extremos.
En 1935,Gutenberg y Richter enunciaron la ley de su nombre para terremotos,de carácter logarítmico,que liga probabilidad y magnitud.
En definitiva,estas medidas reducen el daño potencial,el riesgo,rebajando la probabilidad de fenómenos dañinos hasta la del " peligro de diseño ",límite a partir del cual bienes y personas,hasta allí protegidos,se vuelven vulnerables.
A reducir la vulnerabilidad tienden muchos esfuerzos ingenieriles.
Cuanto menor es el período de retorno de diseño,menos costoso resulta éste y menor el período cubierto.
La utilización de períodos de retorno demasiado cortos es una característica frecuente de los códigos constructivos en países en vías de desarrollo,cuya vulnerabilidad es,consecuentemente,mayor que en los países desarrollados ; en esa mayor vulnerabilidad se encierra la clave de la incidencia de los desastres naturales en el Tercer Mundo.
I grueso del esfuerzo investigador E sobre vulnerabilidad se ha centrado en los terremotos,por la sencilla razón de que la única estrategia viable para limitar el riesgo es adaptar las construcciones.
Para un terremoto de intensidad IX en la escala Mercalli Modificada (MM),que puede corresponder en el sur de la región valenciana a un período de 500 años,la reducción de la vulnerabilidad para una edificación de hormigón armado,según esté o no diseñado antisísmicamente,puede pasar de 0,7 a 0,22.
Para facilitar el trabajo,podemos crear mapas de vulnerabilidad combinados con mapas de microzonación sísmica que reflejan las amplificaciones y atenuaciones locales de la peligrosidad sísmica (en función de la resistencia del terreno,morfología y nivel freático entre otras) ; así lo ha hecho,por ejemplo,el Instituto Tecnológico Geominero de España en la ciudad alicantina de Alcoy.
Otro tipo de instalaciones altamente vulnerables para las personas suelen ser los campings levantados en llanuras de inundación de torrentes.
Tal fue la causa de la muerte de 36 personas en Francia el año pasado.
La reducción de la exposición en que puedan encontrarse bienes y personas es otra vía para atajar el riesgo.
Acorde con ese fin,la ordenación urbana y territorial persigue restringir la construcción o determinados usos del territorio.
Suele ir acompañada de medidas de diseño y reglamentos de uso.
Este tipo de medidas resultan en principio atractivas por su sencillez: localizadas las zonas expuestas con los correspondientes mapas de peligrosidad,se prohíbe la construcción y empleo de las mismas para eliminar el riesgo.
Pero deben salvar dos contratiempos: los usos actuales del suelo y el interés competitivo para otros destinos legítimos.
La verdad es que la población vuelve a utilizar las zonas expuestas a riesgo una vez pasado el desastre,sean las fértiles tierras volcánicas o las llanuras aluviales ; el suelo es allí altamente valorado.
¿Cuáles serían,pues,los criterios a adoptar en la ordenación urbana y territorial compatibles con estos hechos? Ante todo,una comunidad debe ordenar los usos del suelo de tal forma que se proteja la vida de sus habitantes ; éste es el criterio inspirador de las normas sismorresistentes.
Existen situaciones en las que el uso residencial del territorio supone un riesgo para la vida,que podría ser considerado inadmisible si se trata de edificación de alta densidad.
En el caso de las erupciones volcánicas,el riesgo ocurre en las faldas de los volcanes activos y zonas expuestas a lahares (corrientes de lodo volcánico),oleadas piroclásticas derivadas del colapso de columnas eruptivas (nubes ardientes capaces de subir por pendientes) o explosiones laterales.
En el caso de los terremotos,puede suceder en las fallas sísmicas activas que no están disipando energía con desplazamientos.
Los cauces de los torrentes de montaña o ramblas mediterráneas sujetas a inundaciones relámpago,en ambos casos con abundante carga sólida,son también zonas a evitar,como demuestran las inundaciones de Austria en 1988 o la destrucción en la estrategias óptimas de acuerdo con criterios establecidos.
rambla de Puerto Lumbreras en 1972.
Las laderas con arcillas rápidas,que pueden producir deslizamientos instantáneos y muy destructivos,bien conocidos en Suecia o Canadá,son zonas de riesgo real para la vida y los bienes.
Otro tanto puede decirse de las áreas con cavidades kársticas subterráneas,donde la presencia de bóvedas en equilibrio precario que pueden hundirse súbitamente,especialmente en rocas carbonatadas,impone una gran prudencia en su uso.
Dígase lo propio de las costas sujetas a tsunamis,grandes olas que alcanzan varias decenas de metros de altura al llegar a las costas barriéndolas tras originarse en el fondo del mar como consecuencia de terremotos intensos y viajar de forma prácticamente inadvertida a velocidades en torno a los 700 kilómetros por hora,que son también un elemento a considerar en el uso residencial del suelo.
Allí donde no exista un sistema fiable de alerta,como el del Pacífico,no parece prudente la edificación.
La estrategia de respuesta en emergencias requiere que determinados servicios y centros vitales de una población queden a salvo.
Debe asegurarse el mantenimiento del abastecimiento de agua,líneas telefónicas y eléctricas,carreteras de evacuación y suministro,centros políticos,policiales,hospitales y cuarteles de bomberos.
Allí donde no se pueda ir contra la peligrosidad o vulnerabilidad tiene que garantizarse la permanencia operativa de esos servicios con medidas de ordenación urbana o territorial.
No es mala solución el emplazamiento subterráneo de centros y líneas vitales.
Cuando ello no ocurre,ni ha podido procederse a la evacuación preventiva,el desastre se transforma en catástrofe que deja inerme la población.
Garantizada la protección de la vida,la decisión sobre usos del suelo en zonas expuestas al riesgo es un problema de opción comunitaria y análisis coste-beneficio.
El papel de los seguros de riesgos de la naturaleza es múltiple.
En EE. UU. y para el caso de inundaciones,se dividen las zonas inundables según el período de retorno esperable,penalizando las zonas con período de retorno menor de 100 años.
En España,el Consorcio de Compensación de Seguros asume las indemnizaciones para inundaciones,terremotos,erupciones,tempestad ciclónica y caída de meteoritos,con algunas penalizaciones para inundaciones.
Aunque las actuaciones preventivas forman el núcleo central de toda estrategia,siempre es necesaria la planificación de las emergencias.
Un primer campo de actuación es la vigilancia de las crisis,sobre todo en erupciones volcánicas,inundaciones y ciclones.
Suele mediar en esos casos un tiempo de reacción entre la presentación de la crisis y el paroxismo de la misma ; en este lapso de tiempo,debe entrar en acción la protección civil,responsable del seguimiento del episodio mediante comités de expertos ya constituidos de antemano y encargada de dictar las alertas preventivas,ordenar la evacuación y preparar los medios necesarios de acuerdo con la evolución del fenómeno.
El tiempo entre la presentación de una erupción volcánica y el paroxismo es menor de un día en el 45 % de los casos e inferior a una hora en el 23 %.
De ello se desprende que la vigilancia de la crisis eruptiva sólo puede actuar con garantía si el volcán ha sido objeto de estudios y está instrumentado con sismógrafos,inclinómetros y otros medios predictivos ; en su defecto,deben levantarse los correspondientes mapas de peligrosidad y riesgos.
Es necesaria la coordinación con el Servicio Meteorológico que prevenga a la población y compañías aéreas y de transportes de las direcciones de la nube de cenizas.
Si se trata de ciclones y tifones los satélites meteorológicos han posibilitado la predicción y el seguimiento en tiempo real y la emisión de las oportunas alarmas,que resultan vitales en el Caribe,el Indico o el Mar de China.
Gracias a los sensores de infrarrojos,puede estudiarse la evolución de las temperaturas del mar y aire e incluso prever su desarrollo.
Las inundaciones se propagan a lo largo de un río de acuerdo con la " onda de avenida ".
La formación de tales ondas permite,una vez observadas y caracterizadas,prever aguas abajo el momento de llegada.
Cuanto mayor es la cuenca,mayor es el " tiempo de concentración " que tarda el hidrograma en presentar el caudal pico.
En cuencas menores de 100 km2 la alerta puede ser materialmente imposible.
Actualmente las alertas,escalonadas en varias fases,comienzan con la previsión meteorológica de frentes o " gotas frías " generadoras potenciales de intensas precipitaciones,siguen con la medida de precipitación por encima de un umbral crítico y continúan con la observación hidrológica directa.
Este es el sistema seguido por la Protección Civil española y el Instituto Nacional de Meteorología.
En todos los casos anteriores,con posibilidad de predicción,la evacuación de la población hacia refugios o zonas seguras reduce drásticamente el riesgo.
No sucede así con los terremotos.
A pesar de que se conoce desde hace algunas décadas la variación de diversos parámetros antes del paroxismo (emisión anómala de radón,comportamiento electromagnético del terreno,emisión acústica,precursores y otros),no existe ningún método fiable para predecir magnitud y hora aproximada.
Por ello,las actuaciones en emergencia en estas zonas deben concentrarse en la mitigación del propio desastre.
En este sentido,la organización de equipos de rescate tras la localización rápida del epicentro es clave para reducir el número de víctimas,así como el realojamiento en zonas seguras para evitar daños en las réplicas que suelen seguir al terremoto principal y afectan a edificios ya dañados.
En ocasiones,el riesgo principal procede de fenómenos desencadenados por el suceso.
Así aconteció con los incendios en el terremoto de San Francisco en 1906 y en Tokio en el de Kuanto en 1923 o en Longarone en 1963,cuando el deslizamiento súbito del monte Toc,tras unos pequeños movimientos previos,en el vaso del embalse francés de Vaiont,produjo una ola con altura estimada de 100 m que vació buena parte del mismo por encima de la coronación,y arrasó Longarone con un saldo de 2000 víctimas.
Completado el análisis de medidas de reducción del riesgo y evaluada su eficiencia,queda por definir la combinación de las mismas,es decir,configurar la estrategia óptima de reducción de riesgos.
La realización de esta tarea es un problema típico de decisión óptima compleja,ya que los condicionantes existentes son diversos.
Para ello,conviene prestar atención,primero,a los criterios ya descritos al tratar del papel de la ordenación urbana y territorial (protección de la vida y de los servicios básicos).
Deben evaluarse,en segundo lugar,los costos de diversas medidas de reducción que actúan sobre los factores del riesgo.
En el caso de riesgo sísmico,las opciones son escasas: elegir el tipo de diseño sismorresistente de acuerdo con un nivel de vulnerabilidad admisible,construir en las zonas con menor amplificación sísmica o hacerlo en varias hipótesis de densidad de edificación.
En principio,el problema puede decidirse de acuerdo con un análisis coste-beneficio,considerando como beneficio anual el derivado de la reducción potencial de daños correspondiente a la menor vulnerabilidad,repartido entre el período de retorno actual,y teniendo en cuenta el período de servicio de la estructura.
Demos un ejemplo simplificado.
Para la hipótesis de un terremoto de diseño de intensidad X (Mercalli Modificada) con un período de retorno esperable de 100 años,coincidente con la vida útil de una edificación de costo sin diseño antisísmico de 20 millones de pesetas,y con ese diseño de 22,el coste a considerar serían 2 millones de pesetas para un beneficio esperable con una reducción de un 30 % de la vulnerabilidad de 6,6 millones.
Si el período de retorno fuera de 500 años,el costo a considerar sería de 10 millones frente a un beneficio esperado de 6,6.
Este tipo de análisis puede variar sustancialmente cuando se incorporan las indemnizaciones y costes que puedan producirse por muertos y heridos.
En esa línea,podemos construir curvas que expresen la relación entre beneficios y períodos de retorno que ayuden a la decisión.
Tras las actuaciones públicas se esconde un factor determinante,el de los presupuestos disponibles.
Pero es obvio que el problema de los riesgos no debe enfocarse sólo desde una perspectiva estrictamente economicista,sino que las decisiones han de tomarse en un marco más amplio,al tratarse de vidas humanas.
Se ha demostrado,sin embargo,que la reducción de víctimas mortales fruto de desastres naturales en los Estados Unidos resulta más costosa,en términos unitarios,que en otros campos.
A esa condición pueden asimilarse,salvando las distancias,los impactos ambientales negativos.
El diseño óptimo en un campo donde no existen unidades de medida homogéneas tiene que recurrir a matrices de decisión que presenten en las filas las estrategias,descompuestas en sus elementos,y en columnas los diversos criterios (económicos directos y colaterales,sociales y ambientales) con un factor de ponderación.
Evaluados los diversos elementos por un panel de expertos,las sumas ponderadas se comparan para tomar la decisión.
En España,mis últimas estimaciones sitúan las pérdidas medias anuales por inundaciones,durante los últimos veinte anos,en un máximo de 20.000 millones de pesetas.
Las inversiones medias anuales para avenidas en obras hidráulicas públicas en 1990 - 91 han superado los 30.000 millones.
Dadas las bajas cifras de víctimas mortales,menos de 20 al año de media,gran parte en torrentes poco regulables,el costo por muerto evitado es enorme,de miles de millones,especialmente si se compara con inversiones en Sanidad o Salvamento Marítimo.
El ejemplo del Japón merece analizarse.
Desde la promulgación de la Ley Básica de Medidas en Desastres,de 1959,ha adoptado una política nacional activa para prevenir los numerosos desastres que lo amenazan: terremotos,tsunamis,erupciones volcánicas,deslizamientos y ciclones.
Con porcentajes del presupuesto gubernamental en torno al 7 %,durante la década de los sesenta,el 6 % en los setenta y el 5 % en los ochenta,ha conseguido reducciones drásticas de víctimas y daños desde los primeros años de aplicación,con inversiones anuales públicas de alrededor de 1,2 billones de yens,unas 10.000 pesetas por habitante y año en los ochenta.
(En España se pierden unas 2000.
) Así,de una media anual de 150 muertos por deslizamientos en el período 1938 - 58 y 8000 casas destruidas se pasó a 30 muertos y 5000 casas destruidas en los sesenta y 20 muertos en los setenta.
Las cifras no sólo demuestran la viabilidad y eficiencia técnica de las estrategias integradas,sino también la acción rápida de los rendimientos decrecientes en la eficacia de las inversiones para reducir el riesgo.
Importa tener en cuenta que la disminución del riesgo en un marco geográfico y tecnológico dado sigue esta ley decreciente respecto a la inversión: cada vez se necesita mas Inversión para conseguir la misma reducción del riesgo.
Ello es así porque las soluciones más eficaces y menos costosas son las que primero se aplican.
Este hecho vuelve muy difícil,por no decir prácticamente imposible,la reducción del riesgo a cero ; de hecho,no existe esta situación en ninguna región geográfica extensa.
En contrapartida,resalta la rentabilidad de las inversiones en la reducción del riesgo en los países subdesarrollados,azotados por los grandes desastres y donde casi todo está por hacer.
Conversando con el ordenador Reconocimiento automático del habla Uno de los más importantes retos que tiene planteados el sector informático en esta década es el de lograr una comunicación " natural " entre el hombre y el ordenador.
Son muchos los investigadores que,en todo el mundo,trabajan preparando el terreno para el diálogo real entre usuario y máquina.
Si la forma habitual que las personas utilizan para comunicarse es su propia lengua (la llamada " lengua natural "),¿por qué no hacer que los ordenadores sean también capaces de entenderla? Así,la interacción con la máquina sería más sencilla,estaría al alcance de un número mayor de usuarios y se evitaría,en muchos casos,la necesidad de aprender complejos códigos y engorrosos lenguajes de programación.
Si bien es cierto que aún nos encontramos lejos de esos robots parlantes de las novelas futuristas y de las películas de fantasía científica,ya van existiendo herramientas que incorporan ciertos rasgos de la lengua natural a las aplicaciones y procesos informáticos tradicionales.
Un proyecto general sobre " La Tecnología de la Lengua " habrá de proponerse el tratamiento de la lengua natural por medio del ordenador.
Su fin último sería hacer que éste entendiera,generara o tradujera lenguas naturales,para responder en consecuencia a las órdenes y preguntas del sujeto usuario.
Habría de trabajar,pues,en el desarrollo de modelos computacionales y formalismos en morfología,lexicografía,sintaxis,semántica y pragmática ; en la creación de interfases en lengua natural y de herramientas informáticas de ayuda a la escritura,la traducción,la realización automática de resúmenes,la enseñanza y el aprendizaje ; en aplicaciones en recuperación de información,generación de textos y traducción automática,y en el desarrollo de sistemas de reconocimiento y síntesis del habla.
El Centro de Tecnología de la Lengua (CTL) de IBM viene trabajando en algunos de estos apartados desde hace varios años.
En particular,y desde el año 1990,en la preparación de un sistema de reconocimiento del habla para el castellano,proyecto realizado en colaboración con otros países europeos y con el Centro de Investigación T. J. Watson de IBM en Hawthorne (Nueva York),que cuenta con 20 años de experiencia en este campo y dispone de uno de los sistemas más potentes.
El reconocimiento del habla se propone,en definitiva,la conversión fluida con el ordenador,como si se tratara de un sujeto humano más.
A pesar de los enormes progresos conseguidos en los últimos años,ese momento está todavía lejos,pero gracias tanto a la potencia de los ordenadores como a la mejora de los algoritmos,existen hoy ya algunos sistemas comercializados y otras aplicaciones que estarán disponibles en un corto plazo.
Hagamos un breve repaso.
Contamos ya con sistemas que reconocen palabras de un vocabulario reducido pronunciadas por cualquier locutor a través de la línea telefónica.
Pueden utilizarse para la realización de consultas sobre datos bancarios,en sistemas de información municipales o turísticos limitados,en la realización de reservas de viajes y hoteles,o en la transferencia de llamadas telefónicas,por ejemplo.
En este caso,el usuario no necesita un terminal informático,sino simplemente el aparato telefónico ; los sistemas se combinan con sintetizadores de voz,para responder oralmente a las peticiones.
No tardaremos mucho en disponer de ciertas aplicaciones relativas al control de procesos y la introducción de comandos mediante voz,en casos en que las manos o la vista del operador están constantemente ocupados (manejo de maquinaria,de aviones,etc.).
Mayor complejidad revestirá la entrada oral de datos o el dictado de informes que permitan operar con el ordenador sin necesidad de utilizar las manos o en circunstancias de baja iluminación (trabajo en cámaras oscuras).
Se prevé,por fin,amplio uso del reconocimiento del habla en ciertas minusvalías ; pensemos,por ejemplo,en el caso de sordos y de personas con discapacidad en las extremidades superiores.
A más largo plazo,el sistema capaz de reconocer el habla continua,tal y como los humanos lo hacen habitualmente,podrá realizar el subtitulado automático de programas o emisiones de televisión,servicios de interpretación y traducción simultánea aplicable a consultas telefónicas,videoconferencias o conversaciones,etc. Pero son muchas las dificultades que hay que resolver para construir un sistema de reconocimiento del habla.
El propio mecanismo humano de reconocimiento,extraordinariamente complejo,dista mucho de conocerse bien ; y la facilidad con que los humanos entendemos el habla no invita precisamente a ponderar los problemas que ello supone para una máquina.
Pensemos,sin embargo,en las dificultades que conlleva el aprendizaje y comprensión de una lengua extranjera,para hacernos una idea más cabal de la cuestión.
Los problemas principales con los que hay que enfrentarse en el reconocimiento del habla son de distinto tenor.
Unos afectan al tamaño del vocabulario: cuantas más palabras se desea reconocer,más complejo es el sistema,pues aumentan las posibilidades de ambiguedad y confusión y el tiempo de proceso necesario ; es decir,es mucho más sencillo desarrollar un reconocedor para unas pocas palabras que para un vocabulario grande,de varias decenas de miles de palabras,que dé al usuario la impresión de ser ilimitado,de que puede utilizar virtualmente cualquier palabra y expresión que desee.
Otros problemas tienen que ver con el tipo de habla.
Al emitir un sonido,éste se halla condicionado por los sonidos circundantes (coarticulación) ; en el habla continua,las fronteras entre las palabras no suelen mostrarse tajantes,por lo que se producen más confusiones entre ellas.
El habla discreta,donde las palabras se pronuncian de forma aislada,es,en consecuencia,más fácil de reconocer que el discurso continuo,sin pausas entre palabras.
Importa también la dependencia del locutor.
Un sistema dependiente del locutor necesita un entrenamiento con cada nueva voz para adaptarse a sus características.
Compensa los inconvenientes del aprendizaje con la exactitud obtenida en el reconocimiento.
Los sistemas independientes del locutor presentan mayores dificultades,al tener que trabajar con diversos modelos de voces y acentos.
Otra fuente de dificultad reside en los rasgos propios del emisor y en las características de la transmisión.
Cualquier variación en el volumen velocidad,énfasis o claridad del hablante puede influir negativamente en el reconocimiento.
Los ruidos ambientales,las conversaciones cruzadas,el micrófono o la peor calidad del sonido en caso de transmisión a través de la línea telefónica son otras tantas circunstancias que limitan la claridad e ínteligibilidad del habla.
El proyecto Tangora / E,del Centro de Tecnología de la Lengua de IBM,se propone desarrollar un sistema capaz de transcribir por escrito las palabras enunciadas oralmente por distintos locutores humanos,es decir un sistema de dictado automático,lo que podríamos llamar una " máquina de escribir sin teclado ".
El sistema reconoce ya 20.000 palabras pronunciadas de forma aislada.
Es decir,es necesario hablar con pausas entre las palabras,pero bastan unos 4 centisegundos de pausa,por lo que el proceso de coarticulación no desaparece por completo y se puede mantener una entonación " casi " normal.
Con 20.000 palabras se cubre más del 97 % de un texto perteneciente a un dominio determinado de especialidad.
(Por " palabra " entendemos aquí forma de palabra,no lema ; es decir,las distintas formas flexionadas correspondientes a un determinado lema son palabras diferentes para nuestro sistema.
) El reconocimiento se lleva a cabo en tiempo real,con un retardo de unos pocos segundos,necesarios para verificar las distintas hipótesis de palabras y secuencias de palabras.
Tangora / E es dependiente del locutor.
Vale decir,necesita un entrenamiento para cada nuevo usuario que desea ser reconocido por el sistema.
Este se lleva a cabo leyendo un texto preparado de antemano y exige que se pronuncien todos los fonemas un número suficiente de veces en distintos contextos fonéticos.
En esta fase se invierten unos 20 minutos por término medio.
El sistema se basa en métodos probabilísticos,partiendo de la suposición de que el habla se puede modelizar estadísticamente a partir del examen de un conjunto significativo de datos de entrenamiento.
El formalismo utilizado es el de los modelos ocultos de Markov (" Hidden Markov Models "),caracterizados por dos elementos básicos: un conjunto de estados conectados mediante secuencias de transiciones y un conjunto de distribuciones de probabilidad en cada estado.
Basado en esta técnica se elabora el modelo acústico,que se utilizará posteriormente en el proceso de reconocimiento.
El modelo acústico se genera de acuerdo con la pronunciación real de las palabras del vocabulario,partiendo del procesamiento de las grabaciones de las voces de diversos locutores que enuncian varias veces cada palabra.
Para el español,hemos utilizado diez voces masculinas,con pronunciación uniforme,pero con distintos tonos.
En realidad,nuestros locutores han leído el texto de entrenamiento y sólo 6000 palabras del vocabulario,el correspondiente a una versión anterior del actual reconocedor.
A partir de esa información acústica,y gracias al inventario de alófonos resultante,hemos desarrollado herramientas que nos permiten sintetizar los modelos acústicos de las nuevas palabras de forma automática,sin necesidad de grabaciones,con el consiguiente ahorro de tiempo,mayor flexibilidad frente a ampliaciones y modificaciones del vocabulario,y sin merma del rendimiento.
Por otro lado se utiliza el " modelo de lenguaje ",que es también probabilístico.
Este modelo asigna probabilidades a las secuencias de palabras ; codifica,pues,la información lingüística que utiliza el sistema.
Contiene probabilidades de transición entre palabras ; en nuestro caso se utilizan trigramas,por lo que la probabilidad de una palabra viene condicionada por las dos palabras precedentes.
Estas probabilidades se deben extraer del tratamiento estadístico de un cuerpo textual muy voluminoso,en relación directa con el vocabulario de la aplicación.
Nuestro sistema dispone ya de un corpus que supera los 100 millones de palabras,compuesto fundamentalmente por texto periodístico,novela contemporánea y fragmentos de historia y medicina.
- El proceso global que se sigue,en ù gran medida probabilístico,estima cuál es la secuencia más probable de palabras pronunciadas por el locutor a partir de la probabilidad individual,de que cada palabra del vocabulario haya sido pronunciada y de la probabilidad de ocurrencia de una secuencia de palabras determinada.
En resumen,pues,Tangora / E opera como sigue: una vez filtrada,muestreada y normalizada la señal de entrada correspondiente a la emisión del locutor,tras su recepción por el micrófono,se obtiene una sucesión de valores numéricos.
La primera fase,de selección rápida,rastrea todas las palabras del vocabulario del sistema y obtiene una lista de palabras candidatas.
Para facilitar esa labor de comparación,se han creado modelos de las palabras en secuencias de fonos,que tienen la apariencia de una transcripción fonética.
Los fonos están también representados mediante modelos ocultos de Markov,y corresponden a los fonemas y alófonos de la lengua.
Se han definido 51 fonos para el español.
La segunda fase aplica la información del modelo de lenguaje,asignando probabilidades a secuencias de palabras,independientemente del proceso acústico.
Así se reduce la lista de palabras obtenidas por la fase anterior,seleccionándose las secuencias de candidatos más probables.
La última fase,de selección detallada,aplica las probabilidades contenidas en el modelo acústico antes mencionado.
Es más caro,en términos de cómputo,que la selección rápida,pero opera sobre hipótesis más probables.
Al final,se escogen como definitivas las palabras más probables de acuerdo con la puntuación que han ido acumulando a lo largo de las tres fases.
Por supuesto,no se examinan todas las posibles secuencias de palabras,lo cual sería prohibitivo,sino que,a lo largo del proceso,se van descartando las que tienen una puntuación inferior a determinado umbral.
Los resultados obtenidos hasta el momento con nuestro prototipo son extraordinariamente positivos,ya que la tasa de reconocimiento supera en media el 96 % de aciertos en las pruebas realizadas con distintos locutores.
Para que un sistema de este tipo llegue a la comprensión de cualquier locutor,que hable de forma natural y espontánea (y,por tanto,no necesariamente gramatical,como suelen hacerlo las personas,lo que no impide la comprensión),con un gran vocabulario (más de 50.000 palabras) y en unas condiciones ambientales de ruido y conversaciones cruzadas,se requerirán muchos años de trabajo e investigación.
Pero el reconocimiento del habla y,en general,la técnica de la lengua,están dando ya hoy sus frutos y pronto veremos sus resultados en numerosos programas de ordenador.
(Luis de Sopeña,IBM Madrid.
) Tecnología de altura ¿La industria por fin en el espacio? 1 uso de la navegación espacial para la experimentación científica es tan antiguo como los propios lanzamientos de satélites y sondas,tripulados o no.
Se inició con los primeros satélites rusos y americanos,y prosiguió con los satélites europeos,chinos,indios o japoneses.
Ejemplo reciente es el embarque en la nave Columbia,que transporta el laboratorio europeo Spacelab,de dos experimentos sobre materiales sometidos a microgravedad preparados en la Escuela Técnica Superior de Ingenieros Aeronáuticos de Madrid.
Sin embargo,debe señalarse inmediatamente que hasta el momento la experimentación espacial se mantiene en un terreno demostrativo y apenas ha incidido en el ámbito de la industria privada.
En los Estados Unidos,donde podría esperarse una fuerte participación empresarial en esta actividad,la Administración Nacional de Aeronáutica y del Espacio (NASA) intenta convencer,con insistencia rayana en el fastidio,a las compañías privadas de que añadan a sus medios de investigación un laboratorio en órbita.
Pese a ello,las inversiones conseguidas no pasan de simbólicas,y la investigación espacial comercial sigue siendo hoy casi una entelequia.
En éstas,Westinghouse y dos empresas asociadas (EER Systems y Space Industries) creen que serán capaces de hacer que la investigación privada espacial despegue,con algo más que una pequeña ayuda financiera por parte de la NASA.
A la primera misión del COMET (del inglés Commercial Experiment Transporter) seguirán once experimentos en órbita baja,a 550 kilómetros de la Tierra.
Westinghouse espera avivar la demanda de la industria mediante esta estrategia comercial,no demasiado costosa,de alquilar plaza en un vehículo espacial.
Gran parte de los 32 lanzamientos comerciales realizados por los Estados Unidos desde 1982 han colocado en órbita satélites de comunicaciones ; en esas misiones habían los investigadores de aportar su propia estación terrestre y ocuparse por sí mismos de innumerables detalles de planificación.
Westinghouse ha establecido un programa de lanzamiento independiente - - el WESTAR,o Servicio Westinghouse de Transporte Espacial - - que ofrece a cualquier interesado,por 35 millones de dólares,un paquete completo de servicios.
En palabras de Thomas E. Haley,director del programa WESTAR," en esta tienda se compra todo a la vez ; nos encargamos del lanzamiento,la puesta en órbita y la recuperación ".
Por mucho que se hable del sector privado,lo cierto es que la NASA va a correr con 85 de los 100 a 200 millones que se prevé costarán tres lanzamientos del COMET,aparte de financiar los gastos que supone el llevar los experimentos a bordo del vehículo espacial.
Es evidente que sin su apoyo no se habría emprendido el programa.
Los 205 kilogramos de carga útil que suman los once experimentos del primer vuelo del COMET han sido asignados a los centros de desarrollo comercial del espacio.
Estas instituciones,la mayor parte de ellas vinculadas a universidades,fueron creadas,en 1985 la primera,por la oficina de programas comerciales de la NASA,y todavía dependen de ésta como fuente primordial de financiación.
(El Spacehab,módulo experimental de construcción privada que volará en el transbordador espacial,sigue un modelo semejante.
Se han programado ocho vuelos a intervalos de seis meses).
Entre los experimentos asignados al COMET figura uno sobre crecimiento de cristales de proteínas en el que participan Eli Lilly,Du Pont y Merck - -,y unas pruebas de telecomunicaciones de Motorola,firma que,por mediación del centro de comercialización del espacio de la Universidad Atlántica de Florida,participa en experimentos encaminados a determinar los requisitos técnicos del elemento en órbita baja de una red telefónica celular mundial.
El COMET ha de ofrecer una exposición a la microgravedad mucho más prolongada que la prestada por el transbordador espacial o cualquier otro servicio de lanzamiento comercial de los Estados Unidos.
Las anteriores misiones norteamericanas con cohetes desechables solamente permitían unos minutos de microgravedad.
La cápsula,proyectada por los pioneros espaciales Max Faget y C. C. Johnson,de Space Industries,usa una técnica probada a lo largo de 30 años,y recuerda las cápsulas tripuladas de los programas Mercury y Gemini.
El módulo de servicio desechable fue construido por Westinghouse ; contendrá cuatro de los once experimentos,y se mantendrá dos años en órbita.
Ambas cápsulas serán lanzadas por un cohete Conestoga 1620 suministrado por EER,la empresa que construyó la rampa de Wallops Island.
EER proporciona también motores y componentes,y monta los referidos cohetes.
Apenas faltaba un mes para la fecha de lanzamiento tentativamente fijada,y WESTAR todavía buscaba comprador para 50 kilogramos extra de carga útil ; los experimentos que los aprovechasen habrían de estar enteramente financiados por empresas,universidades u organismos del gobierno.
Según Haley,para el otoño de 1994 se podría programar un vuelo WESTAR independiente,aunque muchos,si no todos los experimentos corrieran a cargo del Departamento de Defensa u otra agencia federal.
" La industria y las universidades sienten interés,pero no están listas todavía ",dice.
En realidad,pocos son hoy día los convencidos de que sea rentable investigar en condiciones de microgravedad el crecimiento de cristales o proteínas con fines farmacéuticos,o de materiales semiconductores para la electrónica.
Tras diez años de experimentos,sólo algo más de uno de cada cinco cristales de proteínas producidos en el espacio aventaja a los obtenidos en laboratorios terrestres,según un reciente artículo de Nature.
Un informe del Consejo Nacional de Investigación observaba que el " celo " de la NASA por estimular el uso comercial del espacio podría dar lugar a experimentos industriales de poca calidad,en los que se prescindiese del rigor con que las investigaciones universitarias son examinadas.
Robert F. Sekerka,profesor de física de la Universidad Carnegie Mellon,califica de oportunista la participación industrial en este programa: " Si el gobierno no pusiese el dinero y la industria pagase el flete entero,¿cuánto pondrían? La respuesta,seguramente,sería: nada.
" WESTAR,sin embargo,espera que,si el precio es adecuado,la industria y las universidades se subirán finalmente a bordo.
Proyectan pedir un millón de dólares por una carga útil de cinco kilogramos metida con calzador entre otros experimentos,que,con todo,es suficiente para un pequeño experimento de desarrollo de proteínas.
Según Haley,la posibilidad de repetir las pruebas a intervalos previsibles tal vez permita descubrir las aplicaciones óptimas de la microgravedad.
Si tuviese éxito,WESTAR,predice Haley,podría llegar a realizar hasta 10 misiones por año a fines de siglo,a medida que el costo vaya cayendo de manera constante.
" Muchos científicos desearían poner algo en órbita en enero,recuperarlo,modificarlo y volver a lanzarlo en agosto para observar el efecto del cambio de parámetros ; se ahorrarían así años de experimentación en tierra.
" Según los cálculos actuales,un quinto de los cánceres provienen,al menos en parte,de infecciones víricas crónicas.
Virus convictos - de producirlos son el de la leucemia / linfoma de la célula T humana,el del papiloma humano,el de la n hepatitis B y del síndrome de Epstein-Barr (EB).
El VIH,o virus del SIDA,se añadirá probablemente a la lista.
Hace tiempo que se viene pensando que el uso de antígenos procedentes de estos virus podría movilizar el sistema inmunitario contra el cáncer,pero sólo muy recientemente se ha empezado a notificar resultados positivos al respecto.
Una joven firma británica de biotecnología,Cantab Pharmaceuticals,ha desarrollado una posible inmunoterapia contra el cáncer cervical basada en una proteína vírica.
Y varias empresas han manifestado interés en comercializar una vacuna contra el virus EB,del que se cree que es la causa tanto del cáncer nasofaríngeo como del linfoma de Burkitt,tumor de los ganglios linfáticos habitual entre los niños en Africa.
El ejemplo cundirá probablemente entre otras compañías.
Quizá sea el producto de Cantab el primero que se base en una demostración,mediante pruebas con animales,de que las proteínas víricas provocan una respuesta inmunitaria capaz de evitar tumores e inducir su regresión.
La compañía,que concentra su esfuerzo en el desarrollo de terapias,fue creada en 1989 por Alan J. Munro,quien fuera jefe de inmunología de la Universidad de Cambridge,y Abingworth,un grupo de capital riesgo.
Un acuerdo formal garantiza a Cantab la licencia exclusiva de la comercialización de la investigación inmunológica que la universidad lleve a cabo.
M. Saveria Campo y sus colaboradores,de la Universidad y el Instituto Beatson de Investigación del Cáncer de Glasgow,le abrieron el camino a este producto anticancerígeno.
Los animales a los que se inyectó la proteína E7 del virus del papiloma bovino reforzaron su capacidad de lucha contra los tumores en la boca que la inoculación del virus propiamente dicho les producía.
El tratamiento con E7 estimuló la respuesta inmunitaria celular,elemento esencial del sistema inmunitario,con mayor intensidad que el virus mismo.
Si bien no se conoce aún qué papel desempeña,no es,seguramente,casual que la E7 interacciones con una importante proteína celular que regula el crecimiento.
Los resultados de Campo,que aparecerán dentro de poco en el Journal of General Virology,han dado el espaldarazo a que se investigue si se produce un efecto similar en los seres humanos.
El virus del papiloma bovino es genéticamente muy similar al del papiloma humano tipo 16,causante de lesiones del cérvix que pueden devenir malignas.
Se calcula que este tipo de cáncer mata anualmente a más de 300.000 mujeres en todo el mundo,casi tantas como el cáncer de mama.
Cornelia S. McLean demostró que las proteínas del virus del papiloma humano pueden también inducir una intensa respuesta inmunitaria.
Para ello,McLean (por entonces en el departamento de inmunología de Cambridge y actualmente en Cantab) tomó como vector el virus de la vacuna,tan empleado en la vacunación contra la viruela.
Inyectó a ratones ese virus,modificado de manera que produjese la proteína E7 del virus del papiloma humano tipo 16,y después observó la reacción que tenía lugar cuando se injertaban células productoras de E7 en la piel de los ratones.
El pretratamiento con el virus de la vacuna modificado hizo que los ratones rechazaran los injertos con más rapidez de lo normal ; esto indicaba que los animales se habían vuelto sensibles a la proteína (los resultados de McLean se han publicado en el número de febrero del Journal of General Virology Cantab colabora con Leszek K. Borysiewicz,de la facultad de medicina de la Universidad de Gales en Cardiff,para determinar si el virus de la vacuna productor de E7 genera respuestas inmunitarias en los seres humanos.
Inicialmente,se inyectará el virus modificado (que también produce otra proteína vírica,la E6) a unas 10 pacientes con cáncer cervical avanzado.
Aunque,según Munro,el uso de virus de la vacuna vivo comporte cierto riesgo,los experimentos sugieren que un virus activo da lugar a una respuesta mucho más intensa que una proteína purificada,no reconocida de igual manera por el sistema inmunitaria.
" Aprovechamos las sutilezas del sistema inmunitario ",afirma.
El trabajo de Campo sobre el virus del papiloma bovino fue financiado por la Campaña de Investigación del Cáncer,cuya sede se encuentra en Londres,organización que espera también conseguir una vacuna contra la infección por virus de papiloma ; en enero saltó su nombre a los titulares de los periódicos a causa de su anuncio de otra vacuna preventiva,la vacuna contra el virus EB (de Epstein-Barr).
Se cree que el virus EB origina distintos tipos de cánceres,combinado,probablemente,con otros factores.
Alrededor del 90 % de la población mundial está infectada por este virus,pero en los países desarrollados no suele causar enfermedad alguna ; como mucho,fiebre glandular,una indisposición transitoria.
No obstante,se encuentra el virus en células malignas de quienes padecen la enfermedad de Hodgkin.
En Africa,aparece en las células del linfoma de Burkitt,y se le imputan unos 50.000 fallecimientos al año por cáncer nasofaríngeo en China.
La nueva vacuna contra el EB,a diferencia de la terapia anticancerosa de Cantab,es una versión sintética de una proteína que solemos hallar en el virus.
El producto,culminación de más de veinte años de trabajo,se basa en las investigaciones de Andrew Morgan,de la Universidad de Bristol,y John R. Arrand y Michael Mackett,de la Universidad de Manchester,junto con otros colaboradores de la Universidad de Birmingham.
Se obtiene a partir de células de ratón.
Se ha demostrado que previene un linfoma en cierto tipo de mono.
Si diera buen resultado en los seres humanos,podría convertirse en la primera vacuna concebida especialmente para prevenir el cáncer,si bien las vacunas contra la hepatitis B,utilizadas desde hace más de 10 años,impiden casi con seguridad el cáncer de hígado.
John Green,del Programa para el Desarrollo de Tecnologías Adecuadas para la Salud,de Seattle,institución que colabora con el proyecto internacional contra el virus EB,señala que varias empresas han expresado su interés en perfeccionar la vacuna.
Para ello,no obstante,hay que afrontar muchas dificultades ; ninguna mayor,quizá,que la pobreza de los países que más la necesitan,países que tienen,además,otras prioridades sanitarias.
El uso de la vacuna contra la hepatitis B,por ejemplo,no se ha generalizado en el Tercer Mundo.
Morgan,de la Universidad de Bristol,cree que el trabajo sobre el virus EB conducirá finalmente,además de a una vacuna preventiva,a una vacuna terapéutica contra el cáncer o linfoma nasofaríngeo.
Pudiera ocurrir,sin embargo,que estuviese basada en una proteína diferente de la que constituye el fundamento del producto profiláctico.
En las personas infectadas por el virus EB,el sistema inmunitario muestra un aumento de actividad poco antes de manifestarse clínicamente el cáncer nasofaríngeo.
Una vacuna que estimulase esa misma actividad podría,en su opinión,conseguir efectos terapéuticos.
Hasta hace poco,dice Morgan," la gente que trabaja en el campo de las vacunas no era consciente de la complejidad de la respuesta inmunitaria,ni de que es posible alterarla selectivamente ".
Ahora parece que los virus han ofrecido a los de la Cantab,entre otros,un buen puñado de estrategias prometedoras.
La óptica da en el blanco Ordenador óptico con memoria espaciotemporal l cálculo digital dio un paso de gigante cuando se terminó,en la Universidad de Manchester,allá por 1948,el Mark I,primer computador enteramente electrónico capaz de almacenar en memoria su propio programa.
No menos audaz - - y vacilante - - ha sido el avance operado en computación óptica,tan en su infancia aún,a principios de este año cuando Harry F. Jordan y Vincent P. Heuring,de la Universidad de Colorado,presentaron un ordenador (casi) óptico que almacena y maneja sus instrucciones y datos en forma de impulsos luminosos.
Como el Mark I,el ordenador óptico de bits seriados (OOBS) es grande y obtuso,y se ha construido sólo para probar un principio de la arquitectura de ordenadores.
Apenas hace otra cosa que conmutaciones,recuentos y operaciones de aritmética de carácter básico,y sólo tiene 128 bytes de memoria.
Ahora bien,todo eso lo hace de un modo radicalmente distinto de cualquier ordenador que jamás se haya construido.
La digital y electrónica prole del Mark I ha venido almacenando sus datos en los biestables o basculadores (" flip-flops "),dispositivos que toman y retienen cargas por medio de relés,tubos de vacío o circuitos de semiconductores.
En ellos,la información queda confinada físicamente.
Sin embargo,los equivalentes ópticos del biestable electrónico todavía operan muy lentamente,y por eso el OOBS utiliza en su lugar una memoria dinámica: los impulsos de radiación infrarroja que constituyen sus bits se mueven incesantemente por un circuito de fibras ópticas a la velocidad de la luz.
" La información está siempre en movimiento ; en cierto sentido,se almacena Sin embargo dondequiera que se encuentre,tanto en el espacio como en el tiempo ",explica Heuring.
" El quid está en disponer las cosas de tal modo que la información interaccione en el mismo lugar del espacio al mismo tiempo.
" El OOBS logra esta proeza de sincronización valiéndose de un reloj que funciona a 50 megahertzs y conduciendo la luz por un trayecto que cubre distancias entre conmutadores cuidadosamente medidas.
Antes de cada tic del reloj,los impulsos de control accionan conmutadores que dirigen de un trayecto a otro los bits (luego llegan,haciéndolos entrar y salir de líneas de retardo formadas por bobinas de fibra óptica con metro de longitud.
Al contrario que el pro cesador electrónico,que recoge la información que necesita de una dirección de su memoria,el procesado óptico del OOBS espera que le lleguen los datos.
Los contadores guardan registro de qué hay y dónde.
Los conmutadores son la única parte no enteramente óptica del sistema.
AT&T,suministrador del grupo de Colorado,los construye difundiendo titanio por una superfice de niobato de litio,que es un sólido transparente,a fin de formar dos canales para la luz.
Cuando se aplica una tensión eléctrica,los canales se hacen paralelos,y cuando cesa,se cruzan.
Para accionar el conmutador debe transformarse el impulso de control luminoso en un impulso eléctrico,y amplificarlo después.
Jordar utiliza un ordenador corriente de sobremesa conectado a los amplificado res de conmutación para programa el OOBS y representar sus resulta dos,aunque en teoría un interfaz de fibra óptica funcionaría con pareja perfección.
El OOBS necesita,pues,electricidad para operar.
Pero al relegar los electrones a un papel menor y prescindir por completo del almacenamiento estático,la estructura del ordenador se simplifica bastante.
" Tal vez sea ésta la primera vez que puede representarse un ordenador completo en una sola transparencia de retroproyector ",se ufana Heuring " Normalmente,eso es imposible ; hay tantos biestables,puertas y buses,tantas otras cosas... " La memoria dinámica ofrece otras ventajas: una de ellas es la velocidad ajustable de la máquina.
Dado que e diseño se basa en el tiempo que tar dan los impulsos de luz en pasar de un procesador al siguiente,un diseño funcionará igual a mitad de tamaño y doble velocidad de reloj ; afortunadamente,ya que en su versión actual el OOBS ocupa una mesa entera Jordan y Heuring lo están ahora integrando en un cilindro de niobatc de litio,adherido a un disco de silicio o arseniuro de galio que contendría la electrónica de conmutación El ordenador,que cabrá en la palma de la mano,se habrá achicado unas 400 veces y trabajará 400 veces más deprisa,a 20 gigahertzs.
El simple aumento de velocidad sin cambio de tamaño produce también resultados interesantes,como ha descubierto Jordan.
" Si se cuenta con conmutadores rápidos,se puede duplicar la velocidad de reloj y acaba teniendo dos máquinas con un sol soporte físico,actuando aparentemente en paralelo,pero en realidad intercaladas en el tiempo.
" A 100 megahertzs,en otras palabras,el OOBS desarrolla una doble personalidad.
Las ventajas de la memoria dinámica han de pagarse,no obstante,en capacidad y en rendimiento,pues el almacenamiento espaciotemporal de información requiere gran cantidad de la una o la otra.
Y cuanto más largo sea el circuito,más tiempo tendrá que esperar un procesador a que le lleguen nuevos datos.
Heuring admite que esto limita la complejidad de las posibles aplicaciones.
" Un OOBS sería ideal para una red de conmutación telefónica o de televisión por cable que encauzara y redistribuyera información secuencial procedente de cinco ciudades con destino a otras cinco,pero no valdría para el procesamiento de textos.
" Es casi seguro que las primeras máquinas ópticas que se realicen tendrán un carácter híbrido,con conmutadores optoelectrónicos y algún tipo de almacenamiento estático.
Conscientes de ello,Jordan y Heuring trabajan ya en un proyecto de procesador en paralelo en el que las bobinas de fibras se sustituyen por espacios vacíos y los conmutadores en serie por circuitos integrados optoelectrónicos (CIOE),unas formaciones cuadradas que por una cara tienen fotodetectores en miniatura y por la otra microláseres.
Descansarán sobre sus bordes,a modo de fichas de dominó ; tomarán la alimentación eléctrica por la base y recibirán los impulsos de control por arriba.
En este nuevo diseño de Heuring,se utilizan hologramas generados por ordenador para redistribuir los bits desde un CIOE al siguiente.
Los investigadores de Colorado creen que el poder construir un ordenador con pastillas CIOE que contuviesen al menos un millón de conmutadores sería ya todo un logro.
" El vídeo se ha de leer serialmente,y ése es el verdadero cuello de botella de la realidad virtual y de tantos y tantos gráficos y animaciones por ordenador ",observa Heuring.
" Con los CIOE,podríamos en principio trabajar,de una sola vez,sobre una trama de vídeo entera,o sobre un polígono completo generado por ordenador.
" Ninguno de los dos espera obtener un producto acabado antes de cinco a siete años,aun suponiendo que encuentren financiación.
Pero un año de trabajo debería ser,en su opinión,suficiente para demostrar la validez de los principios en que se basan,y en tres años,vaticina Heuring,podrían conseguirse resultados preliminares.
Sistemática vegetal ¿Una nueva división? La descripción de nuevas especies es el pan de cada día en biología sistemática.
Pero,cuando se trata de crear grupos de alcance amplio ante la aportación de nuevos datos y pruebas,la conmoción producida es notable: se multiplican las especies y se dilatan los criterios de jerarquización sistemática.
Así puede ocurrir con el reciente hallazgo de una nueva rama del árbol de la vida,de nivel parejo al que ocupan los vertebrados o las plantas con semillas.
Nos referimos a la bifurcación descubierta en las algas,ricas en tipos y variantes.
En 1868,el médico Hermann Itzigsohn describió un alga unicelular particular,Glaucocystis nostochinearum,dotada de pared celular y con inclusiones azulverdosas.
Con tales datos no era fácil encajarla en ningún taxón.
Veinte años más tarde,Gustav Lagerheim,botánico sueco,planteaba otra aporía con un alga parecida,Gloeochaete wittrochana.
Entre 1929 y 1930,Lothar Geitler,en Viena,y Adolf Pascher,en Praga,removieron provechosamente el terreno y demostraron que los componentes " azulverdosos " de Glaucocystis y de Gloeochaete procedían verosímilmente de algas azules,las actuales Cianobacterias.
Se trataba,pues,de simbiontes colonizadores de células,a los que Geitler dio el nombre de cianelas.
No parecía plantear mayor problema la separación de algas azulverdosas endosimbiontes o citosimbiontes de otras algas respecto de sus huéspedes para su aislamiento y cultivo,tarea en la que,sin embargo,fracasaron los métodos habituales.
La razón de lo cual la hallaría el grupo de Wolfgang Loffelhardt,de la Universidad de Viena.
En 1980 demostraron que las cianelas ceden buena parte de su material genético al núcleo de la célula huésped.
Sólo les queda entre y el 5 y el 10 por ciento del genoma que posee una cianobacteria independiente.
Viene ello a corresponder con la autonomía genética de los cloroplastos (orgánulos fotosintetizadores) de los vegetales superiores.
Desde el punto de vista de la genética molecular podrían considerarse,pues,como cloroplastos si atendemos a sus pigmentos,de no ser por otras propiedades.
Las cianelas,lo mismo que las cianobacterias y las demás eubacterias,poseen una pared celular de peptidoglicanos y,envueltas en una membrana,yacen en vacuolas de la célula huésped.
Conjugan rasgos propios de orgánulos (dependencia genética del núcleo del huésped) con atributos típicos de las cianobacterias (pared de peptidoglicanos).
Se las puede suponer cianobacterias instaladas en el interior de una célula extraña (inclusión vacuolar).
Tal combinación insólita de propiedades desató la controversia sobre la clasificación de los organismos que las presentaban.
No faltaron propuestas para acomodar A LOS organismos portadores de cianelas entre las algas rojas o la verdes,pero no acababan de convencer.
Uno de nosotros (Kies) ha descubierto,con ayuda del microscopio electrónico,ciertas peculiaridades estructurales de las algas con cianelas que se salen de lo habitual,en particular por darse juntas en un mismo organismo ; así: una zona de corpúsculos de membrana aplastados por debajo mismo de la membrana plasmática,la presencia de raíces flagelares pluriestratificadas y división celular por segmentación.
Estas algas utilizan,como sustancia de reserva,gránulos de polisacáridos (almidón) dispersos por el citoplasma.
Se ha hablado también de peculiaridades en el intercambio de hidratos de carbono y varias diferencias bioquímicas más respecto de otros grupos de algas.
Si con ese manojo de notas propias no encontraban fácil inserción ni conformaban a su vez un taxón algal,pensamos que había que crear un nuevo círculo de parentesco definido por las propiedades siguientes: algas unicelulares dotadas de las características mencionadas,que desempeñan funciones propias de cloroplastos y que garantizan la aportación nutritiva a la célula que las hospeda.
Integran la división de las Glaucocystophyta con una sola clase de momento,la de las Glaucocystophyceae.
Por lo que a tipología concierne,este grupo de parentesco se presenta en situación de igualdad con otros grandes del reino vegetal y se corresponde,en rango taxonómico,con el de las algas verdes,líquenes o plantas vasculares.
Las Glaucocystophyta representan un experimento independiente de desarrollar células con elementos fotosintéticos.
Aunque,hemos de confesarlo,no alcanzaran un éxito rotundo,ya que ni siquiera suman la docena de especies.
Las formas portadoras de cianelas son una suerte de restos fósiles cuyo origen filogenético resulta imposible determinar.
Las dos opciones alternativas son viables desde la genética molecular: o bien se remontan a un pasado lejanísimo en la evolución,o bien se trata de un experimento distinto de la génesis de cloroplastos y serían entonces recientes.
Pudiera ocurrir también que esta nueva división,de escueta diversidad de tipos,se incrementara con otras especies de las que sólo existen estudios anticuados y no se han abordado con las modernas técnicas discriminatorias.
Pensemos,a este propósito,en la unicelular Cyanoptyche gleocystis dispersa,que no ha vuelto a encontrarse desde su descripción en 1925... hasta que la volvió a hallar uno de nosotros (Kies) en el mismo lugar,un estanque vienés.
Investigada en el laboratorio,se comprobó que la estructura mostraba una absoluta coincidencia con la nueva división de las Glaucocystophyta.
(Ludwig Kies,de la Universidad de Hamburgo,y Bruno P. Kremer,de la Universidad de Colonia.
) Dilemas universales ¿Un cosmos eterno? Hasta ahora,a los interesados por el destino del universo les cabía contrapesar sólo dos posibilidades más bien desoladoras: o bien el cosmos continuará expandiéndose indefinidamente y su materia se disipará en un vacío frío y oscuro,o bien se contraerá sobre sí mismo y acabará en un cataclismo de gran implosión (" big crunch ").
Para los que estén dispuestos a ampliar sus horizontes Andrei D. Linde propone un estado de cosas menos desalentador: la autorreproducción eterna del universo.
La teoría de Linde se funda en el concepto de " inflación ",según el cual el universo,nada más producirse la gran explosión (" big bang "),cuando aún era extraordinariamente pequeño,caliente y denso,experimentó un prodigioso estirón antes de aquietarse en su baja velocidad de expansión actual.
El cosmos en su conjunto podría haber surgido de un jirón minúsculo de espacio.
" Lo más probable es que este universo que estudiamos haya sido creado por universos anteriores ",añade.
Las primeras versiones de la teoría Fluctuaciones universos en de la inflación,que se basan en la física de partículas y están empapadas de ella,requerían condiciones muy específicas,muy exactamente " sintonizadas ".
Pero Linde ha mostrado que la inflación puede nacer de procesos más genéricos.
Según la mecánica cuántica,el espacio nunca está enteramente vacío ; a escala muy pequeña,su contenido de energía fluctúa violentamente.
Estas fluctuaciones cuánticas caóticas,dice Linde,podrían engendrar energía de suficiente densidad para detonar la inflación.
La inflación se autolimita: atenúa rápidamente la energía consumiéndola.
Pero Linde sostiene además que la inflación se autoperpetúa: las fluctuaciones cuánticas aseguran que,en algún lugar,de alguna mota de energía seguirán brotando nuevos universos,radicalmente diferentes,en general,del nuestro.
Minúsculas alteraciones en sus condiciones iniciales podrían dar como resultado cambios drásticos en la manera que tengan sus leyes físicas de manifestarse una vez la inflación haya cesado.
Con su hijo Dmitri y otros colaboradores,Linde ha realizado una simulación por ordenador.
Las imágenes muestran un paisaje quebrado,que representa un corte bidimensional del espacio.
Los picos indican regiones de altas energías,inflacionarias ; los valles,regiones de energía relativamente baja,como nuestro propio universo local,donde ha cesado la inflación.
Los colores definen áreas cuyas condiciones iniciales,y leyes físicas,son diferentes.
Linde señala que el patrón montañoso creado por las diferencias de energía es de naturaleza fractal: se repite a escalas billones de veces más pequeñas que el protón o billones de veces mayores que el universo conocido.
la huella,se piensa,dejada por el nacimiento violento del universo,casa bien con las predicciones de la teoría de la inflación.
Parece que las estimaciones de la masa total del universo convergen también hacia el valor predicho por la teoría de la inflación,que es suficiente para refrenar la expansión del universo - - es decir,del universo local - -,pero no para detenerla del todo.
En cuanto a todos esos universos nacientes en la más remota lejanía,están a tales distancias de nosotros,que ningún método de observación hoy concebible podrá salvarlas.
Cromosomas La expresión del ARN El misterio de los gatos calicó va más allá de la coloración de su pelaje.
Los retazos amarillos y negros de su librea son la manifestación externa de una peculiaridad genética más sutil.
Los auténticos calicó son hembras,que,lo mismo que las hembras de los demás mamíferos,portan en sus células dos cromosomas X. Pero sucede que,en una fase precoz de su desarrollo,cada célula del embrión elige al azar uno de los cromosomas X para utilizarlo en el futuro,y el otro queda condensado permanentemente,formando una masa inerte: el corpúsculo de Barr.
(Las hembras se equiparan así a los machos,que portan sólo un cromosoma X y otro Y,inactivo en su mayor parte.
) En los gatos calicó,el mosaicismo que resulta se manifiesta en el exterior porque cada cromosoma X contiene un gen de pigmentación distinto.
Comienza a hacerse la luz en el mecanismo que utilizan las células de mamíferos para inactivar un cromosoma entero.
La clave parece residir en un gen presente en el cromosoma X inactivo,que produce una molécula de ARN de función desconocida.
Hay varias hipótesis para explicar la proeza de ese gen.
" Mi impresión personal ",apunta Carolyn J. Brown,una de las descubridoras del gen,es que la molécula de ARN resulta imprescindible para formar algún tipo de maraña o estructura,que separa al cromosoma X,y permite la inactivación ".
Brown,del laboratorio de Huntington F. Willard,en la Universidad Case Western Reserve,se dedicaba a la expresión génica en el corpúsculo de Barr.
Hay unos cuantos genes que escapan a la señal general de " desconexión " y permanecen activos en los dos cromosomas X. En 1990,el grupo de Willard encontró,sin embargo,un gen que mostraba un comportamiento especial: sólo era activo en el corpúsculo de Barr.
Además,el gen se localizaba en una región del cromosoma X que era esencial para La inactivación de dicho cromosoma.
Todos esos datos apuntaban a que el gen,que el grupo de Willard designó Xist (por X inactive-specific transcript),desempeñaba un papel importante en la inactivación del cromosoma X. El grupo avanzó su propuesta en enero de 1991.
Meses después,Sohaila Rastan y Neil Brockdorff publicaron el descubrimiento del gen Xist de ratón.
El pasado octubre,en la revista Cell,los grupos de Willard y Rastan dieron a conocer sus análisis de los genes Xist humano y de ratón.
Esos genes producen moléculas de ARN excepcionalmente grandes y bastante similares.
Sin embargo,a diferencia de la mayoría de los ARN,que abandonan el núcleo celular y se traducen en proteínas,el ARN de Xist no porta información para ninguna proteína.
Además,nunca deja el núcleo.
Se queda junto al cromosoma X que lo sintetiza.
Estos resultados sugieren varios modelos para explicar la inactivación.
Uno: a medida que se va sintetizando el ARN de Xist,se va pegando al cromosoma,quizá junto con otras moléculas ; la maraña de ARN que resulta puede provocar directamente la incapacitación de la mayoría de los genes.
Otra hipótesis: la presencia del ARN podría hacer que el cromosoma interaccionase con otros factores presentes en la membrana nuclear,o en alguna otra parte,que lo inactiven.
Tercera opción: el propio ARN no realiza función alguna,sino que la transcripción de esa región induce cambios conformacionales en el cromosoma que determinan su inactivación.
En los últimos tiempos ha cobrado fuerza la hipótesis de la asociación entre Xist y la inactivación del cromosoma X,gracias a los trabajos de Larry J. Shapiro,Jacob Wahrman,John R. McCarrey y Donald D. Dilworth.
Han encontrado,que la transcripción de Xist refleja con precisión la inactivación de los cromosomas X en varios tejidos.
En enero,Graham F. Kay publicó que en células embrionarias tempranas se puede observar transcripción del gen Xist un día antes,aproximadamente,de la inactivación del cromosoma X. " Eso nos dice que la expresión de Xist no es simplemente una consecuencia de la inactivación del X,y apoya la idea de una relación causal ",comenta Brockdorff.
Brown coincide con la idea de que Xist es una suerte de " cuerpo del delito ",pero advierte que aún debe demostrarse su importancia durante la inactivación.
Habrá que hacer más experimentos para fijar estas hipótesis.
" Queremos ahora desbaratar los genes Xist de una célula madre embrionaria ",explica Brockdorff.
" Si Xist es necesario,esas células no podrán inactivar el cromosoma X ".
Los investigadores pueden también insertar copias activas de Xist en células,para ver los genes vecinos se inactivan.
Quedan por responder muchas otras preguntas.
" Si Xist está implicado en la inactivación del cromosoma X,debe haber algo que lo active o lo inactive ",apunta Brown.
Importa averiguar el mecanismo de interacción entre el ARN de Xist y el cromosoma.
Por ahora,sólo se puede especular sobre cómo se transmite de una célula a sus descendientes la información relativa al cromosoma X que debe inactivarse.
Swift-Tuttle Un asunto policial a historia del cometa Swift-Tuttle se asemeja a una indagación policial.
En 1862,se vio un gran cometa de magnitud 2 que tenía,al pasar por el punto más cercano a la Tierra - - a 52 millones de kilómetros de ésta - - una magnífica cola de 30 grados de longitud.
A partir de estas observaciones se determinaron los elementos de su órbita y,en particular,su período,evaluado en 120 años.
Poco después de 1862,el astrónomo italiano Giovanni Schiaparelli descubrió que la trayectoria del cometa se solapaba con la de las Perseidas.
Se da este nombre a un conjunto de meteoritos que causa la lluvia de estrellas fugaces que se ve todos los años por agosto,cuando la Tierra pasa por la región de la eclíptica que cruza la trayectoria del enjambre.
Con ello se entendió mejor el origen de esos enjambres de meteoritos: son polvo expulsado por los cometas,que al entrar en la atmósfera terrestre da lugar a las estrellas fugaces.
En los años setenta de nuestro siglo renació el interés por el cometa Swift-Tuttle,y se hicieron los preparativos necesarios para su observación.
Los cálculos predecían que regresaría a finales de 1982.
Pero la decepción fue grande.
Hubo que rendirse a la evidencia: el cometa no volvía.
¿Se había desintegrado totalmente? ¿O se había vuelto tan débil que no se le podía observar ya? Un brillo tan variable explicaría de paso que no haya indicios de visitas del cometa anteriores a 1862.
No obstante,en 1973 Brian Marsden había señalado que el cometa de 1862 quizá fuese el mismo que observó en China el misionero jesuita Ignace Kegler - - corría el año 1737 - -,y también el visto en 1610 y en 1479.
Su período abarcaría,por tanto,más de 120 años,y no sería constante a causa de efectos no gravitacionales.
Marsden llegaba a la conclusión de que,si el cometa no volvía en 1983,habría que identificarlo con el cometa de Kegler,y esperar,por tanto,su paso en 1992.
Hay cometas que,aun teniendo en cuenta las perturbaciones que producen los nueve planetas del sistema solar,llegan al perihelio (su distancia más cercana al Sol) bien con retraso,bien antes de la fecha que el cálculo prevé.
Este desajuste se atribuye a la pérdida anisótropa del gas del cometa,cada vez más caliente por el lado expuesto al Sol,pues esa pérdida hace que retroceda en la dirección opuesta.
Estas fuerzas no gravitacionales se determinan indirectamente de manera que den cuenta de la diferencia existente entre la trayectoria de un cometa prevista por la teoría de la mecánica celeste y la efectivamente observada.
A la vez,se miden estas fuerzas directamente mediante el estudio de la cinemática del gas que envuelve el cometa: cuando hay una pérdida anisótropa de gas se espera que aparezcan en las rayas espectrales de éste desplazamientos debidos al efecto Doppler Fizeau,que variarán con el ángulo entre la velocidad del gas y la dirección del observador.
En la región de ondas de radio este desplazamiento se detecta mejor que en la zona del visible,gracias a la buena resolución espectral de los espectrógrafos que operan en esas longitudes de onda.
Y,efectivamente,el cometa Swift-Tuttle ha vuelto: anunciado por una mayor intensidad de la lluvia de estrellas fugaces en agosto - - pues el cometa reabastace las Perseidas - -,el encuentro se produjo el 26 de septiembre.
Alcanzó su perihelio el 12 de diciembre.
Es menos espectacular que en 1862,ya que pasa a una distancia tres veces mayor que aquel año ; a pesar de todo,con su magnitud 5,sigue brillando bastante.
Un equipo francés,formado por Dominique Bockeleé-Morvan,Francois Colas,Pierre Colom,Jacques Crovisier,Didier Despois,Laurent Jorda,Jean Lecacheux y Gabriel Paubert,ha organizado en muy poco tiempo un conjunto de observaciones coordinadas en el que se han utilizando varios telescopios y radiotelescopios.
El tiempo de observación se ha negociado fuera de los medios habituales,demasiado premiosos.
El equipo ha descubierto,sin ambigüedad alguna,desplazamientos en muchas de las líneas espectrales del gas del cometa,y ha obtenido imágenes de la causa del fenómeno: chorros de gas y polvo que se extienden a lo largo de varios miles de kilómetros.
El radical OH,producto de la descomposición de la molécula de agua,fue el primero en ser detectado,el 15 de octubre,en Nançay ; la fuerte intensidad de la línea,cuya longitud de onda es de 18 centímetros,indicaba una elevada tasa de producción de este radical y,por tanto,una fuerte actividad del cometa.
Las líneas de emisión del metanol (CH30H),cuyas longitudes de onda son 1,8 y 2,1 milímetros,detectadas un mes más tarde con el radiotelescopio de 30 metros de diámetro del IRAM en el pico Veleta,mostraban una abundancia de dicho elemento diferente de la medida en el infrarrojo,a unas longitudes de onda de 3,4 y 3,52 micrometros,por un equipo anglo-americano.
Los radioastrónomos dirigían sus telescopios a ciegas hacia la posición que se le presumía al cometa ; en el infrarrojo,en cambio,era posible detectar la imagen,y percibir que la posición calculada era errónea en 12 ": no tanto como para que se le escapase a la radioantena,pero sí lo bastante para que se subestimase considerablemente la tasa de producción de metanol.
Se supuso,y verificó,que esos 12 " de error se debían a que en el cálculo se pasaron por alto las fuerzas no gravitacionales.
A principios de enero de 1993,se apuntaron las antenas teniendo en cuenta esos 12 ",y se observó una tasa de producción de metanol que casaba con la obtenida en las observaciones infrarrojas.
Las observaciones de radio han permitido,además,el estudio de las moléculas de ácido cianhídrico (HCN),de sulfuro de hidrógeno (H2 S) y de formaldehído (H2CO),estudios que se complementan y permiten comprender mejor los mecanismos de emisión de las moléculas.
Todas esas líneas son asimétricas,con un pico muy pronunciado en las longitudes de onda cortas,de lo que se desprende que la velocidad de eyección está dirigida hacia el Sol,y que tiene una magnitud - - de 600 a 1000 metros por segundo - - no vista hasta ahora: la emisión del gas se produce esencialmente por medio de los chorros,que,mediante el efecto de reacción,generan las fuerzas no gravitacionales que sufre el núcleo del cometa y perturban su órbita.
El equipo francés había ya encontrado,a partir del espectro del cometa Halley,una velocidad de eyección gaseosa de 150 metros por segundo.
Desaparece ahora toda ambigüedad: el cometa experimenta un retroceso considerable debido a la importancia de los chorros de gas.
Los chorros de polvo,a los de gas asociados,se dejan ver claramente en las imágenes obtenidas en el Pic du Midi.
Tanto el gas como el polvo indican que hay varios chorros,cuyas posiciones cambian día a día.
Sólo teniendo en cuenta estas fuerzas no gravitacionales cabe localizar las posiciones del cometa observadas en 1737,en 1862 y en 1992.
El análisis de los perfiles de las líneas correspondientes a las longitudes de onda de radio permitirá cuantificar las fuerzas,y comparando éstas con las perturbaciones de la órbita,se podrá determinar tal vez la masa del cometa.
El aficionado a la astronomía disfrutará en agosto de este año de unas Perseidas que,enriquecidas por los residuos del Swift-Tuttle,serán tan magníficas como las de agosto de 1992.
Ronroneo de los gatos ¿Cómo y por qué? El plácido ronroneo del gato produce,en muchas personas,una sensación agradable.
Es como el de una erre sostenida en todos los felinos domésticos,si bien timbre y volumen d suelen variar de un animal a otro.
El ronroneo,que dura incluso varios minutos,ocurre lo mismo en el movimiento de inspiración que en el de espiración.
Pero en la inversión del flujo de la corriente de aire se interrumpe,de manera casi imperceptible,el sonido.
El oído humano puede distinguir el ronroneo de inspiración del producido durante la espiración.
El sonido se acompaña de las vibraciones de la superficie cutánea del cuello,la laringe y la caja torácica,vibraciones que podemos percibir aunque el ronroneo sea muy tenue.
Se han venido defendiendo distintas hipótesis sobre los órganos y procesos fisiológicos implicados en la formación del sonido.
En un principio se atribuyó a las cuerdas vocales,el diafragma y el paladar blando.
Para dilucidar la cuestión,acometimos ensayos de frecuencias.
Tomamos diez gatos domésticos de diversa edad.
Anotamos la intensidad de su ronroneo y la vibración de la superficie de la piel en distintas regiones del cuerpo y a distancias variables,en uno y dos canales.
Optamos por varios métodos de medición.
Registramos grabaciones de un solo canal de otros gatos domésticos y salvajes.
Se observó que el tono básico del ronroneo es,en promedio,de unos 26 hertz y que el ronroneo de espiración es de unos 2,4 hertz más alto que el de inspiración.
(Por mor de comparación,el hombre genera sus tonos más graves en el rango de los 43 hertz.
) También la laringe vibra con esa frecuencia umbral,sin interrupción durante el cambio de sentido de las fases respiratorias.
Importa destacar que no se pudo deducir ninguna correlación entre la frecuencia del tono base y el tamaño o el peso del animal.
En el ejemplar que controlamos desde su tercer mes de vida hasta los tres años cumplidos no se hallaron diferencias ni variaciones significativas en las oscilaciones de su tono base.
En ese punto,pues,también el ronroneo de los gatos diverge de los sonidos emitidos por otros mamíferos.
Además,mientras ronronean,estos felinos pueden producir otro tipo de ruidos,sobre todo durante la fase de espiración.
En tales casos se emite un sonido compuesto,en el que tienen distinto peso relativo cada uno de los integrantes.
Aunque sea imperceptible la fracción correspondiente al ronroneo,se pueden medir modulaciones de amplitud de unos 26 hertz.
Estos hallazgos respaldan la idea de que el ronroneo,a diferencia de otras formas de sonido producidas por mamíferos,no tiene como único origen el movimiento de las cuerdas vocales generado en la alternancia de tensión y relajación y la oposición al flujo de aire.
Probablemente dependan él y las vibraciones que lo acompañan de una modulación de la corriente respiratoria por la musculatura laríngica,con carácter periódico y coordinada por el sistema nervioso central.
Resulta también verosímil que la formación del sonido se siga de la brusca apertura de las cuerdas vocales.
Además del tono básico,el ronroneo presenta tonos agudos y débiles que modulan el sonido emitido.
Su intensidad disminuye a medida que crece la frecuencia.
El ronroneo de inspiración no sólo tiene el tono base más grave,sino que es más rico e intenso en tonos agudos que el de espiración,lo que determina que lo percibamos distinto.
La investigación del ronroneo plantea inmediatamente la cuestión sobre su significado.
La creencia popular lo asocia a un estado de tranquilidad y bienestar.
Sin duda esta interpretación engloba múltiples situaciones en las que se produce ronroneo,pero no puede explicarlas todas.
Los cachorros adquieren ese hábito muy pronto,y no es raro ver ronronear,al unísono,madre y camada durante el amamantamiento ; dándose contacto físico,cabe inferir que la vibración corporal constituirá también una señal.
Puede admitirse que,en tales casos,el ronroneo sea expresión de bienestar,aunque en el sentido de que todo marcha bien: los cachorros se sienten hartos,protegidos de cualquier agresión exterior y calientes.
Lo que también explicaría la intensidad mínima del sonido emitido (imperceptible a más de tres metros de distancia),evitando de ese modo ser descubiertos por los depredadores.
Mas existen otras situaciones bien estudiadas,como el caso de un gato herido o de la hembra durante el parto y los agudos dolores que les acompañan.
Ambos ronronean,pero difícilmente podemos interpretarlo como señal de bienestar.
Cabe suponer que tenga como finalidad la de calmar al propio animal o de apaciguamiento ante la amenaza potencial del hombre que puede acercársele.
No se ha podido determinar si todos los felinos ronronean.
Ni leones,ni tigres,jaguares o leopardos,por lo que parece,están capacitados.
Aunque a veces se sostiene lo contrario.
La cuestión se dirimirá cuando se aporten las grabaciones que descubran que el sonido registrado ofrece las mismas características observadas en el ronroneo del gato.
El ronroneo quizá no se circunscriba a la familia de los felinos.
En jinetas,ciertos monos y algunos roedores se ha percibido un ruido similar,pero habrá que establecer si es el mismo que caracteriza a los gatos domésticos.
Teoría nuclear Distancias cortas Durante años y años los físicos han ido conociendo mejor la manera en que las fuerzas fundamentales de la naturaleza actúan sobre las partículas elementales más recónditas.
De pronto,han caído en la cuenta de que no saben qué mantiene unido el núcleo de un átomo.
" Por mucho tiempo hemos defendido una idea muy sencilla,que ahora nos parece más bien simple ",comenta George F. Bertsch,de la Universidad de Washington.
Se suponía que los protones y neutrones que constituyen el núcleo de un átomo se atraen entre sí intercambiando una partícula,el mesón pi,o pion.
Los aceleradores han mostrado que al pion sólo se debe la transmisión de la fuerza nuclear a grandes distancias.
Pero,¿qué pasa a distancias cortas? Desde luego,cuando en este contexto hablamos de distancias grandes hablamos de hiatos que,desde el punto de vista corriente,son menos que nada.
Pero el diámetro de un protón es sólo de un fermi (una milésima de billonésima de metro),y por eso los físicos nucleares consideran que una distancia de unos cuantos fermis es grande.
La idea de que una partícula transporta la fuerza nuclear se remonta a los años treinta,y se debe al premio Nobel Hideki Yukawa.
Su teoría fue confirmada en 1947,cuando el grupo de Cecil Frank Powell descubrió el pion.
Yukawa había predicho que esa partícula era la intermediaria de todas las interacciones nucleares.
Pero las cosas se complicaron en los años setenta.
Se demostró por entonces que los protones,neutrones y piones estaban a su vez compuestos por partículas elementales llamadas quarks,de tipo " arriba " (" up ") y de tipo " abajo " (" down "),y gluones.
Un protón está formado por dos quarks " arriba " y un quark " abajo " ; un neutrón es un quark " arriba " y dos " abajo ".
Un pion puede estar formado por un quark " arriba " y la contrapartida de antimateria de un quark " abajo ",pero también por otros pares de quarks.
En los piones,neutrones y protones los quarks se mantienen unidos gracias a los gluones,que transportan la interacción fuerte de la misma manera que los fotones hacen lo propio con la fuerza electromagnética.
En última instancia,gluones y quarks deben ser los transmisores de la fuerza nuclear,pero de lo que se trata es de saber cuál es la combinación de gluones y quarks que realiza esa función.
A comienzos de los años ochenta se había deducido ya que había varios pares de quarks capaces de transportar fuerzas nucleares,pero se seguía creyendo que los piones desempeñaban el papel más importante.
En 1986 se intentó observar el intercambio de piones bombardeando núcleos atómicos con protones.
Por lo que se vio,no parecía que los piones interviniesen en las interacciones nucleares de corto alcance.
Tras una serie de experimentos que concluyó el verano pasado,no quedaba ya otra salida que aceptar que los piones transportan la fuerza nuclear sólo a distancias de 0,5 fermi o más.
" Aunque una fracción de un fermi no parezca demasiado,esa escala de distancias es crucial en todos los procesos nucleares ".
Por desgracia,los nuevos descubrimientos no dan muchos indicios de cómo interaccionan protones y neutrones a distancias cortas.
Es bastante probable que en distancias cortas la fuerza nuclear sea transportada por una partícula más pesada que el pion.
Más desconcertante es la posibilidad de que los propios gluones tengan directamente que ver con su transmisión a esas distancias.
Sólo se sabe que hay gluones dentro de los protones y los neutrones ; si los gluones saltasen entre los protones y los neutrones de los núcleos atómicos,habría que reescribir la teoría nuclear.
Cuando los árabes conquistaron España,parece que aún estaban en funcionamiento los acueductos y sifones que cruzaban por el fondo del mar para abastecer de agua a Cádiz,que entonces era una isla.
Las leyendas con que algunos árabes quieren justificar su construcción recuerdan las orientales del ingeniero persa Farhad quien,loco de amor por Sirin,esposa del rey sasánida Cosroes II Parviz (590628),construyó un canal para llevar rápidamente la leche fresca desde las praderas de Armenia hasta el castillo en que habitaba su amada ; o las que figuran en algún cuento de Las mil y una noches.
En Al-Andalus es una princesa la que quiere que su ciudad esté a cubierto de cualquier contingencia y,en consecuencia,decide evitar que se le pueda cortar el suministro de agua.
A mediados del siglo IX,LOS textos nos hablan de un inventor,Abbas b. Firnas,que se lanzó al espacio con un precedente del ala delta.
Consiguió volar un trecho,pero acabó lastimándose.
Al mismo personaje se atribuye el supuesto invento de una máquina que indicaba la hora del día y de la noche.
En el siglo x sabemos que,en la mezquita de Córdoba,el predicador hablaba a sus fieles desde un púlpito que se colocaba en el lugar conveniente y,después del sermón,se retiraba para guardarlo,adosado junto a la pared,hasta que volviera a necesitarse.
El desplazamiento era fácil,pues estaba montado sobre ruedas.
Lo curioso de estas noticias es que se nos menciona a sus constructores - - que no tienen que ser necesariamente los inventores - - con el nombre genérico de alarifes,palabra que conserva el léxico español con el mismo significado que tenía en árabe hace mil años.
Cuando Abd al-Rahman III decide enviar ayuda técnica a un aliado suyo del norte de Africa (935),manda al jefe de sus ingenieros,Muhammad b. Walid b. Fustayq,al frente de los más hábiles alarifes,expertos en toda clase de oficios.
Gracias a esta ayuda técnica su aliado pudo poner en estado de defensa un viejo castillo.
En la época taifa (siglo Xl),Azarquiel,alarife de Toledo en su juventud y luego el más importante astrónomo de al-Andalus,sabemos que construyó junto al Tajo dos clepsidras que indicaban el día del mes lunar y señalaban la hora del día.
El ciclo se repetía ininterrumpidamente sin errores excesivos y el aparato en cuestión siguió funcionando,cerca de cuarenta años,después de la conquista de la ciudad por los cristianos de Alfonso VI.
A esta época,aproximadamente,cabe referir la existencia de laúdes que tocaban solos,de cintas transportadoras que servían para trasladar las bebidas y los guisos preparados en la cocina hasta el comedor y,desde aquí,llevarlos,una vez utilizados,por el mismo sistema,al fregadero.
El mismo artificio se conocía en Damasco.
Igualmente tenemos noticias de máquinas que permitían elevar o bajar las tuberías que llevaban el agua para hacer funcionar los molinos y,en consecuencia,graduar la velocidad de la muela.
En Oriente,conservamos varios tratados,escalonados a lo largo de cuatro siglos (IX-XIII),que permiten reconstruir los mecanismos que se utilizaban.
No ocurre lo mismo en Occidente.
Por tanto,cuando hace menos de veinte años se descubrió,en la Biblioteca Medicea Laurenziana de Florencia la existencia de un manuscrito misceláneo andalusí de este género,cuyo primer tratado hablaba de máquinas,los investigadores se lanzaron a analizarlo a pesar de que,como subraya Donald R. Hill,el texto,al menos sus primeras páginas,está muy mutilado (sólo es legible el 60 por ciento),pues los folios fueron guillotinados al bies,casi por la mitad,y la humedad ha estropeado otras partes.
Sin embargo,conforme se avanza,la parte legible aumenta y se entienden mejor los dibujos que representan los mecanismos responsables de la acción que figura en el enunciado de actuación de cada máquina.
Pero,afortunadamente,conservamos sin mutilaciones el " plano " de algunas y el correspondiente enunciado en que explica para qué han sido ideadas.
Gracias a ello se ha podido reconstruir,y hacer funcionar la primera,y lo mismo se hará,en plazo breve,con la segunda.
El quinto opúsculo contiene un tratado sobre ruedas hidráulicas,molinos y presas ; no presenta dibujos,aunque están los huecos que debían contenerlos.
En realidad,trata de enseñar a construir los móviles perpetuos conocidos en la época.
Este quinto es anónimo,pero permite relacionar uno de sus ingenios con otro que figura en el Carnet de Villard de Honnecourt.
Las descripciones que se dan en estos tratados primero y quinto manifiestan el estado de la mecánica en al-Andalus y,a su vez,nos permiten entender un texto en que se describe cómo El Corán de la Gran Mezquita de Marrakus salía de su armario y se abría ante los lectores de modo automático.
El manuscrito nos informa de que fue copiado de otro anterior - - por Ishaq b. al-Sid,quien trabajó al servicio de Alfonso X el Sabio.
El original era de un tal Ahmad o Muhammad b. Jalaf al-Muradi," apellido " éste bastante frecuente en al-Andalus del siglo Xl,quien escribió su obra Libro de los secretos de los pensamientos,destinada a enseñar a construir juguetes mecánicos,muchos de los cuales se pueden utilizar como relojes de agua (clepsidras).
Veamos cómo se ha procedido en la reconstrucción de la máquina,que denominaremos uno.
El texto que se encuentra en la parte no mutilada de la página indica la representación teatral que debe desarrollarle: consiste en que dos muchachas encerradas en sus respectivos pabellones salgan (a un jardín).
Cuatro gacelas empiezan a beber y un negro surge del fondo de un pozo para espiarlas.
Al cabo de un momento aparecen tres serpientes que asustan a todos los actores que corren a esconderse en los lugares en que se encontraban al iniciarse la acción.
La representación se desarrolla encima de la cara de una caja que tiene la forma de un paralelepípedo rectangular,aunque la parte superior pueda sufrir ligeras modificaciones,como es su transformación en un octógono.
Debajo,escondidas,se encuentran tres balanzas de agua,las cuerdas,los tubos,válvulas,poleas,etcétera,que desencadenarán,sucesivamente,los movimientos de cada pieza previstos por el constructor.
Hay que subrayar que el aparato permite que la representación se realice con los intervalos de tiempo que se deseen.
Si se quiere que funcione como reloj,bastará con hacer que la acción se repita de hora en hora.
Para conseguir su funcionamiento correcto,es necesario que el depósito de agua que alimenta los distintos platillos de las balanzas mantenga siempre el mismo nivel,lo que conseguían con procedimientos ya conocidos en la Antigüedad,como el de instalar dos depósitos a distinto nivel.
El más alto vertía el agua en el de debajo en mayor cantidad de la que éste podía contener,razón por la cual el líquido que alimentaba el depósito inferior conseguía luego fluir con velocidad constante en la clepsidra.
En cambio,constituye una innovación el llenar con cierta cantidad de mercurio uno de los brazos de la balanza principal,para desencadenar la secuencia de los movimientos programados,a diferencia de lo que ocurría con los medios empleados hasta entonces,por ejemplo,bolas de metal.
La segunda máquina,o dos,presenta a un par de caballeros,lanza en ristre,que corren,uno después de otro,en busca de sendos soldados que se encuentran en el extremo opuesto de una de las superficies de la caja.
Esta se divide en dos mitades,pistas,y en la línea de separación se hallan uno o dos címbalos.
Por el centro de cada una de las pistas corren los caballeros.
Hacia la mitad de la carrera se encuentra una muchacha que levantará los brazos en señal de saludo y,a la izquierda (o derecha,según el caso),el címbalo que golpeará al caballero al cruzarlo (" dando " así,si se quiere,la hora).
Pero,conforme se aproxima al infante,éste se va escondiendo por lo cual,al llegar al sitio donde se encontraba el enemigo,el caballero da media vuelta y regresa.
La muchacha,que había bajado los brazos durante el ataque,vuelve a levantarlos ahora.
En la segunda pista se realiza una acción equivalente al de la primera,de tal modo que su caballero inicia la marcha en el momento en que el primero gira al no alcanzar al infante.
El mecanismo de esta máquina puede funcionar gracias a una rueda de molino de eje vertical,rodete,si existe una fuerte presión del agua,o bien,horizontal,tipo noria,si el flujo es lento.
Sobre este eje motor - - el horizontal - - se encuentra una rueda que tiene 64 dientes al lado de diámetro y ninguno en el otro,lo cual le permite engranar,alternativamente,con dos piñones opuestos,creándose así movimientos alternativos de vaivén.
Igualmente,las cuerdas que han de arrastrar a los caballos se arrollan en una devanadera en forma de cruz y no en un tambor o carrete,con lo cual los animales dan la impresión de moverse a saltos.
Otro de los elementos importantes de esta máquina es el de una varilla articulada cerca de la periferia de una rueda que permite transformar el movimiento circular en lineal,constituyendo así un precedente del mecanismo de biela-manivela.
Gracias a ello,la muchacha levanta y baja los brazos alternativamente durante la representación.
T a parte del manuscrito en que se L,describen las máquinas tercera,cuarta y quinta se encuentra en mal estado de conservación,al menos en algunos lugares estratégicos,y,por el momento,no los hemos podido reconstruir.
De todos modos,se puede deducir que sus mecanismos son similares a los empleados en las dos primeras máquinas y,a partir de trozos legibles pero aislados,se ve que introducen el uso de cintas correderas.
Pasemos al quinto opúsculo que trata,en buena parte,de móviles perpetuos.
De la breve descripción que dimos,hace unos quince anos,no podía deducirse la palabras,en letra pequeñísima,añadidas al mismo.
Más tarde,en 1982 analicé con detalle en un simposio celebrado en Cuelgamuros el opúsculo en cuestión y subrayé el parentesco del mismo con el de otro,traducido al alemán por Schmeller en 1922.
Destaqué entonces la importancia de la expresión contenida en las letras menudas: ruedas hidráulicas que se mueven por sí mismas.
El texto no se publicaría hasta 1988.
Una de las ruedas descritas coincide con la dibujada en el Carnet de Villard de Honnecourt,técnico francés de la segunda mitad del siglo Xll.
Dice éste: " Muchas veces los sabios han discutido cómo se puede hacer girar una rueda por sí sola.
Aquí tenéis cómo se puede hacer con martillos en número impar o con mercurio ".
Este y otros detalles,en los que aquí no voy a entrar,permiten asegurar que el autor francés sabía bien lo que ocurría en el mundo árabe.
Añadamos sólo que los autores de ambos opúsculos describen varios móviles perpetuos,y demuestran conocer algunas máquinas hidráulicas,los distintos tipos de engranajes (de cualquier número de dientes),las ruedas catalinas,muy probablemente el tornillo sin fin.
Pero esta tecnología andalusí no sólo se extendió hacia el norte,hacia Europa,sino también hacia el sur.
Es sabido que la gran mezquita de Córdoba pretendía conservar en su seno El Corán que estaba leyendo el califa CUtman cuando fue asesinado (en el año 654).
El ejemplar en cuestión era venerado por los musulmanes,pues estaba manchado con la sangre del califa mártir,y se conservaba con el máximo cuidado.
En 1158 se trasladó a Marrakus y aquí se inventaron una serie de mecanismos que permitían manejarlo y leerlo sin que le tocasen,ni por azar,unas manos impuras,tal como exige la tradición musulmana.
Sabemos que estaba colocado sobre un atril en aspa,mueble auxiliar que todavía encontramos en el mundo musulmán,e instalado encima de una plataforma móvil.
Todo ello - - El Corán,atril y plataforma - - se mantenía encerrado en un arca guardada en la parte alta de la mezquita.
Al dar vuelta a la llave del arca,se abrían (I),inmediata y automáticamente,hacia el interior sus dos puertas y la plataforma salía sola (2) y llevaba El Corán al lugar más alejado posible previsto.
Simultáneamente (3),el atril en X se desplegaba,mientras las puertas del arca se cerraban (4).
Al introducir de nuevo la llave en la cerradura del arca y girarla en sentido contrario,se reproducían,en sentido inverso,los cuatro movimientos citados,gracias a las " correas y mecanismos escondidos en el interior de la plataforma ".
Todo ello,evidentemente,podía conseguirse con los artificios descritos por al-Muradi.
Muchos de estos automatismos fueron realizadas por alarifes andaluces,llamados a Marruecos expresamente para ello.
Así,el malagueño al-Hayy YaCis,quien no sólo construyó Gibraltar,sino que también automatizó la macsura (lugar destinado al califa) y el almimbar (o púlpito) de la segunda Kutubiyya: montó a su lado una sala de motores (harakat) para que las paredes de gran tamaño - - ya no juguetes,pues - - se movieran simultáneamente.
Al-Maqqari,historiador argelino especializado en la historia de la España musulmana,visitó el lugar en 1601 y aún alcanzó a ver sus restos.
También cabe pensar en el procedimiento seguido por estos alarifes para concebir y ejecutar sus obras.
Si hacemos caso de un excursus que se encuentra en el comentario de El Corán de Fajr al-Din al-Razi,del siglo Xlll,debían empezar,en los ejemplos complicados,trazando planos y construyendo maquetas previas a la obra que querían realizar.
Existe,asimismo,una curiosa anécdota en que parece reflejarse el temor de los arquitectos a adelantar presupuestos.
El presupuesto real se conocía sólo una vez terminada.
Lo dicho muestra que,en Occidente,la mecánica musulmana tuvo en algunos casos una utilidad práctica.
Por otro lado,la terminología empleada aquí difiere de la de Oriente en algunos términos significativos.
La palabra corriente con que se designa esta ciencia allí es hiyal,y aquí handasa.
Y,además,ciertos conceptos cambian: si en las clepsidras orientales la hora la da un ave que suelta de sus garras una bola de metal que cae sobre címbalo,aquí es una muchacha que la suelta de la boca.
Biofísica molecular Interacciones inespecíficas ADN-ligando De entre las diferentes moléculas que interaccionan con el ADN,existe un numeroso grupo que lo hace sin presentar una afinidad especial por ninguna secuencia concreta del genoma.
En virtud de esta propiedad,dichos ligandos se denominan inespecíficos.
Ejemplos de los mismos los constituyen moléculas de principalísima importancia biológica y farmacológica,como son las histonas,las protaminas y las proteínas desestabilizantes de la doble hélice,así como un gran número de drogas antitumorales y antibióticos.
Para obtener información macroscópica de la interacción entre un ligando inespecífico y el ADN,es preciso conocer la distribución de equilibrio de las especies moleculares implicadas.
La descripción de este equilibrio ha dado lugar al desarrollo de una teoría mecanoestadística donde se toma en consideración la unión de ligandos a redes monodimensionales.
Dicho desarrollo ha tenido que abordar diferentes complicaciones,la principal de las cuales es la existencia de exclusión en la interacción: al ser esta inespecífica,los distintos sitios de unión se solapan,pudiendo entonces aparecer regiones de ADN libre con un tamaño lo suficientemente pequeño como para que no puedan unirse otras moléculas de ligando a las mismas (figura a).
Otras complicaciones son las interacciones entre ligandos unidos al ADN,los cambios conformacionales en la red,la interacción simultánea entre varios ligandos,etcétera.
Por supuesto,este tipo de modelos pueden utilizarse en muy diversos sistemas biológicos en los que se dan interacciones de ligandos con biomacromoléculas lineales (ARN,actina microtúbulos,colágeno y otros).
Una técnica usual para obtener experimentalmente la distribución de equilibrio son las isotermas de unión,entendiendo por tal la determinación de las concentraciones de ligando libre y unido a diferentes relaciones ADN / ligando,a una temperatura constante.
En el caso que nos ocupa,la clásica ecuación de Scatchard no suele ser válida para el ajuste a los datos experimentales,por lo que es necesario el planteamiento de nuevas expresiones para las correspondientes isotermas.
Utilizando métodos combinatoriales,el grupo de A. S. Zasedatelev,del Instituto Engelhardt de Biología Molecular de Moscú,derivó en 1971 ecuaciones para las isotermas de unión en el caso de exclusión y con interacciones entre ligandos unidos en posiciones adyacentes.
Ecuaciones similares fueron derivadas en 1974 por James D. McGhee y Peter H. von Hippel,de la Universidad de Oregon,recurriendo a probabilidades condicionales.
En ambos casos,la interacción viene descrita por tres parámetros (figura b): n,o número de monómeros (pares de bases en el ADN doble helicoidal) ocupados por el ligando unido ; K,o constante de asociación del ligando,y,o constante de cooperatividad (constante adimensional que representa las interacciones entre ligandos unidos en posiciones adyacentes).
La derivación de expresiones analíticas correspondientes a la isoterma teórica se complica enormemente cuando se consideran sistemas menos sencillos.
En estos casos es conveniente la utilización de otros métodos matemáticos especialmente concebidos para la descripción de " redes ",que siempre permiten calcular la isoterma teórica numéricamente,y en muchos casos permiten también su construcción analítica.
Nos referimos al método matricial (que describe lo mismo redes finitas que infinitas,así como redes homogéneas o heterogéneas) y al método de funciones generatrices de secuencia (FGS,sólo válido para redes infinitas homogéneas).
Este último fue propuesto en 1964 por Shneior Lifson,del Instituto Weizmann de Ciencias de Israel.
La sencillez matemática y potencia del método de FGS lo hace particularmente idóneo para abordar la interacción de ligandos inespecíficos con el ADN en casos más complejos a los arriba comentados.
El método de FGS se fundamenta en la utilización de un colectivo (conjunto de sistemas termodinámicamente equivalentes) en el que se permite variar el número de monómeros de la red,de forma que en el límite de redes infinitas la función de partición del sistema (suma de pesos estadísticos de los diferentes microestados del mismo) puede aproximarse a la potencia N-ésima de xl,siendo N el número de monómeros de la red,y xl la raíz máxima de una ecuación específica del sistema (la " ecuación secular ").
A partir de esta función de partición se evalúan diferentes magnitudes macroscópicas,tales como la fracción de ligando unido y la fracción helicoidal,así como sus fluctuaciones.
Otra de las técnicas experimentales típicas en el estudio de interacciones inespecíficas ADN-ligando son las curvas de transición térmica.
Dicha técnica se basa en el hecho de que,a medida que el ADN aumenta la temperatura,su estructura en doble hélice se deshace en dos cadenas sencillas.
Este tránsito se mide mediante espectrofotometría en el ultravioleta.
La temperatura de fusión de un ADN se define como aquella a la cual la fracción doble helicoidal de ADN es de 0,5.
Puesto que,al variar la temperatura,varía la relación ADN doble helicoidal / ADN de cadena sencilla,en presencia de un ligando (tanto si interacciona con una u otra especie) una curva de transición térmica en el fondo es una titulación del mismo con ADN.
Además,su aspecto (por tanto su temperatura de fusión) cambiará con respecto al del ADN en solitario.
McGhee utilizó el método de FGS para la descripción de curvas de transición térmica de complejos inespecíficos ADN-ligando.
Para dar cuenta de la transición de ADN doble helicoidal a ADN unicatenario,introdujo dos parámetros: una constante de nucleación,para la formación de un par de bases aislado,y una constante de elongación (con dependencia de tipo van't Hoff con la temperatura),para la formación de pares de bases contiguos a otros ya existentes (figura c).
El equilibrio de asociación ADN-ligando se describe de forma similar que en el modelo de McGhee-von Hippel.
Nuestro grupo de biofísica de la Universidad Complutense de Madrid ha obtenido recientemente expresiones analíticas (derivadas a partir de la teoría de McGhee) que relacionan el valor de la constante de elongación en la temperatura de fusión con los parámetros K,n y que definen la interacción.
De este modo,la estimación de dichos parámetros puede hacerse a partir de la información de la temperatura de fusión de tres o más curvas de transición térmica obtenidas en diferentes relaciones ADN / ligando.
Como se ha dicho anteriormente,el método de FGS permite describir sistemas de gran complejidad.
Por ejemplo,se ha utilizado en las dos últimas décadas para estudiar la transición bidireccional de la forma Z a la B del ADN,inducida por la interacción de drogas intercalantes.
(La configuración Z del ADN es la de una doble hélice levógira ; la B,la de doble hélice dextrógira de Watson-Crick.
) También se ha abordado la asociación de moléculas que interaccionan con el ADN de forma modulable alostéricamente por efectores presentes en el sistema,así como ligandos capaces de interaccionar de varias formas con el ADN.
El método de FGS ha sido mejorado por Yi-Der Chen,del Instituto Nacional de la Salud.
Acaba de demostrar que tanto la ecuación secular como las distintas magnitudes medias pueden obtenerse por manipulación de un determinante característico del sistema,lo que reviste particular interés a la hora de computar numéricamente isotermas y curvas de transición térmica.
Hay,por último,ciertos ligandos inespecíficos cuya interacción con el ADN es superficial,electrostática y deslocalizada (por ejemplo,poliaminas y oligolisinas).
En dichos casos no es posible modelar el ADN como una red,sino considerándolo un continuo unidimensional.
En vez de ocupar posiciones fijas,los ligandos pueden desplazarse libremente en una dimensión (sin interpenetrarse).
La correspondiente teoría mecanoestadística para estos ligandos fue desarrollada hace algunos años por Charles P. Woodbury Jr. Ultimamente,Jolly Ray y Gerald S. Manning la han generalizado para introducir los efectos de polielectrolito (dependencia con la fuerza iónica de las constantes cinéticas y de equilibrio en este tipo de macromoléculas altamente cargadas) en la interacción.
Podemos concluir que los trabajos iniciales de Lifson y otros autores en los años sesenta ha dado lugar a una muy fructífera rama (tanto en lo teórico como en lo experimental) de la biofísica,y de toda la biología molecular,que se plantea la descripción de sistemas inespecíficos ADN-ligando cada vez más complejos,sistemas fuera de equilibrio y metodologías para el estudio de los mismos.
(G. Colmenarejo y F. Montero,del departamento de bioquímica y biología molecular I,de la Universidad Complutense de Madrid.
) Química plana Láminas de polímeros con propiedades insólitas la química de polímeros ha entrado en una nueva dimensión.
La mayoría de los polímeros no son nada más que unidades moleculares idénticas,o monómeros,engarzados en cadenas unidimensionales.
Pero ahora se ha logrado la constitución de láminas bidimensionales.
Algunas de sus propiedades son insólitas.
" Cabe la posibilidad de transformar todos los monómeros conocidos en objetos bidimensionales ",afirma Samuel 1. Stupp,jefe del equipo en la Universidad de Illinois que ha conseguido la síntesis de láminas de polímeros.
" Si esta posibilidad se convirtiese en realidad,tendríamos un nuevo conjunto de materiales,con nuevas propiedades ".
Stupp ya ha demostrado que las láminas de polímeros destacan por su flexibilidad,resistencia y duración.
Podrían servir como lubricantes,semiconductores,materiales ópticos o membranas selectivas.
Los polímeros en láminas de Stupp están entre las mayores moléculas jamás sintetizadas ; se les ha dado el nombre de " gigamoléculas ".
La masa de un polímero se mide normalmente en daltons.
Un átomo de carbono tiene una masa de 12 daltons.
La amilopectina,uno de los mayores polímeros que se conocen y principal componente de los almidones,pesa 90 millones de daltons.
Stupp estima que sus moléculas pesan mucho más de 10 millones de daltons,y afirma que " las de mayor tamaño que vemos por microscopía electrónica están más allá de la resolución de peso molecular de nuestros instrumentos ".
Para fabricar las láminas de polímeros,Stupp,según él mismo ha explicado en la revista Science,prepara en primer lugar una molécula precursora mediante 21 reacciones químicas diferentes.
El resultado es una molécula en forma de varilla con dos sitios reactivos: uno en el centro de la molécula y el otro en un extremo.
Quizá se entienda mejor cómo se ensamblan estos precursores si los comparamos con lápices afilados.
El borrador del lápiz corresponde al extremo reactivo ; la marca que va estampada en el lápiz,al sitio reactivo central.
La " marca " determina que los lápices se alineen costado con costado,en la misma dirección.
Por tanto,los lápices forman una capa con los borradores en un lado y las puntas en el otro.
Una segunda capa se crea simultáneamente encima de la primera,de suerte tal que los borradores de una capa se tocan con los de la otra.
Uno de los hallazgos más importantes de Stupp fue el del método de imbricación de esas capas.
Cuando se les aplica calor,se forman enlaces entre los borradores y las marcas,y así se establecen uniones dentro de las dos capas y entre ellas.
De ese modo Stupp construye láminas cuya área es normalmente de una micra cuadrada,y su espesor uniforme de 0,005 micras.
" La gracia de nuestro método estriba en que tenemos cierto control sobre el tamaño ",puntualiza Stupp.
" Podemos fabricar láminas muy pequeñas o muy grandes ".
En los últimos años se habían construido ya,en Harvard y en otros laboratorios,estructuras moleculares bidimensionales ligadas a láminas de oro o que descansaban sobre la superficie de líquidos.
" El mayor problema de esos otros intentos fue la poca estabilidad de la estructura ",comenta Thomas.
Hasta ahora,Stupp es el único que ha tenido éxito en la fabricación de láminas de polímeros robustas y con libertad de movimiento.
Stupp es consciente de que ni él ni ningún otro químico tienen derecho a alardear demasiado de la creación de los polímeros bidimensionales,pues en la naturaleza los hay desde hace mucho.
La membrana de los eritrocitos,por ejemplo,contiene un gel proteínico que es una especie de polímero bidimensional.
Se cree que el gel hace las veces de esqueleto flexible de las células,y desempeña un papel en que pueden cambiar de forma.
Cuando las láminas son expuestas al calor o se colocan en un medio ambiente ácido,tienden a enrollarse como una hoja de tabaco alrededor de un puro.
Se podrían envolver distintas sustancias dentro del polímero,lo que quizá sirviese para la administración de fármacos.
O podrían construirse membranas selectivas que sólo permitieran el paso de ciertas moléculas.
" Si supiese qué otras aplicaciones son posibles," dice Thomas," ahora mismo me pondría a escribir un artículo sobre ellas.
" Ordenadores agonizantes,Redes neuronales en el umbral de la muerte ¿qué hace un ordenador cuando agoniza? HAL 9000,la computadora de 2001: Odisea del espacio,se puso a cantar " Una bicicleta para dos ",canción que había aprendido en sus primeros días.
Esta memorable escena podría no ser tan irreal como en principio parece.
Stephen L. Thaler,físico de la McDonnell Douglas,ha descubierto que esa vuelta a lo aprendido en una fase temprana es lo que realmente le ocurre a una red neuronal artificial cuando poco a poco se la va " matando ".
A medida que se acerca a la muerte,empieza a emitir,no incoherencias,sino información aprendida con anterioridad ; por así decirlo,su vida de silicio pasa como un destello de ante sus ojos.
Es imposible que esto no nos recuerde las llamadas experiencias en el umbral de la muerte ; al fin y al cabo,los creadores de redes neuronales pretenden al diseñarlas remedar la estructura y función de un cerebro biológico.
Una red neuronal es un tipo especial de programa que se partícula mediante " unidades " que imitan el papel del soma de las neuronas y " enlaces " que actúan a modo de los axones y dendritas que las interconectan.
En una disposición típica,las unidades están organizadas en varios estratos o niveles.
Consecuencia de tal arquitectura es que la red,como el cerebro,es capaz de aprender.
Se cree que en un cerebro real el aprendizaje se produce por modificaciones de la intensidad de las conexiones sinápticas entre neuronas.
De igual manera,una red neuronal va modificando la fuerza de los enlaces (específicamente,el " peso " de las conexiones entre unidades) a fin de producir salidas correctas.
Normalmente,el programador instruye a la red presentándole,repetidas veces,series de configuraciones de instrucción.
Redes neuronales instruidas pueden tomar a su cargo multitud de tareas,desde la compresión de datos hasta la modelización de la dislexia.
Thaler empezó,hará un año,a investigar las redes neuronales en busca de un procedimiento de optimización del control de crecimiento de los cristales de diamante.
Pero por curiosidad,para matar el tiempo libre,se puso a ver qué pasaba cuando se aniquilaba una red neuronal.
Preparó a tal fin un programa que fuera destruyendo la red gradualmente,cortando al azar las conexiones entre unidades.
" Se trataba de imitar la despolarización de las sinapsis en los sistemas biológicos ",explica Thaler.
Tras cada paso,Thaler examinaba la salida de la red.
Cuando se destruían entre el 10 y 60 por ciento de las conexiones,la red escupía un galimatías sin sentido.
Pero cuando el número de conexiones destruidas se acercaba al 90 por ciento,la salida empezaba a estabilizarse en valores bien caracterizados.
En el caso de la red de ocho unidades de Thaler,creada para materializar la función lógica " O exclusiva ",gran parte de lo producido eran los estados 0 y I para los que estaba instruida.
La red generaba a veces lo que Thaler denomina " estados caprichosos ",esto es,valores que ni correspondían a los aprendidos por la red ni aparecerían tampoco en una red " sana ".
Las redes no instruidas,por el contrario,sólo producían al morir números aleatorios.
No es tan descabellado que las exhalaciones de una red moribunda tengan sentido.
" Es comprensible habida cuenta de que son redes que han creado pautas estables," opina David C. Plaut,psicólogo e informático en la Universidad Carnegie-Mellon que se vale de redes neuronales para remedar lesiones cerebrales.
De hecho,el propio Thaler ha proporcionado una explicación detallada del fenómeno.
En una red plenamente instruida y en servicio,todas las señales ponderadas que ingresan en una determinada unidad son más o menos de la misma magnitud y de signos opuestos.
(En jerga matemática,los pesos tienen una distribución gaussiana,de perfil en forma de campana).
Es probable,pues,que la suma de las diversas señales que llegan a una unidad sea casi igual a cero.
Por tanto,si se cortan los enlaces,la unidad quizá no " sienta " la pérdida ; al fin y al cabo,podría haber estado recibiendo anteriormente de ellos una señal total nula.
Con frecuencia,los pocos enlaces que sobrevivan bastarán para que se produzca una salida razonablemente coherente.
Pero trasladar las consecuencias de esta experiencia artificial a las vivencias que los seres humanos tienen cuando están a punto de morir es exagerar demasiado.
" Las redes neuronales no pasan de ser una burda aproximación en el mejor de los casos," señala Plaut.
El cerebro es muchísimo más complejo que las redes neuronales.
Además,no está del todo clara la forma en que mueren las neuronas cuando están agrupadas.
La muerte de unas cuantas,por ejemplo,quizás arrastre a la muerte a sus vecinas.
Y el método que se utiliza en la instrucción de las redes neuronales - - un algoritmo de retropropagación - - no guarda parecido con la manera en que el cerebro aprende.
Aun así,la observación sugiere que algunas de las experiencias cuasimortales de las que con frecuencia se habla pudieran tener un fundamento matemático.
" Quizá no se trate de pura bioquímica-ficción," afirma Thaler,quien en estos momentos trabaja con redes más complejas,entre ellas una que producirá imágenes visuales.
¿Apuesta alguien por una luz que brilla al extremo de un largo túnel? Pólenes y lagos La reconstrucción del clima del pasado no existe variación climática,se haya extendido a lo largo de siglos o de varios millones de años,que no haya modelado nuestro ambiente.
La alternancia de los períodos glaciales e interglaciales se debe principalmente a las variaciones en la cantidad de calor solar que caldea la Tierra.
Estos grandes cambios han dejado sus huellas en los restos fósiles,los glaciares y la vegetación.
Cada año,los granos de polen dispersados por las plantas se acumulan en los sedimentos ; estos granos,cuya cubierta es muy resistente,se conservan en los ambientes húmedos desprovistos de oxígeno.
Al estudiarlos,los paleobotánicos siguen la evolución de la vegetación a lo largo de varios millones de años.
Como sea que las características morfológicas de los granos de polen fósiles se parecen a las de los granos de polen contemporáneos,nos faculta para determinar,en razón de su forma,la planta de la que proceden,de la que nos hacen saber a veces su especie,más a menudo su género.
Con el fin de recoger estas informaciones,se toman muestras a intervalos regulares de un testigo extraído de los sedimentos ; mediante tratamientos químicos adecuados se elimina la ganga y después se observa la muestra al microscopio.
En cada muestra se enumeran las especies o géneros reconocidos,y se determina así un espectro polínico característico de la vegetación de la que procede.
El conjunto de espectros de las muestras sedimentarias proporciona el diagrama polínico característico del testigo.
Toda modificación del diagrama indica un cambio de la vegetación pasada,generalmente como resultado de una modificación del clima.
En la actualidad conseguimos deducir por medios estadísticos el clima que reinaba en una época determinada a partir de la composición polínica de los testigos sedimentarios que estudiamos.
Los espectros polínicos se datan según la capa sedimentaria de la que proceden los granos de polen (mediante el método del carbono 14).
Una vez hecho,se comparan los espectros del diagrama polínico con espectros de referencia contemporáneos (de los que se conocen,evidentemente,las condiciones climáticas).
Por desgracia,un espectro polínico del pasado es a menudo afín a varios espectros actuales.
Las analogías polínicas se precisan entonces mediante otros indicios paleoclimáticos,como los restos fósiles de coleópteros o las variaciones del nivel de los lagos.
De esta manera hemos demostrado que en dos períodos cálidos (hace 125.000 y 6000 años) la temperatura media era superior en uno a dos grados a las temperaturas medias actuales,y que durante el largo período frío que culminó hace 20.000 años,en Francia la temperatura era inferior a la actual en unos cinco grados.
Hemos reconstruido,con datos polínicos extraídos de más de 200 testigos europeos y a partir de muestreos efectuados en 82 lagos de Europa,mapas térmicos y pluviométricos anuales de hace aproximadamente 9000,6000 y 3000 años.
Hace 6000 años,el clima era por término medio más cálido que hoy en día,del orden de uno a tres grados.
Las partes occidental y meridional de Europa eran más áridas,y la septentrional y la oriental,más húmedas.
La insolación anual a latitudes altas era superior,y en las latitudes medias el contraste entre el invierno y el verano,muy marcado.
El anticiclón subtropical,que rige el clima de Europa,se debió de desplazar hacia el norte,Europa septentrional se caldeó y las precipitaciones aumentaron en Escandinavia.
Los datos paleoclimáticos que suministra el análisis del polen y los procedentes del nivel de los lagos no siempre coinciden.
Así,durante el último máximo glacial,hace unos 18.000 años,la vegetación,según los datos polínicos,habría sido esteparia en el norte de Grecia ; sin embargo,los niveles de los lagos de esta región eran al menos tan elevados como en la actualidad,si hay que hacer caso de los análisis geomorfológicos.
Tal contradicción es sólo aparente: hemos imaginado distintas situaciones climáticas hipotéticas (clima más frío,o más seco o más nublado en verano,más frío o más húmedo en invierno,y combinaciones de estas condiciones diversas),y hemos calculado el efecto de estas condiciones sobre el nivel de los lagos y sobre la vegetación.
Hemos demostrado que la distribución estacional de las lluvias desempeña un papel más importante que el total de las precipitaciones: para precipitaciones totales anuales constantes,basta que el 10 por ciento del total anual caiga de más durante los meses de invierno en detrimento de los meses de verano,para que el bosque mediterráneo deje paso a una estepa y para que las aguas de escorrentía aumenten en un 20 por ciento.
Las precipitaciones aumentan en invierno,pero el suelo ya está saturado de agua,y ese aporte no compensa el déficit hídrico de la estación de crecimiento.
Estepas de este tipo ya no existen en Europa,sino sólo en América del Norte,en latitudes comparables,al este de las montañas Rocosas.
Además de las informaciones que estos datos paleoclimáticos aportan acerca de los climas pasados,también nos instruyen sobre el clima del siglo XXI.
Hoy se sabe que la entra da en la atmósfera de gases de efecto invernadero,como el dióxido de carbono o el metano,plantea el riesgo de perturbar notablemente el clima.
Si bien los climatólogos se basan en los modelos de circulación atmosférica y oceánica,la paleoclimatología permite someter a prueba esos modelos en condiciones climáticas extremas y comprobar las consecuencias pasadas de una modificación del clima global del orden de varios grados.
Los modelos nos enseñan que,si la concentración de dióxido de cal-bono se duplicara,las temperatura invernales aumentarían,en Europa.
de tres a diez grados.
Además,por encima del paralelo 40,las precipitaciones abundarían más.
El verano sería igualmente más caluroso,de I orden de tres grados,y las precipitaciones disminuirían al sur de los 50 ° de latitud Norte.
Esto es,a mayor escala,lo que.
por ejemplo,conoció Italia hace 6000 años,pero,diferencia notable.
este tipo de clima se estableció entonces de manera natural y a lo largo de varios milenios.
La vegetación era esencialmente forestal en la mayor parte de Europa,pero nuestro modelo demuestra que basta con aumentar en la región meditarránea el déficit hídrico estival del orden de 100 milímetros por año para que la vegetación forestal sufra una regresión notable.
Estamos elaborando un banco de datos polínicos,a fin de reconstituir la vegetación y el clima del planeta durante los períodos clave del pasado.
Estas informaciones contribuirán a mejorar los modelos climáticos con los que se prevé el clima del mañana.
La quitina es el compuesto orgánico que abunda más en el planeta después de la celulosa,otro polisacárido.
Henry Braconot la descubrió en 1811 en algunas setas y E. Odier la redescubrió en 1823.
El segundo le dio su nombre actual de " chitine ",quitina,cuya etimología griega evoca el significado de túnica,porque la encontró en los élitros de algunos escarabajos y supuso que cumplía una función protectora de los tejidos animales.
La verdad es que la celulosa y la quitina cumplen misiones semejantes de protección y resistencia en plantas y algas,la primera,y en animales inferiores y hongos,la segunda.
A tenor de esa distribución selectiva de ambos polisacáridos,alguien podría concluir que,desde el punto de vista evolutivo,los hongos están más ligados a los animales que a las plantas.
Pero dentro de los hongos distinguimos dos grandes grupos,los hongos inferiores,que portan celulosa,y los superiores,que poseen quitina.
Por cuya razón se habla de un origen polifilético de ese reino orgánico.
La quitina,más versátil que la celulosa,es un polisacárido formado por el azúcar N-acetilglucosamina (abreviado GlcNAc),que se encuentra ampliamente distribuido en la naturaleza.
Quitina y celulosa están constituidas por largas cadenas de monómeros de azúcar unidos covalentemente por enlaces de tipo B 1 - 4 (posiciones de los átomos de carbono),topología que les dota de cierta rigidez.
Las cadenas,a su vez,se unen entre sí por medio de puentes de hidrógeno para formar microfibrillas ; estructuras muy pequeñas cuya asociación origina fibras mayores,algunas de las cuales las percibe el ojo humano desnudo,por ejemplo,las fibras celulósicas de algodón.
Celulosa y quitina forman auténticos tejidos que confieren resistencia y soporte a los organismos.
Lo mismo que los huesos de los vertebrados,la quitina establece complejos con las proteínas y se calcifica para constituir el esqueleto externo (exoesqueleto) de muchos invertebrados.
Celulosa y quitina cristalizan en distintas figuras,según la disposición de las cadenas del polisacárido.
En efecto,la molécula de azúcar adopta,podemos aceptarlo,la forma de un hexágono irregular con dos extremos: uno correspondiente al átomo de carbono número 1,que posee una función aldehído,reductora,y otro correspondiente al átomo número 4 con una función alcohol,que no es reductora.
Al unirse dos moléculas de GlcNac (o de glucosa en el caso de la celulosa) establecen la llamada unión glicosídica entre los átomos de carbono número I de un monómero y el número 4 del monómero vecino.
En una cadena habrá,pues,dos extremos distintos,uno en el que la última molécula de azúcar posea el extremo reductor libre y el otro en el que la molécula tenga libre el extremo no reductor.
Al establecer uniones por puentes de hidrógeno,las distintas cadenas que originan la microfibrilla actúan de acuerdo con reglas estrictas para dar una estructura cristalina.
En la celulosa nativa,las cadenas se organizan paralelamente ; todas ellas se hallan orientadas con los grupos reductores libres en un extremo y los grupos no reductores en el opuesto.
El caso de la quitina es distinto,ya que hay tres formas naturales del polisacárido: alfa,beta y gamma.
En la forma alfa,la más abundante,las cadenas son antiparalelas (como en la molécula de ADN),es decir,cada cadena dispuesta en un sentido se asocia con otra orientada en sentido contrario.
La beta quitina tiene las cadenas paralelas (como la celulosa nativa).
La gamma quitina,la más rara de las tres formas,presenta,por cada cadena dispuesta en un sentido,dos que se orientan en sentido opuesto.
El análisis por difracción de rayos X distingue nítidamente las tres formas,ya que difieren en razón de su estructura cristalina.
No se conocen las razones por lo que la quitina puede cristalizar,en condiciones naturales,en tres formas distintas,pero el fenómeno comporta varias consecuencias.
En primer lugar,las tres tienen propiedades distintas que les permiten acometer funciones diferentes ; así la forma alfa es la más rígida y cumple funciones esqueléticas,en tanto que las otras dos,capaces de hidratarse,desarrollan unas propiedades mecánicas semejantes a las del cartílago.
Por otro lado,el hecho de que las cadenas de la forma alfa sean antiparalelas indica que los procesos de síntesis y ensamblaje de las cadenas para crear las microfibrillas no pueden ser simultáneos en el tiempo,sino separados.
La quitina constituye el polisacárido más insoluble que existe,lo que explica la dificultad que entraña determinar el tamaño de sus cadenas poliméricas.
Para lograr tal medición nos servimos de un método indirecto.
Empezamos por sintetizar quitina in vitro a partir de GlcNAc marcado radiactivamente con tritio.
El polisacárido precursor se usó como substrato para la acción de la galactosiltransferasaN ; esta enzima añade una sola molécula de galactosa al extremo no reductor de las cadenas de quitina.
La galactosa usada para ello estaba marcada radiactivamente con carbono 14.
La proporción de radiactividad que habían adquirido las moléculas de azúcar nos daba la actividad específica del polisacárido y,conocida dicha actividad y la relación de 3 H a 14 C,pudimos calcular que las moléculas de quitina tenían unas 2000 unidades de GlcNAc.
La quitina se encuentra en toda la escala de los seres.
Las bacterias,empero,carecen de ella.
Entre los protozoos,muchos ciliados utilizan la quitina para construir estructuras protectoras (lórigas) a semejanza de los corales ; múltiples amebas,todas las parasitarias de animales,forman quistes de quitina.
Este polisacárido lo hallamos en las colonias de hidrozoos,es un componente estructural de anélidos y moluscos,abunda menos en nemátodos y celenterados,pero crea el exoesqueleto de los artrópodos.
Vuelve a faltar la quitina en equinodermos y cordados,indicio de que se produce un cambio brusco en la estrategia evolutiva de los animales.
Entre las algas,poseen quitina las diatomeas y los crisoflagelados.
En los hongos,con excepción de los oomicetos,ese polisacárido se erige en el principal compuesto del armazón de la pared celular.
Como es sabido,la pared celular es la estructura externa que confiere protección,rigidez y forma a estos organismos.
Las plantas superiores carecen de quitina.
En la estrategia por la supervivencia,llama la atención que plantas y animales superiores estén privados de quitina en tanto que sí la poseen sus múltiples parásitos.
Los hongos constituyen las peores plagas de la agricultura,y los insectos son vectores de otras más.
Hongos e insectos destruyen una proporción muy grande de los granos almacenados,y algunos hongos producen toxinas que hacen sumamente peligroso el consumo de estos granos infestados.
En los animales superiores,hombre incluido,los hongos,amebas,ácaros e insectos causan enfermedades o las transmiten.
Ese panorama sugiere que el diseño de drogas que inhiban la formación de quitina constituye el método ideal de combate de esos parásitos,ya que serían altamente específicas contra ellos.
Recuérdese a este propósito el caso de la penicilina y sus derivados,cuya acción bacteriana selectiva se debe a que inhiben la formación de un compuesto exclusivo de la pared celular de las bacterias (la peptidoglicana),y del que carecen por tanto los eucariotas.
Hasta el momento se conocen dos familias de antibióticos que constituyen las " penicilinas de los hongos ": las polioxinas y las nikomicinas.
Producidas ambas familias por actinomicetos aislados del suelo,actúan inhibiendo la enzima que sintetiza la quitina ; para ello se valen de un mecanismo competitivo,al presentar análogos estructurales al substrato sobre el que opera la quitina sintetasa.
Los antibióticos de ambas familias atacan la inmensa mayoría de los hongos quitinosos.
Su uso viene limitado por la relación beneficio / coste y por los problemas que plantean ; en particular,la baja permeabilidad de las polioxinas contra las micosis humanas.
que implica la necesidad de emplear altas dosis,su pobre absorción intestinal y su poca estabilidad química.
Con la importancia terapéutica hemos de destacar el interés industrial,tanto de la quitina como de la quitosana.
Esta última existe en cantidades significativas sólo en la pared celular de los zigomicetos y es un polisacárido homólogo de la quitina,pero total o parcialmente desacetilado ; se halla constituido fundamentalmente por glucosamina,en vez de acetil-glucosamina.
La quitosana se forma vivo por desacetilación enzimática de la quitina.
Químicamente,la quitina puede desacetilarse por tratamiento con álcalis en caliente.
El potencial anual de producción de quitina se ha calculado en 150.000 toneladas provenientes de residuos de las industrias de fermentación que emplean hongos (32.000 toneladas) y el resto de la industria pesquera.
Por sus características químicas y físicas,la quitina y la quitosana pueden substituir a materiales plásticos con la ventaja de que son biodegradables y,por consiguiente,no contaminan.
Se investiga su aplicación como reactivos químicos para el análisis de metales,basándose en su capacidad de intercambiadores iónicos y de formar complejos,su aprovechamiento en cosmetología por sus propiedades emulsificantes y nula alerginicidad y su mayor explotación en la industria alimentaria y otras.
Quitina y quitosana se emplean ya en la industria alimentaria como fuente de fibra natural en la dieta,para fijar pigmentos artificiales y evitar su absorción en el intestino,como emulsificantes y para absorber grasas.
En forma de película,se ha empleado la quitosana para purificar el agua por medio de ósmosis revertida y se ha sugerido su uso para proteger alimentos,ya que tolera altas temperaturas y es ella misma comestible.
La quitosana sirve también para concentrar material protéico presente en líquidos de desecho de diversas industrias.
Presenta esto una doble ventaja: el material concentrado se puede usar como aditivo en la alimentación animal y,en segundo lugar,purifica el líquido que puede verterse al medio ambiente.
Por su propiedad floculante,el polisacárido puede depurar el agua potable de consumo humano.
En el ámbito médico de la quitosana se destacan sus propiedades hipocolesterémicas e hipolipidémicas,amén de constituir un poderoso secuestrante de pigmentos biliares.
La quitosana es hemostática y varios derivados suyos presentan capacidad anticoagulante.
La quitina es un adyuvante y activador de macrófagos que promueve la producción de anticuerpos.
Ambas,quitina y quitosana,incrementan la defensa antitumoral y protegen contra diversos patógenos.
En cirugía,el hilo de derivados de la quitosana aventaja al formado por otros materiales.
El dominio de las aplicaciones se abre a la producción de textiles y membranas,recuperación y separación de metales,catálisis,purificación de enzimas y otros.
Nosotros hemos centrado la investigación en el papel de la quitina en el desarrollo de los hongos y su mecanismo de síntesis.
Para cumplir con su cometido,la pared celular (la quitina es su principal componente) posee características mecánicas muy definidas,cuyo símil artificial son los materiales compuestos (" composites ").
Estos materiales constan de una matriz y un elemento fibrilar rígido y fuerte,que posee una alta relación entre su módulo elástico y su densidad.
Ese elemento fibrilar es responsable de la resistencia a las tensiones,en tanto que la matriz,débil y dúctil,constituye la zona a través de la cual se transfiere la tensión,soporta las presiones y protege las fibras del daño mecánico y las fracturas.
En el caso de la pared celular de los hongos,la quitina cumple con el papel de elemento fibrilar estructural.
La fuerza tensil de la quitina,muy superior a la de cualquier material natural,está por encima de la que posee la fibra de carbono o el mismo acero.
Si despojamos de la matriz a la pared celular permanecerá la quitina,resistente a la ebullición en ácido y álcali,y su malla conservará la forma del organismo.
Ese papel morfogenético de la quitina se aprecia,por ejemplo,en la forma de los hongos.
La pared celular del micelio de los hongos semeja un extenso sistema tubular por el que avanza protegido el citoplasma para su dispersión y búsqueda de nutrientes.
De ahí el gran éxito colonizador de los hongos,cuyas estructuras de reproducción,las esporas,se forman sobre soportes que abandonan el substrato,para ser dispersadas al aire o ser arrastradas por cursos de agua.
Al encontrar un huésped adecuado,los hongos forman,gracias a la quitina,estructuras especializadas de contacto y penetración.
La morfología del hongo tiene mucho que ver con la síntesis de la quitina,como demostró Salomón Bartnicki-García,de la Universidad de California en Riverside.
Los hongos esféricos sintetizan quitina en toda su superficie (crecimiento isodiamétrico),en tanto que el micelio,tubular,deposita la quitina específicamente en la punta o ápice del mismo.
La biosíntesis de la quitina constituye un ejemplo de una reacción de transglicosilación,en la cual un donador de grupos glicosilo transfiere un glicosilo a un aceptor ; el aceptor es la cadena de quitina,que se incrementa en una unidad,y el donador es un nucleótido.
El sustrato sobre el que opera la enzima quitina sintetasa para producir ese polisacárido es la uridina difosfato-GlcNAc,abreviadamente UDPGlcNAc.
La iniciación de la síntesis plantea ya un problema.
¿Cuál es el aceptor de la primera molécula de azúcar? No se sabe con certeza.
Se ha hablado de lípidos y proteínas.
A propósito de las segundas se recuerda que varios polisacáridos - - así,las mañanas de la pared de hongos y levaduras,el almidón,el glucógeno y las glucanas de los hongos - - utilizan aceptores protéicos.
Pero la mayoría de los autores se inclinan por pensar que no existiría aquí tal aceptor previo.
La estructura de la quitina con uniones de tipo B implica que la orientación de las moléculas de azúcar en el espacio sea alterna.
Para simplificarlo: los radicales N-acetilo que están unidos al átomo de carbono 2 del azúcar deben hallarse orientados en direcciones opuestas en unidades vecinas de azúcar.
Dicho de otro modo,se necesitan dos sitios activos para la unión del substrato a la enzima o bien hay dos polipéptidos distintos que cooperen en la unión de dos moléculas de azúcar simultáneamente.
Así podría iniciarse la síntesis v crecer la cadena.
La quitina sintetasa es una enzima que muestra cooperatividad: la unión de una molécula de substrato facilita la unión de la segunda.
La cooperatividad de esta enzima depende de la unión de UDPGlcNAc a los sitios alostéricos,que,al ser ocupados,alteran la estructura de la enzima y facilitan el acceso de las moléculas de substrato a los sitios catalíticos,o activos.
Se sabe que,en otros sistemas que poseen sitios alostéricos,éstos se hallan en polipéptidos distintos de los catalíticos.
No debe,pues,sorprendernos que las quitinas sintetasas que se han purificado parcialmente aparezcan como agregados de alto peso molecular,cercano al medio millón de daltons formados por varios polipéptidos.
En el laboratorio,el papel estimulador del UDPGlcNAc puede sustituirse por altas concentraciones del propio azúcar libre (GlcNAc).
Y en algunos sistemas incubados en condiciones limitantes de substrato no ocurre la catálisis,si no se añade GlcNAc al substrato.
La acción de la quitina sintetasa se activa en presencia de bajas concentraciones de adenosín trifosfato,ATP.
Esa estimulación no se debe a una reacción de fosforilación,sino a la unión del nucleótido a la enzima,según se desprende del comportamiento de los análogos del ATP,que,sin ser fosforilantes,muestran también un efecto estimulador.
In vitro,la quitina sintetasa requiere ser activada por tratamiento proteolítico (como el caso de los zimógenos) para manifestar su función,pero se desconoce cuál sea el mecanismo natural de activación in vivo.
En Phycomyces blakesleeanus hemos demostrado que la tripsina,o una mezcla de calcio y calmodulina,desencadenan la síntesis de la enzima,y vale la pena señalar que varios sistemas estimulados in vivo por calcio y calmodulina se activan in vitro con proteasas.
Para llevar a cabo su función,la quitina sintetasa requiere de un ión metálico divalente.
En los diferentes sistemas estudiados se ha visto que el Mg2 +,el Mn2 + y el Co2 + son los más eficientes.
Pero bloquean su acción polioxinas y nikomicinas,análogos estructurales del UDPGlcNAc.
El uridín difosfato (UDP) y otros nucleótidos,salvo el ATP a bajas concentraciones,inhiben también la enzima.
En 1904 y en colaboración con Salomón Bartnicki-García,lográbamos la síntesis de microfibrillas de quitina.
Para sintetizar el polisacárido in vitro,partíase de fracciones membranales de distintos hongos ; la tasa de síntesis se medía a través de la incorporación de material radiactivo en un producto insoluble en ácido y en álcali,producto cuyas características físicas se desconocían.
Nosotros obtuvimos las microfibrillas empleando una forma enzimática solubilizada a partir de membranas del hongo Mucor rouxii por exposición a altas concentraciones de UDPGlcNAc a baja temperatura ; no se distinguían nuestros hilillos de los que componían la pared celular,y el espectro de difracción de rayos X corroboró que estaban formados por alfa-quitina.
La levadura Saccharomyces cerevisiae posee tres quitinas sintetasas diferentes,fenómeno que se descubrió al observar la incapacidad de sintetizar quitina in vitro que mostraban los mutantes de la cepa y poseer,no obstante,niveles normales del polisacárido en la pared.
La anulación del gen correspondiente (CHSI) no afectó al crecimiento de la levadura,señal de que podía prescindir del mismo.
Hubo que sugerir,pues,la existencia de otra enzima responsable de la síntesis del polisacárido.
El gen (CHS2) correspondiente se clonó seleccionando recombinantes hipersintetizadores de quitina in vitro,transformados con un plásmido multicopia que transportaba fragmentos del ADN de la propia levadura ; la degradación de este gen causó la lisis de las células,prueba de que constituía un elemento indispensable para el crecimiento celular.
Ahora bien,estudios ulteriores demostrarían que,mediante la variación de las condiciones de cultivo,los dobles mutantes afectados en ambos genes CHSI y CHS2 eran viables,aunque mostraban algunas alteraciones en la deposición de la quitina sobre la superficie,semejantes a lo que ocurre en mutantes alteradas en el citoesqueleto.
De ahí que se postulara la exigencia de otra tercera quitina sintetasa,que sería la responsable de la síntesis in vivo del polisacárido.
Dado el carácter de agregado multiproteico de la enzima,quizá no esté fuera de lugar sospechar que algunos de los genes clonados corresponden a proteínas que son elementos del complejo necesarios para la actividad sintetizadora,aunque no constituyan estrictamente el polipéptido catalítico.
I del lugar donde ocurre la síntesis de quitina es punto importante,porque el producto,una vez cristalizado,resulta altamente insoluble.
De ahí que se le confine fuera de la membrana citoplasmática,si bien queda pendiente el problema de cómo actuaría la enzima sobre el substrato,estando éste como está acumulado en el interior del citoplasma.
Para romper ese círculo vicioso,se ha supuesto que la quitina sintetasa constituye un complejo transmembranal que opera vectorialmente aceptando los radicales glucosilo en la cara interna de la membrana y transfiriéndolos a la cadena naciente que se halla en la cara externa del plasmalema.
Hipótesis que se ha visto respaldada por ciertos experimentos in vitro,donde se comprobó la acumulación de quitina en membranas de levaduras asociada con la cara externa de las mismas.
Pero cabe otra explicación: el polisacárido se sintetiza intracelularmente en algún compartimento aislado,aunque no cristaliza dentro del mismo hasta que el compartimento se fusiona con el plasmalema y se modifican las condiciones fisicoquímicas que impiden dicha cristalización.
Abonan esta idea ciertos ensayos realizados in vivo con el ciliado Eufolliculina uhlighi,que secreta quitina al medio a través de vesículas.
Una vez en el medio,la quitina cristaliza en forma de microfibrillas que sirven de base para la construcción de las lórigas del protozoo.
En los hongos leptomitales,la quitina,sintetizada intracelularmente,se acumula en el citoplasma en gránulos de celulina,cuya función se ignora.
Se sabe que,en todos los organismos eucariotas,la síntesis de las proteínas asociadas a la membrana plasmática,y las de exportación,incluyendo en el caso de los hongos las que se asocian con la pared celular,ocurre en ribosomas que se unen a las membranas del retículo endoplasmático (RE).
Las proteínas sintetizadas por estos ribosomas penetran en el lumen del RE por un mecanismo vectorial asociado con su síntesis y su glicosilación.
Al concluir la síntesis,las proteínas quedan libres en el lumen.
o bien,si tienen determinadas señales y secciones de la cadena formadas por aminoácidos hidrofóbicos,quedan atrapadas en la membrana.
Empieza entonces el tránsito de estas proteínas a través del sistema transmembrana ; pasan al retículo endoplasmático libre de ribosomas (RE liso) y,almacenadas en vesículas,llegan al aparato de Golgi,que dirige el tráfico de proteínas a diversos destinos,entre ellos la superficie celular.
Cabe señalar aquí que,en la mayoría de los hongos,el aparato de Golgi no está morfológicamente tan definido como en otros organismos,y sólo podemos hablar de un " aparato de Golgi funcional " ; del mismo parten vesículas que llevan proteínas destinadas a la membrana plasmática y proteínas de exportación.
En 1975,Bartnicki-García,Charles E. Bracker,de la Universidad de Purdue,y el autor aislaron,a partir de extractos celulares del hongo Mucor rouxii,una población de esas vesículas ; en atención a que sólo se ocupan de la síntesis de quitina las denominamos " quitosomas ".
Se trata de vesículas que miden entre 40 y 70 nanómetros (un nm es la millonésima parte de un milímetro),rodeadas por una finísima membrana.
Estas microvesículas,cuando se activan artificialmente in vitro mediante una proteasa y se incuban con el substrato para la síntesis de quitina (UDPGlcNAc) y activadores,sufren una serie de cambios irreversibles que conducen finalmente a la formación de microfibrillas de quitina.
Cuál es el mecanismo de síntesis de las microfibrillas por los quitosomas? Un estudio minucioso de las microfotografías electrónicas de quitosomas incubados con UDPGlcNAc,proteasas y estimuladores nos manifiesta que,en los primeros estadios de síntesis,aparecen en el interior de los quitosomas fibrillas muy delgadas,que luego aumentan en diámetro y terminan por cristalizar en ovillos constituidos por fibras gruesas de quitina,los fibroides.
Los ovillos se forman en el interior de los quitosomas porque las condiciones artificiales impuestas en el ensayo obligan a la cristalización prematura de la quitina.
En un estadio más avanzado,el ovillo rompe la membrana del quitosoma para originar fibrillas rectas de quitina,iguales a las que existen en la pared celular.
Se tarda más tiempo en explicar el proceso que en producirse.
Basta incubar los quitosomas unos pocos segundos para que se formen masas de fibroides y de microfibrillas.
Estos estudios nos han llevado a sugerir que los quitosomas constituyen la población de vesículas encargadas de transportar la quitina sintetasa ; vendrían a ser el reservorio intracelular de quitina sintetasa,listo siempre para desplazarse hacia los sitios de la célula donde ocurre la formación de la pared.
Con Rafael Sentandreu,de la Universidad de Valencia,y Amelia Martínez,del Instituto de Investigaciones Citológicas de Valencia,hemos sometido a prueba experimental la hipótesis.
Tratamos células de Mucor rouxii con tolueno,que torna permeable la membrana celular al UDPGlcNAc,ya que esta molécula no penetra en la célula en condiciones naturales.
Al incubar estas células con un sustrato marcado radiactivamente con tritio,la quitina se acumuló en el interior celular,allí donde yacían los quitosomas inmovilizados por el tratamiento.
La visualización de la quitina radiactiva se logró mediante la técnica de autorradiografía,que cubre la célula con una película fotográfica que imprime las emisiones del material radiactivo.
No sabemos si la síntesis del polisacárido acontece durante el tránsito de los quitosomas,cuando disponen del substrato disuelto en el citoplasma,ni si las microfibrillas cristalizan cuando la vesícula se fusiona con la membrana plasmática (lo que ocurría con el ciliado mencionado más arriba),ni si,por el contrario,al fusionarse el quitosoma con la membrana crea una complejo sintetizador que produce quitina por el mecanismo de transporte sugerido a propósito de las levaduras.
El quitosoma puede disociarse con ciertos detergentes que liberan los agregados multiproteicos dotados de actividad sintetasa.
En este contexto,D. Rast,de Zurich,incubó agregados provenientes de quitosomas del champiñón Agaricus bisporus y observó que cada complejo enzimático formaba microfibrillas muy delgadas,de sólo dos cadenas de quitina.
Nos indican estos resultados que el quitosoma posee el número de complejos enzimáticos necesarios para sintetizar las cadenas de quitina constituyentes de la microfibrilla La quitina puede depositarse en toda la superficie,lo que origina una morfología celular esférica,o circunscribirse a determinados puntos de la superficie,lo que da lugar al crecimiento tubular del micelio y otras configuraciones.
Ello implica que habrá,en la célula,mecanismos que dirigen específicamente a los quitosomas y otras vesículas hacia sitios prefijados de la superficie.
La microscopía electrónica nos revela que allí donde ocurre síntesis activa de pared celular,se presentan cúmulos de vesículas.
En el caso del micelio que crece apicalmente,las vesículas adquieren tal densidad en la punta que se perciben incluso con un microscopio óptico.
En estas condiciones,la acumulación de vesículas semeja un cuerpo refráctil,un corpúsculo basofílico teñido por Brunswick en 1924 y al que dio el nombre de corpúsculo apical (" Spitzenkorper ").
¿Cuál es el mecanismo que moviliza las vesículas? A falta de seguridad,se manejan dos candidatos plausibles,el citosqueleto y las corrientes iónicas.
En lo concerniente al citosqueleto,lo mismo podrían ser en particular los microtúbulos que los microfilamentos,debidamente organizados ; actuarían como rieles sobre los que se deslizarían las vesículas,según se ha visto que ocurre en otros organismos.
Por su parte,las corrientes iónicas,al crear campos eléctricos,forzarían la migración de las vesículas por medio de electroforesis.
Hay pruebas en favor de ambas hipótesis,pero ninguna determinante.
Cuando se descarga la quitina en el espacio intercelular,debe transcurrir cierto tiempo antes de que cristalice y adquiera una estructura microfibrilar rígida.
Durante ese intervalo,la quitina se halla expuesta a la acción química de otras moléculas e imparte a la pared celular naciente propiedades mecánicas distintas de las que poseerá la pared madura.
Estamos hablando de un momento que reviste especial importancia para el crecimiento del hongo,puesto que permite la expansión celular.
La pared celular en crecimiento es viscoelástica ; la pared madura,elástica.
Entre las modificaciones que sufre la quitina antes de cristalizar citaremos su degradación por quitinasas enzimas hidrolíticas que debilitan la pared y facilitan su expansión cediendo ante la presión osmótica del protoplasma (presión de turgencia).
La quitina podría también desacetilarse y transformarse parcialmente en quitosana,si bien ese proceso ocurre principalmente en los zigomicetos y,en forma muy rara,en hongos de otros grupos taxonómicos.
Por último,la quitina forma uniones covalentes con otros componentes de la pared celular (glucanos y proteínas) para construir la pared madura,muy resistente,y que brinda protección mecánica y química al protoplasma de la célula.
Avanzado el crecimiento celular,cobra interés la hidrólisis controlada del polisacárido por medio de quitinasas que,al ablandar la pared celular,permiten la formación de ramas laterales y la unión de células de sexos opuestos durante los fenómenos de reproducción sexual.
En resumen,la quitina es un compuesto orgánico indispensable para la existencia de un gran número de organismos.
Su síntesis e hidrólisis juiciosamente reguladas permiten el crecimiento y desarrollo de estas especies.
Así mismo,el conocimiento de los mecanismos subyacentes nos ha de llevar al control de las plagas agrícolas y enfermedades animales y humanas,mediante el diseño de drogas que inhiben selectivamente la acción de los patógenos.
Por último,se encierra en la quitina un recurso poderoso de materia orgánica que puede aplicarse a diversos usos industriales,en sustitución de compuestos tóxicos,contaminantes o no biodegradables.

