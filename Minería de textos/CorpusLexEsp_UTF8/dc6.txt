La teoría de la tectónica de placas de Wegener acaba de sufrir,ahora,un brutal ataque por parte de una asociación científica internacional,que cuestiona la famosa deriva de los continentes.
El lema de estos sabios podría muy bien ser el de acabar con el mito de los continentes viajeros.
Trescientos geólogos de todo el mundo,agrupados en la ISCDS (Intemational Stop Continental Drift Society,Sociedad Internacional para detener la deriva de los continentes),acaban de iniciar un debate que,aparentemente al menos,pone en peligro muchas nociones que parecían,por fin,sólidamente establecidas.
En todo caso,lo que estos científicos niegan es que los continentes se desplacen en la superficie del planeta.
Y proponen,para probar sus tesis,toda una serie de experimentos que implican a las más modernas tecnologías: astrofísica,láser,satélites,ordenadores superpotentes... Cuando Wegener propuso,en 1923,su famosa teoría de la deriva de los continentes,lo cierto es que nadie se la tomó muy en serio.
Al finalizar la Segunda Guerra Mundial,se iniciaron las primeras grandes exploraciones de los fondos oceánicos: poco a poco se fueron descubriendo las grandes cadenas montañosas submarinas en el centro de los océanos.
Montañas recientes,en formación,con una especie de valle central en plena dorsal,el ríft,por donde se derraman las lavas basálticas surgidas de las profundidades de la Tierra.
Surgió así la idea de la expansión de los fondos oceánicos: las lavas enfriadas y solidificadas van expulsando hacia los bordes de la dorsal a las rocas más antiguas.
Y este fenómeno se demuestra fácilmente mediante el magnetismo de los óxidos de hierro que contiene el basalto: cuando la roca está en forma líquida,sus partículas férricas se orientan según el campo terrestre,orientación que queda fijada para siempre al solidificarse la roca.
Pero ocurre que el campo magnético terrestre se ha invertido numerosas veces en el pasado,y los geólogos saben cuándo.
Por esta razón,es relativamente fácil determinar la edad de las rocas basálticas según la polaridad que presenten las bandas magnéticas que contienen ; esta polaridad se denomina positiva,de forma convencional,si es igual que la actual,y negativa,si es Inversa.
Estas bandas paleomagnéticas son simétricas respecto a la dorsal,lo que demuestra que en el centro hay rocas recientes y en los bordes rocas cada vez más antiguas (dibujo 1).
Este nuevo dato revitalizó la teoría de Wegener,hasta entonces no demostrada ; y dio lugar a la tesis hoy día aceptada de la tectónica de placas,que implica la deriva de los continentes sobre la corteza de la Tierra (dibujo 2).
La tectónica de placas asocia la idea de los continentes móviles a esta expansión del suelo oceánico que empuja unas placas contra otras.
Hoy día,la teoría se ha ido afinando.
Una placa puede ser destrozada por la subida de la lava en una dorsal,y sus fragmentos pueden derivar en sentido opuesto,haciendo aparecer un nuevo océano.
Así ocurrió seguramente con el Atlántico,nacido entre los restos del gran continente primitivo llamado Pangea.
Otras placas pueden converger y chocar de frente: una de ellas se hunde bajo la otra,sumergiéndose en el manto,en la astenosfera,donde se mezcla de nuevo con el basalto semifundido.
Lo cual viene a compensar la constante formación de nueva corteza bajo los océanos a nivel de las dorsales (dibujo 3).
La placa que se sumerge roza fuertemente con las rocas adyacentes,originando seísmos y produciendo un fuerte calor que colabora a la formación de bolsas de magma que alimentan a los volcanes de superficie.
Los continentes,por su parte,no se hunden en el manto,ya que la placa continental,generalmente granítica,es más ligera que el basalto de la corteza oceánica.
Surgen así las montañas,como vía de escape de las tensiones de rozamiento entre placas.
Finalmente,las crestas de las dorsales oceánicas no forman líneas continuas: numerosas fallas perpendiculares van fragmentando ciertos segmentos de dorsal,creando fallas transformantes (dibujo 4).
En conjunto,muchas cosas quedan así explicadas: la formación de las montañas,los seísmos entre placas,el nacimiento bajo el océano de nuevas rocas... En 1967,prácticamente todos los especialistas norteamericanos de las ciencias de la Tierra aceptan la nueva doctrina.
En la década de los setenta,la mayor parte de los europeos se inclinan igualmente ante la nueva teoría.
Aun así,en el Congreso de Geología de París de 1978,más de la mitad de los participantes discute aún la validez de la teoría de la deriva de los continentes.
En 1980,el empuje de los jóvenes geólogos rusos obliga al famoso académico Bielusov a aceptar,más o menos resignadamente,la teoría venida de occidente... La tectónica de placas integra,en un modelo único,un gran número de descubrimientos recientes cuya explicación nunca antes había sido reagrupada bajo una única teoría.
Por ejemplo,la medida de las bandas paleomagnéticas permite calcular la expansión del fondo oceánico.
Por su parte,aplicada al desplazamiento de las placas sobre el planeta,la geometría de los casquetes esféricos permite extrapolar,hacia el pasado como hacia el futuro,la configuración de los continentes y los océanos (ver al respecto el número 23 de CONOCER," La computadora que viajó al pasado ".
El movimiento relativo de dos placas se reduce a una rotación alrededor de un eje,llamado euleriano (en honor del matemático suizo Euler),que pasa por el centro de la Tierra (dibujo 5).
Este movimiento se produce a lo largo de círculos paralelos centrados en el citado eje,y posee una velocidad angular de rotación que es constante,mientras que la velocidad de desplazamiento a lo largo de los paralelos eulerianos varía,siendo nula en el polo de rotación y máxima en el ecuador.
Las dorsales son más o menos perpendiculares a los paralelos eulerianos,y las fallas transformantes siguen la línea de dichos paralelos.
El año pasado,nadie dudaba de que la deriva de los continentes fuese un hecho científico tan sólidamente establecido como la estructura del átomo o la fórmula química del agua.
¿Nadie? Craso error.
Nada más comenzar el año 1985,se produce el manifiesto público del ISCDS,al que hacíamos referencia al principio.
Un manifiesto firmado por suficientes personalidades científicas,recalcitrantes antiderivistas,como para suponer un auténtico escándalo a nivel mundial.
Entre sus firmantes,un geólogo de la NASA,Paul Lowman,famoso por sus trabajos de interpretación de la superficie terrestre a partir de fotografías tomadas por satélites.
Su posición en realidad no es tan revolucionaria: un " sí " matizado a la tectónica de placas,pero un " no " rotundo a la deriva de los continentes.
Estos,según Lowman y sus colegas,están sólidamente enraizados en la corteza y no viajan en absoluto,ni nunca lo hicieron en el pasado.
Antiguamente,al condenar a los herejes,los Padres de la Iglesia no tenían más remedio que reconocerles cierta utilidad: su mera existencia les obligaba a resolver ciertas cuestiones que,sin ellos,jamás se hubieran suscitado.
El geoide es el nombre que se le da a la forma de la Tierra cuando toda su ' superficie se lleva teóricamente a una misma altitud,la altitud cero.
Respecto a la forma teórica calculada para un globo fluido en rotación alrededor de un eje,pero desprovisto de movimientos internos,el geoide presenta ciertas anomalías ; unas son positivas,los abultamientos,y otras negativas,los ahondamientos.
Estas anomalías se deben a los movimientos de las rocas semifluidas en el interior del manto.
Si se consideran las anomalías que alcanzan superficies de varios millares de kilómetros cuadrados y desniveles de cerca de un centenar de metros,se puede observar que se reparten en dos cinturones discontinuos,encastrados perpendicularmente de forma similar a las bandas de una pelota de tenis (dibujo A).
El eje de las anomalías negativas pasa por los polos ; el de las positivas,por el Ecuador.
Nuestro dibujo muestra en azul la banda positiva y en rojo la negativa.
La forma teórica está indicada en punteado ; por supuesto,la separación con el geoide ha sido exagerada,sin respetar la escala.
El estudio de la programación de las ondas sísmicas en el manto inferior ha revelado la existencia de dos corrientes de convección: una,ascendente,que corresponde al abultamiento ecuatorial,y otra,descendente,correspondiente al ahondamiento polar.
Si consideramos un mapa paleogeográfico de la Pangea,el hemisferio oculto está representado por una corona circular (dibujo B).
Se puede observar inmediatamente que en aquella época,hace 200 millones de años,este continente único se encontraba en un hemisferio,con su contorno pasando por el meridiano que une los polos.
El otro hemisferio estaba ocupado por un superocéano,la Panthalasa,una de cuyas prolongaciones,el mar de Tethys,penetra en cuña dentro de la Pangea,dividiéndola en dos partes: al norte,la Laurasia (América del Norte y Eurasia unidas) y al sur el continente de Gondwna (Suramérica,Africa,Australia,India y Antártida unidas).
El dibujo B muestra el contorno,en negro,de los actuales continentes,que corresponden más o menos a las futuras líneas de fractura de la Pangea.
Los polos no están en el mismo sitio que ahora.
sino allí donde,según el paleomagnetismo,se encontraban por entonces.
El contorno del supercontinente era,entonces,una zona de subducción: la litosfera oceánica se hundía por debajo de la Pangea (dibujo C).
Las corrientes convectivas en el manto superior se acoplaban mecánicamente a las del manto inferior como lo hacen los dientes de un engranaje.
Al igual que las anomalías del geoide,la formación del supercontinente era una consecuencia del sistema de convección del manto inferior,controlado a su vez por la rotación de la Tierra.
El origen común de los dos efectos aparece si se superpone un mapa de la Pangea con otro que represente las anomalías del geoide tal y como las conocemos actualmente (dibujo D).
La corriente ascendente en el manto inferior,al aportar materiales más cálidos y,por tanto,menos densos,engendra anomalías negativas en torno a la Pangea.
Las anomalías positivas en el plano ecuatorial indican los materiales más fríos,es decir,más densos.
Pero desde hace unos 150 millones de años el acoplamiento de los sistemas de convección inferior y superior se ha roto.
La explicación hay que buscarla en la disipación del calor del globo por los océanos,que es cuatro veces más rápida que la que se da en los continentes.
Se produjo,así,una acumulación de calor bajo la Pangea por lo que la corteza continental se fracturó.
Al mismo tiempo,la convección se aceleró en el manto superior desacoplándose del sistema convectivo inferior y haciendo derivar las placas continentales en diversas direcciones hasta llegar a la actual distribución de tierras y océanos.
Pero puede que nos encontremos al final de este período.
La corteza oceánica,refrigerada y con mayor peso,parece hundirse en el manto con mayor rapidez que la formación de nueva corteza en las dorsales.
Si hace 350 millones de años el paleomagnetismo muestra que había un supercontinente único,que luego se separó para volverse a juntar,hace 200 millones de años en la Pangea,quizá ahora nuestro planeta entre en una nueva fase de agrupamiento para dar lugar,de nuevo,a una tercera Pangea futura.
CUANDO la obesidad en las personas se hace llamativa y extremada,sin que tengan éxito los diversos tratamientos y regímenes alimentarios adecuados,se puede recurrir a la cirugía.
La técnica se sustenta en un principio que no puede ser más evidente: puentear una parte del intestino delgado,con el fin de reducir la superficie total de absorción de los principios alimentarios.
El aislamiento del trozo de intestino que se pretende inutilizar se realiza mediante un by-pass quirúrgico,denominado by-pass intestinal ana obesidad.
Este tipo de obesidad extrema no es,como pudiera pensarse,un problema poco frecuente.
En los países desarrollados,sobre todo,los pacientes se cuentan por centenares de miles.
Normalmente,suele considerarse como una molestia sin más pero hoy día existe la tendencia creciente a pensar que se trata de una auténtica enfermedad.
En efecto,numerosos estudios han demostrado que la expectativa de vida de las personas afectadas disminuye notablemente,y aumenta en ellas la incidencia de numerosas enfermedades graves que disminuyen el ritmo social y vital: afecciones cardiovasculares y respiratorias,diabetes,etcétera.
Normalmente se considera que un paciente es un obeso extremo cuando su peso es igual o superior al doble del llamado peso ideal.
En estos casos,los tratamientos médicos habituales suelen fracasar no sólo porque la disminución de peso que se pueda conseguir no sea bastante,sino,sobre todo,porque al terminar,el paciente suele recuperar muy rápidamente su peso primitivo.
Estas personas tienen evidente propensión a enfermar,incluso en plena juventud ; y,de hecho,las estadísticas demuestran que en ellos aumentan significativamente la morbilidad y la mortalidad.
Muchos médicos,sobre todo en los Estados Unidos,acuden entonces a la solución quirúrgica como último remedio.
En la cirugía de la obesidad patológica,el by-pass intestinal comprende un conjunto de técnicas quirúrgicas encuadradas dentro de la moderna cirugía metabólica.
El objetivo no es otro que la creación de un síndrome de mala absorción intestinal tolerable ; así se consigue en poco tiempo un adelgazamiento importante,progresivo y que se estabiliza espontáneamente a partir del tercer año posoperatorio.
Este tipo de cirugía nació,curiosamente,de las observaciones realizadas en numerosos pacientes sometidos a resecciones masivas del intestino delgado compatibles con su supervivencia ; en estas personas (heridos de guerra,lesionados por traumatismos de todo tipo,etcétera) los síndromes de mala absorción originaban un grado intenso de adelgazamiento.
La idea de aplicar,voluntariamente esta vez,una técnica similar para curar a los obesos se abrió camino rápidamente... El doctor Richard L. Varco,de la Universidad de Minnesota,fue el primero que realizó una operación de cirugía antiobesidad en 1953,mediante by-pass del intestino delgado.
En la actualidad,esta técnica,denominada científicamente by-pass yeyuno ileal,se ha generalizado notablemente,estimándose en unas 100.000 las operaciones llevadas a cabo sólo en los Estados Unidos.
Los resultados son positivos: reducción del peso en más de un 35 por 100,descenso de los niveles de colesterol sérico,mejora de las funciones vitales del paciente,etcétera.
No obstante,la mortalidad de estas operaciones es elevada,entre un 4 y 8 por 100,y además suelen aparecer complicaciones médicas y quirúrgicas que han dado lugar a nuevas mejoras técnicas con fundamentos fisiopatológicos diferentes,como,por ejemplo,el by-pass gástrico en sus diversas variantes.
De todos modos,al margen de su utilidad para el tratamiento de la obesidad extrema,esta cirugía se ha revelado como un modelo excepcional para el estudio de la adaptación intestinal y de sus diversos mecanismos de acción.
Y es que,tras el by-pass yeyuno ileal,tanto en el hombre como en los animales de experimentación se produce una hiperplasia (aumento de la superficie) de la mucosa del interior del intestino que sigue en funcionamiento,y una hipoplasia (disminución de la vellosidad intestinal) en la zona del intestino puenteada y que ya no se utiliza.
La hiperplasia se traduce por un aumento de la altura de las vellosidades y de las criptas de la mucosa intestinal ; a este nivel se observa un incremento del índice de producción celular.
Por el contrario,en la hipoplasia de la mucosa intestinal que ha dejado de funcionar,se han descrito cambios opuestos,es decir,una disminución de la altura de las vellosidades y de las criptas,con mucha menor proliferación celular en las mismas ; aunque esto sólo se ha sabido una vez que se ha utilizado microscopía electrónica de barrido.
En efecto,el estudio mediante scanner ha servido para esclarecer la controversia surgida a la hora de medir la altura de estas vellosidades.
Gracias al microscopio tridimensional se han detectado cambios que antes no aparecían con esa misma claridad y definición,lo que ha permitido deducir la estructura real de la mucosa intestinal en plena actividad o,por el contrario,en reposo.
Por ejemplo,y las fotos que aparecen en estas páginas ofrecen suficiente información al respecto,las vellosidades del yeyuno y del íleon en una rata de experimentación tienen forma de pliegues contorneados o foliácea ; en cambio,en los tramos intestinales apartados del tránsito de alimentos,las vellosidades adquieren morfología digitiforme,como las que se aprecian en recién nacidos.
Para ello es necesario el transporte más o menos orientado de ese polen hacia el pistilo de la propia flor.
En algunas especies se realiza este proceso autofecundador con las flores cerradas,lo cual imposibilita la salida del polen hacia el exterior.
La autopolinización es frecuente en plantas aisladas o que necesitan reproducirse muy deprisa por sus propios medios,sin poder contar con ayudas externas en forma de animales o elementos físicos transportadores.
Un buen ejemplo lo constituyen las plantas de países árticos o de zonas desérticas y numerosas malas hierbas,de rápida reproducción.
De todos modos,lo más corriente no es que la planta se autoabastezca,sino que la fecundación se haga con polen procedente de otra planta,a veces incluso muy alejada.
Esta segunda vía,quizá menos segura que la autopolinización,tiene la ventaja de permitir nuevas combinaciones genéticas,y se denomina polinización cruzada.
La planta necesita,en tal caso,de un agente intermediario que se encargue de llevar el polen de una flor a otra.
Normalmente son los insectos los responsables de este transporte ; tarea que realizan,por supuesto,sin enterarse,como engañados por las muy diversas trampas visuales o químicas que las flores utilizan.
Pero también otros animales,por ejemplo,las aves y los murciélagos,y desde luego el viento y otros elementos atmosféricos,realizan esa labor de fecundación a distancia.
En su proceso de evolución y selección natural,las flores de las plantas se han ido adaptando a las características de cada posible agente polinizador,originando asombrosas adaptaciones.
En el caso más común,el transporte es realizado por los insectos ; éstos,al recolectar el néctar,quedan inevitablemente impregnados de polen.
Luego,en sucesivas visitas a otras flores,vuelven a entrar en contacto con los órganos reproductores de la planta,dejando el polen adherido a la zona del pistilo de la flor especializada en su recepción,el estigma,cuya superficie viscosa facilita la adherencia de los granos microscópicos.
Uno de los insectos que más decisivamente interviene en esta polinización entomógama es la abeja.
Ha sido científicamente probado que las abejas son capaces de reconocer olores,colores y contornos.
Por esta razón,generalmente visitan tan sólo flores de un cierto tipo,a las que identifican en seguida,prescindiendo de las demás a no ser que necesiten más alimento.
Las abejas suelen verse atraídas por los pétalos brillantes y vistosos,pero nunca si son de color rojo,ya que sus ojos tienen un espectro de visión algo diferente al humano,más desplazado hacia el violeta y el ultravioleta,pero en cambio insensible al infrarrojo y al rojo.
Diversos investigadores han demostrado,mediante técnicas fotográficas especiales,que las abejas son sensibles a ciertas marcas distintivas en el espectro del ultravioleta ; marcas que presentan a menudo las flores visitadas por estos insectos y que son invisibles para los humanos.
Algunas de las flores más evolucionadas,como las orquídeas,por ejemplo,han desarrollado sofisticadas trampas que obligan a la abeja a seguir en su visita una ruta de entrada y otra de salida,con vistas a asegurar su contacto con los estambres y el estigma.
En la complejidad de esta táctica de polinización llegan aún más lejos las orquídeas del género Ophrys,que mimetizan la forma y el color de la abeja o ciertas avispas con el fin de atraer a los machos.
Como éstos surgen en primavera antes que las hembras,coincidiendo con la floración de estas plantas,intentan copular con la flor con forma de hembra,desde luego infructuosamente ; una frustración del insecto que,sin embargo,le resulta sumamente útil a la orquídea,ya que la carga de polen que inevitablemente acumula el insecto será transferida a la siguiente flor que visite.
Este curioso fenómeno es conocido por los biólogos como peudocopulación,el insecto burlado,cabría añadir... Las mariposas también colaboran en la polinización de las flores,y lo hacen guiadas por una combinación de vista y olfato semejante a la que mueve a las abejas.
En este caso,las flores sí pueden ser rojas y naranJas,porque algunas mariposas distinguen perfectamente estos colores.
También las mariposas nocturnas polinizan ciertas flores,generalmente blancas o de color muy claro,visibles,por tanto,en la oscuridad nocturna ; estas flores seducen también a los insectos por el olor dulce y penetrante A diferencia de las abejas,que se introducen en la cavidad floral buscando el néctar,las mariposas sólo se posan superficialmente en ella extrayendo los líquidos con su aparato bucal chupador.
Por esta razón las flores que polinizan no tienen trampas ni plataformas especiales ni otros mecanismos internos más o menos sofisticados y destinados a propiciar el contacto entre el insecto y el polen que debe transportar.
Otros insectos,como los coleópteros (escarabajos,mariquitas,etc.),también realizan una función polinizadora.
Suelen alimentarse,en general,de savia,frutos y hojas,pero también acuden a las flores atraídos por su olor.
Su sentido del olfato está mucho más desarrollado que el de la vista,por lo que las flores que visitan son de colores apagados o blancas,pero tienen en cambio un fuerte olor,muy distinto del aroma dulce de las flores polinizadas por abejas y mariposas.
Un caso bien curioso es el de algunas especies de flores que imitan el olor del estiércol,a veces incluso el color,con el fin de atraer a las moscas que habitualmente ponen sus huevos en ese medio.
Algunas especies vegetales sin flores,como ciertos hongos (Phallus impudicus y otros) utilizan el mismo artificio con el fin de difundir sus esporas reproductoras.
El nauseabundo aroma no parece ser un obstáculo.
Como ya apuntábamos al principio,no sólo los insectos realizan esta labor auxiliar en la reproducción de las plantas superiores.
Otros animales también colaboran en la polinización llamada zoógama.
Ciertas aves suelen dirigirse a las flores rojas que no polinizan las abejas.
Los pájaros,en general,no suelen verse atraídos por el olor,al que son poco sensibles,pero sí tienen en ellos mucha influencia las formas y los colores de las flores.
En cambio,otros animales prácticamente insensibles al color,como los murciélagos,se basan casi exclusivamente en el olfato.
Y resultan igual de eficaces de cara al transporte del polen floral.
Cuando se trata de atraer a los posibles polinizadores,las flores barajan,como hemos visto,múltiples artificios basados en los colores,las formas o los olores.
Pero si se trata de utilizar a los agentes atmosféricos,esencialmente el viento,la estrategia debe necesariamente cambiar,sustituyéndose el aspecto atractivo de la flor por una estructura más funcional.
Por una parte,la planta producirá mucho más polen,como única forma de superar el tremendo desperdicio inherente al método ; así,por ejemplo,la flor del avellano es capaz de producir dos millones y medio de granos de polen por cada óvulo potencialmente fecundable.
Hasta hace poco más de un siglo no fueron utilizados los primeros anestésicos inhalados ; se trataba de substancias volátiles,como el éter,el cloroformo o el óxido nitroso,que producían un estado de inconsciencia reversible acompañado,según las dosis y el producto,de relajación muscular.
Más adelante,nuevos productos se han ido incorporando para ser inhalados o administrados por otras vías (intravenosa,intrarraquídea,intrarrectal...) ; incluso se han conseguido sustancias que sólo actúan a nivel local,eliminando temporalmente la sensibilidad dolorosa de una parte del cuerpo.
El primero de estos anestésicos locales fue la cocaína.
Hoy día,los equipos de cirugía cuentan con médicos especializados cuyo trabajo consiste en vigilar el nivel de consciencia del enfermo y su posterior recuperación.
CUANDO nos referimos al dolor la primera dificultad que surge es puramente conceptual.
¿Qué es el dolor? ¿Cómo podemos comparar unos dolores a otros,el dolor de una persona y el de otra que aparentemente sufre el mismo mal? Antes de morir,el arzobispo de París cardenal Veuillot,exclamó " La religión nos ha enseñado muy bellas frases sobre el dolor ; yo mismo me he expresado así.
Pues bien,ahora os digo que lo mejor es callarse... " El cardenal murió de un cáncer que le hizo padecer en sus últimos estadios dolores insufribles.
Y es que,ante un hombre que sufre,no valen los razonamientos ni los consuelos morales o espirituales.
Sólo cabe intentar aliviar su dolor,sin más.
Sin embargo,la medicina se muestra reacia a considerar el dolor más que como un simple síntoma,de secundaria importancia frente a muchos males más complejos.
De hecho,si en los Estados Unidos se cuentan por centenares las clínicas especializadas en el tema dolor,en nuestro país sólo existe una unidad específicamente dedicada al problema,en la Residencia Sanitaria Primero de Octubre,de Madrid.
Su director,el doctor José Luis Madrid (vease la entrevista que realiza Carmen Mariño en este mismo dossier),ha afirmado públicamente que los medios que existen para aliviar el dolor no se emplean,o se utilizan inadecuadamente.
Sin olvidar,subsidiariamente,el elevado coste que este síntoma,infravalorado por muchos especialistas,le supone a la comunidad social.
El doctor Bonika,especialista norteamericano en este tema,afirmó en el pasado otoño,en el Congreso mundial sobre el dolor celebrado en los Estados Unidos,que el dolor crónico le cuesta más a la sociedad que el cáncer y las enfermedades del corazón,e incapacita a más personas que esas dos graves enfermedades juntas.
Sólo para intentar atajar el dolor se emplean,en los Estados Unidos,casi 15 billones de pesetas anuales.
..
Antes de seguir adelante,parece indispensable definir lo que es el dolor,para qué sirve.
Sin duda,se trata de un sistema de alarma: cuando acercamos el dedo a la llama,el dolor de la incipiente quemadura nos obliga a retirar la mano,impidiendo daños mayores.
El sufrimiento también constituye un buen elemento de diagnóstico en numerosas enfermedades.
Sin embargo,¿por qué se da a luz con dolor? ¿Por qué millones de personas sufren dolores crónicos que ni avisan ni son síntoma de nada? ¿Por qué,incluso,el dolor aparece en algunas enfermedades cuando ya es tarde para tomar medidas,como en el caso del cáncer,por ejemplo? ¿Por qué más de la mitad de los amputados les duelen sus miembros... inexistentes? Muchos dolores insoportables llevan al suicidio.
Parece como si pudiese preservar nuestra integridad cuando funciona como sistema de alarma,pero también puede llevarnos a la desesperación y a la muerte cuando se desmanda incontroladamente.
Normalmente se conocen dos grandes grupos de dolores clásicos,el externo,típico de un martillazo sobre el dedo,y el de origen interno,propio de las enfermedades de todo tipo,desde el reúma al cáncer.
Además,los especialistas hablan de otros dos tipos de dolores más anómalos: el aberrante,producido por una lesión de algún haz nervioso,que no tiene causa conocida ni solución aparente ; y el psicológico,de origen puramente imaginario pero no menos real ni molesto que el martillazo en el dedo.
La clasificación del dolor es importante de cara a su tratamiento.
Máxime ahora que la medicina posee ya poderosas armas cuyo funcionamiento no siempre se conoce con detalle,pero que resultan sin duda alguna eficaces en un número creciente de casos.
En 1799,Humphrey Davy,joven químico inglés,publica un artículo en el que describe las propiedades anestésicas del protóxido de nitrógeno,generalmente llamado gas hilarante.
El artículo no pasa desapercibido,pero la ciencia considera el tema como una simple curiosidad más.
Hay que esperar medio siglo para que,en 1844,el dentista Horace Wills utilice ese mismo gas como anestésico para sus pacientes.
Hasta entonces,en todos los campos de batalla,y en todos los hospitales del mundo,las operaciones se realizaban en vivo,confortando al enfermo con alcohol o con opiáceos.
Los cirujanos militares del siglo XIX,conscientes del choque que suponía para los heridos una operación en esas circunstancias,habían desarrollado técnicas eficaces y rapidísimas ; se cuenta que un cirujano de Napoleón,Dominique Larrey,era capaz de amputar una pierna en 22 segundos,incluyendo la ligadura de la arteria femoral... En los últimos treinta años,los progresos de la biología,la química y la física han transformado profundamente el tratamiento del dolor.
De cara al diagnóstico de muchas enfermedades,ya no es necesario sufrir,como a principios de siglo,con sondas y aparatos de análisis y auscultación que más parecían instrumentos de tortura que otra cosa.
Hoy,gracias a las fibras ópticas,es posible llegar al interior del cuerpo humano por sus conductos naturales,asociando a tales microsondas toda clase de aparatos miniaturizados y sumamente útiles: minijeringas,micropinzas,laser... Incluso se piensa en introducir endoscopios de láser en las grandes arterias con el fin de destruir,con la radiación,los depósitos arterioescleróticos que impiden la circulación sanguínea.
Y todo ello,sin dolor ni prácticamente molestias.
Paralelamente,los progresos en la anestesia y la reanimación permiten realizar operaciones quirúrgicas cada vez más complejas y seguras,con el máximo confort posible para el paciente y para el especialista que efectúa tan delicado trabajo.
Existen en el mercado aparatos de electroestimulación sumamente eficaces para eliminar los dolores de cicatrización.
La anestesia peridural,mediante inyección en la base de la médula espinal,suprime los dolores del parto sin alterar la consciencia de la futura madre.
La visita al dentista cada vez es menos temible a la vista de los aparatos sofisticados y el arsenal químico que la ciencia pone hoy al alcance de estos especialistas.
El empleo del láser contra toda clase de dolencias en los puntos conocidos por los acupuntores elimina muchos dolores reacios a medicamentos no siempre baratos ni inofensivos.
Ciertos aparatos mecánicos intentan,incluso,puentear directamente el nervio implicado en ciertos dolores de espalda,riñones o Piernas.
Los tres sistemas utilizados en la exploración de recursos petrolíferos submarinos obedecen a condiciones particulares de cada caso.
El buque de posicionamiento dinámico se mantiene en su sitio gracias a las hélices delanteras y traseras orientables,coordinadas por una central informatizada.
Se trata de un buen sistema para mares difíciles,en los que el barco no puede ser anclado al fondo.
Su coste puede llegar a rebasar los 30 millones de dólares (más de 5.000 millones de pesetas).
Las plataformas autoelevadoras poseen patas metálicas sobre las cuales puede desplazarse en vertical gracias a unos motores eléctricos ; son útiles en profundidades de hasta 100 metros y con fondos marinos muy irregulares.
Finalmente,las plataformas móviles semisumergidas reposan sobre unos enormes flotadores en inmersión a más de 20 metros de profundidad.
Para su estabilidad,el sistema está sujeto por un cable al fondo marino.
El incremento de las necesidades energéticas,aún antes de las crisis del petróleo en 1973,impulsó la explotación de reservas submarinas en mares más profundos y mucho menos tranquilos (mar del Norte,península del Labrador) ; una explotación que se ha intensificado y sofisticado hasta extremos inverosímiles después de la crisis energética de hace doce años.
Hoy en día,millares de plataformas pueblan los mares del mundo entero proporcionando la quinta parte del petróleo total producido,que será muy probablemente la tercera parte dentro de cinco años.
Para llegar la situación actual,y a esa situación futura que acabamos de citar,ha sido preciso resolver dos grandes grupos de problemas referidos,el primero,a la explotación de las reservas del fondo del mar,y el segundo,a la explotación comercial de esas reservas,una vez evaluadas sus capacidades productivas.
Por lo que respecta a la exploración,los geofísicos utilizan tres métodos,ya clásicos,para confirmar sus hipótesis sobre la presencia o no de hidrocarburos bajo el fondo marino.
La magnetometría aerotransportada permite medir las variaciones del campo magnético terrestre,detectando zonas en las que las concentraciones de imanes son mayores ; entre ellas,las bolsas de petróleo.
La gravimetría permite medir las variaciones del campo gravitatorio,que también cambia ligeramente según la densidad de las rocas del subsuelo.
Finalmente,la sismología inducida utiliza pequeñas explosiones controladas que producen vibraciones sísmicas,cuyo estudio permite identificar,con mayor o menor precisión,las características de los materiales de la corteza del fondo del mar.
Con todo,y a pesar de estos y otros métodos complementarios utilizados por los prospectores,nunca hay seguridad plena de la existencia de petróleo.
Y se hace imprescindible la realización de pozos de pequeño diámetro allí donde las pruebas dieron un porcentaje más elevado de probabilidades.
Estos primeros sondeos tienen escaso diámetro,no más de 40 centrímetros,y pueden alcanzar gran profundidad,hasta tres y cuatro kilómetros.
Si el petróleo hace acto de presencia,se perforan entonces los pozos de confirmación,a cierta distancia del primero,con el fin de delimitar la extensión del yacimiento e incluso su posible productividad.
De cara a la explotación de los recursos petrolíferos del fondo del mar,las plataformas metálicas enrejadas fueron las primeras en ser empleadas,hasta profundidades de 60 metros.
Las plataformas metálicas reforzadas agruparon las funciones de varias de las anteriores,para lo cual tuvieron que alcanzar considerable tamaño con peso que puede rozar las 100.000 toneladas.
Aún más grandes son las plataformas gravitatorias de hormigón,utilizables en mares muy agitados y a profundidades de hasta 150 metros ; su propio peso las mantiene ancladas en el fondo del mar,gracias a un enorme lastre de hormigón.
Estos mastodontes muy utilizados en el mar del Norte,llegan a alcanzar el millón de toneladas.
Finalmente las torres de obenques se dejan mecer por las olas y las corrientes,sin moverse demasiado gracias a un sistema de cables y anclaje similar al de los puentes colgantes.
Para llevar a cabo esta fase inicial de exploración se utilizan en conjunto tres tipos de sistemas: buques de posicionamiento dinámico,plataformas autoelevadoras y plataformas móviles semisumergidas (ver recuadro pág.
54).
Cuando ya se tiene la seguridad de que existe petróleo y de que su extracción presenta claros indicios de rentabilidad,se pasa a la fase de explotación.
En general,el principio de las plataformas petrolíferas para la extracción del oro negro submarino no difiere gran cosa del de las plataformas de exploración.
Pero la duración de sus trabajos,su mayor tamaño y su muy superior,peso total,exigen de ellas una construcción muy resistente al paso del tiempo y a los elementos atmosféricos y oceánicos.
Estas plataformas de explotación deben contar en su superficie con un puente capaz de soportar las cabezas de perforación,los equipos del pozo para producción y seguridad,el material necesario para el tratamiento de las impurezas del petróleo antes de mandarlo por oleoducto a tierra y,por supuesto,las zonas de alojamiento y descanso del personal de la instalación.
Las actuales plataformas flotantes difieren entre sí por su tamaño y,sobre todo,por el sistema empleado para mantenerlas en funcionamiento en la superficie del agua.
La tecnología de vanguardia utiliza hoy día tan sólo cuatro grandes grupos de plataformas extractoras.
Las más antiguas,aunque perfectamente rentables todavía en fondos no muy importantes (hasta sesenta metros de agua) son las plataformas metálicas enrejadas.
Su pequeño tamaño impide su utilización en mares profundos o bien en yacimientos de climatología adversa.
Estas plataformas metálicas,pero reforzadas y sensiblemente mayores forman el segundo grupo.
POCOS procesos biológicos presentan la belleza plástica del fenómeno de la coagulación.
Algo tan cotidiano y conocido como la formación de un coágulo al hacernos una herida representa,en realidad,una de las obras maestras de los seres vivos,llena de sutiles interacciones,de complejos mecanismos celulares,químicos,físicos y todo ello rodeado de una plasticidad que el microscopio electrónico ha venido a demostrarnos.
La sangre circula por nuestras arterias y nuestras venas sin interrupción,de una manera fluida,sin coagularse,manteniendo un equilibrio líquido perfecto entre los cuerpos celulares - - hematíes,leucocitos y plaquetas - - y los cuerpos químicos y moleculares de gran tamaño.
Pero basta un pequeño desequilibrio,una lesión en la pared de un vaso,para que se ponga en marcha toda una serie de reacciones en cadena que estaban como atrincheradas,esperando esta mínima lesión para manifestarse,para comenzar a actuar.
Pero veamos cuáles son los verdaderos responsables de esta eclosión del sistema estable.
En primer lugar,unas células,desprovistas de núcleo,de forma caprichosa,que se denominan plaquetas.
Se encuentran en un número aproximado de 150.000 a 400.000 por milímetro cúbico de sangre y tienen de dos a tres micras de diámetro.
Presentan tres zonas estructurales diferentes cuando las contemplamos al microscopio electrónico.
La zona más externa,formada por una cápsula de pequeño espesor,es la encargada de su poder de adhesión.
Esta constituye una de las propiedades más interesantes de las plaquetas: su tendencia a adherirse a cualquier superficie extraña.
Su zona interior o citoplasma posee una proteína contráctil muy parecida a la que existe en los músculos,la actomiosina,capaz de contraerse al recibir un estímulo desde el exterior,y,por último.
una zona más interna de gránulos y mitocondrias,responsable directamente de la secreción de sustancias químicas.
Estas células,fundamentales para la correcta coagulación de la sangre,se forman en la médula ósea y se mantienen en circulación durante un término medio de diez días,hasta su muerte.
El 80 por 100 se encuentran en circulación continua por el torrente sanguíneo,y el 20 por 100 restante,en el bazo.
Pero,naturalmente,la existencia de estas curiosas células no sería suficiente para desencadenar todo el proceso de la coagulación.
Veamos más de cerca y a cámara lenta los pasos que ocurren al producirse un corte,herida o ruptura en la pared de un vaso sanguíneo.
Rápidamente las plaquetas circulantes sufren un proceso de atracción hacia la zona lesionada,se acercan a sus bordes,se insinúan entre las fibras de colágeno lesionadas y se adhieren cada vez más fuertemente a ellas.
Se van formando acúmulos de plaquetas que comienzan a liberar una serie de complejas sustancias químicas,como la serotonina,neutralizadoras de la heparina y el difosfato de adenosina (ADP).
El ADP tiene la singular propiedad de causar la agregación de más y más plaquetas entre sí,para ir aumentando el tapón sobre la lesión del vaso.
No conocemos todos los factores que originan esta agregación de las plaquetas ; pero sabemos que en este mecanismo adquieren una gran importancia sustancias tales como las prostaglandinas,recientemente descubiertas.
Sea como fuera,una vez producida la agregación de las plaquetas,la misma superficie de éstas va a desencadenar los pasos siguientes,que llevarán a la verdadera formación del trombo y de la coagulación,la formación de la fibrina.
La fibrina es técnicamente un polímero fibrilar ; es decir,una intrincada tela de araña,una verdadera red de haces de fibrina que forma la estructura fija del coágulo,le da forma y lo mantiene el tiempo necesario.
Es el esqueleto químico de la coagulación.
Pero esta fibrina no está presente en el torrente circulatorio,pues sólo se forma cuando el proceso de coagulación se ha puesto en marcha por alguno de los mecanismos descritos.
¿De dónde proviene,cómo llega a formarse,qué factores desencadenan su producción? Se trata de uno de los capítulos que más estudios ha producido en este intrincado mundo de la coagulación.
Un esquema que nos puede servir para entender este mecanismo sería el siguiente: el producto final de las reacciones químicas provocadas por la agregación plaquetaria es la trombina,una proteasa o enzima capaz de romper las cadenas de proteínas.
Este potente destructor se dirige sobre unas grandes moléculas tridimensionales que circulan de forma fisiológica en la sangre,el fibrinógeno.
Al ponerse en contacto,la trombina comienza a digerir la gran molécula de fibrinógeno,la rompe y altera su estructura ; en términos técnicos,la polimeriza,es decir,se forma la fibrina.
El esquema de esta página nos da una idea de estos pasos.
Pero este proceso requiere una serie de reacciones químicas en cadena,que van actuando sobre los denominados factores de la coagulación,la mayoría de ellos formados en el hígado y dependientes de la vitamina K. Estos factores,que han sido numerados del I al XIII,han de ponerse en marcha en cada proceso de coagulación.
La ausencia o déficit de algunos de ellos pueden ocasionar profundos trastornos,impidiendo la puesta en marcha de toda la cadena de reacciones que dan origen a la coagulación.
Así,la enfermedad conocida por el nombre de hemofilia dependerá de una carencia del factor VIII o IX de la coagulación.
Las deficiencias del factor XIII o del factor VII originan asimismo enfermedades hereditarias,en las que no llega a producirse la formación del trombo ante cualquier tipo de agresión sobre los vasos sanguíneos,abocando al paciente a una muerte por hemorragia incoercible.
Pero la formación del coágulo no va a durar demasiado tiempo,ya que se trata sólo de una solución transitoria a un problema transitorio y debe desaparecer una vez que desaparezca la causa que puso en marcha el proceso de coagulación ; por tanto,ha de existir un proceso paralelo al que formó el coágulo,que logre destruirlo en el momento oportuno.
Así,una nueva enzima,la plasmina,destruirá la delicada red de fibrina,disolviendo el coágulo y restableciendo de nuevo el orden primitivo de las cosas.
El profundo conocimiento de estos delicados y bellos mecanismos ha hecho posible muchos de los avances de la medicina.
La utilización de medicamentos anticoagulantes,como la heparina o los dicumarínicos,en aquellos casos en que el riesgo de formación de trombos sea demasiado elevado ; la aparición de medicamentos que interfieren la agregación de las plaquetas,como la aspirina o el dipiridamol ; la obtención de actores de la coagulación mediante técnicas de microbiología industrial y otros muchos descubrimientos,hace que la esperanza de solución para muchos de los enfermos afectos de coagulopatías llegue a ser una realidad en un futuro que no parece demasiado lejano.
Hace escasamente dos meses ha sido fabricado un anticuerpo puro a partir de la proteína identificada por el equipo de Nueva York.
Este anticuerpo reacciona con efectividad ante la proteína de la corteza de,al menos,una docena de cepas distintas al plasmodio,recogidas en zonas muy diferentes del planeta.
Lo que ahora queda por demostrar es que una vacuna dirigida contra la proteína que recubre al esporozoíto ofrezca a los humanos una protección eficaz y duradera contra el paludismo.
No hay que olvidar que sólo en estado de esporozoíto es vulnerable el plasmodio ; una vez infestada la sangre y el hígado,su transformación en merozoíto le pone a cubierto de ese ataque.
De todos modos,los investigadores se muestran muy esperanzados.
En el caso de un plasmodio que infesta habitualmente a los animales,el Plasmodium knowlesi,se ha demostrado la eficacia de la vacuna en ratones de laboratorio.
Hasta tal punto ha sido positiva la experiencia que ya ha sido prevista una primera campaña de vacunación para este mismo año.
Las vacunas contra el paludismo están ocupando ahora mismo a multitud de laboratorios.
La mayoría de ellas se basan en el método norteamericano de atacar al parásito en el momento de su inoculación mediante destrucción de la coraza de proteínas protectora del esporozoíto.
Pero otras vacunas,todavía en estado experimental,intentan atajar la enfermedad cuando ya se ha declarado,es decir,cuando el plasmodio se ha multiplicado y ha pasado a la fase de merozoíto.
Diversos antígenos de merozoítos ya han sido aislados,y han probado su eficacia para crear inmunidad en ratones ; en cuanto a la especie humana,todavía no hay pruebas concluyentes,pero se estudia la posibilidad de producir una vacuna de doble efecto,profiláctico y terapéutico,es decir,atacante del esporozoíto inicial y del merozoito final.
Otros sistemas de vacunación intentan atacar al parásito en su base sexual,es decir,en sus células reproductoras,los gametos.
Estos alcanzan su madurez en el estómago del mosquito anófeles una vez que este insecto se ha saciado de sangre contaminada.
En ese medio,los gametos se acoplan,dando lugar a la aparición de los esporozoitos,que transmitirán la infección cuando el mosquito le pique a una persona sana.
El sistema de vacunación que se persigue en este caso no protegerá a la persona vacunada,pero impedirá que la infección sea transmitida a otras personas,ya que la sangre humana que portará el mosquito contendrá gametos esterilizados.
No se cura la enfermedad del que la sufre,pero se impide el posterior contagio a personas sanas.
Con todo,todavía no es el momento de echar las campanas al vuelo.
Con la vacuna,aunque llegue a ser eficaz de forma absoluta,no se va a solucionar de forma inmediata el problema.
El coste de su obtención,las dificultades para su distribución a gran escala (recordemos que son más de 300 millones las personas afectadas),las múltiples y pequeñas guerras locales que impiden una eficaz acción sanitaria,los hormigueros humanos de los campos de refugiados en las más precarias condiciones de higiene,son otros tantos obstáculos que se suman a un escollo aún más importante: la inercia debida al desinterés de los países industrializados por un mal endémico que a ellos no les concierne.
De todos modos,ya decíamos al principio que el problema del paludismo no sólo no disminuía,sino que se iba agravando.
Al comienzo de la década de los sesenta,se desencadenó en todo el mundo una gran batalla antipalúdica.
En la India,donde las víctimas se contaban por decenas de millones,no hubo más de un millar de casos en 1965: en Sri Lanka,ex Ceilán,se contaron tan sólo 17 casos en 1963.
Pero la enfermedad resurgió con mayor fuerza poco después.
En Sri Lanka hubo en 1969 más de medio millón de casos,en una población de sólo 12 millones de personas.
Mientras,las víctimas de la India volvían a ser millonarias en cifras: casi siete millones en 1976.
En Africa tropical,con una estructura sanitaria aún más deficiente,la endemia se evalúa actualmente en 200 millones de personas.
El fracaso de la estrategia antipalúdica de hace unos años se debe a varias causas ; por una parte,el mosquito ha sabido desarrollar una defensa contra el DDT,utilizado masiva e indiscriminadamente en casi todo el mundo.
Por su parte,los parásitos del paludismo también se han hecho resistentes frente a los distintos medicamentos ; la quinina,la cloroquina y muchas otras drogas han ido perdiendo paulatinamente eficacia en el sureste asiático,después en Suramérica y en el Pacífico,y finalmente en la India,resto de Asia y Africa.
A pesar de las esperanzas depositadas en las vacunas,lo cierto es que nadie piensa por ahora en la erradicación total de la malaria,sino en su simple contención,para evitar la temida explosión palúdica incluso en el mundo occidental.
Hoy día no existen ya remedios eficaces,a pesar de la aparición de un nuevo compuesto,la mefloquina,que por ahora todavía no se enfrenta al problema de la resistencia del plasmodio ; aunque todo el mundo teme que también ocurra en este caso lo que anteriormente pasó con otros medicamentos.
La esperanza estriba,sin duda,en los distintos tipos de vacuna que parecen a punto de poder ser utilizados.
Quizá porque los países ricos se ven,por primera vez en la historia,amenazados por una enfermedad que hasta ahora era patrimonio casi exclusivo de los países más subdesarrollados.
NO habría alas sin aire.
Volar,en verdad,sólo vuela el viento.
Las aves,los insectos,los murciélagos,muy pocos peces y ciertos objetos inertes surcan el espacio cuando crean corrientes de aire o se dejan arrastrar por las ya originadas en la atmósfera.
Mucho antes de que cualquiera de las actuales especies aladas se desplazaran libremente por los cielos,muchos animales viajaron volando.
En la mayoría de los casos eran transportes forzosos,con dos grandes categorías.
Por un lado,los sin peso y blandos,es decir,infinidad de insectos,eran arrancados de los sustratos sólidos por vendavales y llevados a otros puntos.
Otros utilizaron vehículos.
Las arañas,por ejemplo,usaron,y usan,las sedas que segregan como enorme vela.
A tan poco conocida capacidad,por cierto,se debe la colonización de todos los rincones de este mundo llevada a cabo por los arácnidos.
Sin olvidar,claro,a los numerosísimos seres vivos que ocasionalmente viajan a bordo de hojas y otros objetos susceptibles de ser empujados por el viento.
Pero queremos encarar el vuelo voluntario como función biológica desde la certeza de que su génesis,desarrollo y posibilidades resultan tan inabarcables como apasionantes y atractivas.
No en vano siete de cada diez científicos de la naturaleza o aficionados a su observación,centran sus estudios en las aves.
Tan escueto como ilustrativo dato refrenda la fascinación que produce el ver volar.
Yendo al principio,parece fuera de toda duda que lo primero fue el salto.
No podemos olvidar,al respecto,la prodigiosa capacidad de algunos diminutos pobladores de este mundo como las pulgas,que,si tuvieran nuestro tamaño,salvarían distancias de hasta 200 metros de longitud con un solo brinco que,en los momentos culminantes de su trayectoria,las eleva hasta el equivalente a 120 metros de altura.
Casi igualmente dotados inventores del muelle,y los bien conocidos saltamontes y no pocas arañas.
De la necesidad de estabilizar y dirigir esos saltos prodigiosos nacen las alas.
Y nunca mejor dicho,porque los primeros planos de sustentación conocidos surgen literalmente de la nada,como excrecencias del tórax de algunos insectos y no como transformación de unas extremidades previas,caso de las aves y mamíferos voladores.
No está de más recordar que son más numerosas las especies vivientes dotadas de la capacidad de vuelo que las sujetas a permanecer invariablemente en contacto con el suelo.
Pues bien,las 300.000 mariposas diferentes,el casi medio millón de coleópteros,las 100.000 moscas y mosquitos,las 50.000 especies de abejas y avispas,los 15.000 saltamontes,las 5.000 Iibélulas,las 9.000 aves y los 670 especies de murciélagos demuestran,con su apabullante diversidad,que lo más rentable es volar.
- La aventura fue iniciada por seres muy semejantes a las actuales libélulas hace unos 300 millones de años.
Tal vez por su evidente veteranía los odonatos pueden ser considerados como los más eficaces voladores existentes.
Tienen dos pares de alas que,al actuar independientemente entre sí,permiten todas las maniobras posibles: subir,bajar,avanzar e,incluso,retroceder.
Sin olvidar,por supuesto,su capacidad para permanecer en el mismo punto del espacio como el más perfecto de los helicópteros ; los propios diseñadores de artefactos voladores reconocen abiertamente que ningún logro de la aeronáutica ha superado todavía a las viejas libélulas,capaces,por otra parte,de aproximarse a los 100 kilómetros por hora cuando escapan de sus enemigos naturales.
Es más,la aviación habría surgido mucho antes de no haberse prestado tanta atención a los pájaros como modelo a imitar,en lugar de fijarla en las libélulas.
En la escala del perfeccionismo aerodinámico debemos situar a continuación a las mariposas,un grupo también caracterizado por la posesión de cuatro alas,mientras que el resto de los voladores se contentan con un solo par.
Las gráciles y aterciopelados planos de sustentación de los lepidópteros permiten igualmente un buen número de proezas aéreas,por mucho que aparentemente puedan ser consideradas como lo más delicado que existe.
Baste recordar que las mariposas más lentas avanzan a 30 kilómetros por hora,batiendo sus alas unas diez veces por segundo.
Nada si lo comparamos con las nocturnas esfinges,que,con hasta 70 batidos por segundo,superan los 60 kilómetros por hora.
En cualquier caso,las mariposas pueden ser consideradas lentas frente a los dípteros.
En efecto,muchos mosquitos,y de ahí el aviolinado sonido que producen,alcanzan un ritmo propulsor comprendido entre los 600 y los 1.000 golpes de ala por segundo.
Hasta llegar a los hemípteros,que,sin ser tan raudos,pero no quedando a la zaga,son los voladores por excelencia al serles el vuelo más necesario que a ningún otro grupo zoológico.
Y tanto es así que podría ser admisible la afirmación de que,si las avispas y abejas perdieran sus alas,serían borradas del planeta en pocas horas.
Tengamos presente,al respecto,que la mayoría de los insectos voladores no lo son durante largos períodos de sus ciclos vitales.
Incluso los pájaros pueden vivir sin volar.
Y no diganos los murciélagos.
Las grandes sociedades de las productoras de miel no pueden prescindir del vuelo,entre otras cosas porque deben visitar sus fuentes de alimento con tal rapidez y eficacia que cualquier otro sistema de locomoción resultaría no rentable.
Y presidiéndolo todo,los vertebrados voladores,ese prodigioso plantel de animales que,tras modificar sus extremidades delanteras,vencen su propio peso y surcan los aires.
Los murciélagos partieron de membranas progresivamente más amplias para planear primero de árbol a árbol y luego ampliar el radio de sus desplazamientos aéreos,hasta llegar a su actual capacidad de recorrer miles de kilómetros y orientarse mediante un radar interno.
Pero son,sin duda,los pájaros quienes acaparan nuestros ojos cuando miramos al cielo.
Tienen todas las formas ; ponen en práctica un sinfín de técnicas de vuelo diferentes ; vuelan a todas las alturas imaginables y,literalmente,con todos los grados de aceleración plausibles y recorriendo las distancias más increibles.
Tan amplio catálogo de singularidades bien merece que nos ocupemos,en nuestro próximo capitulo,de las aves como máquinas voladoras.
EL jacinto de Compostela es un mineral español envuelto en los misterios de la Edad Media.
Su nombre,reconocido internacionalmente,se presta a todo tipo de confusiones: por su naturaleza y localización,está ausente de las tierras gallegas y,sin embargo,se trata de una joya vinculada a los peregrinos que recorrían el camino de Santiago ; desde ese foco de religión y cultura se extendió por todo el mundo entonces conocido.
No se puede afirmar a ciencia cierta si era llevado allí desde otras regiones españolas para su venta como recuerdo o souvenir o bien fueron los propios peregrinos los que,recorriendo determinadas partes del Camino donde se encuentra y aprovechando una costumbre tradicional,lo recogían para su posterior venta.
Pero es seguro que el jacinto de Compostela fue el mineral característico de ese importantísimo fenómeno sociológico del culto compostelano.
Como casi siempre,los minerales,y éste en particular,encierran,además de sus propias cualidades científicas,estéticas y económicas,un trasfondo histórico y cultural,cuyo conocimiento supone para el aficionado un motivo más de satisfacción y disfrute ante su estudio.
Pero,¿qué es el jacinto de Compostela desde el punto de vista mineralógico? Pues esta hermosa joya de nuestros yacimientos es un cuarzo,un cristal de roca de una variedad muy original y tan específica de nuestro país,que incluso algunos autores lo han clasificado como un fósil característico del triósico español.
En cuanto a su composición,el jacinto de Compostela es un óxido de silicio coloreado por la presencia en el interior de la masa del cristal de abundantísimas inclusiones de hematite (óxido de hierro) que dan a los cristales de cuarzo una tonalidad uniforme de hermosos tonos rojizos.
Como cristal,lo es en la forma típica de los cuarzos con un hábito muy complejo,resultante de la combinación de tres formas simples,un prisma hexagonal y dos romboedros,directo e inverso,lo que origina la aparición de prismas hexagonales bipiramidados.
Nuestro jacinto de Compostela aparece así como un exponente típico de cristalización mineralógica.
Las caras prismáticas de aristas perfectas dan a muchos de los cristales un aspecto y una geometría que parece tallada artificialmente,dada su perfección.
Como buen cuarzo,es muy frecuente su presencia en forma de cristales maclados.
Cuando el jacinto de Compostela se macla,lo hace en ocasiones hasta límites increíbles,originando verdaderas piñas de cristales.
En cuanto a su localización,pueden encontrarse magníficos ejemplares y a veces muy abundantes entre las arcillas margosas de los terrenos triósicos españoles de Aragón y de Castilla.
Una antiquísima zona de producción que sigue proporcionando las mejores piezas es la comarca natural de Buñol (Valencia) y Minglanilla (Cuenca).
En resumen,el jacinto de Compostela constituye otro de los grandes minerales españoles específico de nuestras tierras.
Descubre al estudioso las características en cuanto a formación y cristalización del maravilloso mundo de los cuarzos y,además,encierra una larga y hermosa tradición cultural y religiosa y presenta una indudable belleza.
Todo ello lo convierte en un pequeño tesoro al alcance del coleccionista español.
Es muy solicitado por los aficionados extranjeros (no en vano fue nuestro primer " recuerdo " turístico).
Se emplea también y con enorme dignidad como piedra preciosa en la joyería moderna.
l Hasta ahora,sólo Francia y Alemania tienen previsto lanzar un satélite dedicado exclusivamente a la difusión de televisión desde el Espacio.
Ambos países pusieron en marcha,hace ya más de seis años,un proyecto común para realizar dos satélites,el TV-SAT y el TDF - 1,que cubrirían respectivamente el territorio alemán y el francés.
Cada uno emitiría por tres canales,ampliables posteriormente a cinco.
El área abarcada por el TDF - 1 francés incluiría a buena parte del territorio español,desde el que,con una antena adecuada,será posible ver directamente esos canales de la televisión francesa,aún no se sabe si privada o pública.
El proyecto sufrió retrasos,pero.
parece seguro ya que estos dos satélites de difusión directa de televisión serán lanzados a finales de este año.
Lo cual significa que en España podremos,el año que viene,ver la televisión francesa sin más que instalar la adecuada antena en el tejado.
Una antena que seguramente será muy cara,pero cuyo precio resultará más asequible si se instala de forma comunitaria.
En Japón hace ya un año que funciona un satélite de difusión directa de televisión,el BS - 2 A,que transmite dos canales en color a todo el archipiélago nipón.
Y el coloso norteamericano,cuyo desinterés inicial por estas cuestiones parecía explicable vista la extensión considerable del territorio a abarcar,parece que está rectificando su postura inicial: ya tiene en marcha nada menos que catorce proyectos,cada uno de ellos con varios satélites de televisión directa,tanto de cadenas privadas como públicas o institucionales.
Volviendo a Europa,Luxemburgo,Italia y los países escandinavos también están encarando proyectos de televisión por satélite directo.
El caso de Italia resulta especialmente interesante para España,porque con antenas especiales podríamos ver sus emisiones y no hay que olvidar que Italia posee la mayor densidad europea de emisoras privadas de televisión.
Los satélites nacionales de difusión directa de televisión no rompen el monopolio de la organización Eutelsat,ya que ningún satélite de telecomunicaciones europeo tiene previsto este servicio de forma exclusiva.
Es evidente que,a corto plazo,todos los países europeos,y muy especialmente los del Mercado Común,por razones económicas evidentes,podrán tener su propio satélite.
Máxime si la industria privada ejerce su iniciativa también en este campo,limitado hasta el momento a los organismos estatales e internacionales.
De momento son ya dos los planes privados anunciados al respecto: uno luxemburgués y el otro escandinavo de origen sueco.
El proyecto GDL (Gran Ducado de Luxemburgo) pretende explotar,mediante una empresa privada llamada Coronet,los satélites artificiales para difusión directa y semidirecta de televisión,pero también quizá para servicios de telecomunicación entre empresas.
Por su parte,el proyecto sueco EBS-AB pertenece a un grupo de inversores nórdicos y pretende lanzar un satélite para empresas entre Gran Bretaña,Noruega y Suecia.
Para ello ha pedido permiso a las autoridades de los tres países e indirectamente a Eutelsat,puesto que necesita utilizar sus instalaciones terrestres.
Si el permiso es concedido,será la primera vez que el monopolio europeo en este campo se rompa y quizá un primer paso para la descomposición de la organización que vela por los intereses de todos sus países miembros.
En el fondo,los próximos años van a deparar una batalla sin precedentes.
Ahora mismo ya existen numerosos partidarios de tres sistemas diferentes.
El primero,una Europa unida en las telecomunicaciones,por medio de Eutelsat,con unificación de criterios y máxima economía en los costes.
El segundo,una Europa con proyectos nacionales independientes,en la que cada país gestiona como quiere o puede sus telecomunicaciones propias y con sus vecinos.
Y un tercer sistema,absolutamente libre,en el que todos (organismos internacionales,países miembros individualmente e incluso grandes y pequeñas empresas privadas) compitan por un mercado que crece cada vez más de prisa: el de las telecomunicaciones,televisión incluida.
En este sentido,España no ha adoptado formalmente ninguna postura.
De manera oficial,nuestra participación en la organización Eutelsat e incluso en la Agencia Europea del Espacio,encargada de lanzar los satélites,es bastante como para definir el presente: integración europea al máximo de posibilidades.
No faltan voces,sin embargo,que claman por la puesta en marcha urgente de un proyecto de satélite propio ; en principio,para difusión directa de televisión,pero quizá también,como medida rentabilizadora,para telecomunicaciones diversas.
En el primer caso,televisión sólo,el precio sería muy elevado,aunque no prohibitivo,con la ventaja adicional de poder transmitir con mayor número de canales,accediendo perfectamente a todo el territorio nacional sin necesidad de enlaces terrestres y sin esas sombras de recepción que todavía hoy impiden siquiera que el segundo canal de UHF sea visto en toda la geografía española.
En el segundo caso,televisión y telecomunicaciones,sería posible rentabilizar mejor la inversión,pero a costa de meterse de lleno en la batalla legal que están a punto de desencadenar los franceses con su Telecom.
No cabe duda de que,a pesar de que fuentes oficiales parecen descartarlo,la solución idónea para España seria el satélite para televisión directa.
AGRADEZCO la invitación para expresar brevemente mi opinión sobre la trascendencia social de la ciencia.
Hasta hace poco,y con excepción de algunos ejemplares,se ha caricaturizado al científico,motivo por el que la sociedad,en general,les ha ignorado y,lo que es peor,también ha ignorado la ciencia.
Todos sabemos que estamos en el momento de mayor impacto científico,en lo que podríamos llamar la Revolución Científica,y que el 90 % de todos los científicos que ha tenido el mundo y la historia viven en la actualidad.
La trascendencia social de la ciencia no es,pues,folclore,sino que se basa en la certeza de que la vida actual y futura depende y dependerá cada vez más de ella.
Y digo cada vez más porque los graves problemas actuales,sobre todo aquellos debidos al enorme aumento de población,con todo lo que ello conlleva,sólo podrán encontrar solución en la ciencia.
Como escribo estas líneas el día siguiente de la entrada de España en el Mercado Común - - tanto político de gran magnitud - -,no puedo dejar de comentar que ello tendrá gran importancia para el desarrollo económico,e industrial y,por tanto,científico del país.
Es fundamental recordar que la primera ley de la termodinámica - - en lenguaje corriente - - es que no hay nada gratis.
Y,por ello,no sólo habrá problemas inmediatos y reajustes,sino que también la entrada supone y supondrá una constante competencia y superación,lo que necesitará de un desarrollo y aumento de la investigación,quizá de un orden muy superior.
En Francia como en España han surgido grandes dificultades derivadas de la reestructuración industrial y la falta de empleo.
Sin embargo,el Gobierno francés y su ministro de Ciencia,Mr. Curien,creen que la investigación y el desarrollo constituyen,en efecto,las llaves para la modernización del país y la base sólida para la renovación industrial.
Yo creo que eso es igualmente cierto para España.
Ahora bien,aunque nos quejamos de Francia,y esto en algunas cosas - - lo comprendo,la verdad es que su presente Gobierno ha aumentado,en términos reales,la ayuda a la ciencia y al desarrollo desde un 1,85 % del producto nacional bruto,en 1980,al 2,25 % en este año.
Después de las distinguidas personas que me han precedido en esta columna,como son el ministro de Educación y el presidente del Consejo Superior de Investigaciones Científicas (CSIC),sólo puedo añadir que me alegrará mucho que las ideas que desarrollaron lleven a la concienciación del país hacia una mayor ayuda a la ciencia y que espero sigan luchando por aquellos ideales tal como expresan.
Pero me preocupa que el hacer organigramas y continuar con la centralización - - de la que no se desprenden ni el Ministerio de Educación ni el CSI mantenga aún muchos problemas.
Valga el caso de lo que dice Enrique Trillas cuando afirma que el Consejo Superior es un organismo autónomo,¡pero dependiente del Ministerio de Educación! Hasta que la Administración no se convenza de hechos reales tales como que las naranjas crecen en Valencia,y no en el Ministerio de Agricultura,y que la investigación se logra creando un clima propicio y no con organigramas restrictivos,la ciencia española tendrá problemas graves y grandes dificultades,por los que la sociedad se resentirá también.
La sociedad debe proteger la ciencia,y de esta manera protegerse a sí misma,que es,en definitiva,la única forma de que el país avance.
En Estados Unidos,el país en que se hace más investigación actualmente,las universidades son libres o estatales,en el sentido de que es cada Estado y sus representantes los que las controlan,protegen y exigen productividad a sus investigadores.
Está claro que para un aragonés lo que haga la universidad de Zaragoza tiene que ser mucho más interesante - - y estará dispuesto a pagarlo - - que lo que haga la de Badajoz,Santiago o París.
Y,desde luego,no veo razón por la que un aragonés deba pagar o preocuparse mucho por los problemas y curriculum de las universidades de Badajoz,Santiago o París,por poner un ejemplo.
Finalmente,estoy convencido de que la ciencia puede y debe ser el mejor pegamento para una indispensable cooperación europea,todavía frágil.
POR qué medio,incluso cabría decir por qué milagro,los innumerables granos de luz que penetran en nuestros ojos se transforman en impulsos nerviosos y en imágenes mentales dentro de nuestro cerebro? La respuesta a semejante pregunta no tiene fácil contestación.
Los mecanismos de la visión entrañan tal cantidad de fenómenos físicos,biológicos,bioquímicos y psicológicos,algunos todavía casi desconocidos,que resulta casi imposible comprenderlos en su totalidad.
En todo caso,lo que sí es cierto es que el ojo no es un simple espejo pasivo que reflejaría dentro de nosotros lo que sucede fuera.
Nuestros ojos están hechos de tejidos de origen cerebral (de hecho,el ojo se separa del cerebro durante el desarrollo del embrión) y se comportan,pues,como una verdadera estructura nerviosa activa.
Esta no sólo detecta y amplifica la luz,sino que realiza una primera selección entre las informaciones que recibe,antes de convertirlas en impulsos que serán transmitidos al cerebro por las fibras del nervio óptico.
En el origen de cualquier imagen se encuentran los fotones,esos granos de energía luminosa asociados a una onda electromagnética.
Existen diferentes tipos de fotones,ya que su energía es proporcional a la longitud de la onda asociada.
El ojo humano sólo es sensible a longitudes de onda situadas en una banda entre 380 y 750 nanómetros (milmillonésimas de metro),que corresponden a energías comprendidas entre 1,7 y 3,1 electrovoltios (eV).
Esta energía de los fotones es la que determina nuestra percepción de los colores ; así,1,7 eV engendra una sensación de rojo,mientras que 3,1 eV crea una sensación de violeta.
Las energías intermedias corresponden a los demás colores del arco iris.
Desde luego,la mayor parte de los objetos que vemos emiten o reflejan fotones con una mezcla de muy diversas energías.
Cuando la mezcla es uniforme,tenemos una sensación de blanco ; si no,habrá una dominancia de uno u otro color según la sensibilidad de nuestras células receptoras de los colores,los conos de la retina.
Existen tres categorías de conos,sensibles a cada uno de los tres colores primarios: rojo,verde y azul.
El cerebro establece después una relación cuantitativa entre las señales procedentes de los tres tipos de conos y,a partir de los diferentes valores de esta relación,elabora toda la gama de colores posible.
Pero subsiste el problema esencial: cómo un acontecimiento físico,la llegada de los fotones a la retina,puede cambiarse en influjo nervioso y luego en sensación de luz y color.
El mecanismo de esta fototransducción (conversión de la energía del fotón en estímulo nervioso) se estudia en los bastones,otro tipo de células de la retina,más numerosas que los conos.
Estos bastones son las células receptoras de la luz monocromática ; no son,pues,sensibles a los colores pero,en cambio,pueden percibir intensidades muy pequeñas de energía luminosa.
Un bastón es una célula nerviosa que comprende,como todas las neuronas,una membrana,un núcleo y dos prolongaciones: el receptor luminoso y el axón.
El receptor está constituido por el apilamiento de al menos un millar de discos que se renuevan constantemente al formarse un nuevo disco en la base cada veinte minutos.
Estos discos son como saquitos aplastados,cuyas paredes contienen una sustancia fotosensible: el pigmento visual,o rodopsina.
Es una molécula tan sensible a la luz que puede reaccionar ante el impacto de un solo fotón ; conviene recordar que la energía de un solo fotón es minúscula: una linterna emite alrededor de 1018 fotones (un trillón de fotones).
La rodopsina está constituida por un carotenoide,el retinal,conjugado con una proteína específica,la opsina.
El retinal es un derivado de la vitamina A ; en realidad,esta vitamina es la forma transportarle del retinal ; al perder dos átomos de hidrógeno en su molécula,la vitamina A se convierte en intransportable,en retinal.
Cuando miramos un objeto,nuestra lente particular,el cristalino,adapta su curvatura para que la imagen se proyecte exactamente sobre la retina: es el proceso de acomodación.
Los conos y bastones de la retina son los fotorreceptores ; en la fóvea,zona central de la imagen,las fibras ópticas van cada una a un solo cono (no hay bastones) ; de ahí la máxima nitidez de la imagen en ese punto privilegiado.
El impacto de un fotón sobre esta molécula modifica su estructura espacial,pero no su composición.
La molécula primitiva,cisretinal,se endereza y se transforma en transretinal.
Esta modificación puramente física de la molécula es el único efecto del fotón de luz en la visión.
Todo lo que ocurre posteriormente,a nivel químico,fisiológico o psicológico,no tiene ya nada que ver con la luz.
La primera consecuencia de la transformación del cisretinal en transretinal es una alteración de la molécula de rodopsina formada,recordémoslo,por retinal y opsina.
El cambio en la forma del retinal disloca la molécula de rodopsina y hace aparecer ciertos puntos muy reactivos que engendran una cascada de reacciones químicas ; éstas,al cabo de unos pocos milisegundos,crearán un potencial de acción,un impulso,que se propagará a lo largo del bastón hasta el extremo de su axón.
Una vez pasada la conmoción,el transretinal vuelve a su forma originaria de cisretinal,la rodopsina se vuelve normal,y de nuevo un fotón podrá reiniciar el proceso.
Al cabo de un cierto número de acontecimientos de este tipo,la rodopsina " usada " es sustituida mediante la renovación continua de los discos.
Lo cual,dicho sea de paso,justifica la tremenda importancia de la vitamina A para la conservación de una buena visión puesto que,no lo olvidemos,el retinal es,por así decirlo,vitamina A inmovilizada.
Lo que hemos llamado potencial de acción o impulso nervioso no es más que un cambio brusco del potencial eléctrico en la membrana celular del bastón,originado por una serie de reacciones químicas nacidas de la alteración de la rodopsina,éstas parecían deberse a la liberación de iones calcio que disminuyen la permeabilidad de la membrana al sodio que se encuentra en el exterior.
Como los iones sodio están cargados positivamente,el fenómeno entrañaría la aparición de ese potencial eléctrico o impulso nervioso,que se propagaría a lo largo del bastón hasta los botones sinápticos que se encuentran al final del axón.
La descripción de este proceso le valió al investigador norteamericano George Wald el premio Nobel de Medicina en 1967.
Sin embargo,en el curso de un importante simposio celebrado en Alemania el pasado mes de noviembre,dedicado a los mecanismos moleculares de la fotorrecepción,se ha aportado una nueva luz a la comprensión del complejo fenómeno de la visión.
A esta presión gaseosa hay que añadir la presión de radiación debida a los fotones liberados en las reacciones termonucleares que tienen lugar permanentemente en el corazón estelar.
A lo largo de su vida,la estrella se va adaptando para que el equilibrio se mantenga ; en este combate,la opacidad del astro juega un papel fundamental,ya que si el gas estelar es transparente,los fotones lo atravesarán sin problema y la energía se evadirá rápidamente.
Si,por el contrario,el gas es opaco,los fotones sufren numerosas colisiones antes de alcanzar la superficie y la energía tardará mucho en salir.
Lo cual significa que la opacidad de la materia controla el caudal energético y,por consiguiente,la luminosidad de la estrella.
El equilibrio vital se ve regulado por un mecanismo automático que impide embalarse al reactor estelar.
Pero cuanto más masa tiene la estrella,menos control es capaz de ejercer sobre sí misma.
Su presión de radiación compensa cada vez más difícilmente su fuerte gravedad,que es proporcional a su masa.
En 1959 se llegó incluso a calcular que una estrella de masa igual o superior a la de 60 soles sería inestable y estallaría.
Y hasta 1970,los astrónomos han negado la existencia de estrellas de ese tipo.
Pero algunos objetos celestes parecían salirse de la norma.
En algunas estrellas dobles,uno de los componentes parecía poseer una masa sospechosamente alta ; aunque es cierto que en nuestra galaxia,por culpa del polvo interestelar y de la difícil evaluación de las distancias,la luminosidad de las estrellas fue siempre objeto de discordia.
En 1960 fueron descubiertos en las dos Nubes de Magallanes (pequeñas galaxias vecinas de la Vía Láctea) objetos extremadamente luminosos.
Esta vez se conocía perfectamente la distancia de estas dos galaxias ; además,tienen muy poco polvo interestelar.
Y se pudieron apreciar estrellas de masa superior a la de 100 soles.
Los astrónomos,en aquella época y en años posteriores,no dieron su brazo a torcer ; simplemente modificaron el límite teórico de la posible existencia de estrellas de 60 a 100 masas solares.
Pero este artificio de cálculo sirvió de bien poco cuando se descubrieron,en la década de los setenta,estrellas de masa igual a mil soles.
Algunos de estos monstruos estelares tienen una acusada personalidad: su comportamiento proporcionó muy valiosas indicaciones acerca de la verdadera naturaleza de las estrellas superpesadas.
La más cercana a nosotros se encuentra en la constelación de Carina ; pertenece a una asociación de estrellas que se aloja en la zona hueca de uno de los brazos espirales más vigorosos de nuestra galaxia.
Esta superestrella se llama Eta Carinae.
Como cualquier star cinematográfica,Eta Carinae está rodeada de una cohorte de starlets secundarias casi tan brillantes como ella.
Seis de ellas son incluso de un tipo muy especial,del que sólo se conoce una decena en todo el Universo: el tipo 03,estrellas extremadamente cálidas,pero muy poco evolucionadas.
De esas seis,la más brillante es la HD93.
129 A,cinco millones de veces más brillante que nuestro Sol y rival de Eta Carinae.
De hecho,son las dos estrellas más luminosas que se conocen en el Universo... Pero la más célebre,por haber sido la primera,sigue siendo Eta Carinae.
Su carrera fulgurante comenzó a mediados del siglo pasado.
Antes de 1830 no tenemos referencias de ella,pero como sólo es visible desde el hemisferio austral de la Tierra,eso no es raro.
Entre 1836 y 1858 presentó variaciones asombrosas y duraderas de su brillo.
En 1843 llegó incluso a ser la segunda estrella más brillante del firmamento.
Más tarde,ya hacia 1870,su brillo se atenuó mucho.
Hasta que,en 1940,comenzó de nuevo a refulgir ; su luminosidad no cesa de aumentar desde entonces.
Hoy día la vemos rodeada de una pequeña nebulosa de gas y polvo,una especie de velo que cubre púdicamente su cuerpo gigantesco.
Este chal celeste ondea en el espacio a la velocidad de varios centenares de kilómetros por segundo y sin duda está compuesto por masa estelar proyectada hacia el exterior por la propia estrella.
La masa total del velo puede muy bien ser similar a la masa de nuestro Sol... Y,a juzgar por la multitud de nebulosas que la rodean,las crisis de histeria explosiva de Eta Carinae no son nada infrecuentes.
Esta estrella supergigante y su colega HD93.
129 A nacieron en la misma región del cielo.
Son muy parecidas en brillo y masa,pero el espectro de Eta Carinae revela que es mucho más fría que la otra,con la mitad de temperatura superficial ; sólo unos 30.000 grados (el Sol tiene unos 6.000).
Y además es mucho más inestable.
Probablemente estas diferencias se deben a que se encuentran en grados de evolución diferentes.
Eta Carinae pudiera muy bien ser excesivamente joven,con un corazón ardiente que todavía no ha tenido tiempo de recalentar su superficie o,por el contrario,muy vieja,con un pie en la tumba,al borde del enfriamiento final.
Esta segunda hipótesis parece más probable por la riqueza en nitrógeno de sus gases exteriores.
El nitrógeno procede de las entrañas de la estrella,lo cual significa que ésta no sólo ha tenido tiempo de fabricarlo,sino de sacarlo hacia el exterior y expulsarlo.
En cuanto a su inestabilidad,la verdad es que no se entiende muy bien por qué se produce ; quizá la existencia próxima de un invisible compañero cuyo roce provocara violentos efectos de marea gravitatoria... En todo caso,Eta Carinae ha perdido en unos cuantos siglos gran parte de su masa.
En promedio,viene a perder entre una milésima y una décima de masa solar cada año ; a este ritmo,la mayor parte de su masa se habrá dispersado antes de 100.000 años.
Una cifra que parece enorme,pero que al lado de los muchos centenares e incluso miles de millones de años que viven otras estrellas resulta casi minúscula.
Cygni es otra superestrella de nuestra galaxia.
Fue descrita en el siglo XVII y su brillo no parece haber variado.
Hoy,su luminosidad es un millón de veces mayor que la del Sol y su temperatura apenas alcanza 20.000 grados en la superficie.
Cygni es parecida a Eta Carinae,pero más fría,menos pesada y menos luminosa.
Supergigante,sí,pero de segunda división.
En cambio,en otras galaxias sí existen rivales de consideración.
Por ejemplo,en la Gran Nube de Magallanes,en el corazón de la nebulosa gigante 30 Doradus (nebulosa de la Tarántula),encontramos un objeto celeste superpesado llamado Radcliff 136 a (R136a).
Según recientes estimaciones,R136a es una estrella superpesada y muy caliente,con una temperatura superficial de 60.000 grados y entre diez y veinte veces más luminosa que Eta Carinae.
Su masa podría superar la de 2.000 soles.
Sin embargo,cabe preguntarse si no se trata de un conglomerado muy apretado de estrellas.
Lo cual parece posible si consideramos que el tamaño de R136a no puede superar 0,005 parsecs,es decir,mil veces la distancia Tierra-Sol.
Desde luego,en esa distancia cabrían como mucho diez estrellas,pero muy bien podría ser que R136a fuese un sistema doble o triple de estrellas supergigantes,pero menos.
En todo caso,hasta los más moderados piensan que se trata de una estrella única de masa comprendida entre 500 y 1.200 soles o de un sistema doble o triple,con masas individuales entre 200 y 1.000 soles.
En todos los casos,muy superiores a los 60 ó 100 del límite teórico.
Antes se pensaba que las reacciones termonucleares sólo afectaban al corazón de la estrella.
La luminosidad intrínseca de una estrella es inversamente proporcional al cuadrado de su distancia.
Se calcula a partir del brillo que de ella nos llega ; es evidente que toda Imprecisión que aparezca en la evaluación de la distancia puede falsear el valor de la luminosidad.
En la foto superior,órgano de Corti normal,donde se aprecian las células ciliadas externas en tres o cuatro hileras,con sus cilios en forma de " V ".
En el centro,detalle en aumento de la anterior: el techo del túnel de Corti,verdadero esqueleto celular del órgano.
Abajo,gran ampliación de los cilios de células externas,con abundantes microvellosidades.
EL órgano de Corti constituye el órgano periférico de la audición,a su nivel se realiza la transducción de la energía sonora (energía mecánica) de las ondas acústicas en impulso bioeléctrico susceptible de ser interpretado por el sistema nervioso central,que lo convierte en sensación psicofisiológica.
En cierto modo,el órgano de Corti es a la audición lo que la retina a la visión: el responsable de transformar un estímulo físico externo (sonido,luz) en impulso nervioso interpretable por el cerebro.
El órgano de Corti está alojado en el laberinto óseo anterior ; adopta forma de caracol y está inmerso en los líquidos laberínticos: endolinfa y periflinfa.
El hecho de que se encuentre situado en un medio acuoso viene a significar que la energía sonora tiene que pasar de un medio aéreo a otro líquido.
Lo cual significa una impedancia muy diferente,que supondrá una amortiguación importante del estimulo ; o,si se prefiere,que va a existir una pérdida energética elevada.
Para obviar esta circunstancia,el oído dispone de un sistema mecánico que condensa enormemente la energía,minimizando así la pérdida que entraña el cambio de medio físico.
Este sistema de condensación está constituido por el oído externo,desde el pabellón (la oreja) y el conducto auditivo hasta el sistema constituido por el tímpano y los huesecillos del oído medio.
El valor completo de este sistema mecánico de transducción es de unos 60 decibelios,cuantitativamente la mayor pérdida que cabe esperar de las lesiones totales del conjunto.
Cuando la causa patológica que provoca la sordera está situada a nivel del órgano de Corti,la pérdida de audición puede ser mucho mayor,incluso total.
Además,y sobre todo,pueden producirse fenómenos de distorsión del sonido de tal forma que,en muchas ocasiones,el sujeto percibe el sonido a partir de ciertas intensidades,pero no acierta a interpretarlo correctamente,debido a las modificaciones introducidas por el órgano de Corti lesionado.
El cerebro recibe estímulos nerviosos distorsionados y no es capaz de " oír " correctamente los sonidos que llegan del exterior.
El estudio mediante microscopio electrónico de barrido (escáner) del órgano de Corti humano resulta extraordinariamente difícil ; en efecto,sólo puede realizarse post-mortem y,en tal caso,las alteraciones debidas al óbito aparecen en los cinco primeros minutos.
Por esta razón,la mayor parte de los estudios disponibles están hechos en animales de experimentación.
En las fotos que presentamos,las preparaciones se realizaron con cobayas.
La fijación del órgano puede realizarse enviando el fijador mediante las últimas emboladas cardíacas,y posteriormente se completa en el mismo momento de la muerte mediante una inyección local de formol.
Uno de los estudios más interesantes,de cara al análisis de las células del órgano de Corti en casos de sordera,consiste en provocar la pérdida de audición con determinados antibióticos específicamente ototóxicos,todos ellos de la familia de la estreptomicina.
El interés de estos trabajos es doble: por una parte,se comprueba la toxicidad experimental de los nuevos agentes de ese grupo de antibióticos,y por otra parte,mediante las lesiones producidas es posible estudiar algunos aspectos todavía no aclarados de la fisiología del órgano.
En todo caso,la visualización tridimensional de una zona tan delicada de nuestro sistema auditivo permite conocer con detalle ciertas degeneraciones celulares que,en otros casos,pasarían inadvertidas.
Arriba,imagen de una intoxicación con kanamicina,en la que se observa la desestructuración de muchas células ciliadas cuyos elementos han perdido rigidez.
En el centro,el mayor grado de intoxicación ha hecho perder la funcionalidad de las células neurosensoriales.
Abajo,hinchamiento de la célula en fase inicial y formación de un cilio gigante.
Un litro de agua de mar contiene como media 35 gramos de sales disueltas ; la variación entre distintas zonas es del orden más o menos de dos o tres gramos,con algunas excepciones,como en el mar Rojo,donde una intensa evaporación eleva esta concentración a cerca de 44 gramos ; o como en el Báltico,frío y de débil evaporación,donde se encuentra una salinidad de cerca de 20 gramos por litro (el mar Muerto es casi un lago salado,cerrado y recalentado,en donde la salinidad se acerca a 275 9 / l).
La principal de estas sales de mar es el cloruro de sodio (cerca de 27 g / l),seguido del cloruro de magnesio (3,8 9),sulfato de magnesio (1,65 9),sulfato de calcio (1,2 9),sulfato de potasio (0,85 9),carbonato de calcio (0,12 9) y bromuro de magnesio (0,076 9).
Hasta hoy se han identificado en el mar más de 70 elementos,y parece que todos existen,pero en concentraciones infinitesimales en algunos casos.
El agua de mar proporciona todos los elementos nutritivos necesarios al plancton plantas microscópicas que se encuentran en la base de la cadena alimentaria de los océanos.
Ciertos elementos,como el nitrógeno y el fósforo,tienen una concentración más o menos igual en el plancton y en el agua.
Los nueve elementos principales cuya concentración marina sobrepasa el 1 / 10.000 son cloro,sodio,magnesio,azufre,calcio,potasio,carbono,bromo y boro,todos esenciales para la vida.
Hay,por tanto,una estrecha adaptación entre vida y medio.
Los elementos llamados menores,cuya concentración varía entre 1 / 10.000 y 1 / 10.000.
000 (estroncio,silicio,flúor,nitrógeno,litio,aluminio,rubidio,fósforo,yodo y bario) son igualmente esenciales para la vida,con la aparente excepción del litio y del rubidio.
Hasta los elementos de los que sólo se encuentran indicios (1 / 100.000.
000,más o menos) llevan metales que,en cantidades ínfimas,juegan un papel esencial en reacciones enzimáticas ; sin embargo,entre los elementos llamados vestigios (uno por mil millones o menos) sólo hay dos,el cobalto y quizá el estaño,que juegan un papel biológico.
En compensación,se encuentran entre estos vestigios metales tales como el berilio,el mercurio,el cadmio y el plomo,de los que una concentración un poco más elevada sería tóxica,de ahí el peligro de los desechos industriales de tales elementos,el plomo,por ejemplo,cuya concentración puede convertirse rápidamente en tóxica.
Los elementos sólidos en disolución en el agua del mar representan el 3,5 % de su peso total.
Hay una dinámica extremadamente compleja definida por el tiempo de permanencia de estos elementos en su seno.
Esta dinámica está Integrada en el sistema biológico de renovación del medio marino por la fauna y la flora.
Absorbidos y concentrados por organismos marinos,estos componentes producen una toxicidad que se propaga a lo largo de la cadena alimentaria.
Enigma: los ríos no paran de echar aguas al mar,que contienen toda clase de sustancias en solución,sustancias que deberían añadirse constantemente a las que ya tiene.
Se podría pensar que la salinidad debería aumentar sin cesar.
Pero no es este el caso.
Hace un siglo que el geólogo danés Johan Forchhammer descubrió que la cantidad de los diferentes elementos del agua de mar no es proporcional a la cantidad de elementos introducidos en el mar por los ríos: es proporcional a la facilidad con que diversos elementos en el agua de mar se vuelven insolubles por reacciones químicas o de química orgánica: " Los océanos del mundo se pueden considerar,a lo largo de millones de años,como un único depósito,donde el agua está perfectamente mezclada,ya que las aguas más profundas en los actuales océanos no cuentan con más de 2.000 años de edad.
" escribe Michael Whitfield,director de investigaciones en la Asociación de Biología Marina del Reino Unido (Plymouth) en la revista New Scientist.
Este investigador propone modelos de nivelación u homeostasia y de control por retroacción de la salinidad de las aguas del mar por procesos geológicos.
Estos procesos son regidos por reglas geoquímicas relativamente simples.
" La concentración de un elemento en el agua de mar dependerá,por una parte de su abundancia en la corteza terrestre y,por otra,de la facilidad con la que puede ser incorporado en materias de sedimentación,que depende evidentemente de la naturaleza química de este elemento - - afirma Whiffield - -.
El reciclaje continuo de este sistema y la simplicidad de los mecanismos de control - - prosigue - - han asegurado probablemente una cierta estabilidad en la concentración relativa de los elementos.
Se deduce que las formas de vida en evolución han podido utilizar abundantemente los elementos disponibles,a condición de que estos elementos les hayan sido útiles.
¿Pero es suficiente un modelo geoquímico para explicar la salinidad del mar? Sin duda,no,porque los organismos vivos tienen también algo que decir sobre lo que contendrá su entorno acuático.
Así,la fotosíntesis genera la mayoría de las partículas en la superficie de los mares ; de hecho,se han descubierto recientemente oasis de vida en las grandes profundidades,en los alrededores de manantiales de agua caliente cuya temperatura puede alcanzar 3000 C. Según Whitfield," la producción primaria por el plancton equivale a 200.000 millones de toneladas de carbono orgánico por año,y por cada tonelada de carbono orgánico se forman 3,8 toneladas de carbonato de calcio y 4,2 de sílice.
Cantidades importantes de otros elementos son asimismo,incorporadas en las partes orgánicas blandas y en los esqueletos de estos microorganismos ".
Además,los desechos de la cadena alimentaria caen en forma de lluvia de las zonas superiores hacia las regiones más profundas y son sometidos a una degradación por las bacterias ; éstas juegan un papel inverso al de la fotosíntesis,consumiendo el oxígeno disponible y liberando nitratos,fosfatos y bicarbonatos inorgánicos,así como los elementos que pueden estar asociados.
Las aguas profundas que vienen de los polos,ricas en oxígeno,nutrimentos y gas carbónico son enriquecidas,además,por los productos de degradación de materias orgánicas y gradualmente,por turbulencia,suben hacia la superficie.
Otras partículas continúan su descenso hacia las profundidades,donde experimentan otros efectos tales como una fuerte presión y una mayor acidez.
A fin de cuentas,esta contribución biológica,gracias a la circulación de las aguas profundas,introduce una cantidad de componentes más importante que la vertida por los ríos.
La vida modifica el medio.
El aporte de organismos vivos afecta,pues,a la concentración relativa de diversos elementos en el agua de mar.
El estudio de diversos modelos teóricos parece confirmar el papel estabilizador de organismos,pero no se sabe mucho más.
¿Qué pasa realmente en el agua? Los mecanismos en juego parecen rápidos a escala geológica,pero de una infinita lentitud para el hombre.
Será necesario estudiar períodos de unos 10.000 años o más para poder evaluar el papel de las aguas profundas y decenas o centenares de años para conocer las reacciones en las aguas poco profundas.
¿El hombre ha vivido siempre a orillas del agua o bien ha tenido él también una fase acuática? Si sobre el origen acuático de la vida ya no hay ninguna duda,hay que preguntarse si la hominización de hace 8 - 4 millones de años no tendrá,como piensan algunos investigadores,un período semiacuático.
En aquella época,el noreste de Africa,cuna del hombre,estaba totalmente inundado.
LA crisis energética ha sumido al mundo civilizado actual en grandes controversias acerca de lo que debe y puede hacerse para salir de ella.
Es obvio que el petróleo será cada vez más escaso y que el carbón es sólo una alternativa transitoria,también en vías de extinción y además extremadamente contaminante.
La energía nuclear,más limpia y eficaz,choca frontalmente con amplios sectores de la población que muestran su virulenta oposición a lo que consideran peligroso.
Y la energía de fusión todavía no es más que un sueño de laboratorio y probablemente no sea tan inocua como se piensa.
Quedan las energías alternativas o blandas,que muchos prefieren denominar complementarias.
Precisamente por eso,porque son por ahora simples complementos de lo que ya hay ; lejos está aún el día en que estas energías puedan llegar a ser una real alternativa al carbón,al petróleo o al uranio.
La utilización de paneles solares en viviendas comunitarias puede suponer un importante ahorro energético y se amortiza en pocos años en zonas favorables.
Y es lástima.
Porque entre ellas nos encontramos con una fuente energética prácticamente ilimitada y madre de toda la vida en la Tierra: la energía solar.
Cada centímetro cuadrado de Sol emite hacia el Espacio toda clase de radiaciones que,en conjunto,suponen nada menos que una potencia de seis kilovatios.
Como el Sol tiene una superficie total de seis billones de kilómetros cuadrados (y un kilómetro cuadrado,no lo olvidemos,contiene 10.000 millones de centímetros cuadrados),resulta sencillo averiguar la potencia total que emite el Sol: 3,8.
1023 kilovatios.
Como dato comparativo,diremos que las más grandes centrales productoras de electricidad no pasan de tres millones de kilovatios y que un rayo de tormenta alcanza una potencia de 13 millones de kilovatios en el brevísimo lapso de tiempo de una cienmilésima de segundo.
Por supuesto,a la Tierra no llega más que una cantidad minúscula de tan formidable potencia.
Y ello es afortunado,porque si no sería imposible la vida tal y como la conocemos.
A nivel del mar,la radiación solar que incide sobre la superficie de nuestro planeta es de 0,7 kilovatios por metro cuadrado.
Fuera de la atmósfera,esta cifra es mayor: aproximadamente 1,3 Kv / m2.
Claro que se trata de potencias medidas perpendicularmente cuando el Sol se encuentra en el meridiano del lugar.
Y en todo caso no es precisamente una energía demasiado elevada si tenemos en cuenta que de noche no existe,que con días nublados se reduce drásticamente y que con rayos solares oblicuos su valor es mucho menor.
Esquema del rendimiento de los paneles solares para obtención de agua caliente según su inclinación,a lo largo del año.
La latitud del lugar determina la inclinación media que se le debe dar a dichos paneles.
Con todo,el Sol es el que,directa e indirectamente,provee de energía a la Tierra.
En primera instancia,para la vida misma,cuya energía fluye a través de los distintos ecosistemas a partir de las cadenas alimentarias básicas cuyo soporte son los vegetales verdes que,sin el Sol,jamás crecerían.
Y en segundo lugar,bajo forma de energía almacenada a lo largo de millones de años en el subsuelo y que ahora utilizamos quemando combustibles fósiles (carbón,petróleo).
Incluso la energía que podemos utilizar almacenando aguas corrientes o atrapando la fuerza del viento proviene también,indirectamente,del Sol,único motor de los movimientos atmosféricos y del ciclo global del agua entre los mares,las tierras y el aire.
Ahora bien,cuando nos referimos a energía solar,solemos aludir a una forma más restringida que implica la utilización directa de la luz del Sol.
Bien sea para calentar fluidos contenidos en recipientes (energía térmica) o para obtener directamente electricidad mediante células de silicio (energía fotovoltaica).
Normalmente se deja de lado la energía mecánica del Sol,que aparece en los vientos de origen térmico,en las mareas debidas a la atracción gravitatoria combinada del Sol y la Luna,e incluso en los saltos hidráulicos que embalsan aguas procedentes del ciclo global gobernado por el proceso de la evaporación.
Tampoco se incluye habitualmente en el concepto restringido de energía solar a la energía química,observable en todos los procesos basados en la fotosíntesis,es decir,la captación por parte de las plantas verdes del carbono atmosférico,con expulsión de oxígeno,gracias a la energía del Sol,indispensable para que la reacción clorofílica tenga lugar.
Esta fotosíntesis de las plantas verdes es un proceso poco eficiente desde el punto de vista del rendimiento,ya que la planta utiliza una pequeña fracción de la radiación electromagnética que le llega.
Pero la biosfera ha diversificado de tal modo el proceso,adaptándolo a toda clase de condiciones diferentes (desde las minúsculas algas verdes monocelulares hasta los más gigantescos árboles),que el mecanismo se ha convertido en extremadamente rentable,como lo prueba,sin ir más lejos,la enorme variedad de seres vivos que poblaron y pueblan la Tierra desde hace 3.500 millones de años... Volvamos,pues,a la energía solar utilizada para calentar fluidos o para producir electricidad.
Sistemas que llevan ya bastantes años en estudio,e incluso en algunos casos a pleno rendimiento.
Reparto espacial de la insolación en España,medida en horas de sol anuales.
El mapa pertenece al Atlas Climatológico Nacional (INM) y abarca un período de treinta años.
El esquema de la página,amablemente cedido por el Centro de Estudios de la Energía,nos muestra los distintos sistemas de captación de la energía solar,utilizando o bien su calor o bien la energía de sus fotones.
De los distintos sistemas,el de calor a baja temperatura es,sin duda,el más extendido a nivel doméstico,gracias a los paneles colectores que proveen de agua caliente sanitaria e incluso de calefacción a viviendas de todo tipo.
Generalmente es necesario apuntalar estos sistemas con fuentes energéticas tradicionales (gas,gasóleo),pero el ahorro es tan sustancial que la instalación se puede amortizar en pocos años.
Al menos en la mayor parte de España,país especialmente bien dotado por la naturaleza para este tipo de aprovechamientos energéticos,como luego veremos.
Otro sistema que ha probado ya con creces su eficacia es el calentamiento de fluidos a altas temperaturas.
En este caso,se obtiene vapor que puede mover una turbina,generando electricidad como cualquier central convencional.
En la localidad almeriense de Tabernas funciona una central española de este tipo.
Todos los seres vivos se caracterizan por su capacidad para fijar energía y utilizarla una vez fijada.
La suma de procesos (reacciones químicas) implicados en la utilización de la energía por los seres vivos se conoce por metabolismo ; los metabolitos son sustancias que intervienen y se originan en esas reacciones químicas.
Estos metabolitos se pueden considerar en dos grandes grupos,aunque en muchos casos la separación no sea muy clara: los metabolitos primarios,que se caracterizan por ser esenciales para la vida (azúcares polisacáridos,proteínas,ácidos nucleicos,etcétera),y los metabolitos secundarios,sustancias que no parecen fundamentales,pero que sí facilitan la supervivencia al aumentar la viabilidad de la especie,por transmitir información (feromonas) o por constituir un sistema de defensa químico (venenos,toxinas,antibióticos,sustancias antigerminativas,inhibidores de la fijación y crecimiento de organismos incrustantes,antifúngicos,etcétera).
Así como los metabolitos primarios son iguales para todos los organismos vivos,no sucede lo mismo con los metabolitos secundarios,que presentan una gran diversidad específica.
Ejemplos de metabolitos secundarios serían las sustancias que permiten la localización de un compañero sexual (una feromona diferente para cada especie) o bien las sustancias tóxicas que protegen contra los depredadores o los parásitos,e incluso los venenos que se emplean para neutralizar a las presas.
Por lo que respecta al extenso grupo de las esponjas,dos especies ya han proporcionado sustancias interesantes.
De la Cryptothetya cripta se han obtenido los nucleósidos espongouridina y espongotimidina,que sirvieron como modelos para la síntesis de sus análogos químicos el ara-C (D-arabinosil citosina) y el ara-A (Darabinosil adenina).
El primero de estos medicamentos ha resultado ser un potente inhibidor de ciertas formas de cáncer y leucemia,hasta tal punto que actualmente se le atribuye,asociado a otros medicamentos,la curación del 50 por 100 de las leucemias tratadas.
Tan esperanzador resultado se achaca a su parecido químico con uno de los nucleósidos,la citidina,que constituyen el ADN de los cromosomas de las células cancerígenas ; éstas,en presencia del ara-C,lo incorporan a su ADN impidiendo así la síntesis normal y el crecimiento posterior de la célula enferma.
Por su parte,el ara-A presenta una actividad antivírica muy marcada y ha sido empleado con resultados muy prometedores en el tratamiento de la encefalitis,por ejemplo.
Otra esponja,la Aplysinopsis reticulata,le ha servido a unos laboratorios australianos para sintetizar un producto metilado análogo a la aplisinopsina extraída de la citada esponja.
Ese producto actúa como inhibidor temporal de una importante enzima del cerebro,la monoaminooxidasa,con efectos antidepresivos muy esperanzadores.
Otros seres vivos marinos,los cnidarios (medusas,actinias,gorgonias hidrozoos,corales),han sido estudiados concienzudamente por sus defensas tóxicas.
Las gorgonias,por ejemplo,han resultado ser portadoras de prostaglandinas ; destaca sobre todo la especie Plexaura homomalla por su altísima concentración en prostaglandina (15 F) - PGA.
De este producto se deriva otro similar el (15 S) - PGA2,del que se investigan diversos efectos fisiológicos ; esencialmente,su capacidad estimulante de la actividad de la musculatura lisa,su capacidad tranquilizante del sistema nervioso y su acción como agente hipotensor.
Los trabajos con el coral blanco Sinularia flexibilis,llevados a cabo por la Universidad de Bruselas,están comprobando el alcance farmacológico de las cualidades citotóxicas y amebicidas del diterpeno sinulariolida obtenido de dicho animal.
Por su parte,el Instituto de Biología Marina de Hawai estudia el antozoo Palythoa toxica,portador del principio activo palitoxina.
Se trata de una toxina no proteica y tan efectiva que bastan 15 microgramos para acabar con la vida de un hombre.
En cantidades apropiadas se está empleando,sin embargo,por su alto poder vasoconstrictor y antitumoral.
Laboratorios australianos han extraído de anémonas de mar un par de sustancias,la congestina y la talasina,con la propiedad de actuar sobre el músculo cardíaco (miocardio) aumentando su fuerza de contracción sin modificar su ritmo.
Esto se traduce inmediatamente en una circulación más eficaz y abundante sin aumentar el trabajo del corazón.
No hace falta señalar la importancia de un medicamento de este tipo.
Las medusas también están siendo objeto de la atención de rusos,japoneses y americanos,habiéndose conseguido cardiotoxinas y neurotoxinas cuyas aplicaciones están en estudio.
Observaciones de centros japoneses sobre los gusanos han dado como resultado la extracción de una nereistoxina del Lumbrineris heteropoda.
La acción de esta toxina bloquea la transmisión del impulso nervioso en los ganglios de enlace del sistema nervioso.
Aunque su aplicación medicinal aún está en estudio,se ha utilizado para elaborar un potente insecticida que ya lleva diez años de buenos resultados en la agricultura nipona.
De diversos peces tropicales un laboratorio japonés ha obtenido una de las toxinas más activas que existen,la tetrodotoxina.
Actúa bloqueando la transmisión del impulso nervioso y su actividad es 160.000 veces superior a la de la cocaína,bastando sólo diez microgramos por kilo de peso para matar a un hombre.
Aun así,en los últimos años se ha convertido en una de las sustancias más útiles en la investigación neurológica,ya que la tetrodotoxina actúa bloqueando la permeabilidad de la membrana a los iones Na + (sodio positivo).
Este bloqueo selectivo está permitiendo el estudio de los diferentes mecanismos de la transmisión del impulso nervioso.
Finalmente,cabría citar la obtención de aceites terapéuticos y diversas vitaminas a partir de gran número de peces ; los mamíferos marinos,por su parte,son fuente de productos como la insulina,la cortisona y la hormona corticotropina (ACTH).
Después de treinta años de creciente actividad,la farmacología marina no es todavía más que una promesa de futuro,a pesar de los espectaculares logros ya alcanzados.
Pero basta con pensar en ese más de medio millón de especies de seres vivos marinos,para mirar con optimismo el futuro de los medicamentos extraídos de las aguas marinas.
La farmacia del mar entreabre sus puertas.
LA perfección y belleza de un organismo vivo,sus inestables mecanismos de supervivencia plenamente relacionados entre sí,nos recuerdan a esos increíbles apilamientos de frutos que vemos en los mercados,formando pirámides tan bellas como inestables.
Retirar una de las frutas de las líneas más altas puede afectar a la estética del conjunto,sin más ; pero apropiarnos de alguna de las que forman la base o el interior nos demostrará hasta qué punto cada fruta depende de las demás para mantener el difícil equilibrio y la belleza del conjunto.
Algo así ocurre cuando estudiamos el mundo de las secreciones internas de los organismos ; las glándulas encargadas de tales secreciones y de sus interconexiones se encuentran relacionadas entre sí de forma sutil y perfecta.
Y todavía son muchas las cosas que ignoramos de estas acciones bioquímicas,aunque una cosa es segura: cuando una se estropea,el conjunto deja de funcionar.
Como la pirámide de fruta a la que le retiramos una pieza tan aparentemente inofensiva como las demás,pero cuya ausencia desmorona a las otras.
Una de estas glándulas,el tiroides,fue descubierta y minuciosamente descrita hace unos trescientos años ; pero en realidad poco se ha sabido realmente de ella hasta principios del presente siglo.
Su principal hormona,la tirosina,fue descubierta en 1915 ; su otra secreción,la hormona triyodotironina,sólo fue descubierta hace veinte años.
Las dos hormonas secretadas por la tiroides,conocidas generalmente como T3 y T4,están químicamente compuestas por una misma proteína,la tironina,y además dependen exclusivamente de un elemento no metálico,el yodo.
Gracias a la presencia de yodo en las moléculas de estas hormonas,se ha podido estudiar de cerca la función del tiroides.
Utilizando yodo radiactivo,de peso atómico 131 en lugar del 126 del yodo normal,ha sido posible seguir las radiaciones según se iba incorporando el elemento halógeno a las hormonas tiroideas ; se puede levantar así un mapa de la distribución y el metabolismo de tales hormonas en el organismo.
El yodo es un elemento bastante escaso en nuestro planeta,y por eso necesitamos un organismo de búsqueda y captura del preciado halógeno ; una función que le está reservada precisamente a la glándula tiroides.
El yodo existe en forma libre en el suelo y en el agua,pero en pequeñísima proporción.
Normalmente,lo ingerimos en forma de yoduros,y los residuos no utilizados son eliminados por las heces.
De las cantidades de yodo circulante,el tiroides ha de extraer más de la mitad para formar sus hormonas.
Cómo puede el tiroides buscar,capturar y almacenar las exiguas proporciones de yodo que pasan a su alcance,es aún un secreto celosamente guardado.
Resulta,sin duda,un proceso difícil,ya que la captación ha de realizarse contra corriente y supone un gran consumo de energía a partir del adenosintrifosfato (ATP) ; un proceso que puede ser bloqueado o alterado por la acción de gran cantidad de compuestos iónicos.
Sabemos también que otra hormona,procedente de la hipófisis del cerebro,influye de manera decisiva en esta delicada misión de captación de yodo.
Esta hormona auxiliar estimulante del tiroides,o TSH,es tan importante que a veces una deficiencia en la hipófisis llega a poner en peligro la formación de hormonas tiroideas.
En cierto modo,volvemos a la idea de la pirámide de frutas,en la que todas dependen de todas para el equilibrio global.
Una vez capturado el yodo,comienza otro mecanismo aún más delicado si cabe: la unión del halógeno y de la proteína tironina,proceso denominado yodinación.
Una reacción química difícil que es realizada por el organismo a una velocidad y con una perfección sorprendentes.
La tiroides,responsable final de estos procesos,es una glándula situada en la parte anterior de la base del cuello,por delante de la tráquea y con una forma que recuerda a una mariposa con las alas extendidas: un estrecho cuerpo central y dos lóbulos más grandes.
Pesa unos 20 a 35 gramos,y su tejido está compuesto por una multitud de vesículas o acinos,de forma más o menos esférica que contienen una sustancia gelatinosa,la secreción coloide almacenada de la glándula.
Las células que forman estos acinos poseen unas microvellosidades características de todas las células secretoras del organismo,y el conjunto se ve irrigado por una riquísima red vascular que permite una óptima circulación sanguínea en su interior.
Es precisamente en esa sustancia coloide donde quedan almacenadas las hormonas tiroideas,dispuestas para su eliminación en el momento oportuno hacia el resto del cuerpo.
Como la cantidad total de yodo que se encuentra en el organismo de una persona sana es de unos seis microgramos por cada 100 centímetros cúbicos de plasma,y buena parte de tan exigua cantidad se elimina en las heces,es fácil percatarse de la precisión con que debe trabajar el tiroides para fabricar sus hormonas incansablemente con tan escaso elemento a su disposición.
Estas hormonas yodadas son mal conocidas ; desconocemos aún algunas de sus funciones,pero,en cambio,sabemos bastante más de las alteraciones que producen tanto su defecto como su exceso.
Sus efectos más destacados son dos: uno sobre el crecimiento y la diferenciación celular,y otro sobre los procesos de metabolismo de las células (consumo de oxígeno y combustión interna).
La glándula tiroides puede enfermar por diversas causas.
Muy especialmente si cualquier agente o situación impide la formación de sus hormonas (la falta de yodo en el ambiente o en la dieta suele ser una buena causa),se produce un aumento de la hormona estimuladora del tiroides,la TSH,lo cual induce un crecimiento de la glándula ; aparece así el bocio o crecimiento no funcional de la glándula.
Una enfermedad que llega a ser endémica en zonas como las Hurdes o determinadas regiones gallegas.
Curiosamente,este tipo de enfermedad podría tener fácil remedio aumentando ligerísimamente el yodo en la dieta habitual de las regiones afectadas.
Sólo un detalle más,por si hiciera falta,para corroborar la teoría de la pirámide de fruta: todo el equilibrio vital no es más que una suma de pequeños esfuerzos colectivos que consiguen,de forma precariamente maravillosa,que el conjunto funcione a la perfección.
Aunque quitando una sola fruta,todo pueda venirse abajo.
La quiralidad es muy importante en bioquímica y afecta a numerosas moléculas.
La glucosa,por ejemplo,puede ser de izquierdas (alfa) o de derechas (beta),según una convención establecida por los químicos para identificar el sentido de las moléculas quirales.
El azúcar natural que se extrae de la remolacha,por ejemplo,es glucosa de izquierdas.
El azúcar de síntesis química proporciona mitad y mitad de glucosa alfa y beta.
Y si mezclamos azúcar artificial con agua conteniendo bacterias,éstas sólo absorberán las moléculas de izquierdas.
La explicación de estas extrañas preferencias de las moléculas orgánicas es un misterio.
Todo ocurre como si la biosfera fuera una inmensa fábrica en la que unas máquinas estuviesen ajustadas con tornillos que giran a izquierdas y otras con tornillos que giran a derechas.
Con lo que,aun siendo exactamente iguales,ninguna pieza del mundo de derechas puede integrarse al mundo de izquierdas,y viceversa.
Los seres vivos son casi siempre quirales,es decir,que reconocen las moléculas según su orientación izquierdista o derechista.
En cambio,los átomos y las partículas elementales que los componen no tienen preferencias diestras o siniestras ; no son quirales,sino idénticos a su reflejado en un espejo.
El átomo es un sistema dominado por fuerzas electromagnéticas,que son,a su vez,insensibles a la quiralidad.
Las fuerzas de la física también parecen comportarse de manera simétrica,conservándose la paridad.
Sin embargo,hace ya algunos años que los físicos intentan encontrar un fallo a esa simetría física.
Para que un fenómeno viole la paridad sería necesario que su imagen en el espejo fuese un fenómeno imposible,es decir,contrario a las leyes admitidas por la física.
Ello resulta difícil de imaginar así,en abstracto,pero podemos comprenderlo con un ejemplo: el código de la circulación.
Un coche con el volante a la izquierda y circulando por su derecha nos mostraría una imagen,en el espejo,de un coche con el volante a la derecha y circulando por la izquierda.
Lo cual resulta imposible en España,por ser contrario al código de la circulación (además de sumamente peligroso).
La paridad ha sido violada,porque el coche y el código son,por así decirlo,quirales.
Desde luego,las normas legales son factores arbitrarios ; nada impide,en efecto,la existencia de coches con el volante a la derecha y circulando por su izquierda,tal y como ocurre en el Reino Unido y el Japón.
Las nociones de derecha e izquierda no tienen ningún valor absoluto en el Universo,son simples convencionalismos terrestres.
El macrocosmos no contiene ningún punto de referencia que permita determinar un sentido de derechas o de izquierdas.
Parece impensable que las leyes universales de la física puedan,como lo hace el código de la circulación,privilegiar arbitrariamente un lado respecto a otro.
Ninguna de las cuatro fuerzas fundamentales del Universo (gravitatoria,electromagnética,nuclear fuerte y nuclear débil) se opone a la conservación de la paridad: en el cosmos,la mano izquierda y la derecha no tienen diferencias respecto a un espejo.
Pero en 1956,la duda afloró por primera vez.
Dos jóvenes físicos chinos emigrados a los Estados Unidos,Ning Yang y Dao Lee,comprobaron asombrados cómo un mesón K,partícula obtenida de las colisiones atómicas en los grandes aceleradores de partículas,se desintegraba a veces en tres mesones Pi,y a veces,en sólo dos.
En realidad nada se opone a que una misma partícula presente dos modos diferentes de desintegración,pero en este caso preciso,según el principio de paridad,no podía ser la misma partícula.
Y sin embargo se trataba del mismo mesón K..
. La mayoría de los físicos,fieles seguidores de la intocable ley de la paridad,pensaron que se trataba de un conocimiento insuficiente del fenómeno,que tendría su explicación racional tarde o temprano.
Pero Yang y Lee supusieron que en la fuerza débil que gobernaba los extraños avatares del mesón K,la paridad no era respetada.
Y le encargaron a una eminente colega,la doctora Chien Shiung Wu,también de origen chino y catedrática de física experimental de la Universidad de Columbia,que verificara experimentalmente tan revolucionaria teoría.
La doctora Wu demostró,con emisiones de electrones por átomos de cobalto,que había una disimetría hacia la izquierda del experimento,como si la naturaleza hubiera polarizado a izquierdas el eje de rotación del núcleo de cobalto.
La simetría universal quedaba hecha añicos: la paridad no era respetada.
Un descubrimiento que les valió a Lee y Yang el Nobel de física de 1957.
Al contrario de lo que entonces se pensaba,no era ése un caso aislado.
La paridad es violada incluso en los átomos estables,esos sólidos ciudadanos del mundo de la materia que no son tan proclives a las veleidades como sus colegas,los átomos radiactivos.
Ya se suponía que eso era así en la década de los setenta,cuando se intentaba teóricamente unificar la fuerza electromagnética y la fuerza débil,responsable esta última de los fenómenos que violaban la paridad.
A pesar de que el electromagnetismo es muchísimo más poderoso,de alcance casi infinito y respetuoso con la ley de la paridad ; todo lo contrario que la fuerza débil,de alcance sólo atómico y descaradamente ofensora de la ley de la paridad.
Con todo,tres físicos teóricos obtuvieron el Nobel de 1979 por demostrar que ambas fuerzas eran,en realidad,la misma: Glashow,Weimberg y Salam.
Y,a finales de 1984,el Nobel a Rubbia y Van der Meer coronaba el éxito experimental que Lui demostró la certeza de tales teorías.
Volviendo al tema de la simetría,es evidente que lo que ocurría con An la fuerza débil también podría pasar P° en el electromagnetismo: la paridad puede ser violada en el mundo real.
En un átomo,las fuerzas electromagnéticas son aproximadamente D cien millones de veces más elevadas que las fuerzas débiles.
La violación de la paridad interviene en una proporción de una vez cada cien mil billones de veces en el comportamiento de los electrones de un átomo estable.
Ningún laboratorio podría tener la pretensión de alcanzar una precisión tan inconcebible.
Pero entre las partículas elementales,la proporción no es tan descabellada,y ya en 1973 se llegó a observar los efectos,a este nivel,de las fuerzas débiles.
Sin embargo,había que encontrar estas fuerzas en el seno mismo de los átomos estables.
Los electrones periféricos,los menos ligados al núcleo,determinan el carácter del átomo,sus relaciones con los demás átomos y con la luz....
en suma,todas sus propiedades.
Son,por tanto,los más fáciles de estudiar en su relación de fuerza débil con el núcleo.
Claro que éste se halla muy alejado,más allá del muy corto alcance de la fuerza nuclear débil ; pero la trayectoria muy elíptica de los electrones a veces los lleva a distancias muy próximas.
Para detectar experimentalmente el fenómeno de asimetría en átomos estables,un grupo de investigadores franceses de la Escuela Normal Superior de París eligió el átomo de cesio.
Este contiene 55 protones en su núcleo,y otros tantos electrones en su zona exterior.
Pero estos electrones están dispuestos de un modo muy peculiar: sólo uno ocupa el nivel energético más elevado,es decir,el más exterior,mientras que los demás se agrupan en una especie de nube esférica alrededor del núcleo.
Todo lo cual permite,desde el punto de vista de los cálculos teóricos,considerar al cesio como un átomo con un único electrón ; casi,casi,como el más simple de los átomos pesados,o el más pesado de los átomos simples.
En suma,el candidato ideal para la búsqueda del efecto de antiparidad de las fuerzas débiles.
Los físicos franceses intentaron medir el efecto de la fuerza débil en el salto entre dos niveles de energía del electrón exterior del cesio ; un salto impulsado por láser.
En tal caso,el fenómeno de violación de la paridad ocurre de forma muy fugitiva,el tiempo del salto del electrón.
Sin embargo,el átomo que ha sido excitado conserva,por así decirlo,una huella de desexcitación como fluorescencia infrarroja.
La asimetría izquierda-derecha del sistema no se observa,pues,instantáneamente,sino en la fase de desexcitación,gracias a la polarización circular de la luz fluorescente ; la luz polarizada es,en efecto,una luz quiral,que nos permitirá detectar una eventual violación de la simetría.
Claro que para ello es preciso que el dispositivo de laboratorio también sea quiral ; punto esencial en este tipo de experiencias.
UN investigador francés,el doctor Claude Nicolau,del Centro de Biofísica Molecular de Orleáns,acaba de demostrar en ratones que el gen implicado en la síntesis de la insulina puede ser inoculado a los diabéticos.
Para trasplantar el gen,primero hay que extraerlo de las células del páncreas.
Lo complicado es introducirlo en las células enfermas.
El gen no puede ser introducido al desnudo,necesita una envoltura apropiada.
El doctor Nicolau ha utilizado minúsculas gotas de grasa llamadas liposomas.
La grasa forma una envoltura esférica cuyo diámetro varía entre 50 y 300 nanómetros (millonésimas de milímetro).
Los genes que se meten dentro son los productores de preproinsulina,y deben ser obtenidos en múltiples ejemplares ; lo cual no resulta fácil.
Por ello,el investigador francés primero obtiene múltiples copias del gen sano inicial,gracias a un vector,un plásmido (molécula de ADN en forma de anillo).
Estos plásmidos,muy numerosos y portadores de copias del gen que se quiere trasplantar,se introducen en las microgotas de grasa.
Después se inyecta a la rata enferma el conjunto de los liposomas,y se espera el resultado.
La experiencia resultó bien,ya que la rata comenzó a fabricar de nuevo insulina por sus propios medios.
Aunque con una sorpresa: el gen trabajaba correctamente en su labor productora de insulina,pero no dentro de las células del páncreas ; ni siquiera en las del hígado o las del bazo,sino en los macrófagos,grandes células del sistema inmunitario cuyo papel es el de absorber los desechos de tejidos y microbios destruidos por las defensas orgánicas.
Lo que había ocurrido es que estos macrófagos se habían tragado literalmente a los genes que se quería trasplantar y los habían incorporado a su propio programa genético,comenzando a producir insulina.
La cosa no sería grave si no fuera porque los macrófagos tienen una vida efímera.
No todos llegaron a su destino,porque los macrófagos absorbieron muchos de ellos ; pero llegaron en cantidad suficiente como para producir insulina de forma duradera.
Funcionando separadamente dentro de la misma célula,unos genes se copian a sí mismos (contenido del vector) y otros fabrican las proteínas de la cubierta (envuelta del vector).
Y así,las partículas ARN de ingeniería,con las proteínas de la cubierta que suministran los genes del virus auxiliar,pueden ya invadir otras células.
El vector está terminado.
Los investigadores suelen decir que sólo lleva billete de ida.
La partícula llega a la célula que queremos curar,le transmite su información genética y allí se queda.
Algunos investigadores tampoco ven muy claros los beneficios de los vectores construidos a partir de retrovirus.
Hay que estar seguros de haber despojado por completo al virus de los genes que causan tumores.
Además,teniendo en cuenta que las células humanas tienen 46 cromosomas,podría producirse una mutación inesperada,ya que no sabemos en qué punto de ellos se introduce el vector.
Las señales reguladoras virales son muy potentes y,colocadas en medio de un determinado gen un efecto no deseado.
Por todo ello,algunos investigadores se dirigen a detectar las señales genéticas de cada tipo de tejido y también a la posibilidad de construir vectores con señales de regulación directamente humanas.
Por lo que a España respecta,el Ministerio de Educación intenta unificar los esfuerzos para el desarrollo del sistema Ciencia-Tecnología en nuestro país.
Acaba de tener entrada en las Cortes el proyecto de Ley para la coordinación y promoción de la Investigación,la llamada Ley de la Ciencia.
..
EL Góbierno ha decidido fijar la Biotecnología como una prioridad del desarrollo científico y tecnológico español - - dice Emilio Muñoz,director general de política científica.
Entre las medidas concretas que implica esta decisión está la creación del Centro de Investigación de Ingeniería Genética y Biotecnología,y la formulación de un programa movilizador en estas materias.
En España existen 14 centros para la detección,prevención y tratamiento de enfermedades moleculares.
El que mayor volumen de muestras procesó durante el pasado año es el Centro de Enfermedades Moleculares,ubicado en la Universidad Autónoma de Madrid,que realizo un número de análisis cercano a 60.000.
Pero esa cifra supone solamente un 60 por ciento aproximado de los nacidos en el mismo período entre la población bajo cobertura del centro.
Por supuesto,la ansiedad no es sólo un estado de ánimo.
Comporta también manifestaciones orgánicas: actúa sobre la presión sanguínea,el ritmo cardíaco,el filtro renal,las secreciones de las glándulas suprarrenales... Mientras que estas variaciones se mantengan en límites razonables,no supondrán ninguna incidencia en la salud.
Pero si se produce una exasperación de la ansiedad,además del sufrimiento psíquico,pueden producirse desórdenes orgánicos más o menos graves.
Se entiende,en tales condiciones,el éxito de los medicamentos que calman la ansiedad y su empleo para todo tipo de dolencias.
Dicho esto señalemos que también tienen detractores,que dudan tanto de su eficacia como de su inocuidad.
Hay diversas técnicas psicológicas que permiten combatir la ansiedad.
¿Por qué entonces - - preguntan algunos - - tomar medicamentos? Y,puesto que la ansiedad es asunto psíquico,¿no será totalmente ilusoria la acción de los tranquilizantes,como la de cualquier placebo? Ciertamente,los tranquilizantes plantean interrogantes,incluso para quienes están convencidos de su eficacia.
¿A partir de qué nivel de ansiedad hay que tomarlos? ¿Hasta qué punto se puede o se debe soportar esa tensión interna que forma parte de la vida misma? Rechazar la ansiedad vital,bajo pretexto de incomodidad,¿no es sacrificar la conciencia psíquica y la vivacidad del espíritu? ¿No será una búsqueda de esos paraísos artificiales que,del amodorramiento,pueden conducir al envilecimiento? Sobre este último punto,al menos,hay datos tranquilizadores: un estudio epidemiológico exhaustivo,que abarca varios años,ha demostrado que las benzodiazepinas,clase química a la que pertenecen la mayoría de los tranquilizantes,no crean dependencia alguna (a diferencia de los opiáceos,el alcohol o los barbitúricos).
Lo que no impide que todos los interrogantes suscitados por los ansiolíticos (medicamentos contra la ansiedad) hayan estimulado a los investigadores a intentar comprender su acción.
Y es que,por muy curioso que parezca,su descubrimiento fue casual.
En los años sesenta,algunos laboratorios estudiaban ciertos cuerpos químicos dotados de poder antidepresivo.
Para ello,fabricaban series de nuevas moléculas,próximas a las ya conocidas,y comprobaban su eficacia en animales,tomando como criterio ciertos síntomas muy precisos de la depresión.
Si la molécula se mostraba incapaz de corregir los síntomas,era abandonada.
Tal fue el caso del clordiazepóxido (Librium).
Pero,un día,en uno de esos laboratorios,un investigador experimentó casualmente con esta molécula rechazada sobre ratas,y constató que su comportamiento se transformaba.
Se trataba de animales con distintos condicionamientos,destinados a servir de modelo para diferentes estados.
La reacción más sorprendente se dio en los modelos de ansiedad: no sólo las ratas ya no estaban ansiosas,sino que afrontaban con perfecta tranquilidad las situaciones más ansiógenas.
De ahí el nombre de píldora de la felicidad que se dio a este primer tranquilizante cuando fue lanzado al público.
Desde entonces,han sido identificadas,siempre en la familia de las benzodiazepinas (BZ),nuevas moléculas dotadas de las mismas virtudes tranquilizantes,algunas de las cuales,administradas en fuerte dosis,actúan incluso sobre las crisis de epilepsia.
Asimismo,modificando ligeramente la molécula tipo de las BZ,ha sido obtenido un sedante capaz de combatir numerosas formas de insomnio.
Trabajos todos ellos guiados con el solo objetivo de poner a punto nuevas sustancias cada vez más eficaces.
Dicho de otro modo,se ha investigado principalmente lo que funcionaba,sin interesarse demasiado por cómo funcionaba.
Es cierto que,hasta hace cuatro o cinco años,no se dominaban las técnicas de exploración fina de los mecanismos íntimos de las células cerebrales.
Así pues,era difícil ver lo que ocurría a nivel de las neuronas.
El gran cambio,en este campo,fue el descubrimiento de puntos receptores en la membrana possináptica (en una neurona,la parte de membrana que se encuentra frente al botón terminal de la neurona anterior,y que de hecho,recibe el influjo transmitido por ese botón terminal).
Cuando el profesor Eric Simon,de la Universidad de Nueva York,anunció que había detectado.
en la membrana de las células cerebrales,receptores específicos de los morfínicos,los medios científicos se vieron a un tiempo cautivados e intrigados.
En efecto,era difícil pensar que la naturaleza hubiera previsto que al hombre le iba a dar por la morfina: la presencia de dichos receptores indicaba,pues,que el organismo era capaz de fabricar opiáceos por sí mismo.
Fueron buscados y,muy pronto,tres laboratorios distintos dieron con ellos: se trataba de las endorfinas.
En 1978,un investigador danés,Claus Braestrup,mientras estudiaba las reacciones de las células cerebrales a las benzodiazepinas (BZ),descubrió por su parte,en numerosas regiones del cerebro y de la médula espinal,neuronas que poseían enclaves receptores a las BZ (enclaves de alta afinidad).
Su conclusión fue que,como en el caso anterior,el organismo fabricaba por si mismo sustancias ansiolíticas con la misma conformación que las BZ.
Empezó,pues,la búsqueda de los neurotransmisores naturales (o endógenos) de los enclaves receptores de BZ.
Aún hoy,la búsqueda prosigue.
Y,aunque no haya dado resultados todavía (¿los dará alguna vez?),no ha sido vana,ya que ha dado lugar a descubrimientos apasionantes que arrojan una nueva luz sobre el funcionamiento del cerebro,la función de lo psíquico y,sobre todo,el significado de la ansiedad.
Primer descubrimiento: el enclave receptor de BZ no es una proteína aislada en la zona possináptica,sino que forma parte de un complejo receptor en el que varias unidades asociadas trabajan de forma coordinada.
El complejo comprende (ver dibujo) una proteína de alta afinidad para las BZ ; una proteína de alta afinidad para el GABA (ácido gamma - aminobutírico),neurotransmisor de acción generalmente inhibidora (cuando es emitido por una neurona presináptica,hace a la neurona possináptica menos sensible a la acción de un neurotransmisor excitante que la alcanzaría al mismo tiempo),y una o varias células llamadas ionóforas para los iones cloro (Cl -),ya que autorizan o impiden el paso de estos iones.
Las células emisoras de GABA se reparten por todo el cerebro,y prácticamente todas las neuronas están expuestas a recibir en cualquier instante alguna descarga de GABA.
Descargas que no bastan para bloquear la neurona,ya que compiten con descargas excitantes más numerosas,sino que,simplemente,moderan y modulan la excitación global que recibe constantemente cada neurona cerebral.
Es precisamente al disminuir esa acción generalizada de frenado cuando aparecen trastornos más o menos graves: una disminución débil provoca sólo manifestaciones de ansiedad.
Una disminución más acentuada se traduce en un estado de agitación continuo.
Finalmente,una disminución extrema puede engendrar crisis comiciales (epilépticas).
Ahora comprendemos por qué las benzodiazepinas (BZ) combaten la ansiedad,la agitación y hasta las crisis comiciales.
Al acoplarse a su proteína receptora,la activan.
La activación del receptor de BZ favorece,pues,la acción del GABA.
Este esquema relativamente simple de la acción de las benzodiazepinas se complica en la práctica con la intervención de otros elementos que se oponen a esa acción.
Así,varios investigadores americanos observaron,en el curso de sus experiencias,que las BZ compiten con una proteína,presente a nivel del complejo receptor,que disminuye la afinidad de enlace entre el GABA y su enclave receptor.
Bautizaron a esta proteína con el nombre de GABAmodulina y,probando todas las diferentes BZ conocidas,consiguieron establecer una correlación entre su eficacia clínica y su poder concurrencial respecto de la GABA-modulina.
Paralelamente,mientras buscaban el neurotransmisor endógeno de los receptores de BZ (es decir,como hemos visto,la sustancia secretada por el organismo y destinada a esos receptores),unos investigadores británicos descubrieron en 1981,en la orina humana,en el cerebro de diversos mamíferos e incluso en secciones de cerebro humano.
En el transcurso de esta metamorfosis,nuevos órganos reemplazan a los primitivos,algunos de los cuales desaparecen por fagocitosis,es decir,por expansiones citoplasmáticas del cuerpo que destruyen los órganos larvarios.
Pero ¿cómo se produce este fenómeno? Varía según las especies.
En los insectos,por ejemplo,la metamorfosis se debe a la acción de una hormona juvenil,que retrasa la diferenciación de larva en adulto a fin de favorecer el crecimiento.
En cuanto a los anfibios,el cambio viene desencadenado por la secreción de la hormona tiroidea.
Las investigaciones que se han llevado a cabo en este sentido han demostrado que la adición de tiroides fresco a la alimentación del renacuajo lo transforma en ranas enanas y que la ablación del tiroides o del lóbulo anterior de la hipófisis impide la metamorfosis.
Fue en 1648 cuando el científico italiano Francesco Redi descubrió que las larvas se convertían en moscas una vez que habían puesto los huevos.
Fue también este investigador quien comprobó que muchos insectos cambiaban de forma a lo largo de su desarrollo.
Ciertos grupos experimentan una metamorfosis llamada incompleta,ya que las larvas sólo se diferencian de los adultos en la ausencia o estadio rudimentario de algunos órganos.
Los que sufren una metamorfosis completa,sin embargo,tienen un carácter particular que no pierden hasta pasar a la fase denominada de crisálida o de ninfa ; en este caso,la larva pasa por una etapa intermedia,conocida como fase de pupa.
Algunos invertebrados están protegidos por una cubierta externa de capacidad limitada,que pronto se convierte en una coraza que les impide el crecimiento.
De ahí que tengan que cambiarla varias veces durante el desarrollo: es la muda.
Para ello la larva deja de comer y busca un lugar adecuado,un sitio donde se encuentre a salvo,ya que debe restringir su movimiento.
Allí esperará hasta que,por secreción subcutánea,se le forme una nueva piel bajo la antigua.
Entonces,el aumento de su presión sanguínea provoca el resquebrajamiento de la armadura,tras lo cual la larva comienza a moverse para liberarse de ella.
A continuación,estira y aprieta la nueva muda,que está todavía plegada y,horas después,cuando la piel se le ha secado y endurecido,reanuda su vida normal.
El número de mudas varía de un animal a otro.
El saltamontes o la oruga realizan 5 ó 6,mientras que las arañas experimentan esta evolución de 8 a 10 veces.
La araña presenta un proceso aún más complicado,ya que posee unos miembros bastante desarrollados incluso en las etapas más tempranas.
Por este motivo,ha de realizar una especie de ejercicios malabares.
La operación consiste en colgarse de la tela cabeza abajo,con lo que su presión sanguínea se eleva de 1,8 atmósferas a más de 2. Su vieja cubierta,incapaz de soportar ahora la tensión,se abre alrededor de la boca hasta que llega a la parte trasera del cuerpo.
Entonces saca sus ocho patas milímetro a milímetro,hasta que sólo queda unida a su caparazón por medio de un hilito.
Este es el momento más peligroso para su vida.
LA utilización de los cultivos de tejidos en la investigación biológica y,en particular,en la investigación médica ofrece cada vez más aplicaciones.
El cultivo de tejidos vivos como método experimental tiene su origen en la demostración realizada por Harrison en 1907 ; en fragmentos de médula espinal de ranas,aislados en linfa coagulada,se pudieron observar cómo crecían fibras nerviosas.
Los tejidos vivos pueden,pues,crecer fuera del organismo al que pertenecen.
Desde entonces,han sido cada vez más numerosos los métodos utilizados para la propagación en serie de cualquier clase de célula.
En la actualidad,los medios de cultivo " in vitro.
, han alcanzado un desarrollo espectacular,al mismo tiempo que han ido adquiriendo una simplicidad y una definición química cada vez mayores.
En las imágenes que presentamos podemos observar dos buenos ejemplos de este tipo de cultivos,analizados mediante microscopía electrónica de barrido (scanner): neuronas ganglionares,en su conexión con células del músculo cardíaco,y células nerviosas tumorales uniéndose unas a otras.
La unión entre los nervios y los músculos presenta un enorme interés de cara a la comprensión de los mecanismos que permiten transmitir la información desde el sistema nervioso al muscular.
Una forma de analizar esta unión neuromuscular estriba en estudiar cultivos de neuronas ganglionares (en este caso,pertenecientes a embriones de rata) sobre mioblastos cardíacos.
Las neuronas ganglionares forman parte de la última etapa de la transmisión nerviosa,y tienen su origen en los ganglios del sistema simpático,en la parte lumbar y dorsal de la columna vertebral ; los mioblastos cardíacos son las células musculares del corazón.
Cuando se observa un cultivo de este tipo mediante el scanner es posible observar cómo las neuronas emiten ciertas prolongaciones,las neuritas,que establecen la unión con la célula muscular.
Esta observación confirma,una vez más,lo acertado de la famosa teoría neuronal de Ramón y Cajal,quien defendía la contiguidad entre las células nerviosas ,.
, en contra del " reticularismo " entonces imperante,defendido por Georlach y Golgi,y que consideraba al sistema nervioso como una red continua.
Por lo que respecta al estudio de células nerviosas cancerosas,con la ayuda del scanner podemos observar la diferenciación de este tipo de células en cultivos " in vitro ".
Un glioma o glioblastoma es un tumor de la parte del sistema nervioso conocida como glia ; ésta está formada por células nerviosas,generalmente de sostén y terminales.
Las células propiamente nerviosas rara vez tienen capacidad para formar tumores.
Por eso resulta interesante estudiar un cultivo de esas células nerviosas cancerosas,con el fin de comprobar cómo se diferencian para establecer contactos entre ellas.
En nuestro caso,se puede observar cómo esa diferenciación se hace siguiendo la línea astrocitaria (los astrocitos son células de la glía) que es radiada,como corresponde a su función de establecer contactos nerviosos terminales.
Se observa cómo las células emiten delgadas prolongaciones que conectan con las células contiguas,a modo de pies chupadores como los de las células astrocitarias normales.
La presencia de una densa red fibrilar es constante en el crecimiento " in vitro " de los tumores de este tipo.
En conjunto,el cultivo de células proporciona un método de experimentación con tejidos humanos que es éticamente aceptable y que permite aislar diversos elementos vivos del complejo medio ambiente del organismo.
Pueden así ser sometidos directamente a la acción de diversos agentes,cuyos efectos," in vivo " podrían ser interferidos o enmascarados por respuestas generales del organismo,que podrían impedir el estudio detallado de sus distintas reacciones.
Toda una sistemática de trabajo que presenta excelentes perspectivas de futuro.
La dificultad reside en evitar la formación de cristales de hielo en cada célula,por lo que éstas deben ser deshidratadas al mismo tiempo que enfriadas.
El embrión se coloca en una solución protectora que va siendo enfriada lentamente hasta llegar a la temperatura del ázoe líquido (- - 196 ° C).
Este proceso admite una automatización total.
Para descongelarlo,basta con sacarlo del ázoe líquido e introducirlo en otra solución que disuelve el agente protector.
El embrión,a veces,no soporta la operación,y muere.
Pero lo normal es que sobreviva y,al parecer,su desarrollo ulterior también es normal.
La congelación de embriones constituye una consecuencia asistencial de la fecundación in vitro.
Si una mujer a la que se le han extraído varios ovocitos no queda embarazada en el primer ciclo,bastará con congelar los embriones excedentes y esperar el siguiente ciclo.
No es aconsejable implantar en el útero más de tres embriones a la vez,pues,aunque aumenta las posibilidades de embarazo,se corre el riesgo de que sea múltiple.
La congelación permite conservar los embriones hasta una nueva petición: unos meses después,si el embarazo ha fracasado,o unos años más tarde,si se desea tener otro hijo.
Puede darse el caso de que la pareja ceda el embrión a otra pareja o a un centro ; en este caso,habrá que descongelar el embrión coincidiendo con el período adecuado del ciclo menstrual de la receptora.
Sin embargo,una duda importante asalta a los científicos: ¿Cuánto tiempo puede vivir un embrión congelado sin sufrir alteraciones genéticas?,¿tres meses?,¿tres años?,¿treinta años? No se sabe.
Y,por otro lado,¿tiene derechos el embrión? ¿Hasta cuántos días de edad se consideraría ética su manipulación? Para dar respuesta a estas preguntas se impone entrar en una seria reflexión moral y jurídica,de la que hablaremos al final del articulo.
Parece obvio recordar que el progreso científico y técnico no puede detenerse.
La mayoría de los especialistas coinciden en que la próxima etapa de esta carrera de la fertilización artificial será la congelación de óvulos: en un futuro habrá bancos de óvulos como hoy tenemos de esperma.
Lógicamente,las investigaciones abordan este tema con grandes precauciones.
El óvulo es una célula acuosa en la que los cromosomas se mueven libremente en torno al núcleo.
En la congelación,la formación de cristales podría ocasionar daños considerables,por lo que se deben hallar los protectores adecuados.
Además,resulta más difícil obtener óvulos que espermatozoides,ya que los primeros aparecen por unidades y los segundos se producen por millones.
Asimismo,la única manera de comprobar si un óvulo ha resistido la congelación consiste en hacerlo fecundar por un espermatozoide ; dicho de otro modo,estas investigaciones no podrán sacar conclusiones claras hasta que hayan sido fecundados en el laboratorio numerosos embriones,experiencia que no agrada a todo el mundo y que choca con los prejuicios morales.
Pero nadie duda del resultado final.
En un futuro será técnicamente factible la constitución de bancos de óvulos para las mujeres estériles,de igual manera que hoy existen bancos de esperma destinados a suplir la esterilidad masculina.
Se adivinan también otras revoluciones derivadas de esta técnica.
El doctor Robert Edwards,padre del primer bebé-probeta y director del principal centro de fecundación in vitro,en Inglaterra,se muestra esperanzado,desde la perspectiva científica,en la posibilidad que tienen las primeras células del embrión para reproducirse de manera idéntica.
En ese estadio,la célula contiene instrucciones para fabricar la totalidad del ser humano,pues las células no empiezan a diversificarse hasta más adelante.
Consecuentemente,un embrión que tenga entre dos y cien células puede ser separado en dos partes iguales,sin que se altere su desarrollo posterior.
Y,así,uno de los dos conjuntos celulares obtenidos serviría para fabricar un embrión gemelo.
Si un embrión gemelo ha sido fecundado por un solo óvulo,se podrán colocar en el vientre de la madre dos embriones,lo que eleva las posibilidades de embarazo.
El doctor Edwards apunta,asimismo,la idea de desarrollar el embrión gemelo en un cultivo adecuado hasta la formación de sus órganos principales ; de este modo,el doble del embrión gemelo implantado en la madre representaría una reserva de tejido a la que se accedería en caso de accidente.
Si la congelación de embriones llega a convertirse en algo común,no sólo se podrá esperar el momento más idóneo para su implantación,sino que también permitirá efectuar su cariotipo,es decir,el examen de sus cromosomas.
El cariotipo posibilitaría conocer el sexo y también localizar cualquier anomalía genética,algunas de las cuales se corregirían en ese mismo estadio.
Pero estos avances de la ciencia quedan inmersos en un enorme vacío jurídico.
¿Se impone estudiar una nueva definición de persona? Se están empezando a dar los primeros pasos en busca de una normativa.
Así,los trabajos de fecundación in vitro van a ser homologados por un consejo internacional que preside Robert Edwards y de la que por España forma parte el doctor Barri,del Instituto Dexeus (Barcelona).
Por otra parte,una comisión británica,la comisión Warnock,ha sido la pionera en el mundo en redactar un informe con propuestas sobre temas tan delicados como los derechos del embrión,derechos y deberes de los donantes,de las madres inseminadas... Pero no dejan de ser eso: propuestas.
Científicos,juristas y defensores de la moral muestran profundas divisiones al respecto.
Por ejemplo,el Instituto Nacional de la Salud y la Investigación Médica de Francia se declaraba rotundamente contrario a la congelación de embriones...,hasta que un embrión congelado se convirtió en una niña preciosa.
Por su parte,la comisión Warnock,que se pronunció a favor de la congelación y su explotación científica,fijó en el día 14 la fecha límite a partir de la cual toda experimentación con el embrión debía establecerse ilegal,ya que en este momento empieza a formarse el primer rasgo reconocible de forma embrionaria.
Australia,que sólo permitía investigar hasta el séptimo día,proyecta ampliar el límite hasta el decimoséptimo,en que aparece el canal neural.
Los daneses consideran tolerable el margen hasta el día 21,en que surgen los primeros pliegues neuronales.
Y el Consejo para la Ciencia y la Sociedad,organismo oficial británico,aumenta el período hasta los 40 días,antes del cual el embrión es incapaz de sufrir dolor.
Discusiones aparte,sólo investigando con embriones humanos se avanzará en el estudio sobre la fertilidad.
El embrión podrá servir también para comprobar el efecto de medicamentos y sustancias tóxicas en los tejidos humanos.
La realidad se anticipa a las suposiciones más rocambolescas.
En Australia permanecían congelados dos embriones cuando sus padres,unos latinoamericanos millonarios,fallecían en un accidente.
Si fueran transferidos al útero de una mujer,¿tendrían derecho a la inmensa fortuna de sus padres naturales? Una comisión australiana recomendó su destrucción.
¿Creerán los moralistas que esto es un crimen? Puede también suceder que un hombre que ha aceptado por escrito la inseminación artificial de su mujer con semen de donante entable un proceso para negar su paternidad.
Y el tribunal,hoy por hoy,le dará la razón,con lo que el niño se convertiría en hijo natural.
Tampoco hay que descartar la circunstancia de un niño que tenga cinco progenitores: la mujer que ha donado el óvulo,el hombre que ha cedido el esperma,otra mujer que ha recibido.
el embrión en su útero y la pareja estéril que solicitó tener este hijo.
¿Cuál es la frontera entre las peticiones aceptables y las inaceptables? ¿Qué condiciones deben exigirse a los donantes y a los receptores? También se han elevado peticiones de parejas lesbianas que piden una fecundación in vitro con el óvulo de una,para ser insertado después en el útero de la otra.
En definitiva,todo lo que se quiera imaginar.
Sobran,como se ve en estos ejemplos,las palabras.
Pero,también como se deduce de estos casos.
faltan las leves.
A Albert Rosenfeld,que fue responsable de la sección científica de la revista " Life ",un amigo le preguntó por qué dirigía aquella sección: - - ¡Ciencia! ¿Cómo te has remontado a esas alturas? - - En esas alturas - - respondió el periodista - - es donde se encuentra la acción.
Efectivamente,una parte sustancial de la información tiene hoy su base en la ciencia y en la tecnología,cuyo desarrollo está ligado directamente a nuestro bienestar.
Y el periodismo científico,tal como lo conocemos y lo practicamos hoy,se inicia precisamente cuando la actividad científica empieza a transformarse en actividad económica,cuando este trabajo se convierte en capital.
Hoy,los horizontes que se abren al divulgador de la ciencia son fascinantes y podría decirse que infinitos.
Si se me permite un ejemplo personal,desde que me inicié en el periodismo científico,en la década de los 50,han ocurrido cosas tan impresionantes como éstas: se ha descifrado el código genético ; se han roto las cadenas de la gravedad,se han sondado los planetas,se vencen enfermedades mortales,se alarga la vida de la gente (se añaden años a la vida,aunque no siempre vida a los años),la Tierra va camino de ser la " aldea global " de McLuhan,se han inventado el radar,el láser,la penicilina y la endoscopia,se han desterrado (aunque no para todos,desgraciadamente) el hambre,el dolor y la enfermedad y se prepara el viaje a las estrellas.
El dominio del átomo,del electrón y de la bacteria nos permite transformar todo lo que queramos.
La humanidad ha completado la conquista de la Tierra.
Y el desarrollo aterrador del microchip puede convertir en inteligente a todo lo que teníamos hasta ahora.
El cambio tecnológico del último tercio del siglo,a sólo quince años del tercer milenio de nuestra era,ha sido comparado a la caída del Imperio romano,a la Reforma,a la Revolución Industrial,al descubrimiento del fuego y a la invención de la rueda,la palanca,la brújula y la imprenta.
¿Se han duplicado nuestros conocimientos? ¿Se han triplicado? Y en este contexto apasionante y sugestivo surgen unos profesionales,los periodistas científicos,con una pretensión inaudita: acercar al pueblo el conocimiento.
Pocos objetivos tan decisivos podrá proponerse una democracia.
La información se confunde con el conocimiento y quienes dominen una y otro dominarán el futuro.
Esto lo saben muy bien los japoneses,que han abordado el empeño de dioses del ordenador de quinta generación.
En España,para poder llegar algún día a acometer una empresa parecida,nos falta un largo camino de educación popular y de creación de un clima de interés hacia la ciencia.
Y para tratar de conseguirlo,! os miembros de la Asociación Española de Periodismo Científico hemos propuesto,en los cuatro congresos iberoamericanos celebrados hasta ahora (Caracas,1974 ; Madrid,1977 ; Méjico,1979,y Sao Paulo,1982) y últimamente en el I Encuentro patrocinado por la Fundación Ramos Areces,la apertura de un diálogo entre científicos,periodistas y educadores para buscar cómo poner los medios de comunicación de masas al servicio de la educación popular,del desarrollo integral y de la democracia cultural.
El Estado,los entes autonómicos y las corporaciones públicas y privadas tienen la obligación urgente de promover y extender el conocimiento,para que la mayoría pueda participar en las conquistas de la minoría y para impedir que lo más noble del espíritu humano,el saber,sea fuente de desigualdad y desequilibrio entre los habitantes de un país o se degrade hacia un analfabetismo electrónico,o resulte incapaz de ayudar al hombre a combatir los terrores del año 2000.
Por su parte,los medios informativos se enfrentan con una grave responsabilidad social al tener que asumir nuestra cuota de participación en la tarea de ayudar a nuestros contemporáneos a entender lo que está pasando y a ofrecerles herramientas conceptuales que les permitan tomar unas decisiones colectivas.
Parece necesario,por cuanto llevamos dicho,extender el campo de acción del periodismo científico y buscar sistemas de ayuda a los medios informativos que se propongan como objetivo básico la difusión de la cultura y de la ciencia.
Necesitamos apoyo de la sociedad para que deje de ser realidad la dolorida queja de Laín: " Hacer ciencia en España es llorar.
" Tengo el máximo respeto por cualquier instrumento de comunicación de masas,aunque se dedique a actividades que a mí puedan parecerme frívolas,negativas o deseducadoras.
Pero quienes difunden el conocimiento y popularizan la ciencia de modo riguroso y periodístico han de ser más asistidos por el Estado y la sociedad,por aquello tan conocido de " Rebelión en la granja ".
Todos los animales son iguales,pero unos son más iguales que otros.
El autor,un psiquiatra llamado Liebowitz,no preconiza en absoluto la utilización de tranquilizantes ni sustancias similares,cuyo único efecto es el de anestesiar provisionalmente la angustia sin afectar a las causas.
Su argumentación,teórica y práctica,parte de los fenómenos que ocurren en nuestro cerebro cuando somos felices o desgraciados en amores.
Lo cierto es que existen muy diversas formas de enamoramiento: el clásico flechazo,la amistad que poco a poco se va transformando,la convivencia laboral con alguien del que,un buen día,nos sentimos enamorados sin haber reparado nunca en ello... También existen múltiples formas de ser fiel,celoso,posesivo,de reaccionar ante una ruptura,un desamor... Son rasgos de nuestra personalidad que o bien nos incitan a enamorarnos muy frecuentemente,sin conseguir nunca una estabilidad prolongada con una sola pareja,o bien nos convierten en seres conyugales,que pueden ser capaces de enamorarse dos o tres veces en su vida,pero siempre conservando en cada caso una absoluta fidelidad a ese amor.
Algunos conservan su propia autonomía,respetando la del ser amado ; otros son incapaces de vivir sin él un solo día.
La separación,que puede no ser para algunos más que un mal rato,se convierte para otros en un auténtico drama que puede llevar a enfermedades,incluso al hospital o al suicidio.
Toda nuestra vida sentimental,mucho más compleja aún de lo que hemos sugerido en el párrafo anterior,está íntimamente relacionada con la actividad de nuestro cerebro.
Las diferencias entre las distintas maneras de ser - - y de amar - - deben corresponder,pues,a rasgos biológicos individuales.
Si las sustancias que actúan sobre el psiquismo humano (medicamentos,café,tabaco,drogas) no se inventan las reacciones afectivas que podamos sentir,sino que las modulan,amplificándolas,disminuyéndolas o perturbándolas de muy diversas formas,las emociones actúan de la misma manera,aumentando o frenando la actividad de los circuitos nerviosos responsables de la afectividad,es decir,del placer o del sufrimiento.
Por ejemplo,las endorfinas reducen la actividad de los sistemas de neuronas implicados en el dolor.
La feniletilamina (PEA) actúa de forma similar a las anfetaminas,que estimulan la actividad de las neuronas implicadas en los estados de excitación.
Las betacarbolinas aumentan el estado de alerta,a veces hasta el nivel de ansiedad,frenando las neuronas inhibidoras.
Y,en conjunto,las vicisitudes de nuestra vida afectiva dependen de estas y otras drogas endógenas,fabricadas por nosotros,cuyos efectos son comparables a los de las drogas exógenas,ingeridas o inyectadas.
Placer y penas de amor no son más que el resultado de la activación de estos sistemas neuroquímicos por las emociones que sentimos a cada momento.
Las emociones son necesarias.
Estimulan la actividad biológica frente a las circunstancias exteriores que reclaman una respuesta eficaz.
El ritmo cardíaco y la respiración se aceleran,las reservas bioquímicas son vertidas a la sangre para aportar a los músculos y a los órganos un suplemento de energía.
La emoción responde al estímulo que nos produce un objeto o una situación que pone en peligro la supervivencia del individuo o de la especie.
Por eso existen el miedo,el hambre,la sed,la ira... y el amor.
En los seres humanos,el factor desencadenante de la emoción puede ser incluso reemplazado por su símbolo: una imagen,un simple nombre,una frase.
Nuestro cerebro está interconectado de tal modo que se pone en estado de emoción al recibir la señal,directa o simbólica.
Los centros cerebrales que organizan ese estado de emoción son el lóbulo límbico y el hipotálamo.
Son regiones de la corteza situadas en la cara interna de cada hemisferio y habitualmente denominadas cerebro primitivo,ya que se encuentran,con distintas variaciones,en todos los vertebrados.
Lo cual,dicho sea de paso,es buena prueba de su importancia vital.
Desde que nacemos,esas regiones cerebrales responden a estímulos precisos: los que significan bienestar (en un bebé,la satisfacción de sus necesidades primarias,hambre,calor) o los que significan sufrimiento o angustia (carencia de esas necesidades).
A medida que nuestra personalidad va madurando,los aprendizajes se multiplican,dejando su huella en nuestra memoria.
Si las situaciones memorizadas se han asociado al bienestar o al sufrimiento,se inscriben en el lóbulo límbico.
Con lo cual un adulto posee toda una reserva de memorias emocionales superpuestas a las memorias innatas de su propia especie y que varia para cada uno de nosotros según nuestra propia experiencia vital.
Cualquier percepción nueva es comparada por nuestra corteza cerebral a esas memorias innatas o aprendidas.
Si existe suficiente coincidencia,la emoción aparece.
Lo cual supone,según las interconexiones establecidas en el cerebro,la activación del placer o del sufrimiento,según los casos.
Y además la nueva percepción se fija,a su vez,en la memoria,enriqueciéndola aún más.
Constatación fundamental: cuantas más memorias felices posea una persona,más posibilidades tiene de vivir un nuevo estímulo de forma placentera,e inversamente.
Y esto es válido desde los primeros momentos de nuestra vida.
Todo lo cual va a desempeñar,como es lógico,un papel importante en las diferencias individuales frente al amor.
La zona de las memorias del lóbulo límbico que va a desencadenar la activación emocional cuando una percepción encuentra allí eco,se sitúa al nivel de la quinta circunvolución temporal,entre el córtex y la formación arcaica del hipocampo.
El lugar clave que parece desencadenar,según los casos,el placer,el sufrimiento o la angustia parece estar situado en los dos extremos de la herradura que forma el lóbulo límbico: el septum y la amígdala.
Así nacen las emociones.
Cuando vemos,sentimos,escuchamos,gustamos... (en el ejemplo,estímulo gastronómico) (1),el cerebro trata la información (2),la pasa al lóbulo límbico (3),que la envía a la sede de memorias (4).
Si la percepción no tiene recuerdo (5),será neutra ; si lo tiene,desencadenará una emoción.
Numerosos trabajos han probado que las reacciones placenteras se deben a la activación de una red de fibras y células que va del lóbulo frontal al tronco cerebral,pasando por el hipotálamo.
La zona que parece desempeñar un papel muy especial en la activación del sistema de placer se encuentra al nivel del septum,formación muy arcaica situada por delante del lóbulo límbico.
En los seres humanos se han implantado electrodos en la región correspondiente (en enfermos gravemente inhibidos) y se ha podido observar un comportamiento de autoestimulación: los sujetos experimentaban un placer o una felicidad extremos cuando se les estimulaba eléctricamente.
Tales descubrimientos han servido para estudiar,por ejemplo,el aprendizaje.
Se ha observado así que las neuronas que estimulan normalmente al sistema de fibras del placer descargan neurotransmisores de la familia de las catecolaminas (noradrenalina,NA,y dopamina,DA).
Lo cual significa que todas las drogas que actúan liberando catecolaminas deben,por la misma razón,influir en la activación del sistema de recompensa.
CUANDO,en 1932,Fernando de los Ríos,a la sazón ministro de Instrucción Pública y Bellas Artes,firma el decreto fundacional de la Universidad Internacional Menéndez Pelayo (UIMP) deja constancia expresa de lo que constituye la misión primera de esta institución: " Organizar cursos generales y conferencias sobre temas variados de interés general en sus diferentes ramas cientificas y literarias ",con objeto de satisfacer " los requerimientos universales de cualquier conciencia sensible a la contemporaneidad ".
Una misión ésta,la extensión cultural,que ha adquirido en los años recientes una especial trascendencia al haberse decantado el aumento de la demanda de bienes y actividades culturales como uno de los hechos más sobresalientes y alentadores de la España democrática.
La progresiva consolidación institucional de la UIMP,su creciente proyección social y protagonismo en la vida cultural española imponen como tarea a todos los que trabajamos en tan singular empresa cultural afirmar al máximo nuestra receptividad hacia los requerimientos que plantea esa " contemporaneidad ",que ya se contempla en el decreto fundacional.
Se trata,en suma,de seleccionar adecuadamente las materias tratadas,con objeto de elevar la calidad de los cursos,conectar más estrechamente con las demandas culturales de la sociedad y contribuir,desde el modesto papel de esta institución cultural,al necesario diseño de la España del futuro.
En este sentido,me interesa subrayar,como propósito que ha presidido la programación de la UIMP de este año,la creciente atención otorgada a aquellas áreas y temas que conforman hoy la frontera del saber científico.
Se ha tratado así de contribuir a una cada vez más profunda integración de la cultura científica en la reflexión intelectual para sensibilizar a la opinión pública sobre el permanente desafío que supone el acelerado desarrollo científico y las nuevas tecnologías ; para contribuir,en definitiva,a la necesaria renovación y modernización de la universidad española.
Acorde con este planteamiento se ha procecido a una selección rigurosa de los temas,realizada,en una gran parte de los casos,en colaboración con el Consejo Superior de Investigaciones Científicas,la Comisión Asesora Científica y Técnica,el Centro para el Desarrollo Tecnológico e Industrial y otras instituciones especializadas.
Con satisfacción hemos podido comprobar cómo las áreas de actuación preferente del Plan Europeo de Cooperación Tecnológica aprobado en Milán coinciden prácticamente con las aquí incluidas.
Las razones que explican esta orientación son fácilmente resumibles: en primer lugar,las nuevas tecnologías forman parte de nuestro mundo cotidiano y son ya determinantes de primer orden en nuestro modo de vida.
Además,y en segundo lugar,y más en una universidad como ésta que se califica de internacional,vencer el tradicional acientifismo de la cultura española y romper la esquemática identificación de la cultura con las humanidades.
Y,por último,porque compartimos la idea de que esta hora de una España incorporada a la Comunidad Económica Europea,con un sistema democrático consolidado y un admirable capital humano,es una hora crucial para no perder otra ocasión,recuperar el terreno perdido y romper,en definitiva,el maleficio de anteriores frustraciones históricas de nuestro itinerario como nación.
Hemos dado,pues,un especial realce a esos temas,que no ha mermado,sin embargo,el interés que en nuestro programa de seminarios y actividades tienen desarrollos concretos y ciencias sociales y saberes humanísticos.
Y no por prurito de elemental simetría o artificial equilibrio,sino por creer que el propio desarrollo tecnológico también depende de la riqueza y diversificación del patrimonio cultural de quienes lo reciben.
" Cultura y nuevas tecnologías " es,muy consecuentemente,el titulo que hemos elegido para el conjunto de nuestras actividades,convencidos de que es el cruce de lo científicotécnico con lo literario.
EL hombre siempre ha soñado con reemplazar parte de su anatomía por elementos artificiales que pudieran eventualmente mejorar lo que la naturaleza nos depara.
Sin embargo,pocas cosas son más difíciles: la extraordinaria complejidad de los tejidos vivos impide una sustitución eficaz y mínimamente utilitaria.
Por eso,hablar de repente del corazón artificial se nos puede antojar como una especie de herejía biológica,aun a sabiendas de que esa víscera no es más que una bomba que distribuye la sangre por todo el organismo y no,como se creyó antaño,la sede del alma y los sentimientos.
Cuando los trasplantes cardíacos se han ido generalizando en todo el mundo con un éxito todavía discutible,crece la esperanza de conseguir una máquina artificial capaz de suplir en su vital misión al músculo cardíaco irrecuperable.
Sólo que,por ahora,los resultados no son nada brillantes,a pesar de que sólo en la investigación del corazón artificial denominado Jarvik - 7 se gastaron,en 1982,nada menos que 350 millones de dólares (unos 60.000 millones de pesetas).
Son,sin duda,los primeros pasos balbuceantes de unos trabajos que quizá nos lleven,con el tiempo y mucho más dinero,a éxitos más esperanzadores.
Pero son muchos los que se preguntan si todo este esfuerzo humano y económico se debe emplear en investigaciones tan azarosas,existiendo como existen otras prioridades mucho más urgentes en el mundo de la medicina.
Incluso hay personas que acusan a los investigadores privados de preparar,mediante cuantiosas inversiones no necesariamente rentables a corto plazo,el futuro monopolio de los corazones artificiales,que se parece mucho a un sucio negocio... Lo cierto es que en los Estados Unidos las grandes empresas compiten por financiar a pequeñas iniciativas privadas,como Novacor,Thermedics y otras que quizá sean capaces de inundar el mercado de corazones artificiales en la próxima década.
Arabia Saudí apuesta,en cambio,por el módulo experimental francés desarrollado por el doctor Lapeyre,en colaboración con la empresa aeronáutica Aérospatiale.
Y los doctores Kolff y Jarvik,americanos,y Akutsu,japonés,de la Universidad de Utah,están consiguiendo financiar su proyecto Jarvik - 7,implantado con éxito dudoso en tres pacientes recientemente,a base de cifras que superan al año los 50.000 millones de pesetas.
Al principio,tras los primeros trasplantes de corazón realizados por Barnard,los propios cirujanos se volcaron en la posible solución de un corazón artificial que limitara o evitara totalmente los problemas de rechazo.
El desafío tecnológico era,no obstante,inmenso: la máquina debería responder a imperativos de una severidad sin precedentes.
Especialmente,respecto a la biocompatibilidad y duración de los materiales,la miniaturización,el peso,la forma anatómica,la fuente de energía y el mismo principio de funcionamiento.
Kolff y Akutsu prueban en animales el primer modelo,de aire comprimido,el 12 de diciembre de 1957.
Supervivencia máxima: 90 minutos.
En 1959,una ternera llamada Fumi Joe aguanta 221 días con un prototipo de laboratorio precursor del actual Jarvik - 7. Diez años más tarde,en mayo de 1969,se inaugura la era del corazón artificial aplicado a humanos.
El doctor Denton Cooley,de la Universidad de Houston (Texas),implanta en un hombre de cuarenta y siete años un corazón mecánico de aire comprimido,puesto a punto por el argentino Domingo Liotta.
El enfermo vive 54 horas con este corazón artificial,hasta que se consigue un corazón natural para trasplantárselo.
Día y medio después muere de pulmonía ; el corazón mecánico sólo sirvió para obtener unas horas de plazo hasta la llegada de un corazón humano trasplantable.
La polémica se hace muy agria: ¿para qué tanta operación si sólo se ganan unas horas? ¿No es eso cobayismo humano? El doctor Cooley se defiende: su enfermo tuvo así una posibilidad que jamás hubiera tenido de otra forma.
En esa época,la cirugía de trasplantes se enfrentaba todavía al agudo problema del rechazo con todas sus secuelas.
Lo cierto es que no parecía demasiado justificada una operación de ese calibre con tan poca experiencia clínica al respecto.
El escándalo salpica a Cooley,que no había obtenido autorización oficial para su operación ; pierde las subvenciones públicas de investigación y enseñanza,y es expulsado del hospital.
Pero la iniciativa privada toma el relevo de las autoridades estatales.
Y doce años más tarde,ya en 1981,Denton Cooley reincide: su enfermo,un hombre de treinta y seis años,vive durante tres días con un corazón artificial a la espera de un corazón trasplantable.
Una vez efectuado ese trasplante,muere una semana más tarde de septicemia.
HACE unos dieciocho años apareció en los radiotelescopios una nueva categoría de estrellas,los pulsares.
Su nombre refleja una característica esencial,expresada en inglés por pulsating stars,estrellas pulsantes o,abreviadamente,pulsares.
En realidad,este nombre no es apropiado,aunque perduró,ya que las señales de radio de estos astros,perfectamente rítmicas,no se correspondían con una pulsación,sino con un barrido comparable al que realizan con la luz los faros giratorios.
Dos nuevos descubrimientos vinieron a complicar las cosas.
En febrero de 1983,dos astrónomos americanos,Seward y Harnden,descubrieron un pulsar que giraba sobre Sí mismo siete veces por segundo.
Lo cual no es mucho,ya que normalmente giran unas veinte veces por segundo.
Pero lo importante era que ese pulsar se encontraba en el emplazamiento exacto de una antigua supernova,al lado de la estrella Beta Circinus,en la constelación austral del Compás,al lado de la célebre Alfa Centauri,la estrella más próxima al Sol.
La tesis más corriente admitida hoy día es que,en efecto,los pulsares corresponden al corazón de antiguas supernovas,es decir,estrellas que explotaron esparciendo su masa por el Cosmos.
Lo que ocurre es que de los 350 pulsares censados hasta hace un par de años,sólo tres ocupan el antiguo emplazamiento de supernovas conocidas.
A la satisfacción por la coincidencia se vino a sumar la perplejidad: mientras que la edad del pulsar Circinus se evalúa en torno a los 2.000 años (lo cual se deduce del ritmo de decrecimiento de su rotación),la de la nebulosa que le rodea,residuo de la supernova que estalló,es de unos 20.000 años,calculados según su extensión.
Parece un misterio,porque ambas edades deberían coincidir,y la diferencia es enorme,aun contando con los posibles errores de cálculo.
Para complicarlo aún más,un segundo descubrimiento se ha venido a añadir a la sorpresa producida por estas diferencias de edad.
Gracias al radiotelescopio gigante de Arecibo,en Puerto Rico,el doctor Carl Heiles,de la universidad de California,ha descubierto el más rápido de los pulsares conocidos,en la constelación del Zorro menor,muy cerca del primer pulsar descubierto en 1 967.
El pulsar nuevo,bautizado 4 C - 21.53,giraba sobre sí mismo nada menos que a razón de 642 vueltas por segundo.
Se había calculado que para un pulsar normal,de unos 20 kilómetros de diámetroy una masa como la del Sol (1,9.
102 toneladas),el límite de ruptura mecánica originada por la fuerza centrífuga se sitúa en unas 2.000 vueltas por segundo ; es decir,sólo unas tres veces más que en el caso del pulsar 4 C - 21.53.
Hay que considerar que en la superficie de este objeto superdenso la velocidad lineal alcanza 40.440 km / seg,es decir,un 13,5 por 100 de la velocidad de la luz,se trata,en efecto,de una circunferencia de 63 kilómetros para una rotación de 1,558 milisegundos.
Un poco más,y el astro salta en pedazos... Teóricamente la velocidad de rotación de los pulsares es tanto mayor cuanto más joven es el astro.
El pulsar del Cangrejo,que gira a treinta vueltas por segundo,corresponde a la supernova que explotó en el año 1054 en la constelación de Tauro.
Según esta misma proporción,el pulsar 4 C - 21.53 no debería tener más de unos veinte años.
Pero ninguna supernova ha sido observada desde 1604,antes incluso de los primeros telescopios.
Y además,alrededor de este extraño pulsar no se observa nebulosa alguna que pueda corresponder a los residuos de la explosión ; y para colmo,el ritmo de sus pulsaciones parece mantenerse constante,en contra de lo que ocurre indefectiblemente en todos los demás casos,en que se aprecia una disminución progresiva del ritmo de rotación.
La estrella de neutrones constituye la fase final de la vida de las estrellas superpesadas.
Después de pasar por varias etapas (determinadas,en su secuencia,por la masa inicial de la estrella),el diámetro se estabiliza alrededor de los 20 kilómetros,con una masa equivalente como mínimo a la del Sol.
Se piensa que,en el núcleo,la densidad llega a ser de 1015 ; debe estar constituido por una mezcla de neutrones,protones y partículas elementales pesadas.
Por encima existe un manto de neutrones superfluidos,como el helio - 3. La corteza,de neutrones y hierro,es superrígida y está rodeada de una fina atmósfera de algunos centímetros de espesor,con neutrones aislados.
Sabemos,pues,describir matemáticamente la forma ovoide con gran precisión,pero resulta más difícil explicar por qué posee el huevo una forma tan particular.
Recordemos,antes que nada,que aunque la mayoría de los huevos de pájaro tienen una forma cercana a la del huevo de gallina,hay excepciones notables.
Existe,de hecho,una gran diversidad según las especies,desde los huevos casi esféricos del pingüino o del milano real hasta aquellos,casi cónicos,del chorlito real o del pájaro bobo.
Y no sólo eso: dentro de una misma especie podemos encontrar huevos con importantes diferencias,aunque tengan un parecido general.
Ni siquiera todos los huevos puestos por un mismo pájaro son rigurosamente idénticos.
En la gallina tienden a redondearse a medida que el animal va envejeciendo.
Los primeros huevos de una pollita suelen ser bastante atípicos,a veces anormalmente alargados ; sólo al cabo de un cierto tiempo van tomando la forma característica de la especie.
¿A qué se deben todas estas variaciones? Aristóteles pensaba que los gallos venían de los huevos más puntiagudos,y las gallinas de los más redondos.
A principios del siglo XIX,algunos naturalistas lanzaron la hipótesis de que el contorno del huevo indica la forma general del cuerpo del pájaro que se está desarrollando dentro.
Más adelante,los defensores de la selección natural trataron de demostrar,más elaboradamente,que los huevos de las diferentes especies habían tomado la forma más apta para su supervivencia,de acuerdo con el medio en el que evolucionaban.
Pero en 1914,Pearl y Surface,dos biólogos americanos,demostraron que los contornos de los huevos podían presentar variaciones tan importantes en la gallina doméstica,sobre la cual no actúa evidentemente la selección natural,como en las especies salvajes.
Actualmente,se reconoce generalmente que los factores fisiológicos son los principales responsables de la diversidad de formas en los huevos.
De hecho,ya en 1772 el ornitólogo alemán Gunther había afirmado que el contorno del huevo era el resultado de la presión ejercida por los músculos del oviducto,órgano en el que se forma.
En lo esencial,su teoría sigue siendo válida.
La teoría de Gunther,cuando se conoce el proceso de formación de un huevo,parece de lo más sensato.
Al principio,el huevo existe bajo la forma de un oocito ubicado en el ovario de la gallina (a diferencia de los mamíferos,los pájaros suelen tener un sólo ovario).
Este oocito,o huevo inmaduro,constituirá más tarde la yema.
El ovario de la gallina contiene una gran cantidad de oocitos,fijados a unas membranas llamadas folículos,y todos ellos en un estadio de desarrollo distinto.
Suelen madurar uno por uno,a razón de uno por día en las gallinas que ponen con regularidad.
A veces,sin embargo,se liberan dos oocitos a un tiempo de un mismo folículo,en cuyo caso el huevo tendrá dos yemas en vez de una.
La formación del huevo propiamente dicho se desarrolla en el oviducto,órgano correspondiente a las trompas de Falopio de la mujer.
Como con el ovario,la gallina adulta tiene un oviducto.
De hecho,al principio,en el pollito hembra se desarrollan dos ovarios y dos oviductos,pero los órganos del lado derecho regresan rápidamente al estado embrionario y sólo subsisten el ovario y el oviducto izquierdos.
Lo mismo ocurre con la mayoría de los pájaros,excepto con ciertas especies salvajes que conservan los órganos derechos,lo que no significa que pongan dos huevos al día.
Una vez desprendido de su folículo,el oocito,que llamaremos yema a partir de ahora,se mete por una especie de embudo,el infundibulum,puerta de entrada del oviducto.
Allí se produce la fecundación,cuando la gallina es inseminada.
La yema prosigue su avance por un conducto tubular,el magnum,en donde se produce el depósito de las capas de albúmina que formarán la clara.
Todo el proceso dura alrededor de cuatro horas.
El huevo,provisto ya de su clara,llega entonces a un paso estrecho,el istmo,en donde dos membranas conchíferas se colocan en torno a la albúmina.
Al principio,las membranas constituyen un envoltorio apretado,pero se distienden rápidamente.
El huevo entra luego en el útero.
En ese momento,las membranas conchíferas están completamente distendidas y flotan en torno a la clara.
Durante las cinco horas siguientes se desarrolla un proceso conocido bajo la apelación anglosajona de plumping (hinchamiento).
Consiste en una introducción gradual de agua y de sales a través de las membranas hasta que éstas se tensan de nuevo y el huevo queda hinchado.
Preliminar indispensable para la calcificación de la cáscara,que ocupa las quince o dieciséis horas siguientes.
La cáscara está constituida principalmente por calcita ; es decir,por cristales de carbonato de calcio.
Comprende tres capas: en el interior,una capa mamilar,a base de cristales cónicos ligados a la membrana conchífera por puentes de queratina,los núcleos mamilares (esta capa representa un tercio del espesor total).
Viene recubierta por la capa palisádica,la más espesa,constituida por columnas de cristales de calcita apretados los unos contra los otros ; por último,el conjunto está recubierto por una fina cutícula que se deposita justo antes de ser puesto el huevo.
Si el huevo no hubiera estado sometido a fuerzas exteriores cuando aún era maleable,tendería hacia la forma esférica.
En efecto,toda membrana elástica que contiene un fluido se contrae cuanto puede,con vistas a alcanzar un equilibrio en el que las tensiones superficiales sean mínimas.
Idealmente,ello corresponde a la más pequeña superficie posible para un volumen dado ; es decir,la esfera.
Las pompas de jabón,las gotas de agua y los balones (excepto... los de rugby) ilustran este principio a la perfección.
Cuando vuelvan a nuestras latitudes habrán mudado su plumaje y del capuchón pardo no quedarán más que un par de manchas oscuras a ambos lados de la cabeza,en la zona auricular.
Hace doscientos años,la gaviota reidora presentaba una distribución de tipo paleártico centrada durante el período estival en una ancha banda que cruzaba el continente eurasiático de un lado a otro,por sus latitudes más templadas.
Con la llegada de los primeros fríos,la mayor parte de los individuos se desplazaban hacia el sur,ocupando toda la periferia del Mediterráneo y las costas atlánticas de Africa.
Sin embargo,desde principios del siglo pasado,es decir,coincidiendo con el comienzo de la revolución industrial y con el imparable crecimiento de las ciudades que trajo consigo,las observaciones realizadas sobre las poblaciones de reidora constatan un continuo aumento de sus efectivos y,como consecuencia lógica,una no menos continua expansión de su área de distribución.
Este proceso,acentuado sobre todo durante el presente siglo y sin visos de irse a detener por el momento,amplía inexorablemente de año en año los límites del área de cría,tanto hacia el norte como hacia el sur.
Así,a principios del siglo XIX se observaron por primera vez sus nidos en Finlandia,en 1867 en Noruega,en 1911 en Islandia,en 1969 en Groenlandia,en 1965 en Cerdeña... Tanto en Italia como en España,la primera nidificación data de 1960 ; en el caso de nuestro país,se ha producido desde entonces una lenta expansión,por lo que hoy contamos con algunas pequeñas colonias de cría en la Albufera de Valencia,el Delta del Ebro,la laguna de Gallocanta o las zonas húmedas manchegas.
Paralelamente,el área de invernada va dilatando también sus límites hacia el corazón de Africa y sus costas orientales,donde las gaviotas reidoras son habituales desde los años setenta,o incluso hacia América,donde empiezan a ser frecuentes en el Labrador,Nueva Inglaterra o Barbados.
La clave de esta extraordinaria proliferación,que ha situado la población europea actual de gaviotas reidoras por encima del millón de parejas reproductoras,debe buscarse ante todo en el notable eclecticismo que les permite vivir prácticamente en cualquier sitio,alimentándose de cualquier cosa.
A diferencia de otras gaviotas,la reidora puede vivir indistintamente en lugares costeros y del interior ; y aunque generalmente elige tierras bajas,se la ha llegado a ver a más de 2.000 metros de altitud.
En realidad,el único requisito verdaderamente imprescindible para su asentamiento es la existencia de masas de agua tranquila y poco profunda,ya sea dulce,salada o salobre,en torno a las cuales pueda centrarse la vida de la bandada,que las utilizará como dormidero.
fuente de alimentos o lugar para construir sus nidos.
Espacios con estas características no faltan ciertamente en los países que habitan las reidoras: lagos,lagunas,ríos de corriente lenta,albuferas,marismas,deltas,estuarios y,también,masas de agua artificiales como embalses,canales,graveras o salinas,son algunos de los lugares que suelen colonizar estas aves.
Además,mantienen hacia ellos una fidelidad que les hace volver a los mismos parajes año tras año,sin importarles los drásticos cambios que puedan sufrir.
Las gaviotas suelen mostrar una sorprendente capacidad para adaptarse rápidamente a cualquier circunstancia nueva,alterando si es preciso sus costumbres habituales.
Ello constituye,obviamente una razón más de su desbordante expansión actual.
Otra causa importante es su alimentación,tan variada en sus fuentes y modos de obtenerla como en su composición,que abarca prácticamente todo tipo de materia orgánica.
En áreas rurales,el grueso de la dieta lo constituyen insectos y lombrices,obtenidos en gran medida de las tierras removidas por las labores agrícolas.
Por eso es muy frecuente encontrar en los campos nutridas bandas de gaviotas siguiendo de cerca a los arados y tractores.
Y como complemento,también se alimentan de una variadísima gama de pequeños animales (peces,crustáceos,moluscos,huevos o pollos de otras aves o aun de sus propios congéneres,ratillas,topillos) y también de materia vegetal,fundamentalmente frutos o granos de cereal.
Sin embargo,en la periferia de las ciudades una única fuente de alimento eclipsa casi por completo a todas las demás: los desechos orgánicos procedentes de las actividades humanas,en los que se engloban tanto las basuras domésticas como los restos generados en mataderos,puertos o factorías pesqueras.
Como consecuencia,miles de gaviotas,atraídas por la posibilidad de alimentarse sin esfuerzo,acuden en invierno a instalarse en torno a las ciudades en aglomeraciones tan notables como los 300.000 individuos que han llegado a contabilizarse en Londres.
Se las puede ver durante el día afanarse en los vertederos urbanos o pasearse por parques y calles,y desplazarse por la noche organizadamente a sus dormideros en embalses,lagos o ríos situados a veces a muchos kilómetros de sus lugares de alimentación.
Hacia febrero o marzo,según las latitudes,las gaviotas reidoras sufren una muda: la cabeza se cubrirá con el capuchón oscuro característico del período estival.
Precisamente coincidiendo con ella se inician los vuelos migratorios,más o menos largos,que las llevarán a sus lugares de reproducción,situados casi siempre en zonas encharcadas del interior,muy raramente en la costa.
En ellos,las reidoras formarán colonias de cría abigarradas,ruidosas y en apariencia anárquicas,pero sometidas en realidad a una rígida compartimentación en territorios que cada pareja reproductora defiende bravamente.
El tamaño de estas colonias es muy variable,oscilando entre el caso,muy frecuente,de miles de parejas y el más raro de nidificación aislada ; este último tiene siempre menores probabilidades de éxito que los nidos agrupados,pero con frecuencia significa el primer paso en la colonización de un nuevo enclave.
Las parejas,que comienzan a formarse ya durante el viaje migratorio,permanecerán unidas a lo largo de todo el período reproductor y,con frecuencia,durante muchos años sucesivos,aunque la relación se establece de nuevo al principio de cada temporada.
Dotadas,como todas las especies sociales,de un complejo abanico de pautas de comportamiento que se despliega especialmente en este período de la reproducción,las gaviotas constituyen uno de los objetos de investigación favoritos de los etólogos,que han descrito minuciosamente todo el ritual que regula las diferentes etapas de la vida de la colonia.
En el caso de las reidoras,su llegada al área de cría va acompañada por el desarrollo de una tendencia a la territorialidad de la que carecen durante el resto del año.
Los machos delimitan inicialmente un espacio para el apareamiento y lo defenderán contra los otros machos.
Desde él,atraerán a las hembras desparejadas mediante una serie de pautas específicas.
Cuando la hembra que acude es la misma de los años anteriores,los lazos de pareja se establecen de nuevo con rapidez y sin problemas.
Pero cuando se trata de hembras que se aparean por primera vez o que han perdido su pareja,suelen mostrarse vacilantes y recorren los territorios de varios machos antes de decidirse por uno.
Una vez formadas las parejas tiene lugar la cópula,que se repetirá,en el territorio de apareamiento,primero,y en el de nidificación,después,hasta el comienzo de la incubación.
El nido se construye en un territorio diferente del de apareamiento,aunque a veces incluido en él ; siempre de menor tamaño.
De hecho,en las grandes colonias se dan apiñamientos tan considerables que los nidos pueden llegar a tocarse.
El lugar elegido puede estar colocado en tierra firme,sobre islotes o entre la maraña de vegetación de las aguas poco profundas.
El macho y la hembra se turnan para llevar ramas finas,tallos y hojas de vegetación palustre.
REALIZAR un experimento,ciertamente imposible pero no por ello menos deseado,seria lograr traer hasta nuestros días,al siglo XX,a nuestras estruendosas y vertiginosas ciudades,a uno de aquellos sabios contemplativos de la antigua Grecia o a un romano circunspecto y señorial de la ciudad de los Césares,o incluso a uno de aquellos monjes dedicados a copiar interminables pergaminos en las abadías del siglo X,o bien a un intrépido señór de las Cruzadas,con su brillante armadura y sus aceradas lanzas.
Traerles y observar sus reacciones,contemplar sus caras de asombro o de disgusto,o incluso de pavor.
Mucho nos tememos que el experimento no durase más de unas horas o,a lo sumo,unos días.
Probablemente el griego,el romano,el monje o el noble señor de la lanza morirían del susto,de miedo ante lo que les rodeaba.
O quizá no ; seguramente,en la primera fase de su llegada a nuestro civilizado mundo lo pasarían muy mal,francamente mal.
Pero si logran superar estos primeros momentos,descubriremos que pueden sobrevivir en este medio,que pueden adaptarse a algo tan aparentemente dañino.
Sus oídos,sus ojos,su aparato respiratorio... comienzan a soportar las agresiones.
Si sobreviven,el daño sufrido puede ser importante: una cierta sordera,un poco de tos crónica,un parpadeo apenas perceptible ; pero sobrevivirán.
Han logrado adaptarse al medio.
Han alcanzado,tras un período de desequilibrio ciertamente peligroso,un nuevo equilibrio importante con el medio.
En términos científicos,diríamos que han superado una fase de estrés para lograr una adaptación y un equilibrio correcto.
Si bien las palabras estrés y adaptación son relativamente recientes,vemos que los seres vivos han sufrido procesos como éstos desde hace más de 4.000 millones de años.
Algo más reciente,el hombre viene sufriendo etapas de estrés y adaptación de forma constante y casi a diario.
Al hablar de estrés nos referimos precisamente a las respuestas del organismo ante una situación brusca de cambio,o bien de peligro para su integridad.
No al accidente que ha motivado el peligro o el desequilibrio,sino precisamente a la forma y manera de responder ante él.
Esta reacción de estrés o síndrome general de adaptación es siempre la misma,sea cual sea el agente que la desencadena ; es,pues,una respuesta involuntaria del organismo que se desencadena con el único objetivo de sobrevivir.
Desde luego,aún no conocemos cuál es el primer eslabón de la cadena que pone en marcha todo el mecanismo de respuestas,desconocemos el desencadenante químico que enciende la mecha del proceso ; pero sea cual sea,comienza por activación de lo que se ha denominado el eje hipotálamo - - hipófisis - -,corteza suprarrenal.
Estas tres estructuras,bastante alejadas las unas de las otras,anatómicamente diferentes,y sin lazos de conexión visibles,comienzan a excitarse para lograr la respuesta deseada.
El hipotálamo y la hipófisis se encuentran alojadas en el interior del cráneo,formando parte del sistema nervioso central ; la corteza suprarrenal forma parte de una glándula de secreción interna localizada en la parte superior de ambos riñones.
La respuesta a esta estimulación es la rápida secreción de una serie de hormonas emparentadas,los corticoides.
En primer lugar,se libera a la circulación general una importante cantidad de glucocorticoides,encargados de la rápida liberación de energía y la puesta en marcha de diferentes caminos metabólicos que permitan una mayor producción.
El objetivo primordial de esta liberación y creación de energía es el de hacer frente a las necesidades que los músculos,el corazón y el cerebro van a necesitar para comenzar el camino de la adaptación.
Al mismo tiempo se produce la liberación de otro grupo de corticoides,los mineralocorticoides,encargados de poner en marcha y sostener todas las reacciones de defensa local estimulando la aparición de células defensivas,como los leucocitos,y poniendo en marcha mecanismos inmunológicos defensivos contra cualquier tipo de agresión exterior que pudiera producirse a nivel local.
Existen,además,otras secreciones no corticoides,como la de hormona somatotropa,o del crecimiento,encargada de preparar a los tejidos para la agresión,aumentando la producción de células óseas y de la piel.
Vemos cómo la rapidísima respuesta del sistema nervioso central ha servido para hacer frente a una primera agresión.
Pero también el sistema nervioso autónomo o parasimpático va a intervenir desde el primer momento en la reacción de estrés.
Gracias a su temprana estimulación se van a liberar importantes cantidades de adrenalina y noradrenalina,que van a producir la rápida respuesta de las vísceras: el corazón aumenta en seguida su velocidad de contracción y la fuerza de la misma.
Se incrementa así la cantidad de sangre expulsada por minuto ; se eleva de esta manera la tensión arterial,aumentando significativamente la cantidad de sangre que llega al cerebro,a los músculos y a todos los órganos que han de intervenir en los primeros momentos de la defensa.
Todas estas respuestas van encaminadas en una sola dirección: aumentar el poder del organismo,prepararle para la lucha e intentar prevenir los daños que con toda seguridad se van a producir.
Pero ello implica irremediablemente que otros sistemas se vean peor atendidos ; así,el tiroides,el timo,las glándulas de reproducción y diversos sistemas específicos de defensa pasan a ocupar un segundo plano.
Si esta respuesta se hiciera crónica,es decir,se prolongase más de lo debido,el sufrimiento del organismo podría ser realmente importante.
Lo que comenzó siendo una respuesta necesaria y fisiológica para nuestra defensa se acaba convirtiendo en una agresión nueva contra nosotros mismos ; y si el ataque es demasiado intenso o la respuesta de estrés es corta,la muerte será inevitable.
Sabemos hoy que muchas de las enfermedades que padecemos están producidas por respuestas de estrés mantenidas o perpetuadas ; la úlcera duodenal,la hipertensión,el infarto de miocardio,la aterosclerosis,la sordera,determinadas infecciones....
están producidas o favorecidas por estas respuestas anómalas del organismo.
Otros padecimientos,como las neurosis o las disfunciones sexuales,son el precio a pagar por la desatención producida ante la necesidad de una adaptación superior a nuestras posibilidades.
Tratar de evitar las respuestas excesivas puede estar en nuestras manos,para impedir así un daño quizá irreparable.
EN la primera mitad de los años sesenta,poco después de la creación de la Comunidad Económica Europea (CEE,1957),los países occidentales son ya plenamente conscientes de que para poseer un elevado nivel de desarrollo científico y tecnológico - - imprescindible para el progreso económico - - es necesario recurrir a organizaciones supranacionales,capaces de aunar y potenciar los esfuerzos individuales de cada país,sobre todo en algunas actividades con elevada proyección de futuro que,por su complejidad,requieren cuantiosos medios materiales y humanos.
Surgen entonces diferentes instituciones,muchas de ellas al margen de la propia CEE - - constituida en esa época solamente por seis países - -,entre las que merece destacarse al Centro Europeo de Investigación Nuclear (CERN,1961) y la Organización Europea de Investigaciones Espaciales (ESRO,1964),que posteriormente se convirtió en la Agencia Espacial Europea (ESA,1975).
En ambos casos nuestra participación en estas organizaciones se lleva a cabo en calidad de socio fundador.
No obstante,en el caso del CERN España se retira,por diversas razones,en 1968,para volver a incorporarse en 1982,mientras que la permanencia en ESA no se ha interrumpido hasta la fecha.
El resultado de tal participación ha sido aceptable en algunos casos y casi nulo en otros.
Hemos obtenido indudables progresos en determinadas áreas tecnológicas e industriales,que hubieran podido ser notablemente mejores si nuestras estructuras fuesen menos rígidas y más agresivas.
Quizá por ello nuestras actividades en estos campos denotan la existencia de claroscuros.
Sin embargo,nuestro fracaso más evidente corresponde a las actividades científicas: no hemos participado en ninguna de las casi 80 experiencias científicas llevadas a cabo por ESA,siendo así que en este campo es en el que España realiza un mayor esfuerzo económico,al tratarse de una actividad obligatoria de la agencia en la que debemos participar en función de nuestro producto interior bruto.
Lógicamente,un fracaso de tal envergadura obedece a profundas y complejas razones.
Sin ánimo de realizar un análisis completo de las mismas,me gustaría apuntar aquí algunas de las más importantes que,quizá,pudiéramos clasificar en tres grupos: de tipo conceptual,de tipo estructural y de tipo financiero.
Entre las del primer grupo hay que decir que en España todavía no hemos logrado entender que el programa científico de ESA - - que representa casi el 15 % del coste total - - no es en absoluto un lujo destinado a desarrollar la ciencia básica,sino el hilo conductor de todas las demás actuaciones a las que sirve de esqueleto o soporte articulado,a la vez que les proporciona una perspectiva de viabilidad futura.
Es en este sentido como lo entiende la agencia,de forma que,apoyadas sus acciones en tal esqueleto,se consigue dar coherencia,tanto a los desarrollos tecnológicos e industriales - - muchas veces producto de coyunturas muy circunstanciales - - como a la determinación de objetivos más profundos,alrededor de los cuales se generan las demás actividades.
En lo que se refiere a los factores estructurales,cabría señalar,entre los más importantes,la carencia de las dotaciones mínimas necesarias para los grupos científicos ya constituidos,así como la dispersión de los mismos,la ausencia de mecanismos capaces de estructurar y coordinar los esfuerzos realizados,la desproporción entre los objetivos fijados y las posibilidades reales,la falta de contactos científicos suficientes con otros grupos europeos,escasa información,carencia de estímulos adecuados,etcétera.
Por último,las dificultades de tipo financiero resultan del desequilibrio existente entre las cuotas que se pagan a la agencia y los fondos que se dedican al fomento interno en nuestras actividades.
En efecto,con el pago de las cuotas lo que obtenemos es el derecho a usar unos medios y a participar en su concepción y desarrollo,medios que la agencia pone a disposición de sus asociados.
Pero para usarlos efectivamente,cada socio debe financiar independientemente sus propios programas.
Es decir,con las cuotas solamente se paga el vehículo,y para hacer uso del mismo cada usuario debe sufragar a sus expensas los costes correspondientes.
Por eso es necesario comprender que si pagamos la cuota de la agencia y no usamos sus medios,lo que hacemos en realidad es financiar a los países que los usan,es decir,a los más desarrollados.
Parece necesario resolver esta situación de una vez por todas,reconociendo de antemano que la solución no es fácil.
En el modelo utilizado por la agencia se establece un reparto entre los esfuerzos dedicados a la investigación básica (15 %) y a la orientada,aplicada y de desarrollo (85 %) que,hasta el momento,está dando buen resultado.
La figura de los " Planes Nacionales " que aparece en el borrador de la Ley de la Ciencia puede ser un buen camino para intentar esa solución.
En cualquier caso,la próxima discusión en el Parlamento de dicha Ley ofrece una ocasión inmejorable para ello.
El espárrago común no es más que la parte escondida de una planta,que requiere una tierra que no sea ni fría ni compacta ni asfixiante.
Se trata de una planta que vive de diez a veinte años,gracias a unas raíces que se renuevan cada año a partir de un rizoma que aumenta anualmente y produce brotes nuevos.
El número de brotes determina el número de turiones que se podrá recolectar al año siguiente.
Estos turiones deben ser cortados y desterrados,pues de lo contrario inhiben el crecimiento de los brotes sucesivos.
Una vez que la planta ha repuesto sus reservas,se dejan de cortar los turiones,cuya punta sale a la superficie,se abre y se ramifica.
En otoño se nutre de las reservas que mantienen las raíces,las cuales a su vez cuentan con unas raicillas especiales para la absorción de agua y de los elementos nutritivos.
Como los pies masculinos son distintos de los femeninos,la fecundación debe ser cruzada: los insectos transportan el polen de las flores masculinas a los pistilos de las flores femeninas.
La técnica del esqueje es una forma de multiplicación vegetativa que conoce cualquier jardinero: se toma un fragmento de la planta madre (tallo,tubérculo o bulbilla) y se vuelve a plantar en tierra ; se desarrollará,convirtiéndose en una planta completa,que reproduce los caracteres de la planta original,ya que el individuo resultante es genéticamente idéntico a aquel del que procede.
El clonaje es una técnica equivalente,que utiliza en laboratorio (in vitro) el cultivo de los tejidos,produciendo células a partir de una sola,de la que todas son la réplica cromosómica exacta.
Desgraciadamente,el espárrago no se deja manipular con facilidad.
Sólo se pueden sacar esquejes del rizoma,y no más de dos o tres al año.
Antes de los cultivos in vitro,la mejora de la especie pasaba necesariamente por la reproducción sexuada,con todos sus azares genéticos.
Los criterios de selección son múltiples: uno de ellos atiende a que la punta del espárrago no abra demasiado rápidamente sus escamas (hojas) cuando el tallo asoma fuera de la tierra.
Según la mano de obra que haya,puede darse un margen de varias horas entre ese momento y el de la recolección,y no conviene que las cabezas se abran entretanto.
Cuando el agricultor planta una capa de espárrago,el terreno movilizado - - alrededor de 1 m2 por pie - - será ocupado durante más de diez años.
Es comprensible que desee garantías de que la variedad que ha plantado no mudará de comportamiento.
El caso es que,por un extraño capricho de la naturaleza,la mayoría de las cualidades requeridas suelen darse más en los pies de espárragos machos que en los pies de espárragos hembras.
Las plantas macho tienen fama de ser más productivas,más precoces,más robustas,más duraderas que las del otro sexo (aunque estas últimas,aunque menos productivas,suelan dar espárragos de mayor calibre).
Pero para garantizar la conformidad de las plantas a un tipo dado,el de la variedad comercializada,es decir,para garantizar la constancia de sus caracteres a través de las semillas vendidas,hay que tener la seguridad de que las plantas que han criado esas semillas son homocigotas.
Como se sabe,las características de un individuo vienen determinadas por los genes de los cromosomas.
Cada gene es el resultado de dos formas,semejantes o distintas,situadas en el mismo locus de dos cromosomas homólogos: los alelos.
Si los alelos son idénticos,el gene BS homocigótico.
Si no,heterocigótico.
Para que un individuo sea homocigótico,todos sus genes deben serlo también.
Técnicamente hablando se dice entonces que es un individuo ouro.
Un espárrago supermacho es una planta que sólo puede dar origen a otros espárragos masculinos.
El sexo de los espárragos viene determinado,como el del hombre,desde la concepción misma,por la naturaleza de los cromosomas heredados del padre y de la madre.
La fecundación de los gametos da lugar a un individuo que tiene las mismas probabilidades de ser masculino que femenino (dibujo 1).
El espárrago supermacho pertenece a un tercer sexo,representado por YY.
Su polen es portador exclusivamente del cromosoma sexual Y,por lo que al fecundar un óvulo X sólo puede dar lugar a un individuo XY,es decir,a una Planta masculina (dibujo 2).
Para satisfacer plenamente los deseos de los agricultores,habría que crear,pues,una variedad constituida exclusivamente por machos y,además,nacidos todos de progenitores homocigóticos.
En primer lugar,¿cómo se las arregla uno para producir más bien machos que hembras? El espárrago,como el hombre,procrea sujetos que en principio tienen tantas posibilidades de ser machos como hembras.
Situación sabiamente decretada por la naturaleza,y que no parece vaya a alterarse en toda la eternidad... Pues no.
Sí puede alterarse,al menos en el caso del Asparagus officinasis: los investigadores han logrado,mediante una extraordinaria sucesión de manipulaciones in vitro,confeccionar unos espárragos supermachos cuya descendencia directa se compone exclusivamente de individuos masculinos.
Los hijos de los supermachos,además,pueden ser homogéneos,si nacen de espárragos hembras de línea homocigótica.
La similitud biológica entre un espárrago macho normal y un supermacho de la misma especie es total.
Excepto en un punto: la repartición del sexo en la progenitura.
Los granos recogidos en una planta hembra fecundada por un macho normal dan un 50 % de plantas hembra y un 50 % de plantas macho,conforme a la regla.
Pero los granos fecundados por el polen de un supermacho dan un 100 % de plantas macho.
La naturaleza del sexo,como toda instrucción genética que concierna a la vida y al desarrollo del individuo,viene determinada por los cromosomas de la célula.
Es bien conocida la teoría cromosómica de la herencia,que explica la constancia de los factores de transmisión,descubierta por Mendel.
En todas las células del cuerpo (excepto en las células sexuales) cada uno de estos cromosomas se presenta en dos ejemplares apareados,homólogos de dos en dos,que constituyen dos lotes complementarios cuya información genética procede mitad del padre mitad de la madre.
Tales células son llamadas diploides,ya que cada una posee dos lotes de cromosomas homólogos.
El número de cromosomas de cada lote es el número " haploide ",especifico de la especie: n = 23 en los humanos,n= 10 en los espárragos.
Las células diploides del hombre contienen 2 x 23 cromosomas,y las del espárrago,2 =1 0 cromosomas.
Pero las células reproductoras que se fusionan en el momento de la fecundación (los gametos) no tienen más que un ejemplar único de cada cromosoma.
Son haploides ; en el momento de su formación heredan sólo uno de los cromosomas asociados por pares en las otras células.
En la mujer,el sexo esta determinado por la presencia,en cada célula diploide,de un par de cromosomas específicos e idénticos,los cromosomas sexuales X. En el hombre,ese par de cromosomas que determina el sexo está constituido por dos cromosomas distintos,el cromosoma sexual Y y el cromosoma sexual X. Los óvulos (gametos femeninos) sólo tienen,pues,cromosomas X. Por el contrario,hay tantas posibilidades de que los espermatozoides del hombre sean portadores del cromosoma X como del cromosoma Y. El sexo del futuro bebé lo decide el tipo de espermatozoide que fecunde el óvulo.
Si el gameto macho lleva el cromosoma X,el embrión se desarrollará como niña XX.
Si lleva el cromosoma Y,el embrión se desarrollará como niño YX.
Y como el hombre produce tantos gametos con cromosoma X como gametos con cromosoma Y,un bebé tiene a priori tantas posibilidades de nacer niño como de nacer niña.
Idéntica situación se da en el espárrago.
Los óvulos de la planta hembra tienen un determinismo sexual X y los granos del polen pueden tener lo mismo el factor X o el factor Y. La pequeña planta del espárrago tiene tantas posibilidades de dar flores estaminadas macho como flores con pistilo,hembras.
EL período de incubación de esta enfermedad puede ir de algunos meses a seis años.
El contagio pudo haberse producido por muchas vías.
La más corriente es la relación sexual con un portador-transmisor,que no tiene por qué desarrollar la enfermedad.
Pero también es posible contagiarse a través de las transfusiones de plasma sanguíneo contaminado - - vía de particular riesgo en el caso de los enfermos que dependen de estas transfusiones,como los hemofílicos - -,a través de una aguja hipodérmica compartida,como ocurre con los drogadictos o de la placenta de la madre,en el caso de los niños.
La sintomatología es difusa,al menos hasta que no aparecen dolencias asociadas y la enfermedad inicia su fase terminal que,en un porcentaje abrumador,termina con la vida del paciente en unos pocos meses.
Entre estos síntomas,destacan las fiebres,con una evolución de tres meses,la fatiga,el malestar general y la sudoración nocturna.
Pero también hay otros síntomas: trastornos al hablar,pérdida de memoria,dificultades de movimiento,jaquecas,brotes de demencia y,con mayor frecuencia,violentas diarreas que provocan pérdidas espectaculares de peso (hasta veinte kilos en dos semanas,según casos registrados).
En general,la detección se produce cuando aparecen síntomas más graves,vinculados a las afecciones particularmente asociadas al SIDA: sarcoma de Kaposi,con lesiones dermatológicas de color morado oscuro,hipertrofia del hígado y del bazo,o de los ganglios linfáticos,e infecciones oportunistas,como la neumonía por Neumocystis carinii.
En estos últimos casos,el paciente presenta cuadros de tos,disnea,anorexia y dolor pleural.
También puede haber bloqueos y lesiones en el esófago,con dificultades para tragar (disfagia) cuando la infección se produce por cándidas de esófago.
Es frecuente que estas infecciones oportunistas aparezcan en muchos casos combinadas con el sarcoma de Kaposi.
La tasa de mortalidad es muy elevada.
De los 39 casos registrados en España desde 1983 han muerto 31.
En algunos cuadros - - SIDA asociado al sarcoma de Kaposi - -,esta mortalidad ha alcanzado el 100 %.
La ciencia no dispone aún de medios eficaces que permitan suponer que estas tasas van a descender en los próximos años.
Por otra parte,y a pesar de la avalancha de informaciones más o menos sensacionalistas,puede decirse que aún se ignora el origen de la enfermedad.
Hasta el momento,la hipótesis del origen africano - - a través de la mordedura del mono verde que,según estudios,en la zona de Kenia presenta un 30 % de ejemplares portadores del SIDA - - o la pista de los corderos no permiten afirmaciones categóricas.
Todavía se sabe poco.
Y lo que se sabe preocupa sobremanera a las autoridades sanitarias de los países desarrollados.
No porque el SIDA no afecte a la población africana - - el 80 % de las prostitutas de Ruanda son portadoras del virus - -,sino porque en estos países las parasitosis endémicas producen muchas más muertes y el presupuesto sanitario alcanza cuotas inferiores a las 170 pesetas anuales de gasto por habitante.
Actualmente,los estudios realizados permiten suponer que no se trata de una enfermedad de fácil contagio.
El personal médico sanitario que ha tratado a los primeros pacientes sin medidas de precaución especiales,e incluso los familiares no cónyuges,no se han contagiado.
Fundamentalmente,este contagio se produce a través de los fluidos corporales - - sangre,semen y quizá por la saliva - - por un contacto íntimo continuado con el agente infeccioso.
Sin embargo,también se ha detectado la presencia del virus en el sudor y las lágrimas.
Aunque,según los médicos,puede descartarse el contagio por contactos corporales no íntimos si existe la debida higiene.
A su vez,los datos que aportan las investigaciones son suficientes para saldar algunas falsas ideas generadas a partir del sensacionalismo que ha rodeado a la enfermedad.
En primer lugar,no sólo afecta a homosexuales y bisexuales,aunque es cierto que éstos,con los drogadictos por vía intravenosa,constituyen el perfil de más alto riesgo.
También se ha visto que puede afectar a los heterosexuales,a los niños - - a través de la placenta de la madre - - y a los pacientes que dependen de transfusiones sanguíneas.
En el caso de los hemofílicos,la enfermedad mortal les ha atacado al recibir un tratamiento sanguíneo que necesitan para seguir con vida.
El hemofílico debe inyectarse un concentrado de factores de coagulación denominado factor VIII de la sangre imprescindible para que ésta pueda coagular.
Se trata de un hemoderivado que producen y comercializan diversos laboratorios a partir de sangre de donantes.
Ocurre que gran número de drogadictos y homosexuales,principales víctimas y portadores del SIDA,venden su sangre a los laboratorios,sobre todo en Estados Unidos,país de donde suele importarse en España el hemoderivado o el plasma.
Hasta que el SIDA fue detectado,ni los laboratorios estadounidenses ni los españoles observaban un control rígido del producto.
Así,en el caso de que la sangre proceda de una persona contagiada,el hemofílico tratado puede llegar a padecer el síndrome.
El hemofílico,pues,se convierte en una víctima que en principio no la transmite.
En la actualidad,el factor VIII es calentado a alta temperatura,operación que al parecer destruye el virus que ocasiona el síndrome y que reduce en un 90 % las posibilidades de evitar el contagio entre los enfermos hemofílicos.
Por otra parte,las bolsas procedentes de Estados Unidos reúnen ya las suficientes garantías,por lo que se supone que el SIDA va a dejar de afectar a los hemofílicos.
No obstante,la Asociación Española de Hemofílicos ha recomendado unas medidas profilácticas y de protección para reducir los riesgos de contagio.
De hecho,el 75 % de los hemofílicos estudiados en el Centro Nacional de Microbiología,Virología e Inmunología ha sido considerado transmisor de la enfermedad,aunque a lo mejor no la sufre.
De acuerdo con la Organización Mundial de la Salud (OMS),el grupo de más alto riesgo es el de los homosexuales.
Se trata de un área polisensorial,con neuronas que responden a estímulos visuales,auditivos o táctiles.
Sobre 452 neuronas exploradas con microelectrodos,menos de la mitad se mostraron sensibles al modo auditivo o táctil.
Incluso se daba una circunstancia curiosa: en ausencia de estímulo visual,esas neuronas no respondían tampoco a los demás modos.
El área polisensorial aparece,pues,como integradora de las informaciones auditivas o táctiles en las informaciones visuales,con mucho las más importantes.
Los experimentadores se inclinaron,pues,preferentemente sobre los estímulos visuales.
Como ya esperaban,los estímulos móviles eran los más eficaces ; pero a veces sólo había respuesta si el movimiento era en profundidad,alejándose o acercándose al mono.
Con desplazamientos laterales o verticales,no había reacción.
Es más,entre esas neuronas especiales sólo unas reaccionaban al alejamiento,y otras,diferentes,al acercamiento.
Por otra parte,una neurona de cada cinco prefería los movimientos radiales partiendo del eje de visión (movimientos centrífugos) o convergiendo hacia él (centrípetos).
Siete neuronas habían sido capaces de responder tan sólo a una combinación de movimientos en profundidad con giros centrífugos o centrípetos,y tres neuronas más prefirieron la rotación al desplazamiento rectilíneo.
La mayor parte de las neuronas estudiadas eran sensibles al movimiento,con velocidades muy diversas ; pero algunas sólo reaccionaban ante movimientos muy lentos,otras sólo respondían ante las aceleraciones,otras a los movimientos por impulsos... La combinación de estas y otras características nos lleva a una desconcertante variedad de especializaciones.
Que,además,se agrava si se consideran,amén de las particularidades del movimiento,la naturaleza de los estímulos.
En efecto,para las dos terceras partes de las neuronas no tiene ninguna importancia cómo es el estímulo ; sólo importa el movimiento.
Algunas preferían ciertos tamaños,ciertas formas u orientaciones,ciertos contrastes luminosos.
Muchas de ellas respondían a estímulos muy pequeños desplazándose muy deprisa.
En cambio,un 30 % de las neuronas estudiadas tenían preferencias muy concretas respecto a la naturaleza del estímulo,al margen de su movimiento.
Entre ellas,catorce sólo reaccionaban ante la cara de un hombre o de un mono.
¿El factor desencadenante era la globalidad o uno de sus componentes? Una vez más la especialización se impone: para la mitad de estas neuronas,sólo funciona el conjunto del estímulo.
Por ejemplo,la respuesta se debilita si tapamos los ojos,pero no desaparece ; en cambio,se anula cuando recortamos la cara en trozos y los presentamos desordenadamente.
Para la otra mitad de las neuronas,sólo una componente muy precisa del estímulo (la boca o los ojos) parece actuar,sin que el resto de la cara tenga nada que ver con la respuesta.
El estudio de las células nerviosas del córtex temporal que sólo responden a las manos o a la cara ha proseguido en Gran Bretaña y en los Estados Unidos con monos anestesiados o no,en los dos últimos años.
En el caso de monos no anestesiados,un sistema de recompensa (zumo de fruta) tendía a mantener la mirada del mono en la dirección en la que aparecían los estímulos.
En el segundo caso,para compensar el debilitamiento de las respuestas a causa de la anestesia,se imprimía una leve oscilación a las imágenes u objetos presentados.
Se utilizaban como estímulos,además de las manos y las caras,objetos atractivos,tales como plátanos o manzanas,o repulsivos (serpientes,arañas agitando sus patas).
El balance de estas experiencias muestra que son numerosas las neuronas del lóbulo temporal inferior,al igual que ocurre en otras áreas visuales del cerebro,que responden a estímulos muy variados y no muestran,por tanto,especialización alguna.
Pero existen unas cuantas que responden de forma muy selectiva a formas,colores o texturas bien precisas.
Por lo que respecta a las neuronas que sólo responden a estímulos visuales de manos o caras,se ha observado que forman una categoría muy especial y se encuentran localizadas en zonas muy estrechas.
Su sensibilidad no es modificada ni por el tamaño ni por la orientación del estímulo.
Su especialización es aún más sutil de lo que los primeros estudios hacían suponer: algunas de ellas responden mejor a las caras presentadas de frente que a los perfiles,mientras que para otras ocurre lo contrario.
Células de este tipo,sensibles sólo a las caras,también han sido encontradas en otras zonas del cerebro,aunque en menor número.
Sin duda,forman parte de un sistema de reconocimiento de las caras cuya sede principal se encuentra,seguramente,en el córtex temporal inferior.
Los neurólogos denominan prosopagnosia a una afección que tiene implicaciones sociales de evidente gravedad: la imposibilidad de reconocer las caras.
Las personas que sufren esta enfermedad identifican a las personas que les son familiares por la voz,pero no pueden reconocerlas sobre una foto o mirándolas a la cara ; ni siquiera en el caso de su cónyuge o sus propios hijos.
Hoy día se sabe ya que esta patología está ligada a lesiones del córtex,occipital y temporal.
Es muy probable que el estudio en primates de las células nerviosas especializadas en el reconocimiento de las caras nos ayude,en un futuro próximo,a mejor comprender la prosopagnosia y,quién sabe,quizá a curarla.
Un grupo de neurofisiólogos de la universidad escocesa de Fife acaba de demostrar que las células especializadas en ver caras no sólo reconocen la figura como tal,sino que perciben de forma integral la identidad del sujeto presentado,diferenciando perfectamente una cara familiar de una cara desconocida.
Incluso aparece una característica aún más curiosa: la intensidad de la respuesta no es la misma para las caras de personas diferentes ; las neuronas tienen sus propias preferencias... El problema estriba en comprender cómo se realiza la identificación.
En la mayor parte de los casos,curiosamente,esta identificación no obedece a un elemento señalado de la cara,sino al conjunto de los rasgos que la conforman.
Lo cual demuestra que la especialización de estas neuronas ha alcanzado una perfección casi increíble.
Incluso se dio el caso de que algunas células sólo respondían ante representaciones planas (en dos dimensiones) de las caras,mientras permanecían impasibles ante la cara real,en tres dimensiones.
La maquinaria visual del cerebro comprende diversos niveles,que van haciéndose más y más complejos a medida que la información circula desde la retina del ojo hacia el interior del cerebro.
Existen,además,bucles de retroacción,conexiones múltiples,interferencias entre estructuras y funciones diferentes.
Una vez más,la pregunta podría ser: ¿cómo puede una neurona del córtex procesar simultáneamente los mensajes que le llegan de un gran número de otras neuronas,utilizando también simultáneamente informaciones previamente memorizadas? Quizá deberíamos imaginarnos el sistema nervioso como un complejo ordenador capaz de efectuar simultáneamente en paralelo un enorme número de operaciones.
Algunos pretenden que el cerebro es incapaz de comprender al cerebro ; probablemente,se trata tan sólo de una brillante paradoja metafísica.
Nada se opone,por principio,a que conozcamos el cerebro peor de lo que podemos conocer el riñón o el hígado.
La dificultad estriba en que hace poco se estimaba en 10.000 ó 12.000 millones el número de neuronas que hay en el cerebro y los neurofisiólogos hablan hoy día de 100.000 millones.
Para comprender tan complejo sistema,habrá que investigar mucho y durante bastante tiempo.
Trabajo no va a faltar para que,por fin,el cerebro comprenda al cerebro.
Otros observaron una rotación ciclónica en el torbellino y un relámpago en el seno de la tromba.
Nada que ver,pues,con una simple tormenta.
Y la destrucción no se detiene ahí: un automóvil es proyectado a más de 40 metros,los pequeños estanques son literalmente desecados (al aspirar el agua la tromba) y los campesinos ven " volar " a sus ovejas... El campanario de la iglesia fue derribado,una casa decapitada y,en definitiva,el pueblo destruido en un 80 por 100,con 200 víctimas mortales.
A la entrada del pueblo,la tromba derribó los árboles de un bosquecillo resinoso,disponiéndolos meticulosamente en abanico,formando una doble espiral.
Hay que precisar que ese violento viento turbillonario no vino acompañado de precipitación alguna,ni antes ni durante ni después de su paso.
La formación de un tornado requiere la conjunción de diversas condiciones atmosféricas.
En primer lugar,fuertes vientos en altura (corrientes en chorro entre 90 y 350 km / h) que chocan con la parte superior de un cumulonimbo,empujando hacia abajo el aire cálido y húmedo que sube desde el suelo (1).
Por efecto de cizalladura de los vientos en sentido opuesto,este torbellino báscula en un plano horizontal (2),ganando velocidad conforme se va cerrando ; se genera así un torbellino ciclónico que se estira hacia arriba y hacia abajo.
Cuando en una pequeña zona de este ciclón se intensifica la convergencia de los vientos hacia tierra,se produce un tornado (3) que se puede extender hacia el mismo suelo ; el viento gira con velocidades de 250 km / h (4).
La fuerza centrífuga en la punta de la manga crea entonces una succión que genera,en el suelo un arbusto que absorbe,como un gigantesco aspirador,todo lo que encuentra a su paso (5).
Sobre el borde del tornado principal pueden nacer diversos minitornados (6) aún más temibles,ya que a su velocidad propia de rotación hay que añadir la del tornado principal ; lo cual supone unos 500 km / h. El conjunto se desplaza a unos 60 / 100 km / h sobre una estrecha franja de unos 300 metros de anchura.
Las trombas se produjeron en una zona de convergencia,en la vanguardia de una línea de bajas presiones,poco después de la hora de máxima insolación,correspondiente a los claros de nubes.
Situación tormentosa,recalentamiento intenso,ascendencia,concentración de energía turbillonaria convergencia de dos frentes: el cóctel estaba servido.
Ocurrió en septiembre.
Pero las trombas afectan generalmente a Europa en julio y agosto.
En nuestras regiones,suelen tener unos diez metros de diámetro y son un fenómeno muy localizado.
Aunque a veces aspiren el agua de una charca y luego se produzca la asombrosa " lluvia de ranas " tan frecuente en el sureste español.
En la India y en Japón aparecen durante el período de los monzones y durante la estación de los tifones.
Si hablamos de trombas en estos casos es porque los meteorólogos reservan el nombre de tornados a las trombas de grandes dimensiones (varios centenares de metros de diámetro) y de intensidad muy fuerte,como se observan en los Estados Unidos y Australia.
Europa occidental,India y Japón sólo conocen la versión menos violenta,pero el mecanismo parece ser el mismo.
Y aunque la palabra tornado significa " tormenta " designar con este nombre a los vendavales tormentosos que se producen en el Africa occidental y ecuatorial es impropio.
Todos los meteorólogos están de acuerdo en que el encuentro de una masa de aire caliente e inestable con otra de aire frío es una de las condiciones indispensables para la formación de un tornado.
A este respecto,el continente americano tiene el triste - privilegio de ser el número uno.
Se - habla incluso de un " Tornado alley " (avenida de los Tornados),región que va de Texas a Dakota,pasando - por Kansas y Oklahoma (el Estado más afectado).
En estas zonas de convergencia,LOS tornados son muy frecuentes y muy violentos.
No aparecen ni bajo los trópicos ni al norte del paralelo 60.
(Generalmente,bajo latitudes entre los paralelos 20 y 50.
) En invierno se desencadenan a lo largo del golfo de México y en el sudeste del país,pero en primavera y en verano su actividad máxima se desplaza con el aire tropical y sube hacia las grandes planicies del centro y luego del norte de los Estados Unidos,para volver hacia el sur en otoño.
Los tornados aparecen siempre en el punto caliente de una zona de depresiones.
Se cuentan unos 1.000 tornados por año en el continente norteamericano,con una frecuencia máxima en mayo y junio.
Aunque pueden producirse en cualquier momento del día su horario favorito es entre las 15 y las 19 horas,momento en que,el calor diurno acelera la actividad convectiva.
Los tornados matan a unos 100 americanos cada año y causan pérdidas de centenares de miles de dólares.
En 1925,al tornado más asesino conocido hasta la fecha causó 689 muertos y 1.980 heridos.
Recorrió 350 kilómetros,de Missouri a Indiana,pasando por Illinois,a una velocidad de 100 km / hora.
Todo un récord.
Como estos fenómenos son tan peligrosos como inevitables,los meteorólogos americanos intentan predecirlos,para alertar a la población.
Con ese objetivo fue creado el centro nacional de las tempestades violentas (el National Severe Storms Laboratory o NSSL) en Kansas City,Missouri,que cuenta con múltiples antenas,una de ellas especialmente activa,en Oklahoma.
Según estos especialistas,la situación propicia a la aparición de un tornado reúne cinco condiciones: inestabilidad acompañada de importantes desfases de temperatura y humedad ; humedad abundante en el suelo hasta menos de mil metros presencia de una capa estable que impida la convección ; mecanismo que permita atravesar esa capa estable (como,por ejemplo,una poderosa corriente de convección que,al penetrar en el aire superior inestable,inicia una aspiración de aire provocadora de un tornado) ; y,por último,vientos de altura muy violentos,tal como se encuentran en las zonas de transición entre aire caliente y aire frío,en donde se forman las tempestades más violentas.
Tetsuya Fujita,un investigador japonés especialista en tornados,ha establecido una clasificación para cuantificar la intensidad del fenómeno.
La escala de Fujita abarca seis grados: F0,vientos de 65 a 11 5 km / hora y daños escasos ; F1,vientos de 115 a 180 km / hora y daños moderados ; F2,vientos de 180 km / hora y daños considerables ; F3,vientos de 250 a 320 km / hora y daños severos ; F4,vientos de 320 a 415 km / hora y devastación ; F5,vientos superiores a los 420 km / hora y daño ilimitado.
Las estaciones meteorológicas americanas difunden esta escala,con fotografías de referencia que permiten evaluar los daños causados por los tornados.
Hasta el presente no existe instrumento alguno capaz de medir la intensidad de un tornado,ya que ni los anemómetros ni los barómetros pueden resistir la fuerza de tales vientos.
El profesor Fujita observó su primer tornado en Hiroshima,causado por el aire caliente que se elevaba de la ciudad en llamas al encontrarse con el aire frío en altura.
Desde entonces,los tornados se han convertido en su pasión.
Enseña meteorología en la Universidad de Chicago y el territorio americano le proporciona todo tipo de tornados posibles e imaginables.
Antes de la década de los setenta aparecen los circuitos integrados: en pastillas del grosor de una uña son colocados algunos componentes (integración a pequeña escala o SSI,small scale íntegration).
Poco después aparecen las pastillas con varias docenas de transistores por centímetro cuadrado ; es la integración a escala media o MSI (medium scale integration).
En 1971,dos jóvenes californianos montan un calculador completo capaz de realizar operaciones matemáticas a base de un único y minúsculo circuito integrado: acababa de nacer el microprocesador,conjunto miniaturizado que puede constituir el elemento central de un microordenador.
Así nació el mito del Silicon Valley,al sureste de San Francisco,que acabaría convirtiéndose en el centro mundial de la investigación en semiconductores a base de silicio.
En 1974,nuevo salto hacia adelante: la tecnología del transistor con efecto de campo,que inaugura la etapa de integración a gran escala o LSI (large scale integration).
Centenares,luego millares y,por último,centenares de miles de transistores en un solo chip de unos pocos milímetros cuadrados.
En la actualidad,los laboratorios ya son capaces de fabricar chips a base de un millón de transistores por milímetro cuadrado,que estarán disponibles comercialmente dentro de un par de años: la era de la integración a muy gran escala o VLSI (very large scale integration) acaba de comenzar.
Esta miniaturización asombrosa no se debe a un deseo de alcanzar proezas tecnológicas gratuitas.
La densidad de integración de un circuito se ha convertido en una necesidad económica tanto como tecnológica: menor consumo,mayor velocidad de procesamiento,mejor fiabilidad,menor coste por elemento de circuito... Resulta casi milagroso que se puedan realizar,automáticamente y en gran serie,toda la serie de operaciones físicas y químicas,con precisión de una milésima de milímetro,que permitirán albergar un millón de componentes en menos de un centímetro cuadrado.
La fabricación de estos chips minúsculos debe satisfacer condiciones draconianas,más rigurosas aún que las que rigen en la industria farmacéutica: ambiente absolutamente desprovisto de polvo,ausencia total de vibraciones,humedad y temperatura con estabilidad a prueba de bomba... Y todo para obtener un circuito integrado,un chip,esa especie de minúsculo cerebro que ya lo controla casi todo en el mundo de finales del siglo XX.
Repasemos rápidamente cómo se fabrica este minúsculo bloque semiconductor y sus componentes.
Es sabido que los ordenadores hablan un lenguaje binario: lo dicen casi todo,pero con un vocabulario muy pobre,sólo dos palabras,cero y uno.
Su trabajo se limita a manipular estas dos unidades elementales de información o bits.
Estos bits cabalgan materialmente sobre señales eléctricas que circulan por un laberinto de hilos y elementos interconectados.
El elemento clave de todos estos circuitos es el transistor ; él es el que desvía o no la señal (uno) o la ausencia de señal (cero).
Todo circuito no es más que un conjunto de transistores unidos entre sí por conexiones y elementos pasivos,como las resistencias y las capacidades.
La aptitud de los transistores para dejar pasar la corriente o para cerrarle el paso se debe al silicio,que es un semiconductor (dibujos 1,2 y 3).
Las dos clases de semiconductores P y N se unen para formar conjuntos que pueden ser o no conductores (dibujos 4,5,6 y 7).
Un transistor es un simple bocadillo de semiconductores NPN o PNP.
Su interés reside en su capacidad de amplificador de la corriente o de conmutador.
Estos transistores son ya antiguos.
La microelectrónica utiliza hoy día transistores con efecto de campo (dibujos 8 y 9).
En todo caso,los transistores,con sus diferentes capas,son objetos tridimensionales,aunque los electrones sólo circulan linealmente en un espacio bidimensional.
El circuito se fabrica,pues,capa a capa,cada una de ellas constituida por un determinado material (impurezas,aislante,rejilla,etcétera) y con un dibujo específico de su función.
Estos dibujos laberínticos corresponden a cada piso del circuito y son creados por auténticos maestros grafistas de la electrónica,auxiliados por la concepción asistida por ordenador (CAO),absolutamente indispensable dada la complejidad de su trabajo.
Para fabricar el circuito integrado se utiliza la técnica de la fotolitografía,similar al offset de las imprentas modernas.
El circuito será la resultante de toda una serie de tratamientos físicos y químicos que consiguen implantar ciertos materiales y grabar en relieve otros.
La base está constituida por una placa circular de silicio de diez centímetros de diámetro y medio milímetro de espesor,sobre la cual se hacen crecer las diferentes capas y que,posteriormente,será troceada en varios centenares de chips.
El silicio es,después del oxígeno,el elemento más abundante en la litosfera terrestre.
Se extrae de la sílice,componente principal de la arena.
Después de ser purificado se le cristaliza por estirado: un germen de cristalización fijado a una varilla en movimiento origina un grueso monocristal que luego se corta en finas rodajas cuidadosamente pulidas (dibujos 10 y 11).
Estas rodajas de silicio,guardadas en un tubo de cuarzo,se calientan a mil grados y se oxidan a base de vapor de agua purísimo.
El espesor del óxido depositado sobre el silicio se controla,mediante computadora,con una precisión del orden de la diezmilésima de milímetro (dibujos 12 y 13).
Sobre esta superficie de óxido de silicio aislante hay que grabar ahora millones de motivos en hueco para señalar el emplazamiento de los futuros transistores.
Se usa para ello una mascarilla de nitruro de silicio,grabado,él también,en relieve (dibujo 14).
Este nitruro desempeña el mismo papel que la piedra litográfica sobre la que los artistas grabadores trazan sus dibujos con lápiz graso.
En esta técnica artística se usa después ácido nítrico que,salvo en los lugares dibujados,produce por reacción con la piedra calcárea una capa de nitrato de calcio,una sustancia higroscópica que no chupa la tinta ; se consigue así que sólo las partes dibujadas aparezcan a la hora de sacar una prueba en papel.
La técnica microelectrónica se inspira en ese mismo principio,pero de forma mucho más sofisticada y automatizada: es la fotolitografía,utilizada en diez ocasiones sucesivas durante la fabricación del transistor.
Las cinco grandes etapas de este proceso son detalladas al pie de las figuras 15 a 26.
Queda por implantar en el silicio las impurezas que le van a conferir sus propiedades semiconductoras,lo que se denomina familiarmente dopado.
Las zonas a dopar se encuentran a ambos lados del espacio de control que está bajo la rejilla (dibujo 27).
La operación se realiza mediante inclusión iónica ; se trata de un haz de iones enviados a un acelerador lineal de partículas que barre la placa de silicio.
Ahora bien,la longitud de la zancada también importa a la hora de valorar la velocidad,ya que se trata de recorrer el mayor espacio posible en el mínimo tiempo.
El aumento en la longitud de la zancada se consigue mediante el alargamiento de las patas y la flexión de la columna vertebral,unido a la fuerza ejercida por la musculatura de las extremidades y las rotaciones adicionales de la cadera y de la cintura escapular.
La columna vertebral proporciona rigidez al cuerpo - - junto con los tendones y los músculos - -,al mismo tiempo que sustenta estructuras como las extremidades,por medio de la cintura escapular y pelviana,y el tórax,rodeado por las costillas.
Aunque parece rígida,la columna posee cierta flexibilidad y compresión,a causa de la disposición metamérica de las vértebras.
Por otra parte,los discos intervertebrales,de tejido cartilaginoso,ayudan a una mayor elasticidad y,por consiguiente,a una mayor gama de deformaciones.
A lo largo de la lenta evolución de las especies,se han producido diversos cambios adaptativos para conseguir incrementar la velocidad.
Comparando la carrera de un reptil con la de un mamífero terrestre,por ejemplo,comprobamos que ambos lo hacen de forma diferente.
Los reptiles actuales flexionan la columna vertebral en un plano horizontal (recordemos a las serpientes) y tienen los miembros orientados hacia fuera del cuerpo.
Los mamiferos,sin embargo,flexionan la columna en un plano vertical y tienen sus miembros orientados hacia delante y hacia dentro ; estos cambios con respecto a los reptiles han venido producidos,por una parte,a causa de torsiones hacia delante en los miembros posteriores,con lo que los pies también modificaron su postura,y,por otra,debido a una inclinación hacia atrás a nivel del hombro y un posterior giro a la altura del codo ; esto motivó la rotación del antebrazo hacia delante,con lo que los pies de los miembros anteriores apuntaron también hacia allí.
Estas son las razones por las que la locomoción de ambos grupos animales es tan distinta.
Aunque las dos formas resultan eficaces,los mamíferos han alcanzado una mayor armonia,lo que convierte a algunos de ellos en máquinas perfectas para la carrera.
Dentro de estos cambios adaptativos se encuentran,asimismo,la reducción del grosor y el alargamiento de los segmentos de la pata (pie,caña y antebrazo),que quedan más alejados del cuerpo.
En el hombre mamífero poco veloz,los huesos son pequeños y presentan una disposición que hacen que la superficie plantar sea mayor (plantígrado).
En el caso del perro,y más aún del caballo,los metatarsos han aumentado de longitud considerablemente,lo que les permite lograr una zancada más grande ; además,han perdido algunos huesos de la pata,por lo que andan y corren de puntillas (digitígrado y ungulígrado).
Finalmente,el hombro ha sufrido también unas modificaciones,a consecuencia de las cuales las extremidades anteriores gozan de mayor libertad de movimientos.
Todas las variaciones estructurales de los animales terrestres corredores van encaminadas a obtener un máximo de velocidad,dentro de las limitaciones propias de la mecánica muscular.
¿Qué papel juegan aquí los músculos y los tendones? Pues una función trascendental: soportan el cuerpo y resisten la tracción.
Las fuerzas que produce el tejido muscular,por tanto,son las responsables de que nos movamos o de que nos quedemos estáticos.
El músculo consta de células fibrilares (fibras) que se contraen ante la estimulación,acercando las inserciones de sus dos extremos.
La fuerza máxima que puede ejercer un músculo depende del área de su sección transversal,que en el caso del hombre es de 3 - 4 kilopondios por centímetro cuadrado.
Los músculos están generalmente unidos en sus extremos a dos huesos por medio de los tendones ; éstos proporcionan la unidad rígida del músculo al hueso y transmiten la fuerza muscular con un mínimo de pérdida,gracias a la elasticidad y a la gran resistencia a la tracción que proporcionan las fibras paralelas de colágeno.
Pero,en definitiva,es el hueso el que aguanta toda esta serie de tensiones.
Aunque parezcan frágiles y rígidos,los huesos tienen cierta capacidad de deformación,que en el caso del fémur de vaca,por ejemplo,llega hasta el 4 %.
Y las articulaciones,dispositivos estructurales que unen los huesos a nivel de sus superficies de contacto,permiten los movimientos relativos de los miembros.
la humanidad parece vivir una edad de oro de la tecnología.
No hay amanecer en que los medios informativos no nos sorprendan con noticias sobre nuevos y espectaculares avances tecnológicos.
Los hombres tenemos a nuestra disposición ingenios prodigiosos.
Los robots son ya algo habitual ; la ingeniería genética nos anuncia la posibilidad de planificar seres humanos con determinadas características ; funcionan corazones artificiales que reemplazan con éxito al organismo vital que regula la circulación de la sangre en el organismo.
En el ámbito de la empresa periodística se utiliza la transmisión optoelectrónica,la telecopia telefónica,la reproducción facsímil,el acceso inmediato a bancos de datos... Apretando un simple botón se consiguen tareas que ayer exigían tiempo y dedicación.
Ante semejante espectáculo,no resulta sorprendente que el hombre se sienta deslumbrado ante la apertura de un horizonte mágico y esperanzado donde todo,o casi todo,parece posible.
Todos estos avances tecnológicos han convertido al saber científico en una especie de nueva religión capaz de proporcionar al hombre,en plena tierra,el paraíso donde se logran objetivos sin apenas esfuerzo físico ni intelectual.
Los avances son vertiginosos: lo que es nuevo hoy queda viejo mañana.
Como en anteriores etapas de la humanidad,no faltan en este terreno resistencias a toda innovación,fruto en ocasiones de la ignorancia y en otras del temor que suele despertar lo desconocido.
No hubo progreso humano que no tuviera que vencer recelos por parte de lo establecido.
Algunos científicos - - recordemos el caso histórico de Galileo Galilei - - fueron tratados por los poderes sociales como vulgares delincuentes.
No hemos superado todavía situaciones parecidas.
El miedo que despierta la utilización de la energía nuclear es un ejemplo reciente y directo.
Hay quien sueña con un bucólico retorno a la naturaleza primitiva como remedio a los males de nuestra sociedad.
En prensa se siente nostalgia hacia los viejos sistemas donde el hombre era protagonista exclusivo de la actividad informativa.
Se experimenta el temor de que la máquina desplace al hombre de su puesto de trabajo.
En este tema,como en tantos otros,la verdad está en el justo medio.
Las posturas extremas,por desequilibradoras,son siempre peligrosas.
No debemos rendir culto fanático a fetichismos tecnológicos ni poner zancadillas a avances que contribuyen claramente al progreso de la humanidad.
La técnica ahorra al hombre esfuerzos físicos y mentales,crea nuevas posibilidades y abre fronteras de futuro.
Sin embargo,no debe perderse nunca de vista que la ciencia tiene un valor instrumental,no es un fin en sí misma.
Desde la más remota antigüedad el hombre ha acudido a la técnica para reformar la naturaleza con vistas a la satisfacción de sus necesidades.
Desde el invento del fuego para protegerse del frío o la construcción de las cavernas para abrigarse de las inclemencias del tiempo hasta los complejos y sofisticados mecanismos tecnológicos recientes va un largo camino de logros y fracasos,de frustraciones y éxitos.
Los avances técnicos constituyen un eficaz instrumento de ayuda para que los medios informativos cumplan con su importante y decisiva función social,pero no podemos dejarnos deslumbrar por falsos espejismos y olvidarnos que detrás de cada noticia hay un hombre.
En el centro neurálgico de la empresa se encuentra el periodista de carne y hueso.
Como muy bien dijo Ortega y Gasset,cuyo aniversario de su muerte se ha celebrado ahora," sólo en una sociedad donde la inteligencia funciona al servicio de una imaginación,no técnica,sino creadora de proyectos,puede constituirse la capacidad técnica ".
Hace unos meses,Barbacid y su equipo constataron que el cáncer de mama en las ratas aparecía sesenta días después de la inyección de metilnitrosurea.
Establecieron entonces que todas las células tumorales poseían un tipo de oncogén particular,el del Rat Sarcoma Virus (RAS),que se había convertido en tal a partir de un gen del cáncer por la acción del citado producto.
Este gen descubierto hace unos años en la rata,fue localizado luego en el hombre,tanto en forma silenciosa como activada,es decir,como oncogén,especialmente en las células cancerosas del pulmón,del intestino grueso y de la vesícula.
El doctor Barbacid logró demostrar que el gen RAS se hace oncogén porque la metilnitrosurea provoca su mutación a nivel del segundo nucleótido del duodécimo codón.
Para valorar y comprender el enorme alcance de este descubrimiento hay que recordar que un gen es una secuencia del ADN cromosómico,formado a su vez por una cadena de nucleótidos,cada uno de los cuales contiene una de las cuatro bases nitrogenadas siguientes: adenina,citosina,guanina y timina.
Sus iniciales A,C,G y T son las cuatro letras fundamentales del alfabeto genético,y el orden en que vienen dispuestas en el interior del gen determina cuál será la proteína específica codificada por ese gen.
Toda célula del organismo tiene el poder de descifrar ese mensaje palabra a palabra ; cada palabra,o codón,es un conjunto de tres letras,también llamado triplete,y constituye la unidad de información que ordena el ensamblaje de aminoácidos en las proteínas.
Por tanto,cada codón corresponde a uno de los veinte aminoácidos constituyentes esenciales de las proteínas.
Así,el triplete CTA codifica la serina ; el TCG,la leucina,y el CGT,la alanina,tres aminoácidos indispensables al hombre y constituyentes de numerosas proteínas.
Así pues,los codones determinan el alineamiento de los aminoácidos en la cadena proteica.
Mariano Barbacid ha encontrado que la metilnitrosurea había falseado el mensaje del gen sustituyendo la letra G por la letra A. Esto hizo introducir un aminoácido equivocado en el seno de la proteína,de forma que esta proteína codificada por el gen mutante se convirtió en cancerígena.
Gracias a este descubrimiento puede pensarse ahora en probar otras sustancias cancerígenas para conocer su modo de acción a nivel genético.
Además de diversos productos químicos,los agentes cancerígenos pueden ser también radiaciones ionizantes e,incluso,y éste es el tercer gran descubrimiento del año,sustancias segregadas por el propio individuo.
Por muy extraño que parezca,el doctor Tomás Stossel,de la unidad de oncología del Massachusetts General Hospital,ha conseguido probar la existencia de esta autotoxicidad.
Y lo que resulta aún más sorprendente,estas sustancias cancerígenas son producidas en el organismo por células normalmente destinadas a su defensa inmunitaria,los neutrófilos polinucleares,una variedad de los leucocitos o glóbulos blancos de la sangre.
Curiosa desviación ésta de la naturaleza,si tenemos en cuenta que este tipo de leucocitos,lo mismo que los macrófagos,no tienen más vocación que fagocitar los agentes infecciosos y las materias peligrosas para el organismo.
La demostración de su influencia patógena se hizo in vivo e in vitro.
La experiencia consistió en cultivar,juntos,neutrófilos humanos y fibroblastos (células del tejido conjuntivo) de ratón.
La idea,aparentemente incongruente,de cultivar en probeta (in vitro) estos dos tipos de células juntas se debió a que hacía algún tiempo ya que se sospechaba que los glóbulos blancos del hombre segregan cuerpos tóxicos.
El profesor Stossel se preguntó si no serían también cancerígenos ; y efectivamente al cabo de seis u ocho días,los fibroblastos se habían cancerizado.
El estudio en el organismo vivo dio resultados más impresionantes: la inyección de estos fibroblastos en ratones provocó la aparición de tumores cancerosos entre trece y veintidós semanas después de la contaminación,mientras que en ratones testigos,a los que se había inoculado fibroblastos normales,no apareció lesión alguna.
Quedó,pues,demostrado que los elementos neutrófilos de la sangre segregan sustancias mutágenas capaces de despertar los genes del cáncer presentes en los fibroblastos.
Al parecer,se trata de aminas y de ácidos,poderosos oxidantes.
Todo ello,a primera vista,podría poner en tela de juicio la estrategia terapéutica que combate el cáncer mediante el estímulo del sistema de defensas inmunitarias.
La inmunoterapia descansa en una observación hecha hace cien años por William Cooley,cirujano de Nueva York,que comprobó que el estado de algunos cancerosos mejoraba notablemente después de una infección.
De ahí su idea de inyectar al paciente microbios inactivados para aumentar sus reacciones inmunitarias.
El doctor Cooley actuaba empíricamente,pero luego se demostró que los glóbulos blancos (hoy sabemos que hay que exceptuar a los neutrófilos) fabrican en efecto,sustancias susceptibles de luchar eficazmente contra el cáncer.
Entre esas sustancias está el TNF (Tumor Necrosis Factor),aislado en 1975 por los investigadores del Sloan Kettering Institute,de Nueva York.
Los primeros ensayos clínicos se están efectuando ahora y,al parecer,con resultados prometedores.
Queda aún por saber por qué,de todos los tipos de glóbulos blancos,sólo los polinucleares neutrófilos emiten productos cancerígenos.
Quedan,sin duda,muchos otros porqués todavía no contestados.
Pero,sin duda,los trabajos de tres grupos de investigadores en Estados Unidos,uno de ellos dirigido por un español,han supuesto,en este año de 1985 que ahora termina,tres auténticos pasos de gigante en la investigación cancerológica.
Además de la riqueza ecológica y paisajística que en sí mismas albergan,suponen auténticas barreras contra los temporales,cuya intensidad amortiguan,eficazmente contra los vientos marinos y su salitre,que son absorbidos y desviados,y contra la misma arena que la constituye,atrapada por su cobertura vegetal.
Todas estas acciones protectoras preservan en primer lugar la playa misma,y también,por supuesto,el interior costero ; lo cual resulta tanto más imprescindible cuando se trata de zonas turísticas de sol y playa,o de regiones habitadas cuyas casas y cultivos se benefician de esa protección de las dunas.
La función protectora de las dunas litorales se explica principalmente por el intercambio entre la playa y la anteplaya,zona esta última siempre sumergida bajo las olas,pero formando un todo con la playa emergida.
En verano,el perfil se levanta,ya que el mar,con oleaje poco violento,alimenta a la duna y la hace engordar.
Las olas rompen muy pronto y sin violencia y tienen un efecto constructivo: en su movimiento de vaivén,el empuje hacia la costa aporta arena a la playa y es superior al reflujo de resaca que se la lleva mar adentro.
Los sedimentos de la anteplaya siempre sumergida van emigrando hacia la playa emergida y se van acumulando en pequeños taludes.
La pendiente de la playa se va acentuando poco a poco hasta alcanzar un punto de equilibrio en el que el aporte y la retirada de arena se compensan.
Por esta causa el verano no es una estación propicia para detectar si,en promedio,la playa se encuentra en regresión.
Sólo en invierno cuando las tempestades y sus enormes olas producen una violenta destrucción del equilibrio playero,es cuando se puede apreciar si las cosas se acercan a lo irreversible o si por el contrario,el litoral aguantará sin problemas.
Cuando una ola violenta choca contra una duna,su energía se disipa en tres frentes simultáneos.
En primer lugar,es frenada por la pendiente suavemente inclinada de la duna,que se comporta así como un obstáculo.
En segundo lugar,la ola ve amortiguada su energía por la arena en lugar de verse repelida violentamente por un obstáculo impermeable ; es el efecto amortiguador o parachoques.
Finalmente,la duna debilita la fuerza del agua que avanza al absorberla y mezclar su arena con ella ; es el efecto de duna-reserva.
Las olas no le quitan la arena a la playa,sino que desestabilizan el pie de la duna,transformándolo en un miniacantilado inestable.
Si el ciclo de dunas es estable,este negativo efecto se compensará más adelante,en verano ; la arena de la zona comprendida entre la marea más baja y la marea más alta (el estrán) remontará con el suave oleaje del buen tiempo y con la brisa del mar.
Esta modificación estacional de una playa equilibrada hace intervenir así,a lo largo del año,a más de 500 metros cúbicos de arena viajera por cada metro lineal de costa.
El balance erosión-reconstruccióV0V sólo estará equilibrado si el mar aporta tanta arena al sistema como la que se lleva.
Pero puede ocurrir que en una determinada zona del litoral lleguen sedimentos cuyo origen sea exterior al sistema playero ; y,por el contrario,que algunos sedimentos se escapen definitivamente del sistema.
Los ríos pueden ser un ejemplo del primer caso,las corrientes marinas del segundo.
Esquemáticamente,los materiales remodelados por el oleaje pertenecen a los fondos marinos situados entre la playa seca y una profundidad de agua equivalente a unas tres veces la altura media de las olas.
Lo que significa entre 10 y 15 metros para las costas atlánticas,y entre 3 y 8 metros en las mediterráneas.
La deriva litoral,nacida de la oblicuidad del oleaje respecto a la línea de costa,arrastra esos sedimentos a lo largo de la orilla,llevándose la arena según la fuerza de las olas,su dirección,las corrientes de marea y la propia morfología costera.
En las costas muy bajas,como las del golfo de Cádiz,este arrastre puede suponer casi un millón de metros cúbicos de arena transportados cada año en dirección norte-sur,mayoritariamente,o sur-norte.
Las olas.
Las corrientes de marea.
las corrientes más profundas y los vientos se conjugan,pues,para remover y trasladar la arena.
En el caso de los vientos,la arena fina (0,2 milímetros de diámetro) y seca es levantada a partir de 15 km / h de velocidad del viento,medida a dos metros del suelo.
Un viento de 18 km / h es capaz de transportar un kilo de arena fina por hora y metro lineal de playa.
Un viento con velocidad doble,es decir,36 km / h,puede desplazar 100 kilos.
Y un viento de 72 km / h es capaz de mover ¡800 kilos por hora y metro! Cada porción de la costa posee,pues,su propio presupuesto sedimentario.
Si recibe tantos sedimentos como pierde,el balance es nulo: la playa tiene muchas posibilidades de permanecer estable.
Si pierde más arena de la que recibe,entonces el balance sedimentario negativo se traduce por una recesión de la playa,con desaparición de las dunas y,a medio plazo,eliminación completa de la playa.
Si,en cambio,el balance es positivo,la playa engorda y su superficie desnuda se convierte en una auténtica pista de despegue para la arena ; el viento la empujará hacia el interior,mediante dunas móviles (sin plantas que las fijen,debido a su juventud) que lo invadirán todo.
Una duna puede ser fijada mediante las plantas que acaban por poblarla.
Se trata de vegetales que soportan el salitre (por ejemplo,el Agropyrum junceum o junco de las playas) y cuya presencia aumenta la rugosidad de la duna ; la velocidad del viento disminuye y su carga arenosa se deposita antes de penetrar más hacia el interior.
Otras plantas crecen entonces en esta duna semiestable son vegetales que necesitan arena para crecer.
La Ammophila arenaria o carrizo playero constituye el ejemplo más extendido de estas especies llamadas psamófilas ; suele asociarse a otras plantas,como el cárex de raíces horizontales y la euforbia de raíces verticales pivotantes.
Algunas células,como por ejemplo las del hígado,se destruyen y regeneran en períodos inferiores a una semana.
El biólogo belga Christian René de Duve,premio Nobel de Medicina en 1974,escribió que las células son como esas casas de vieja fachada que aparentan ser iguales a como eran en el momento de su construcción,pero que por dentro han sido completamente restauradas,no conservando más que algunos elementos aislados del original ".
Esa renovación celular,al igual que la restauración de edificios viejos,presenta notables ventajas,esencialmente la adaptabilidad.
A una vieja casa se le pueden añadir elementos modernos de confort: calefacción central,tuberías de cobre,puertas blindadas... Del mismo modo,una célula puede adaptarse a necesidades nuevas,reemplazando una parte destruida por otra nueva y quizá diferente,segregando nuevos productos,erigiendo barreras inmunológicas ante invasores que antes no existían.
A menudo se piensa que,no obstante,existe una zona de la célula nunca reemplazada: la larga molécula de ADN que contiene su núcleo y que es portadora de su mensaje hereditario.
Pero lo cierto es que cuando se produce una división celular (mitosis),el ADN es resintetizado y puede sufrir modificaciones,a veces muy importantes: trasposición,pérdida o amplificación de algún gen,conversiones y recombinaciones de otros e incluso cambios en la conformación de los cromosomas,formados por la agregación de múltiples genes (ver figura página 16).
Se piensa incluso que tales acontecimientos pueden producirse en las células después de su división.
En todo caso si el ADN se ve modificado se debería poder identificar,mediante dicha modificación,a la célula que envejece,y se podría seguir la pista a las causas genéticas de su envejecimiento.
Las modificaciones genéticas producen mutaciones,cambios muy diversos acaecidos en el momento de una división celular más o menos lograda o debidos a distintas agresiones químicas o físicas o a las reparaciones genéticas efectuadas por enzimas,que son proteínas fabricadas por los mismos genes.
Ahora bien,¿cuál es la relación que existe entre esas modificaciones celulares y el fenómeno global del envejecimiento del organismo? ¿Podríamos modificar el curso de esas transformaciones o paliar las deficiencias de las células que envejecen,introduciendo en el organismo alguna sustancia que las células no son ya capaces de producir? Estas y otras cuestiones son hoy vivamente debatidas,y constituyen la base de investigaciones intensivas,incluida la posibilidad de establecer con precisión los riesgos genéticos de los hijos de padres y madres de muchos países,el sector de los jubilados,en los sistemas de seguridad social,representa ya el gasto más importante.
Por otra parte,los ancianos son,todavía,más un fardo para ellos mismos y para la sociedad que un elemento activo.
Por eso la genética,la biología celular,la inmunología,la neurología y la endocrinología se inclinan sobre el problema del envejecimiento.
Y si a veces se han obtenido resultados espectaculares,lo cierto es que cada vez resulta más evidente la enormidad y la complejidad del problema de la senectud biológica.
Porque no existe una sola causa del envejecimiento,sino múltiples fenómenos no siempre relacionados entre si y,a veces,ni siquiera puramente biológicos.
Numerosos investigadores abordan el problema desde la óptica de la célula que envejece.
Uno de los hallazgos básicos en ese sentido fue el de los biólogos norteamericanos Hyflick y Moorehead,que demostraron,en 1970,que el potencial de multiplicación de un cultivo de células no era el mismo si procedían de un organismo joven o de uno viejo.
Según estos investigadores,la pérdida de capacidad de multiplicación es una expresión de envejecimiento celular.
La única célula inmortal que se conoce,capaz de multiplicarse indefinidamente,es una célula totalmente anormal: la célula cancerosa.
También se constató,poco después,que el potencial de reproducción de las células variaba,tanto en el hombre como en los animales,en función de ciertas enfermedades.
Además,se pudo comprobar que el medio en el que se desarrollaba el cultivo celular también influía sobre el número de multiplicaciones ; por ejemplo,si se introduce suero procedente de animales jóvenes,el cultivo de células se multiplica mejor que si el suero añadido a su medio nutritivo procede de animales viejos.
De todos modos,lo esencial no es que la división celular se detenga más pronto o más tarde,sino que en cada caso la célula hija sea igual o no a la célula madre.
Cuando no lo es,algún cambio en el ADN ha debido aparecer en el proceso.
Y estos cambios son los que hay que estudiar preferentemente si se quiere comprender el proceso global del envejecimiento.
Los más recientes estudios sobre las transformaciones genéticas que aparecen en cultivos de células muestran claramente la diversidad del proceso.
Diversos cambios aparecen en las secuencias que forman las bases de ADN,bases que constituyen las letras del código genético.
También surgen modificaciones en la forma misma de la doble espiral de ADN y en la cantidad total del material genético.
Finalmente,incluso los cromosomas pueden verse afectados por ciertos desarreglos estructurales.
Los investigadores han observado pérdidas de secuencias genéticas repetitivas conforme las células en cultivo envejecían.
Más sorprendente aún resulta la adición de material genético a lo largo de dicho envejecimiento ; un fenómeno aparentemente contradictorio,a no ser que ese exceso sea el responsable de los problemas posteriores.
También se ha conseguido medir el contenido de ADN en las células hermanas y se ha verificado que no siempre era el mismo en todas: al menos un 20 % de ellas tienen un contenido de ADN diferente al de sus hermanas y su célula madre.
También los cromosomas,formados por los genes y compuestos,por tanto,como éstos de ADN,presentan a veces modificaciones.
En esos errores de duplicación aparece asimismo una modificación que evoca en nosotros el envejecimiento tal y como lo conocemos a escala humana: el material genético pierde parte de su plasticidad,se hace menos elástico.
En efecto,cuando aparecen modificaciones en células jóvenes,éstas suelen tener tendencia a repararlas y recuperar su estado anterior ; en cambio,las células viejas suelen conservar la modificación genética que apareció en alguna etapa de su multiplicación.
Parece como si la célula perdiese con la edad su facultad de reorganización,con lo que la mayor parte de las modificaciones casuales impuestas por el tiempo acabarían por ser nefastas.
En 1981,una serie de medidas demostró que,en diez minutos,el operador puede obtener un perfil de humedad de la atmósfera entre 0 y 1.500 metros,con una resolución vertical de 30 metros y una precisión del 5 %.
Para alturas mayores se precisan cincuenta minutos,con una resolución vertical de 300 metros y una precisión del 10 %.
Bastaba,pues,concebir una estación móvil,automática,manejable y fiable,que pudiera desplazarse en un pequeño camión sin requerir siquiera la presencia de un especialista en el terreno.
La estación lidar móvil de EDF demostró definitivamente su carácter operativo durante la campaña de Fos en 1983.
Utilizando el lidar DIAL para medir la humedad atmosférica,y otro láser que sólo mide la concentración en partículas,el equipo pudo verificar la concordancia de las mediciones de humedad y aerosoles y caracterizar la capa de mezcla.
Lógicamente esa concordancia puede atribuirse a una correlación puramente dinámica,en la que la estructura vertical del perfil de temperatura provoca intercambios verticales en la atmósfera,que influyen tanto en el reparto de la humedad como en el de los aerosoles ; o bien puede deberse a una correlación más física,en la que ciertas partículas de aerosoles (cristal de cloruro de sodio,por ejemplo),tienen tendencia a captar el agua,por lo que su tamaño y su índice de refracción varían en función de la humedad relativa.
Actualmente se están realizando ensayos con un nuevo tipo de láser más compacto,en el que la cuba de colorante ha sido sustituida por un cristal de alejandrita.
Además del perfil de humedad,este láser podrá determinar el perfil de temperatura y de presión,que exigen una precisión muy superior.
La puesta a punto de este láser con cristal de alejandrita interesa a otros muchos laboratorios franceses y europeos.
No hay que olvidar que todas estas aplicaciones meteorológicas y climáticas vienen precedidas por una abundante investigación teórica.
En general,todavía está pendiente la tarea de demostrar la viabilidad y la utilidad de estos experimentos.
Los investigadores han de elaborar además,a partir de estas medidas,modelos matemáticos que permitan comprender científicamente la evolución del clima o la estructura fina de la atmósfera.
Para la modelización matemática es preciso,esencialmente,liberarse del carácter puntual de las mediciones en el suelo,y pasar a una escala más amplia,la mesoescala (entre 100 y 500 kilómetros).
Porque lo que interesa no es tanto conocer los parámetros de forma escalonada a lo largo del tiempo,sino tenerlos todos a la vez.
Además,las medidas en el suelo sólo pueden realizarse en una quinta parte aproximadamente del globo.
Para alcanzar una cobertura global se necesita un sistema que no tenga problemas de logística para realizar su trabajo en todas partes,ya sea por encima de una superficie habitada,de un desierto o de un océano.
Por ejemplo,un sistema montado en un satélite ; aunque una vez lanzado ya no puede modificarse el equipo en varios meses o incluso años... La solución,por supuesto,es la estación orbital conectada con tierra por medio de lanzaderas espaciales.
Existe un proyecto ESA-NASA que prevé,para 1988,la puesta en órbita,por la lanzadera espacial americana,del Eureca (European Retrieval Carrier,portador recuperable europeo).
En esta plataforma espacial se instalarán lidares operacionales a partir de 1992.
A más corto plazo está la solución relativamente modesta de un lidar embarcado en un avión.
Ya hay dos proyectos en fase de puesta a punto ; el primero es un proyecto francés CNES-INSU (Centro Nacional de Estudios Espaciales e Instituto Nacional de las Ciencias del Universo) llamado " Leandre Ara " acróstico que permite recordar la naturaleza del proyecto (Lidar Embarcado Aerosol Nube Dinámico Radiactivo Embarcado en Avión de Investigaciones [Recherches] Atmosféricas).
El avión en cuestión es un Fokker 27 que vuela a 8 kilómetros de altura y realizará las mismas medidas que se realizan desde el suelo (nubes,aerosoles,humedad,temperatura,capas de mezcla en donde se bloquea la polución).
Su primer vuelo está previsto para 1987,puesto que " Leandre ",debe participar en la gran experiencia " Front87 " destinada al estudio de los frentes cálidos o fríos que vienen del Atlántico y de los que dependen literalmente la lluvia y el buen tiempo.
El otro proyecto,llamado LASERER 82 (Laser Atmospheric Sensing Experiment),en colaboración con la NASA,estará destinado a estudios meteorológicos a mesoescala que abarcarán toda la troposfera,ya que el ER2 es un avión que vuela a 20 kilómetros de altura.
Este programa comporta también el estudio de los cirros,esas nubes de hielo estiradas en finas capas,a 10 kilómetros de altura,cerca de la estratosfera.
En Francia el lidar cuenta con auténticos hinchas en la Dirección General del Armamento,que contribuye desde hace unos diez años a la financiación de numerosas investigaciones con el objetivo de hacer operativo el sistema lidar en telemetría (detección a distancia).
Las voluminosas lámparas y los tubos de vacío fueron sustituidos por componentes electrónicos,transistores en circuitos impresos,que a su vez desaparecieron frente a los circuitos integrados de silicio,cada vez mejor miniaturizados gracias a los " chips " de silicio.
Actualmente,se sabe esculpir casi cien mil transistores en una sola esquirla de silicio.
Cuanto más se miniaturizan,más rápidas se hacen las máquinas,puesto que los electrones precisan de menos tiempo para ir de un componente electrónico a otro.
Hoy en día se están rozando prácticamente los límites físicos de la más extrema miniaturización.
Para ganar aún más velocidad de cálculo,habría que abandonar el silicio y reemplazarlo por otros materiales,como el arseniuro de galio,en el que los electrones viajan de 7 a 10 veces más deprisa,o bien por componentes superconductores en los que las velocidades se multiplican asimismo por diez.
De momento,ninguna de estas tecnologías está a punto,aunque se trabaja intensamente en ellas.
Si la electrónica ha progresado a pasos de gigante en cuarenta años,no ocurre lo mismo con la estructura interna de las máquinas - - los informáticos la llaman,ya lo hemos visto,arquitectura - - que no ha cambiado un ápice.
Las máquinas rápidas de hoy trabajan,como sus predecesoras,según los principios establecidos por Von Neumann.
Si se consiguiera hacer ejecutar al ordenador varias operaciones a la vez y no una sola,se ganaría un tiempo considerable.
El cerebro de la máquina ya no estaría constituido por un solo procesador,sino por toda una serie de procesadores trabajando en paralelo.
La idea es simple,y hace tiempo que los informáticos habían pensado en ella.
Desgraciadamente,su realización - es muy compleja.
¿Por qué? En la mayoría de los casos,los resultados de una operación son indispensables para efectuar el resto de los cálculos.
Los distintos procesadores deben estar en continuo contacto entre ellos,por tanto,y esperarse los unos a los otros antes de proseguir con sus respectivos trabajos.
Así pues,un ordenador-supervisor debe controlar constantemente lo que hace cada procesador,y eso complica aún más la arquitectura de la máquina.
Además,todos los programas existentes están escritos para máquinas en las que las operaciones son realizadas una después de otra ; la puesta a punto de programas en los que las operaciones puedan ser realizadas en paralelo resultará muy compleja,ya que hay que dividir la aplicación global en el máximo número de tareas independientes,cada una de las cuales ha de poder ser tratada por procesadores diferentes.
Las primeras investigaciones sobre los ordenadores paralelos se iniciaron a principios de los años sesenta,y dieron como resultado algunos prototipos excesivamente voluminosos y costosos,como el ILLIAC IV,que acabó siendo un ejemplar único.
Ante las dificultades que presentaba el tema,los distintos proyectos acabaron en el baúl de los recuerdos.
Y,sin embargo,tanto la ciencia como la industria necesitaban,con creciente urgencia,ordenadores cada vez más rápidos.
Para calcular con precisión los movimientos del aire en torno a un motor de cohete,por ejemplo,el más grande de los ordenadores en servicio necesita más de dieciocho horas de cálculo.
La síntesis de los datos meteorológicos y el establecimiento de previsiones precisan horas,incluso jornadas enteras de cálculo.
La simulación del deslizamiento del aire en torno al perfil de un avión puede llegar a requerir la ejecución de cinco billones de operaciones ; las más poderosas máquinas hoy disponibles efectúan " sólo " cien millones de operaciones por segundo: hay que esperar por lo menos catorce horas,por tanto,para ver realizada esa simulación.
A veces,el coste y el espacio que ocupan los ordenadores constituyen un auténtico obstáculo.
Así,la comunicación vocal de los robots y la visión en las máquinas sólo podrán generalizarse cuando los ordenadores que las dirigen sean menos voluminosos y resulten más baratos.
Ultimamente,la demanda de muchos microorganismos,paso esencial para llegar a comprender su estructura molecular y,por tanto,su funcionamiento como agentes infecciosos.
Las imágenes del microscopio electrónico,convenientemente procesadas por el ordenador,permiten visualizar los virus en dos y tres dimensiones,haciendo mucho más comprensibles las imágenes.
Astrofísica: Estudio sobre las galaxias y sus mecanismos de formación,que aplica técnicas informáticas que suponen un avance muy importante en la observación y comprensión del Universo,al mismo tiempo que suponen un ahorro de tiempo y dinero.
Con los circuitos electrónicos que se han diseñado y los algoritmos de trazado que se han conseguido se podrán estudiar los supercúmulos de galaxias,las galaxias enanas cercanas y los grupos compactos de galaxias.
También se analizará la composición y dinámica de la atmósfera terrestre.
Televisión por satélite: Un proyecto de gran interés práctico,ya que desarrolla un prototipo de sistema propio para recibir imágenes de televisión por satélite de forma perfectamente competitiva con los que actualmente tienen en el mercado japoneses y norteamericanos.
Han colaborado un organismo público y dos empresas privadas catalanas.
Diseño de fármacos por ordenador: El ordenador selecciona de entre varios millares la estructura química más adecuada según el modo de actuación que de ella se espera.
El sistema permite economizar tiempo y posibilita además la obtención de fármacos más específicos y eficaces.
Por lo que respecta a los recursos naturales,se presentan los cuatro proyectos siguientes: Acuicultura: España es el segundo país del mundo en cuanto a consumo de pescado,y cada vez tiene más dificultades para pescar en los tradicionales caladeros lejanos.
El proyecto investiga los problemas que plantean determinados cultivos marinos en tierra y en zonas costeras (mejillón,rodaballo,ostra...) De especial interés resulta el análisis del impacto de estos cultivos sobre el ecosistema,ya que se ha observado,por ejemplo,que la implantación masiva de bateas provoca un importante incremento de la sedimentación de materia orgánica.
Geología marina: Se trata de un estudio oceanográfico integral del margen continental español,que pretende conocer mejor nuestros fondos marinos próximos: sus recursos naturales potencialmente aprovechables,su edad,los procesos de formación,sus estructuras interiores... Flora ibérica: Con los resultados de este trabajo se pretende redactar una obra que recoja,científicamente ordenadas,todas las plantas que crecen espontáneamente en nuestro país (península e islas).
El estudio ya ha descubierto nuevas especies de enorme interés botánico.
Plantas medicinales: La idea básica de este proyecto gira en torno a determinados productos naturales que,según la tradición,tienen poderes medicinales.
El aumento del uso integral de plantas con fines curativos suele chocar con la ausencia de estudios rigurosos sobre los efectos de los extractos de tales plantas,tanto tóxicos como medicinales.
Finalmente,en el área de las ciencias humanas y sociales se presentan dos proyectos.
Vidrieras medievales catalanas: Un proyecto que presenta los resultados de un estudio que amplía el conocimiento de las técnicas artesanales utilizadas para la confección de vidrieras: su composición química,los temas predominantes en su iconografía,los mecanismos de su conservación,el color y el estilo y los métodos para diferenciar los vidrios originales de los que provienen de otras vidrieras ya desaparecidas.
Cartografía urbanística americana: Un estudio sobre la evolución de los núcleos urbanos en la colonización española de América,en la que la ciudad constituyó el eje de aglomeración del elemento hispano llegado al Nuevo Mundo y del elemento indígena,convirtiéndose en el centro rector de la vida del continente americano durante los tres siglos de la colonia.
La variedad de los proyectos presentados permite obtener una idea global suficientemente representativa de la actividad investigadora en nuestro país.
Mucho más variada e intensa de lo que suele pensarse.
Cada vez está más claro que los científicos españoles no están de acuerdo con el nefasto " que inventen ellos ".
Ahora hace falta que la sociedad española lo sepa y comparta esa idea ; la política de puertas abiertas del CSIC parece uno de los caminos más fructíferos para conseguirlo.
Nuestros sueños prehistóricos,con toda seguridad,debieron quedar bajo el dominio de una obsesiva imagen: un gran animal quedaba al alcance,estaba lo suficientemente cerca como para apresarlo.
Fueron centenares de miles de años de esperar la cotidiana plasmación del íntimo deseo que,lógicamente,fue satisfecho en innumerables ocasiones.
Secuencias en las que los animales se muestran al alcance.
Estamos sencillamente satisfaciendo de nuevo un anhelo tan viejo y arraigado como el ansia de vivir.
Sólo que ahora,con las modernas técnicas de filmación,se consigue mucho más.
Mundos antes inaccesibles se han convertido en algo cotidiano como,por ejemplo,las altas montañas,las selvas enmarañadas,las tierras polares y,sobre todo,el fondo de los mares.
El desarrollo de la óptica nos permite viajar fuera de nuestro planeta o penetrar en lo ínfimo,en ese cosmos que por su reducido tamaño queda fuera de nuestra vista desnuda.
Las cámaras de alta velocidad,por otra parte,consiguen descomponer los movimientos más rápidos,mientras que las filmaciones a pocas imágenes por segundo aceleran los acontecimientos.
Es decir,que el cine de la naturaleza contribuye a un nuevo enfoque de las realidades más inmediatas.
No es,con todo,lo más importante.
El verdadero magnetismo,al menos para quien esto escribe,reside no en la espectacularidad,sino en la naturalidad que emana de las imágenes.
Me explico,la gran mayoría de las visiones que el cazador,el naturalista e incluso el fotógrafo tienen son de animales en estado de máxima alerta o en franca huida.
Los rodajes actuales pasan por la ineludible premisa de conseguir que los animales se comporten tal y como son,sin desconflanza alguna.
Penetran en la vida íntima de la fauna y sin alterar ni un ápice su conducta,la capturan en el celuloide para ofrecérsela a grandes masas de espectadores.
Y he utilizado la palabra captura intencionalmente,pues también se caza con la cámara.
Hasta aquí la sencilla clave del éxito,lo que,entre otras cosas,ha convertido a varios divulgadores de la zoología a través del cine para televisión en personas de indiscutible peso social.
Nada es mejor recibido que la satisfacción de un anhelo.
Sin embargo,no debemos olvidar la cultura,ese proceso que nos convierte en analizadores de porqués,en sensibles hacia nuestras propias adquisiciones espirituales y tendentes a un orden moral que mejore las relaciones entre los seres humanos.
Al valorar el documental de la naturaleza como un producto cultural llegamos todavía más lejos: añadimos al patrimonio genético,al siempre recordado inconscientemente mundo de la naturaleza,el placer de recibir información,conocimientos tratados con presupuestos estéticos y,en la mayoría de los casos,con un mensaje ético.
Dijo Unamuno que nada nos hacía más humanos que una clara,relajada y sentida contemplación de la naturaleza.
Acortando el camino,es decir,sin salir de casa,el cine zoológico consigue,creo,ese pequeño milagro al que se refiere don Miguel.
Parece increíble que millones y millones de pesetas gastadas en inventos nucleares y en guerras no puedan terminar con esta lacra de la humanidad,con estos diminutos organismos,unos diez millones de veces más pequeños que el hombre.
El reino de los protozoos es amplísimo,pero afortunadamente sólo unos pocos de ellos son patógenos para el hombre,capaces de producir enfermedades.
Atendiendo a las diferentes formas de desplazarse,se han dividido en flagelados,ciliados,amebas y esporozoarios.
Los primeros pueden desplazarse mediante uno o varios apéndices a modo de cola o látigo,que les permiten un desplazamiento rápido ; los ciliados presentan en su superficie unas pequeñas barbas o pelillos que pueden,al moverse,permitir el desplazamiento de la célula ; las amebas poseen la particularidad de prolongar su cuerpo en forma de falsos pies (pseudópodos).
Los esporozoarios son generalmente inmóviles por carecer de órganos para la locomoción.
En cuanto a sus formas de vida,separaremos aquellos que precisan necesariamente vivir en el interior de otras células de los que pueden hacerlo en los medios líquidos del organismo,como la sangre o el interior del intestino.
Esta diferente forma de vivir marca también la forma de enfermar.
Por ejemplo,los extracelulares pueden contagiarse de persona a persona,como los Trichomonas vaginalis,por contagio sexual,o indirectamente,como la Entamoeba histolytica,por medio de las heces de los pacientes.
Por el contrario,aquellos que viven en la sangre han de utilizar un vehículo apropiado para pasar de la sangre del enfermo al sano,como el caso del paludismo,en el que su productor,el plasmodio,necesita de un artrópodo para el contagio.
Vamos a repasar algunas de las enfermedades más frecuentes producidas por los protozoos en el hombre.
La mayoría de ellas son ya prácticamente desconocidas en España,pero su interés radica en el carácter de puente de nuestro país entre Europa y Africa,y en el masivo aflujo de visitantes y turistas a nuestras tierras,que en algunos casos pueden llegar a producir verdaderos focos de infección difíciles de descubrir y de combatir.
Distinguiremos,en primer lugar,la amebiasis,la enfermedad de la pobreza y el subdesarrollo.
Se manifiesta por la aparición de diarreas incontrolables,que pueden llegar a ser mortales.
Producida por la Entamoeba histolytica,existe un gran número de portadores sanos,de pacientes capaces de transmitir la enfermedad aunque no padezcan síntomas.
En algunas zonas de Sudámerica y de Asia se ha encontrado un orden de prevalencia del 50 %.
El paludismo es otra de las enfermedades producidas por orotozoos.
Se conocen cuatro clases de plasmodios: malariae,vivax,falciparum y ovale,capaces de producir la enfermedad en el hombre.
Precisa de la existencia del mosquito anófeles para su transmisión.
La extinción de esta familia de mosquitos bastaría para hacer desaparecer esta enfermedad.
El mosquito es el huésped verdadero,mientras que el hombre es sólo un huésped intermediario.
El plasmodio infecta el aparato digestivo del mosquito,donde se produce su fertilización.
Al picar al hombre pasa a la sangre,donde se multiplica y parasita los glóbulos rojos,destruyéndolos.
Más interés tiene para nosotros otra enfermedad de protozoos,la toxoplasmosis,producida por el Toxoplasma gondii.
Este unicelular puede dar origen a una enfermedad adquirida,casi siempre asintomática,pero durante el embarazo puede pasar al feto y producir severas lesiones.
Afortunadamente,el 80 % de las madres que padecen toxoplasmosis durante el embarazo dan a luz a un hijo totalmente sano,por lo que sin minimizar el problema no se puede ser alarmista.
Otra de las enfermedades leves producida por uno de estos microorganismos es la tricomoniasis,enfermedad de obligado contagio sexual producida por el Trichomonas vaginalis,que afecta tanto al hombre como a la mujer,dando lugar a molestias y lesiones en la vagina,en la uretra o en la próstata.
Como vemos,la gran mayoría de las enfermedades producidas por protozoos son debidas a la pobreza,a la falta de higiene.
Sólo un nivel educativo y los esfuerzos de los países para suplir las diferencias sociales pueden terminar con estas plagas de la humanidad.
Los pacientes cuyas quejas podían escucharse intentaban curar las neurosis que ensombrecían sus vidas desde la infancia y que hasta entonces ningún tratamiento había sido capaz de aliviar.
Y el edificio era la sede del Instituto Primal Europeo (IPE),fundado y dirigido por el psicoterapeuta norteamericano Arthur Janov.
Janov reivindicaba,desde la aparición de su primer libro sobre el tema " The Primal Scream ",el grito primal) en los Estados Unidos,hace quince años,la paternidad en exclusiva de su método para los institutos primales por él fundados.
Aunque se da el caso,actualmente,de que la sede central de París,sin aviso previo,acaba de cerrar definitivamente.
Al parecer,por agotamiento físico y psiquico de su fundador.
Lo que no ha impedido la proliferación,en los Estados Unidos y en Francia,pero también en otros países europeos y en Japón,de jóvenes psicoterapeutas que ofrecen sus servicios con terapias similares.
Y a todo esto,¿qué es exactamente la terapia primal,celebrada por su descubridor y sus seguidores como una revolución en el tratamiento de las neurosis e incluso de las psicosis? Conviene señalar,ante todo,que el adjetivo " primal " no es español ; en inglés correspondería a original,fundamental.
Según el credo janovista,todos llegamos al mundo con nuestra personalidad real,portadora ya de nuestras necesidades primales: ser alimentado,estar protegido y resguardado,crecer y desarrollarnos según nuestro propio ritmo,ser tomados en brazos y acariciados,ser estimulados.
..
Si estas necesidades básicas no se ven satisfechas,o lo son sólo parcialmente,el niño sufre.
Para sobrevivir,debe expulsar ese sufrimiento del campo de la consciencia.
Sin embargo,las necesidades no desaparecen por el hecho de ser inhibidas y,tarde o temprano,reaparecen bajo forma simbólica.
Un ejemplo,quizá,podría ser el del niño destetado antes de tiempo ; dejará muy pronto de sentir la necesidad de mamar,pero,una vez adulto,quizá fume sin parar,chupando ansiosamente el cigarrillo.
Por supuesto,la neurosis no aparece en cuanto el niño reprime sus necesidades por vez primera.
Pero si empieza en ese momento el proceso que,por acumulación,desembocará en la neurosis.
Y es que el niño vive en un contexto en donde no sólo tiene que satisfacer sus necesidades,sino también las de sus padres.
Janov no evoca solamente la condición dolorosa de los niños maltratados,humillados,privados de todo amor,ni los casos que pertenecen a los tribunales,como los niños martirizados o victimas de agresiones sexuales.
El niño sufre,según él,cuando se le obliga a sonreír " para que parezca feliz " ; cuando se le enseña a decir " adiós " con la mano y luego a decir " gracias " por favor " ; cuando se le impide ser demasiado ruidoso o demasiado hablador,y más adelante,cuando se le pide que tenga buenas notas en el colegio,etcétera.
El niño sufre,siempre según Janov,porque cada vez que se ve en la imposibilidad de ser a la vez real y amado,se va haciendo más irreal y neurótico.
Y llega un día en el que un acontecimiento inclina la balanza a favor de lo irreal y aparece la neurosis.
Acontecimiento que,según Janov,supone la escena primal mayor.
La forma que tome la neurosis corresponderá a los puntos débiles de cada individuo.
Puede encarnarse en trastornos orgánicos (sobre todo,digestivos) de naturaleza psicosomática o en la búsqueda incontrolada de satisfacciones simbólicas que sustituyan,sin que el paciente pueda saberlo,a las satisfacciones reales a las que renunció antaño.
Según los casos,se convertirá en bulímico,alcohólico,toxicómano ; buscará siempre nuevas aventuras sexuales,será autoritario y tiránico,oprimirá a sus propios hijos del mismo modo que él fue oprimido... De esta manera,la neurosis se transmite de generación en generación.
No puede aliviarse por la satisfacción de las necesidades neuróticas,puesto que éstas sólo son sustitutos simbólicos de necesidades reales no satisfechas.
Por ello,el neurótico está condenado a vivir en un estado de tensión permanente.
La técnica de la terapia primal descansa en esta construcción teórica.
El paciente es incitado a sumergirse en su pasado,yendo al encuentro de las escenas traumatizantes que le empujaron a abandonar su yo real y cuyo recuerdo yace oculto en el inconsciente.
Se le lleva a no evitar ya lo que le hace daño,sino,por el contrario,a experimentarlo para liberarse así de los comportamientos simbólicos resultantes y de la tensión que les acompaña.
Revivir una de esas escenas olvidadas es lo que Janov llama hacer un primal.
Necesariamente,también se revive el sufrimiento que había sido conjurado mediante un comportamiento simbólico.
De ahí,el famoso grito prima /,que puede ir desde el gemido,con llanto y lágrimas,hasta un verdadero alarido de agonía cuando se revive la escena primal mayor.
En la fase inicial,el tratamiento es individual,bajo la dirección de un terapeuta profesional.
Después de algunos meses,los pacientes pueden ayudarse mutuamente.
Los más aguerridos alcanzan sus primales sin ayuda ninguna.
Poco a poco,los primales se espacían y el paciente comienza a vivir una nueva vida.
Se ha hecho real.
Se acabaron las drogas,el alcohol,los medicamentos,las depresiones,las pesadillas,el insomnio... En una época en la que muchas personas no se sienten a gusto consigo mismo y viven con un malestar continuo a rastras,el Instituto Primal,fundado en Los ´ngeles en 1970,acogió a clientes acaudalados,algunos de ellos muy conocidos.
Luego,Janov abrió otro centro en Nueva York.
Pero su rentabilidad,sin embargo,no correspondía a sus deseos.
En 1982,en parte por motivos económicos y en parte por presiones de su segunda mujer,de nacionalidad francesa,Janov inaugura el Instituto Primal Europeo,cuyas puertas se han cerrado hace pocos meses en París,como ya hemos visto.
Los pacientes,muchos de ellos con el tratamiento a medias,recibieron una carta del maestro en la que,entre otras cosas,expresaba que " ya no puedo vivir más en medio del dolor y la miseria ; después de treinta y cinco años viendo pacientes,ya es hora de que viva mi propia vida ".
Quizás,además del cansancio,influyera la competencia,cada vez más numerosa y a precios mucho más asequibles que los de la terapia primal,de otros psicólogos que retomaban,bajo una forma a menudo simplista,la idea freudiana de una reactivación de los recuerdos que duermen en el inconsciente.
Hace unos treinta años,un escritor de ciencia-ficción,Lafayette Ron Hubbard,obtuvo en los Estados Unidos un éxito imprevisible con una obra titulada " La Dianética,ciencia moderna de la salud mental ".
Según esta ciencia,los desórdenes psicológicos vienen causados por los engramas registrados en nuestro inconsciente,al que llama espíritu reaccional,cuando,por la razón que sea,el espíritu analítico se encuentra momentáneamente inhibido.
Estos engramas quedan en reserva y su acumulación es la causa de las psicosis,de las neurosis y de numerosos trastornos orgánicos.
Los engramas más peligrosos y numerosos se producen antes incluso del nacimiento,durante la vida fetal.
Basta con que la futura madre estornude o que haga un ejercicio que comprima al bebé (especialmente si realiza una actividad sexual,de la que debería abstenerse durante todo el embarazo) para que el reaccional del niño registre un engrama que,más adelante,le ocasionará serios disgustos.
Entonces será necesario,para curarlo,hacerle remontarse en su memoria hasta que recuerde la circunstancia incriminada.
Sentado detrás de su cliente,Hubbard lo pone en estado de " ensoñación dianética ".
A medida que va recordando sus experiencias antiguas,ocultas en el reaccional,el paciente debilita el poder de los engramas,que acabarán por desaparecer en cuanto tales,ya que habrán pasado del banco reaccional a la memoria corriente,que pertenece al espíritu consciente.
La cura termina cuando el paciente consigue alcanzar su primer engrama: el resto se borra entonces completamente.
El paciente ha sido clarificado.
LA inteligencia artificial es,sin duda,una de las áreas de investigación más prometedoras.
Existen claros indicios de que esta disciplina se encuentra en el umbral de conseguir la tan largamente esperada construcción de máquinas inteligentes.
Una reciente encuesta entre laureados con el Premio Nobel reveló que la Informática era la tecnología que ejercería más influencia en el próximo siglo.
Ya,actualmente,los sistemas expertos están siendo utilizados en una gran variedad de aplicaciones con resultados enormemente interesantes,así como los métodos de la inteligencia artificial,utilizados en robótica,diseño asistido por ordenador,psicología,lingüística,etcétera.
Esta disciplina comprende diversas áreas de investigación: lenguaje natural,visión,robótica,reconocimiento de la palabra y sistemas expertos,cada una de ellas con sus problemas característicos.
Sin embargo,hay varios problemas de tipo general comunes a todas ellas.
El más importante es el problema del aprendizaje.
Los programas requieren,para ser eficaces,una gran cantidad de conocimientos que,una vez representados podrán ser utilizados por el ordenador para resolver un problema especifico.
Pero resulta extremadamente difícil decidir cuáles son los hechos acerca del mundo que deben ser representados,puesto que son innumerables y cambiantes ; por tanto,un programa inteligente debe ser un programa que aprenda,con el fin de poder responder a nuevas informaciones y situaciones,de la misma forma que una persona,y para adaptarse a los cambios de su entorno.
El problema de la representación es a la vez teórico y tecnológico.
El problema teórico es cómo organizar estos conocimientos,y el tecnológico,cómo construir programas grandes y complejos.
Los ordenadores son cada vez más potentes y admiten programas mucho más complicados ; sin embargo,no podemos asegurar que los ordenadores de mañana sean capaces de soportar el crecimiento geométrico de los requerimientos de la programación en inteligencia artificial y siempre existirá la necesidad de aparatos cada vez más potentes.
Los problemas relacionados con la representación,adquisición y organización del conocimiento seguirán estando en el núcleo de esta disciplina durante las próximas décadas.
Quizá existan diversas formas de desarrollar programas inteligentes,pero el enfoque predominante en la actual investigación es que los programas complejos deben estar psicológicamente motivados,es decir,que deberían ser modelos del comportamiento cognoscitivo humano.
Hay varias razones que apoyan este enfoque ; la más importante es que los seres humanos constituyen una demostración de la existencia de la inteligencia.
Una segunda razón es que se puede aprovechar la metodología y los experimentos psicológicos en la formulación de nuestras teorías.
Por otra parte,nuestros programas y teorías pueden ser útiles a los psicólogos para llevar a cabo nuevos experimentos.
La tercera razón,y sin duda la más importante,es el deseo de que los programas sean psicológicamente correctos con el fin de aumentar nuestros conocimientos acerca del cerebro humano.
Los especialistas en inteligencia artificial somos conscientes de la importancia que supone el construir máquinas inteligentes,pero somos también conscientes de que llegar a comprender la inteligencia humana es un objetivo mucho más ambicioso.
En los próximos cincuenta años la inteligencia artificial podría muy bien convertirse en una especie de metaciencia,como la matemática,es decir,una herramienta para atacar problemas de numerosas áreas científicas y tecnológicas,puesto que un programa capaz de razonar y aprender resulta valiosísimo en cualquier aplicación.
Entre muchos investigadores existe la creencia de que los ordenadores serán mucho más inteligentes que las personas ; incluso,muchos afirman que un programa inteligente podría muy bien merecer el Premio Nobel.
Todo esto es muy discutible,pero no hay que olvidar que un sistema experto llamado dendral ha sido decisivo en la obtención de resultados originales de química orgánica,que han sido publicados en prestigiosas revistas especializadas.
Una visión de ciencia-ficción sobre lo que habrá logrado la inteligencia artificial dentro de cien años nos puede conducir a imaginar robots esclavos,que comprenderán el lenguaje hablado y escrito,capaces de realizar cualquier tarea,por muy compleja que sea ; pero pensar que la investigación culminará con la construcción de estos robots es lo mismo que decir que el principal resultado del descubrimiento del electromagnetismo es la televisión en color.
Además de la importancia tecnológica,existe un aspecto científico mucho más importante que implica el estudio de la mente.
¿Cómo comprendemos?,¿cómo organizamos la información en nuestro cerebro?,¿cómo tomamos decisiones?,en definitiva,¿cómo funcionan los mecanismos del pensamiento? El género humano está en constante evolución y por eso resulta imposible hablar de límites en la investigación en inteligencia artificial,la cual,en tanto que metaciencia,mostrará el camino a descubrimientos importantes en otras áreas,y en tanto que tecnología,lo más probable es que llegue a subsimir a toda la informática en general.
Como todas las células del organismo,la fibra muscular está rodeada por una membrana,el sarcolema,dentro de la cual se encierra el citoplasma o sarcoplasma.
Pero,al contrario que la mayoría de las células,que sólo tienen un núcleo,la muscular es multinucleada ; puede llegar a tener centenares de núcleos repartidos en su periferia,siguiendo el eje principal de la fibra.
La razón estriba en que estas células resultan,en el proceso de desarrollo biológico,de la fusión de varias células llamadas mioblastos.
De los distintos tipos de músculos,nos vamos a detener en el estriado,llamado así porque al microscopio muestra una estriación uniforme.
También se conoce como voluntario,porque actúa bajo las órdenes directas del sistema nervioso central.
Pero,además de algunas diferencias en los puntos de contacto con las terminaciones nerviosas y en sus extremos,la fibra muscular estriada se caracteriza esencialmente por su sistema reticular,que forma un gran circuito de canales comunicantes,y por su organización en haces de fibrillas paralelas.
Cada una de estas fibras musculares contiene un haz de fibrillas (miofibrillas) cilíndricas,orientadas paralelamente,que producen una estriación longitudinal.
La estriación transversal,ya observada tiempo atrás al microscopio óptico,se debe a la alternancia de bandas claras (1,isótropas) y oscuras (A,anisótropas).
Las primeras poseen propiedades físicas uniformes,independientemente de la dirección en la que se midan,mientras que las segundas tienen un índice de refracción que no es uniforme en todas las direcciones.
La banda I está atravesada por una banda denominada Z y la unidad comprendida entre dos de estas bandas Z se llama sarcómero.
Por otra parte,cabe añadir que la franja A incluye una zona media denominada H,un poco más clara que sus zonas laterales,y una estría central M ligeraménte abombada.
La inestimable aportación de la microscopía electrónica consiste en haber posibilitado una interpretación de la estriación,al revelar la estructura interna,también filamentosa,de las miofibrillas.
En efecto,cada miofibrilla está compuesta por filamentos colocados en paralelo ; unos finos,los filamentos 1,formados principalmente por una proteína,la actina,y otros gruesos,los filamentos A,constituidos por la proteína miosina.
Todos estos elementos dan lugar a una estructura bastante compleja.
Para tratar de imaginarla podemos pensar en un haz de palillos más bien gruesos (que representan los filamentos de miosina),clavados en una rodaja de pepino (una banda anisótropa),y en otro haz de palillos más bien finos (filamentos de actina),clavados en una rodaja de rábano negro (banda isótropa).
Colocando una rodaja de rábano a cada lado de una rodaja de pepino,de forma que los palillos se imbriquen sin que las puntas se hundan en la carne de las rodajas de enfrente,obtendremos una imagen de la organización espacial de un sarcómero,parte elemental de la miofibrilla y unidad de base del trabajo muscular.
Gracias al microscopio electrónico se ha podido constatar que entre los filamentos gruesos y finos existen puentes,que se extienden de los primeros a los segundos prolongando las moléculas de miosina ; las cabezas de éstas se inclinan,por acción química,hacia las cadenas de actina.
Y es precisamente ese movimiento,el deslizamiento de las cabezas,esos brazos musculosos de los músculos,lo que origina la contracción muscular.
Son auténticamente los músculos interiores de los músculos.
Para que los filamentos se interpenetren,es preciso que haya producción de energía química.
Así,el desencadenamiento de la contracción se produce en el punto de contacto entre el nervio y el músculo,en la unión neuromuscular.
Allí,un neurotransmisor almacenado en vesículas de la terminación nerviosa,la acetilcolina,ordena a un receptor específico de la membrana de la fibra muscular que libere los iones de calcio que contiene en reserva.
Este flujo de iones de calcio desencadena,a su vez,la actividad del sistema ADP-ATP,que a continuación explicamos.
El ATP,trifosfato de adenosina,es el transportador de energía más importante en las células y está constituido,como su nombre indica,por tres elementos de fosfato.
A medida que el ATP transfiere energía a otras moléculas,pierde su grupo fosfato terminal y se transforma en ADP (difosfato de adenosina),que es la forma descargada o pobre de energía,pero que puede,a su vez,captar energía química y recuperar un grupo fosfato para convertirse de nuevo en ATP.
De todas ellas,sin duda las más llamativas son las feromonas sexuales,que intervienen normalmente en la atracción entre los miembros de una pareja animal.
Por ejemplo,en el caso de los insectos,en los que se conoce bastante bien el proceso,resultan muy útiles a la hora de la reproducción.
Estos animales voladores necesitan un método de atracción muy potente,ya que la dispersión de los individuos en el medio en que viven es muy grande.
Por ejemplo,el bombicol es la feromona sexual de la mariposa hembra Bombyx mori.
En el laboratorio,la atracción del macho hacia la hembra se consigue con una concentración de la feromona tan pequeña como 10 - ' 2 microgramos.
El macho,sometido a los vapores de esta sustancia,se ve inexorablemente atraído hacia la hembra,entrando en un estado característico de excitación sexual debido a la llegada a sus antenas de tan sólo unas pocas moléculas de bombicol.
Se trata,por tanto,de un fenómeno muy diferente al del olfato,y muy difícil de estudiar,por otra parte,ya que el aislamiento y la identificación de estas sustancias es un proceso harto complejo: sólo se producen en cantidades mínimas y a partir de hembras vírgenes (la fecundación detiene la emisión de bombicol).
Así,A. Butenandt y sus colaboradores han conseguido extraer apenas doce miligramos de esta sustancia a partir de nada más y nada menos que 250.000 mariposas Bombyx.
En el caso de los vertebrados,el sistema de feromonas es mucho más complejo que en los insectos,y su acción,menos precisa.
Las mismas secreciones pueden intervenir,por ejemplo,en el marcaje de un determinado territorio,en el reconocimiento de la especie o en el acoplamiento.
Así,se establecen numerosas interrelaciones químicas y el proceso se complica cada vez más.
Las glándulas que segregan las feromonas están situadas cerca de los órganos sexuales y su funcionamiento parece estar bajo las órdenes de determinadas acciones hormonales.
Cuando una ratona,por ejemplo,huele las secreciones de la glándula apocrina (ubicada en el pene del macho),se siente atraída hacia él,aunque no esté en celo.
Si el macho orina,otra feromona contenida en esa orina parece provocar en la hembra un estado de estro que la hace sexualmente receptiva.
Apenas se habían hecho públicos estos descubrimientos cuando ya se empezaron a buscar feromonas en las secreciones de diversas glándulas sudoríparas humanas,especialmente las situadas en las axilas,donde se pensaba que podían abundar.
El fisiólogo Auguste Galopin escribía en 1886 que la interacción recíproca de los olores era la esencia del amor sexual.
" La unión más pura que pueda establecerse entre un hombre y una mujer es la engendrada por el olfato y sancionada por la asimilación normal en el cerebro de las moléculas animadas producidas por la secreción y evaporación de dos cuerpos en contacto y en simpatía " El alemán Wilhem Fliess,amigo de Freud,trató de probar la existencia de una relación íntima entre el proceso olfativo nasal y el proceso sexual.
Freud,por su parte,decía que el hombre había reprimido su olfato y,con él,su sexualidad.
Sin embargo,otros investigadores pensaban que las feromonas humanas no existían por el hecho de que,en el hombre,el córtex cerebral se había impuesto en los centros primitivos del olfato.
Otros aseguraban que esas moléculas inefables debían conservar una importante función en el comportamiento humano.
En 1980,el investigador británico George Dodd,de la Universidad de Warwick,subrayó la existencia de una sustancia química aislada en el sudor masculino que hechizaba a las mujeres.
Creía incluso tenerla identificada: un esteroide,más precisamente el alfa-androsterol.
Quizá Dodd sabía algo de las indiscreciones del novelista Somerset Maugham sobre su colega H. G. Wells,autor de ciencia-ficción.
Wells,que no era precisamente un adonis,era un hombre mujeriego,y Maugham no pudo resistir la tentación de preguntar a una de sus amantes por las razones de su éxito.
Yo esperaba que ella me hablara de su aguda inteligencia y de su afición por las bromas.
Pues nada de eso: ella me dijo que su cuerpo tenía un olor de miel.
En cualquier caso,no se ha vuelto a oír hablar de la miel alfa-androsterólica del doctor Dodd... Los científicos Cutler y Preti,citados más arriba,llevan estudiando desde hace años los sentidos químicos en el Centro Monell,laboratorio de la Universidad de Pensilvania totalmente consagrado al estudio del olfato y del gusto.
Y han observado que mujeres de ciclo menstrual irregular,al ser expuestas a feromonas segregadas por el hombre,regularizan gradualmente su ciclo hasta aproximarlo a la norma de veintinueve días y medio,mientras que las que no tienen la ocasión de recibir regularmente el influjo de tales esencias masculinas tienen unos ciclos más largos que otros.
Además,dicen,la exposición a las feromonas masculinas reduce la infertilidad femenina y hace más llevadera la menopausia.
Los investigadores tomaron muestras de sudor de axilas de hombres y las colocaron sobre el labio superior de mujeres que no tenían relaciones sexuales y cuyo ciclo menstrual presentaba una duración anormal.
Al cabo de doce a catorce semanas de la aplicación de este tratamiento,sus ciclos menstruales se acercaban a la norma.
De todo ello dedujeron que las feromonas existen en el hombre y quizá tenían un efecto comparable al de las relaciones sexuales regulares,durante las cuales el hombre emite y la mujer percibe sustancias odoríferas.
Sus experiencias confirmaban numerosas observaciones más o menos anecdóticas según las cuales las mujeres que tienen relaciones sexuales regulares tienden a tener ciclos menstruales regulares y una menopausia gradual y menos penosa.
Puestos ya a especular,estas feromonas masculinas,según la doctora Cutler,podrían ser utilizadas para la fabricación de cremas,perfumes o vaporizaciones que " mejorarían de forma clara el bienestar de las mujeres ".
De hecho,el Centro Monell ya ha solicitado la patente de cuatro feromonas,aunque todavía no hayan sido ni siquiera identificados los numerosos componentes químicos de estas supuestas esencias.
La comunicación entre la terminación nerviosa y la célula muscular se efectúa mediante un mediador químico o,si se prefiere,un neurotransmisor: la acetilcolina.
Pero su acción es muy efímera.
En efecto,la membrana de la célula muscular no tarda en segregar una enzima,la acetilcolinesterasa,que destruye la acetilcolina.
Por ello,la permeabilidad de la membrana vuelve a su estado inicial,y el músculo se distiende,hasta la próxima estimulación La succinilcolina ocupa los enclaves receptores de la acetilcolina e impide así al neurotransmisor que ejerza su acción habitual.
Consecuencia: la placa motriz ya no responde a las descargas de acetilcolina y el músculo permanece en estado de relajación ; es la parálisis fláccida.
Además,la succinilcolina resiste mucho mejor que la acetilcolina la acción destructora de la acetilcolinesterasa,de forma que la relajación muscular puede prolongarse durante varios minutos.
Por último,antes de apropiarse de los enclaves receptores de acetilcolina,la succinilcolina empieza por obligar a los botones terminales de las neuronas motoras a vaciarse de todas sus reservas de acetilcolina.
De ahí esas olas de contracciones anárquicas que pueden observarse en todo el cuerpo del paciente (tórax,abdomen,brazos,piernas,rostro,etc.).
Por último,se inyecta en la perfusión el contenido de las dos últimas jeringuillas.
Por un lado,cinco miligramos de Pancuronium,y por otro,cuatro décimas de miligramo de Fentanyl.
El Pancuronium es,como la succinilcolina,un curare,pero su acción es mucho más prolongada: unos treinta minutos.
Interviene también en la unión neuro-muscular de la placa motriz,ocupando los enclaves receptores de la acetilcolina e impidiendo toda contracción del músculo.
No obstante,su degradación es mucho más lenta,de forma que la parálisis fláccida que provoca persiste mucho más tiempo.
El Fentanyl,por su parte,es un derivado sintético de la morfina,200 a 300 veces más activo que la morfina natural.
Este producto hará al señor X insensible a todo dolor.
Introducido en la circulación sanguínea,llega rápidamente al cerebro,en donde se fija.
El efecto analgésico,que aparece al cabo de treinta segundos,es máximo a los tres minutos y dura unos treinta minutos.
Ahora que el señor X está profundamente dormido (por el pentotal),totalmente relajado (por el Pancuronium) y enteramente insensibilizado (por el Fentanyl),los cirujanos pueden ponerse al trabajo.
Cuando la operación llega a su fin y sólo queda prolongar por unos minutos el sueño,el anestesista preferirá,más que reinyectar pentotal,utilizar uno de los gases anestesiantes (fluotano,enflurano,isoflurano...),introducidos bajo forma líquida en el difusor de la máquina ventiladora.
Una vez en los pulmones del paciente,se difunden a través de las paredes de los alvéolos pulmonares y penetran en los vasos sanguíneos que riegan el tejido pulmonar.
La cantidad de gas que pasa así a la sangre es función de dos factores: la solubilidad del gas y la velocidad circulatoria,es decir,el caudal cardíaco.
Existe una gran preocupación acerca de la seguridad de la anestesia.
En un estudio publicado el pasado agosto en el Journal of the American Medical Association,se indicaba que,de los 20 millones de sujetos dormidos cada año en los Estados Unidos,unos 2.000 aproximadamente morían por causas directamente imputables a la anestesia.
De una encuesta realizada en Francia se deduce que,cada año,más de dos millones y medio de franceses sufren una anestesia general (93,3 % de los casos) o loco-regional (anticuerpos,lo que puede llevar varios meses,o bien si ya no sabe hacerlo,como ocurre precisamente en el caso de un SIDA desarrollado,el test serológico no detectará nada.
Digamos que no ocurre nunca que una persona afectada por un SIDA desarrollado done su sangre.
Pero sí puede ocurrir que alguien que tema haber sido contaminado por un contacto sexual reciente,por ejemplo,vaya a donar su sangre para aprovecharse del test.
Tal tipo de comportamiento resulta catastrófico porque,aunque las sospechas sean fundadas,el test dará negativo,al ser demasiado reciente la infección,y la sangre contaminada será transfundida.
Se podría pensar en conservar la sangre,para evitar esto,durante un máximo de seis meses,hasta que el donante pase una segunda prueba.
¿Pero cuántos donantes de éstos se presentarían seis meses más tarde? Y el coste de los almacenamientos y las dobles pruebas sería enorme.
Se han aplicado,pues,otras precauciones suplementarias: en primer lugar,toda donación de sangre viene precedida por un interrogatorio riguroso,bajo la garantía del secreto médico.
En segundo lugar,ciertos productos de la sangre son lo bastante resistentes para ser calentados o tratados de forma que muera el virus.
Tal es el caso,por ejemplo,de los concentrados de proteínas de coagulación destinados a los hemofílicos,que pierden al calentarse el 100 % de sus virus,pero sólo un 20 % de eficacia.
La albúmina,los anticuerpos de protección (gammaglobulinas humanas antitetánicas por ejemplo) o la vacuna contra la hepatitis B,todos ellos preparados a partir de sangre humana,son tratados así y en consecuencia no of recen ningún peligro.
Desgraciadamente,no se puede desactivar el virus en las fracciones llamadas lábiles de la sangre (glóbulos rojos y blancos,plaquetas,sangre total o plasma fresco congelado).
Otro problema lo plantea la variabilidad del virus del SIDA.
Así,los anticuerpos dirigidos contra el virus n. o 2 (VIH 2),descubierto en Africa del Oeste,no se detectan siempre con los tests preparados para el virus n1 (Vl H 1).
Afortunadamente,este virus es muy raro en Europa.
Además,la firma Diagnostic Pasteur,ligada al Instituto Pasteur,va a comercializar pronto un test mixto VIH 1 + VIH 2. Por último,con todas las precauciones tomadas actualmente,puede estimarse que el riesgo de recibir sangre contaminada es inferior al 1 por 10.000.
Cifra que aún podría disminuir considerablemente con la puesta a punto de los tests basados en la técnica de las sondas genéticas.
Los riesgos disminuirán aún más con la mejora de las técnicas de detección del virus.
Pero,sobre todo,entre todos podemos disminuir de forma evidente el peligro: hay que multiplicar las donaciones de sangre,sí,pero sobre todo se debe evitar hacerlas si uno piensa que puede ser portador del virus del SIDA.
Porque la donación de sangre se ha convertido en la única " profesión " de miles de toxicómanos norteamericanos y de innumerables habitantes miserables del Tercer Mundo,sobre todo en América Latina.
Allí no hay legislación que establezca límite a la cantidad de sangre vendida por un individuo,y los pobres se dejan puncionar las venas varias veces al mes ; ya subalimentados,acaban totalmente anémicos.
En Brasil,de cada seis millones de perfusiones comerciales realizadas cada año,200.000 se realizaron con la sangre de donantes enfermos de sífilis,de la enfermedad de Chagas - - parasitosis muy grave - -,de malaria y de hepatitis varice,todas ellas infecciones transmisibles al receptor.
Se recuerda el escándalo de Puerto Príncipe,donde el jefe de los tontonsmacoutes cobraba sus diezmos a los parados bajo forma de donación de sangre,con la que hacía un comercio muy lucrativo con los laboratorios privados de Occidente.
Los Estados Unidos se interrogan en este momento sobre su política transfusional.
El mercado libre de sangre humana es hoy muy criticado por la medicina norteamericana,que pide la instauración del voluntariado.
Por una cuestión de ética,ya que el dinero suele ser mal consejero cuando se trata de la salud de todos.
Pero también porque la sangre mercenaria que transita por los circuitos comerciales of rece menos garantías de seguridad que la sangre recogida por los organismos sanitarios adecuados.
Las cargas inorgánicas no se utilizaban normalmente en caucho,y este grupo de trabajo ha obtenido resultados muy satisfactorios en el empleo de minerales arcillosos como la sepiolita,de la que España es primer productor mundial.
La única condición que ha de tener una carga para ser útil en caucho,a diferencia de los plásticos,es que sea muy fina: un tamaño de partícula no superior a una micra.
La sepiolita podrá sustituir ahora en parte al negro de carbono en formulaciones de banda de rodamiento en neumáticos para turismo y camión,sin que mermen sus propiedades mecánicas.
Paralelamente a estos estudios de modificación de cargas inorgánicas,la unidad de caucho lleva a cabo trabajos contratados por la industria para desarrollar técnicas o formaciones específicas,y en particular el estudio de materiales a los que se les exigen determinados comportamientos (resistencia al frío,al calor,a la tracción,amortiguamiento,dureza...).
En colaboración con la Cátedra de Física del Aire y Medio Ambiente de la Universidad Complutense de Madrid,la unidad de Caucho ha preparado distintas formulaciones de este producto para usarlas como patrón de medida de la concentración de ozono ambiental.
La unidad estructural de física y fisicoquímica de polímeros está estudiando la relación que hay entre la estructura y las propiedades de los polímeros en estado sólido.
Otro grupo de trabajo se ocupa de la síntesis de nuevos polímeros aplicando la denominada polimerización iónica.
Los resultados obtenidos en estas investigaciones han dado lugar a diversas publicaciones en revistas extranjeras del sector.
La unidad estructural de investigación de química molecular es la que cuenta con más personal del centro,y aun así,dada la amplitud del objeto de su investigación - - la química de los polímeros - - no puede abarcar todo el que sería su campo de estudio.
En la actualidad mantiene cinco líneas fundamentales que tienen que ver con lo que hoy está en la punta de la investigación de la química macromolecular.
El primer grupo se dedica a síntesis y caracterización de polímeros especiales,principalmente de alta temperatura.
Se trata de aquellos polímeros que dan lugar a plásticos de alta temperatura capaces de competir con los metales.
Un segundo grupo se dedica a modificar polímeros naturales ; polímeros que luego pueda destruir el organismo.
Se utilizan no sólo como cubierta,sino también como soporte de medicamentos.
Otro grupo se dedica a la modificación de polímeros industriales.
Son estudios básicos para comprobar cuáles son los puntos en los que el polímero se degrada más fácilmente.
Empezaron tratando de mejorar la estructura química del policloruro de vinilo para hacerlo más estable químicamente ; de ahí se desembocó en la actual línea general de modificación química de polímeros.
Un cuarto grupo se dedica a la obtención de polímeros por vía radical vía química distinta de la habitual,en la que los polímeros se logran por condensación.
El poliestireno,las resinas acrílicas y algunos otros se obtienen generalmente por vía radical en la industria.
Los investigadores se dedican a conocer el mecanismo de las reacciones,hacen sus propios polímeros,no descritos anteriormente ; sintetizan nuevos monómeros,estudian la reacción de polimerización por vía radical (cuando se sintetiza un monómero nuevo es conveniente conocer los pasos que conducen del monómero al polímero).
No se investigan polímeros industriales pero estos trabajos han servido para que la industria contrate poliestirenos modificados por ellos.
Un conocimiento básico que se aplicará siempre que lo demande la industria.
La última línea de investigación de esta unidad se centra en la fotoquímica.
Su origen es la preocupación,surgida hace una década,de que el plástico contamina el ambiente.
Hubo entonces que inventar plásticos que se degradarán a la luz del sol.
Este grupo obtuvo buenos resultados en la obtención de plásticos fotodegradables,y derivó ' posteriormente hacia el aprovechamiento de la fotoquímica como un medio de estudiar,sintetizar y modificar polímeros.
La unidad estructural de investigación de tecnología de plásticos se ha ocupado fundamentalmente durante los últimos meses del estudio y desarrollo de materiales plásticos a base de polímeros vírgenes y procedentes de residuos y materiales inorgánicos cristalinos y vítreos.

